{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//import sys.process._\n",
    "//\"wget https://github.com/adrianulbona/osm-parquetizer/releases/download/v1.0.0/osm-parquetizer-1.0.0.jar -P /FileStore/group14\" !!\n",
    "\n",
    "//import sys.process._\n",
    "//\"wget http://download.geofabrik.de/europe/sweden-latest.osm.pbf -P dbfs:/FileStore/group14\" !!\n",
    "\n",
    "//import sys.process._\n",
    "//\"java -jar /FileStore/group14/osm-parquetizer-1.0.0.jar /FileStore/group14/sweden-latest.osm.pbf\" !!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "import scala.sys.process._\n",
    "import org.apache.spark.sql.functions.{col}\n",
    "\n",
    "def toMap(tupesArray: Seq[Row]): Option[Map[String, String]] = {\n",
    "    if (tupesArray == null) {\n",
    "      None\n",
    "    } else {\n",
    "      val tuples = tupesArray.map(e => {\n",
    "        (\n",
    "          e.getAs[String](\"key\"),\n",
    "          e.getAs[String](\"value\")\n",
    "        )\n",
    "      })\n",
    "      Some(tuples.toMap)\n",
    "    }\n",
    "  }\n",
    "\n",
    "def handleCommon()(df:DataFrame):DataFrame = {\n",
    "  val toMapUDF = udf(toMap _)\n",
    "  df.drop(\"uid\", \"user_sid\", \"changeset\", \"version\", \"timestamp\")\n",
    "    .withColumn(\"tags\", toMapUDF(col(\"tags\")))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import org.apache.spark.sql._\n",
    ">     import scala.sys.process._\n",
    ">     import org.apache.spark.sql.functions.col\n",
    ">     toMap: (tupesArray: Seq[org.apache.spark.sql.Row])Option[Map[String,String]]\n",
    ">     handleCommon: ()(df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.setConf(\"spark.sql.parquet.binaryAsString\",\"true\")\n",
    "val nodeDF = sqlContext.read.parquet(\"dbfs:/FileStore/group14/sweden-latest_osm_pbf_node.parquet\").transform(handleCommon())\n",
    "nodeDF.createOrReplaceTempView(\"nodes\")\n",
    "\n",
    "val wayDF = sqlContext.read.parquet(\"dbfs:/FileStore/group14/sweden_latest_osm_pbf_way.parquet\").transform(handleCommon())\n",
    "wayDF.createOrReplaceTempView(\"ways\")\n",
    "\n",
    "//val relationDF = sqlContext.read.parquet(\"dbfs:/FileStore/group14/sweden_latest_osm_pbf_relation.parquet\").transform(handleCommon())\n",
    "//relationDF.createOrReplaceTempView(\"relations\")\n",
    "print(wayDF.columns.toSeq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     WrappedArray(id, tags, nodes)nodeDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: bigint, tags: map<string,string> ... 2 more fields]\n",
    ">     wayDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: bigint, tags: map<string,string> ... 1 more field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In order to reduce the number of nodes and "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions.{udf, explode,arrays_zip,lag, lead, concat,array, lit}\n",
    "\n",
    "//val wayDF_tmp2 = wayDF_tmp.withColumn(\"exploded\", explode(arrays_zip($\"nodes\", $\"nodes\".drop(1))))\n",
    "//wayDF_tmp1.withColumn(\"tesrasrdasd\", concat($\"nodes\",$\"nullcol\"))\n",
    "val wayDF_exploded = wayDF.withColumn(\"exploded\", explode(arrays_zip(concat($\"nodes.nodeId\",array(lit(-1L))), concat(array(lit(-1L)),$\"nodes.nodeId\"))))\n",
    "val wayDF_filtered = wayDF_exploded.filter($\"exploded.0\" > 0 && $\"exploded.1\" > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import org.apache.spark.sql._\n",
    ">     import org.apache.spark.sql.functions.{udf, explode, arrays_zip, lag, lead, concat, array, lit}\n",
    ">     wayDF_exploded: org.apache.spark.sql.DataFrame = [id: bigint, version: int ... 4 more fields]\n",
    ">     wayDF_filtered: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: bigint, version: int ... 4 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val wayDF_filtered = wayDF_exploded.filter($\"exploded.0\" > 0 && $\"exploded.1\" > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     wayDF_filtered: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: bigint, version: int ... 5 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wayDF_filtered.select(\"exploded\").take(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res27: Array[org.apache.spark.sql.Row] = Array([[2416576872,2416576873]], [[5159477197,2416576872]], [[2416576867,5159477197]], [[5159477198,2416576867]], [[2416576866,5159477198]], [[5159477199,2416576866]], [[1960564960,5159477199]], [[5159477196,1960564960]], [[5915403101,5159477196]], [[97772279,5915403101]], [[1960564965,2416576876]], [[5159477214,1960564965]], [[5159477215,5159477214]], [[1960564961,5159477215]], [[5159477217,1960564961]], [[5159477218,5159477217]], [[1960564958,5159477218]], [[2416576873,1960564958]], [[5159477329,2416576873]], [[2416576864,5159477329]], [[2416579043,2416579038]], [[2416579045,2416579043]], [[2416579046,2416579045]], [[2416579038,2416579046]], [[2416579127,2416579021]], [[2416579024,2416579127]], [[2416579163,2416579024]], [[2416579021,2416579163]], [[2416579054,2416579146]], [[2416579067,2416579054]], [[2416586043,2416579067]], [[2416579184,2416586043]], [[2416579146,2416579184]], [[2416579099,2416579104]], [[2416594473,2416579099]], [[2416579069,2416594473]], [[2416579136,2416579069]], [[2416579104,2416579136]], [[2416579161,2416579029]], [[2416579097,2416579161]], [[2416579072,2416579097]], [[2416579029,2416579072]], [[2416579200,2416579036]], [[2416579198,2416579200]], [[2416582165,2416579198]], [[2416579025,2416582165]], [[2416579036,2416579025]], [[2416579065,2416579059]], [[2416609471,2416579065]], [[2416579144,2416609471]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wayDF_exploded.select(\"exploded\").take(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res26: Array[org.apache.spark.sql.Row] = Array([[2416576873,-1]], [[2416576872,2416576873]], [[5159477197,2416576872]], [[2416576867,5159477197]], [[5159477198,2416576867]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val wayNodeDF = wayDF_exploded.select($\"exploded.0\".as(\"start\"), $\"exploded.1\".as(\"end\"),$\"tags.highway\", $\"tags.maxspeed\")\n",
    ".filter($\"highway\" isin (\"motorway\",\"trunk\",\"primary\",\"secondary\", \"tertiary\", \"unclassified\", \"residential\",\"motorway_link\", \"trunk_link\",  \"primary_link\", \"secondary_link\", \"tertiary_link\"))\n",
    "//  .filter(array_contains($\"tags.value\", \"motorway\") || array_contains($\"tags.value\", \"trunk\") || array_contains($\"tags.value\", \"primary\") || array_contains($\"tags.value\", \" \tsecondary\") || array_contains($\"tags.value\", \"tertiary\") || array_contains($\"tags.value\", \"unclassified\")  || array_contains($\"tags.value\", \"residential\") || array_contains($\"tags.value\", \"motorway_link\") || array_contains($\"tags.value\", \"trunk_link\") || array_contains($\"tags.value\", \"primary_link\") || array_contains($\"tags.value\", \"secondary_link\") || array_contains($\"tags.value\", \"tertiary_link\"))\n",
    "  //.select($\"id\".as(\"wayId\"), $\"user_sid\", explode($\"nodes\").as(\"indexedNode\"))\n",
    "wayNodeDF.createOrReplaceTempView(\"wayHighway\")\n",
    "//val rdd = wayNodeDF.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import org.apache.spark.sql.functions._\n",
    ">     wayNodeDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [start: bigint, end: bigint ... 2 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "We then add default speeds to each edge. If it is a motorway it gets a\n",
    "speed of 110km/h, residential areas gets 15 km/h while all other gets 50\n",
    "km/h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val wayNodeDF_nonull = wayNodeDF.withColumn(\"maxspeed\", when($\"maxspeed\".isNull && col(\"highway\") == \"motorway\", 110)\n",
    "                     .when($\"maxspeed\".isNull && col(\"highway\")==\"primary\", 50).when($\"maxspeed\".isNull && col(\"highway\")==\"secondary\", 50).when($\"maxspeed\".isNull && col(\"highway\")==\"motorway_link\", 50)\n",
    "                     .when($\"maxspeed\".isNull && col(\"highway\")==\"residential\", 15).when($\"maxspeed\".isNull, 50)\n",
    "                     .otherwise($\"maxspeed\"))\n",
    "wayNodeDF_nonull.createOrReplaceTempView(\"wayHighway\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     wayNodeDF_nonull: org.apache.spark.sql.DataFrame = [start: bigint, end: bigint ... 2 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val nodeLatLonDF = nodeDF\n",
    "  .select($\"id\".as(\"nodeId\"), $\"latitude\".as(\"startLat\"), $\"longitude\".as(\"startLong\"))\n",
    "\n",
    "val endnodeLatLonDF = nodeDF\n",
    "  .select($\"id\".as(\"nodeId2\"), $\"latitude\".as(\"endLat\"), $\"longitude\".as(\"endLong\"))\n",
    "\n",
    "val wayGeometryDF = wayNodeDF_nonull.join(nodeLatLonDF, $\"start\" === $\"nodeId\").join(endnodeLatLonDF, $\"end\" === $\"nodeId2\")\n",
    "\n",
    "val wayGeometry_distDF = wayGeometryDF.withColumn(\"a\", pow(sin(radians($\"endLat\" - $\"startLat\") / 2), 2) + cos(radians($\"startLat\")) * cos(radians($\"endLat\")) * pow(sin(radians($\"endLong\" - $\"startLong\") / 2), 2))\n",
    "  .withColumn(\"distance\", atan2(sqrt($\"a\"), sqrt(-$\"a\" + 1)) * 2 * 6371)\n",
    "  .filter($\"endLat\"<55.4326186d) //South of malmÃ¶\n",
    "  .withColumn(\"time\", $\"distance\"/$\"maxspeed\").select(\"time\", \"start\", \"end\", \"distance\", \"maxspeed\")\n",
    "wayGeometry_distDF.createOrReplaceTempView(\"wayGeometry_distDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import org.apache.spark.sql.functions._\n",
    ">     nodeLatLonDF: org.apache.spark.sql.DataFrame = [nodeId: bigint, startLat: double ... 1 more field]\n",
    ">     endnodeLatLonDF: org.apache.spark.sql.DataFrame = [nodeId2: bigint, endLat: double ... 1 more field]\n",
    ">     wayGeometryDF: org.apache.spark.sql.DataFrame = [start: bigint, end: bigint ... 8 more fields]\n",
    ">     wayGeometry_distDF: org.apache.spark.sql.DataFrame = [time: double, start: bigint ... 3 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//import org.graphframes._\n",
    "//val g = GraphFrame(\n",
    "  //verexDf.select($\"id\", $\"name\"), \n",
    " // edgeDf.select ($\"sourceID\" as \"src\", $\"destID\" as \"dst\", $\"relationship\"))\n",
    "// Convert to GraphX\n",
    "//val gx: Graph[Row, Row] = g.toGraphX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "val eps = 0.000001\n",
    "val edges: RDD[Edge[Double]]  =  wayGeometry_distDF\n",
    "    .select(\"start\", \"end\", \"time\").rdd.map(line => Edge(line.getAs(\"start\"), line.getAs(\"end\"), line.getAs(\"time\")))\n",
    "val revedges: RDD[Edge[Double]]  =  wayGeometry_distDF\n",
    "    .select(\"start\", \"end\", \"time\").rdd.map(line => Edge(line.getAs(\"end\"), line.getAs(\"start\"), line.getAs(\"time\")))\n",
    "\n",
    "val graphorg = Graph.fromEdges(edges, \"defaultname\")\n",
    "graphorg.cache()\n",
    "\n",
    "val revgraph = Graph.fromEdges(revedges, \"defaultname\")\n",
    "//val edge = edges.toDF(\"id\", \"node\")\n",
    "\n",
    "val graph = Graph(\n",
    "   graphorg.vertices,\n",
    "    graphorg.edges.union(revgraph.edges)\n",
    ").groupEdges((attr1, attr2) => scala.math.max(eps, scala.math.min(attr1, attr2)))\n",
    "\n",
    "graph.cache()\n",
    "println(graphorg.edges.toDF.count())\n",
    "println(graphorg.vertices.toDF.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     25402\n",
    ">     24148\n",
    ">     import org.apache.spark.graphx._\n",
    ">     import org.apache.spark.rdd.RDD\n",
    ">     eps: Double = 1.0E-6\n",
    ">     edges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Double]] = MapPartitionsRDD[472993] at map at command-1716517738645653:5\n",
    ">     revedges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Double]] = MapPartitionsRDD[473113] at map at command-1716517738645653:7\n",
    ">     graphorg: org.apache.spark.graphx.Graph[String,Double] = org.apache.spark.graphx.impl.GraphImpl@48b6952b\n",
    ">     revgraph: org.apache.spark.graphx.Graph[String,Double] = org.apache.spark.graphx.impl.GraphImpl@69df44ba\n",
    ">     graph: org.apache.spark.graphx.Graph[String,Double] = org.apache.spark.graphx.impl.GraphImpl@2a139bec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.reflect.ClassTag\n",
    "import org.apache.spark.graphx._\n",
    "\n",
    "/*  \n",
    " * Computes shortest weighted paths to the given set of landmark vertices, returning a graph where each\n",
    " * vertex attribute is a map containing the shortest-path distance to each reachable landmark.\n",
    " * Currently supports only Graph of [VD, Double], where VD is an arbitrary vertex type.\n",
    " *\n",
    " * The object also include a function which transforms the resulting graph into a path_graph between a \n",
    " * specific starting node and goal node. Each edge in the path_grpah is either 1 or 0 depending if it is \n",
    " * the shortest path or not.\n",
    " *\n",
    " */\n",
    "object ShortestPath extends Serializable {\n",
    "\n",
    "  // When finding the shortest path each node stores a map from the itself to each goal node.\n",
    "  // The map returns an array includeing the total distance to the goal node as well as the\n",
    "  // next node pn the shortest path to the goal node. The last value in the array is only \n",
    "  // populated with the nodes own id and is only used for computational convenience. \n",
    "  type SPMap = Map[VertexId, Tuple3[Double, VertexId, VertexId]]\n",
    "  \n",
    "  // PN holds the information of the path nodes which are used for creating a path graph\n",
    "  // PN = ('Distance left to goal node', 'Next path node id', 'Goal node', 'Is on path')\n",
    "  type PN = Tuple4[Double, VertexId, VertexId, Boolean] \n",
    "  \n",
    "  private val INITIAL_DIST = 0.0\n",
    "  private val DEFAULT_ID = -1L\n",
    "  private val INFINITY = Int.MaxValue.toDouble\n",
    "\n",
    "  private def makeMap(x: (VertexId, Tuple3[Double, VertexId, VertexId])*) = Map(x: _*)\n",
    "  \n",
    "  private def incrementMap(spmap: SPMap, delta: Double, id: VertexId): SPMap = { \n",
    "    spmap.map { case (v, d) => v -> (Tuple3(d._1 + delta, d._3, id)) }\n",
    "  }\n",
    "\n",
    "  private def addMaps(spmap1: SPMap, spmap2: SPMap): SPMap = {\n",
    "    (spmap1.keySet ++ spmap2.keySet).map {\n",
    "    k =>{\n",
    "        if (spmap1.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._1 < spmap2.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._1) \n",
    "                k -> (Tuple3(spmap1.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._1, \n",
    "                             spmap1.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._2, \n",
    "                             spmap1.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._3))\n",
    "        else \n",
    "                k -> (Tuple3(spmap2.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._1, \n",
    "                             spmap2.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._2, \n",
    "                             spmap2.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._3))\n",
    "        }\n",
    "    }.toMap\n",
    "  }\n",
    "  \n",
    "  // at this point it does not really matter what vertex type is\n",
    "  def run[VD](graph: Graph[VD, Double], landmarks: Seq[VertexId]): Graph[SPMap, Double] = {\n",
    "    val spGraph = graph.mapVertices { (vid, attr) =>\n",
    "      // initial value for itself is 0.0 as Double\n",
    "      if (landmarks.contains(vid)) makeMap(vid -> Tuple3(INITIAL_DIST, DEFAULT_ID, DEFAULT_ID)) else makeMap()\n",
    "    }\n",
    "\n",
    "    val initMaps = makeMap()\n",
    "\n",
    "    def vProg(id: VertexId, attr: SPMap, msg: SPMap): SPMap = {\n",
    "      addMaps(attr, msg)\n",
    "    }\n",
    "\n",
    "    def sendMsg(edge: EdgeTriplet[SPMap, Double]): Iterator[(VertexId, SPMap)] = {\n",
    "      val newAttr = incrementMap(edge.dstAttr, edge.attr, edge.srcId)\n",
    "      if (edge.srcAttr != addMaps(newAttr, edge.srcAttr)) Iterator((edge.srcId, newAttr))\n",
    "      else Iterator.empty\n",
    "    }\n",
    "\n",
    "    Pregel(spGraph, initMaps)(vProg, sendMsg, addMaps)\n",
    "  }\n",
    "  \n",
    "  def create_path_graph[VD](graph: Graph[SPMap, Double], goalId: VertexId, startId: VertexId): Graph[PN, Int] = {\n",
    "    // For a given goal node we remove the lookup map and extend the state to a Tuple5 with the goal id and a boolean\n",
    "    val path = graph.mapEdges(e => 0)\n",
    "              .mapVertices((vertixId, attr) => {\n",
    "                if (attr.contains(goalId)) {\n",
    "                  val path_step = attr(goalId)\n",
    "                  if (vertixId == path_step._3 && path_step._2 == -1L)\n",
    "                    (path_step._1, goalId, goalId, false) // while we are at it, we clean up the state a bit\n",
    "                  else  \n",
    "                    (path_step._1, path_step._2, goalId, false)\n",
    "                } else// If the vertice does not have a map to our goal we add a default value to it\n",
    "                    (INFINITY, -1L, -1L, false)\n",
    "              })\n",
    "\n",
    "      def mergeMsg(msg1: VertexId, msg2: VertexId): VertexId = { // we should only get one msg\n",
    "          msg2\n",
    "      }\n",
    "\n",
    "      def vprog(id: VertexId, attr: PN, msg: VertexId): PN = {\n",
    "        // Check that the current node is the one adressed in the message\n",
    "        if (id == msg)\n",
    "          (attr._1, attr._2, attr._3, true)\n",
    "        else // If the message is not addressed to the current node (happens for inital message), use the old value \n",
    "          attr\n",
    "      }\n",
    "      def sendMsg(triplet: EdgeTriplet[PN, Int]): Iterator[(VertexId, VertexId)] = {\n",
    "        // If dstId is the next node on the path and has not yet been activated\n",
    "        if (triplet.srcAttr._2 == triplet.dstId && triplet.srcAttr._4 && !triplet.dstAttr._4) \n",
    "          Iterator((triplet.dstId, triplet.dstId))// Send next msg\n",
    "        else\n",
    "          Iterator.empty// Do nothing\n",
    "      }\n",
    "\n",
    "      Pregel(path, startId)(vprog, sendMsg, mergeMsg).mapTriplets(triplet => {\n",
    "        if(triplet.srcAttr._2 == triplet.dstId && triplet.srcAttr._4)\n",
    "          1\n",
    "        else\n",
    "          0\n",
    "      })\n",
    "  }\n",
    "}\n",
    "\n",
    "println(\"Usage: val result = GraphXShortestWeightedPaths.run(graph, Seq(4L, 0L, 9L))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Usage: val result = GraphXShortestWeightedPaths.run(graph, Seq(4L, 0L, 9L))\n",
    ">     import scala.reflect.ClassTag\n",
    ">     import org.apache.spark.graphx._\n",
    ">     defined object ShortestPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val cc = graph.connectedComponents()\n",
    "val ccDf = cc.vertices.toDF\n",
    "ccDf.groupBy($\"_2\").count().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val start = 2416576872L\n",
    "val goal = 2416609471L\n",
    "val landMarkVertexIds = Seq(goal)\n",
    "val result = ShortestPath.run(graph, landMarkVertexIds)\n",
    "\n",
    "//println(path_graph.vertices.collect.mkString(\"\\n\"))\n",
    "//println(path_graph.edges.collect.mkString(\"\\n\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     start: Long = 2416576872\n",
    ">     goal: Long = 2416609471\n",
    ">     landMarkVertexIds: Seq[Long] = List(2416609471)\n",
    ">     result: org.apache.spark.graphx.Graph[ShortestPath.SPMap,Double] = org.apache.spark.graphx.impl.GraphImpl@72ea9eec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val path_graph = ShortestPath.create_path_graph(result, goal, start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     path_graph: org.apache.spark.graphx.Graph[ShortestPath.PN,Int] = org.apache.spark.graphx.impl.GraphImpl@7128ebb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result.vertices.toDF.filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(graph.edges.toDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//val path_df = path_graph.edges.toDF\n",
    "//display(path_df)\n",
    "//val path_df2 = path_graph.edges.toDF.filter(col(\"attr\")>0)\n",
    "//display(path_df2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     1016395"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph.vertices.toDF.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     736490"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select count(*) from wayGeometry_distDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import com.vividsolutions.jts.io.WKTWriter\n",
    "import com.vividsolutions.jts.geom.{Coordinate, Geometry, GeometryFactory}\n",
    "\n",
    "case class IndexedNode(index: Int, coord: Coordinate)\n",
    "\n",
    "def nodesToWKT(nodes: Seq[Row]): String = {\n",
    "    val indexedCoords = nodes.map { \n",
    "      case Row(index: Int, lat: Double, lon:Double) => IndexedNode(index, new Coordinate(lon, lat))\n",
    "    }\n",
    "    val coords = indexedCoords.sortBy(_.index).map(_.coord).toArray\n",
    "    new WKTWriter().write(new GeometryFactory().createLineString(coords))\n",
    "}\n",
    "\n",
    "val wkt = udf {\n",
    "  (nodes: Seq[Row]) => nodesToWKT(nodes)\n",
    "}\n",
    "\n",
    "val wayWKTDF = wayGeometryDF\n",
    "  .filter(size($\"geometry\") > 1)\n",
    "  .select($\"wayId\", $\"user_sid\", wkt($\"geometry\").as(\"geometry\"))\n",
    "wayWKTDF.createOrReplaceTempView(\"wayWKT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import com.vividsolutions.jts.io.WKTWriter\n",
    ">     import com.vividsolutions.jts.geom.{Coordinate, Geometry, GeometryFactory}\n",
    ">     defined class IndexedNode\n",
    ">     nodesToWKT: (nodes: Seq[org.apache.spark.sql.Row])String\n",
    ">     wkt: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,None)\n",
    ">     wayWKTDF: org.apache.spark.sql.DataFrame = [wayId: bigint, user_sid: string ... 1 more field]"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
