{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ScaDaMaLe, Scalable Data Science and Distributed Machine Learning](https://lamastex.github.io/scalable-data-science/sds/3/x/)\n",
    "==============================================================================================================================\n",
    "\n",
    "Word Count on US State of the Union (SoU) Addresses\n",
    "===================================================\n",
    "\n",
    "-   Word Count in big data is the equivalent of `Hello World` in\n",
    "    programming\n",
    "-   We count the number of occurences of each word in the first and\n",
    "    last (2016) SoU addresses.\n",
    "\n",
    "**prerequisite** see **DO NOW** below. You should have loaded data as\n",
    "instructed in `scalable-data-science/xtraResources/sdsDatasets`.\n",
    "\n",
    "#### DO NOW (if not done already)\n",
    "\n",
    "In your databricks community edition:\n",
    "\n",
    "1.  In your `WorkSpace` create a Folder named `scalable-data-science`\n",
    "2.  `Import` the databricks archive file at the following URL:\n",
    "    -   <https://github.com/lamastex/scalable-data-science/raw/master/dbcArchives/2017/parts/xtraResources.dbc>\n",
    "3.  This should open a structure of directories in with path:\n",
    "    `/Workspace/scalable-data-science/xtraResources/`\n",
    "\n",
    "An interesting analysis of the textual content of the *State of the\n",
    "Union (SoU)* addresses by all US presidents was done in:\n",
    "\n",
    "-   [Alix Rule, Jean-Philippe Cointet, and Peter S. Bearman, Lexical\n",
    "    shifts, substantive changes, and continuity in State of the Union\n",
    "    discourse, 1790–2014, PNAS 2015 112 (35) 10837-10844;\n",
    "    doi:10.1073/pnas.1512221112](http://www.pnas.org/content/112/35/10837.full).\n",
    "\n",
    "![](http://www.pnas.org/content/112/35/10837/F5.large.jpg)\n",
    "\n",
    "[Fig. 5](http://www.pnas.org/content/112/35/10837.full). A river network\n",
    "captures the flow across history of US political discourse, as perceived\n",
    "by contemporaries. Time moves along the x axis. Clusters on semantic\n",
    "networks of 300 most frequent terms for each of 10 historical periods\n",
    "are displayed as vertical bars. Relations between clusters of adjacent\n",
    "periods are indexed by gray flows, whose density reflects their degree\n",
    "of connection. Streams that connect at any point in history may be\n",
    "considered to be part of the same system, indicated with a single color.\n",
    "\n",
    "Let us investigate this dataset ourselves!\n",
    "------------------------------------------\n",
    "\n",
    "1.  We first get the source text data by scraping and parsing from\n",
    "    <http://stateoftheunion.onetwothree.net/texts/index.html> as\n",
    "    explained in [scraping and parsing SoU\n",
    "    addresses](/#workspace/scalable-data-science/xtraResources/sdsDatasets/scraperUSStateofUnionAddresses).\n",
    "\n",
    "-   This data is already made available in DBFS, our distributed file\n",
    "    system.\n",
    "-   We only do the simplest word count with this data in this notebook\n",
    "    and will do more sophisticated analyses in the sequel (including\n",
    "    topic modeling, etc).\n",
    "\n",
    "Key Data Management Concepts\n",
    "----------------------------\n",
    "\n",
    "### The Structure Spectrum\n",
    "\n",
    "**(watch now 1:10)**:\n",
    "\n",
    "[![Structure Spectrum by Anthony Joseph in\n",
    "BerkeleyX/CS100.1x](http://img.youtube.com/vi/pMSGGZVSwqo/0.jpg)](https://www.youtube.com/watch?v=pMSGGZVSwqo?rel=0&autoplay=1&modestbranding=1&start=1&end=70)\n",
    "\n",
    "Here we will be working with **unstructured** or **schema-never** data\n",
    "(plain text files). \\*\\*\\*\n",
    "\n",
    "### Files\n",
    "\n",
    "**(watch later 1:43)**:\n",
    "\n",
    "[![Files by Anthony Joseph in\n",
    "BerkeleyX/CS100.1x](http://img.youtube.com/vi/NJyBQ-cQ3Ac/0.jpg)](https://www.youtube.com/watch?v=NJyBQ-cQ3Ac?rel=0&autoplay=1&modestbranding=1&start=1)\n",
    "\n",
    "### DBFS and dbutils - where is this dataset in our distributed file system?\n",
    "\n",
    "-   Since we are on the databricks cloud, it has a file system called\n",
    "    DBFS\n",
    "-   DBFS is similar to HDFS, the Hadoop distributed file system\n",
    "-   dbutils allows us to interact with dbfs.\n",
    "-   The 'display' command displays the list of files in a given\n",
    "    directory in the file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/datasets/sou\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/datasets/sou\")) // Cntrl+Enter to display the files in dbfs:/datasets/sou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Let us display the *head* or the first few lines of the file\n",
    "`dbfs:/datasets/sou/17900108.txt` to see what it contains using\n",
    "`dbutils.fs.head` method.  \n",
    "`head(file: String, maxBytes: int = 65536): String` -&gt; Returns up to\n",
    "the first 'maxBytes' bytes of the given file as a String encoded in\n",
    "UTF-8 as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.head(\"dbfs:/datasets/sou/17900108.txt\",673) // Cntrl+Enter to get the first 673 bytes of the file (which corresponds to the first five lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "##### You Try!\n",
    "\n",
    "Uncomment and modify `xxxx` in the cell below to read the first 1000\n",
    "bytes from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//dbutils.fs.head(\"dbfs:/datasets/sou/17900108.txt\", xxxx) // Cntrl+Enter to get the first 1000 bytes of the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### Read the file into Spark Context as an RDD of Strings\n",
    "\n",
    "-   The `textFile` method on the available `SparkContext` `sc` can read\n",
    "    the text file `dbfs:/datasets/sou/17900108.txt` into Spark and\n",
    "    create an RDD of Strings\n",
    "    -   but this is done lazily until an action is taken on the RDD\n",
    "        `sou17900108`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sou17900108 = sc.textFile(\"dbfs:/datasets/sou/17900108.txt\") // Cntrl+Enter to read in the textfile as RDD[String]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### Perform some actions on the RDD\n",
    "\n",
    "-   Each String in the RDD `sou17900108` represents one line of data\n",
    "    from the file and can be made to perform one of the following\n",
    "    actions:\n",
    "    -   count the number of elements in the RDD `sou17900108` (i.e., the\n",
    "        number of lines in the text file\n",
    "        `dbfs:/datasets/sou/17900108.txt`) using `sou17900108.count()`\n",
    "    -   display the contents of the RDD using `take` or `collect`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sou17900108.count() // <Shift+Enter> to count the number of elements in the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sou17900108.take(5) // <Shift+Enter> to display the first 5 elements of RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sou17900108.take(5).foreach(println) // <Shift+Enter> to display the first 5 elements of RDD line by line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sou17900108.collect // <Cntrl+Enter> to display all the elements of RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### Cache the RDD in (distributed) memory to avoid recreating it for each action\n",
    "\n",
    "-   Above, every time we took an action on the same RDD, the RDD was\n",
    "    reconstructed from the textfile.\n",
    "    -   Spark's advantage compared to Hadoop MapReduce is the ability to\n",
    "        cache or store the RDD in distributed memory across the nodes.\n",
    "-   Let's use `.cache()` after creating an RDD so that it is in memory\n",
    "    after the first action (and thus avoid reconstruction for subsequent\n",
    "    actions).\n",
    "    -   count the number of elements in the RDD `sou17900108` (i.e., the\n",
    "        number of lines in the text file\n",
    "        `dbfs:/datasets/sou/17900108.txt`) using `sou17900108.count()`\n",
    "    -   display the contents of the RDD using `take` or `collect`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Shift+Enter to read in the textfile as RDD[String] and cache it in distributed memory\n",
    "val sou17900108 = sc.textFile(\"dbfs:/datasets/sou/17900108.txt\")\n",
    "sou17900108.cache() // cache the RDD in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sou17900108.count() // Shift+Enter during this count action the RDD is constructed from texfile and cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sou17900108.count() // Shift+Enter during this count action the cached RDD is used (notice less time taken by the same command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sou17900108.take(5) // <Cntrl+Enter> to display the first 5 elements of the cached RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "#### Lifecycle of a Spark Program\n",
    "\n",
    "**(watch now 0:23)**:\n",
    "\n",
    "[![Spark Program Lifecycle by Anthony Joseph in\n",
    "BerkeleyX/CS100.1x](http://img.youtube.com/vi/HWZUqNYAJj4/0.jpg)](https://www.youtube.com/watch?v=HWZUqNYAJj4?rel=0&autoplay=1&modestbranding=1&start=1)\n",
    "\n",
    "##### Summary\n",
    "\n",
    "-   create RDDs from:\n",
    "    -   some external data source (such as a distributed file system)\n",
    "    -   parallelized collection in your driver program\n",
    "-   lazily transform these RDDs into new RDDs\n",
    "-   cache some of those RDDs for future reuse\n",
    "-   you perform actions to execute parallel computation to produce\n",
    "    results\n",
    "\n",
    "### Transform lines to words\n",
    "\n",
    "-   We need to loop through each line and split the line into words\n",
    "-   For now, let us split using whitespace\n",
    "-   More sophisticated regular expressions can be used to split the line\n",
    "    (as we will see soon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sou17900108\n",
    ".flatMap(line => line.split(\" \"))\n",
    ".take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### Naive word count\n",
    "\n",
    "At a first glace, to do a word count of George Washingtons SoU address,\n",
    "we are templed to do the following:\n",
    "\n",
    "-   just break each line by the whitespace character \" \" and find the\n",
    "    words using a `flatMap`\n",
    "-   then do the `map` with the closure `word => (word, 1)` to initialize\n",
    "    each `word` with a integer count of `1`\n",
    "    -   ie., transform each word to a *(key, value)* pair or `Tuple`\n",
    "        such as `(word, 1)`\n",
    "-   then count all *value*s with the same *key* (`word` is the Key in\n",
    "    our case) by doing a\n",
    "    -   `reduceByKey(_+_)`\n",
    "-   and finally `collect()` to display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sou17900108\n",
    ".flatMap( line => line.split(\" \") )\n",
    ".map( word => (word, 1) )\n",
    ".reduceByKey(_+_)\n",
    ".collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Unfortunately, as you can see from the `collect` above:\n",
    "\n",
    "-   the words have punctuations at the end which means that the same\n",
    "    words are being counted as different words. Eg: importance\n",
    "-   empty words are being counted\n",
    "\n",
    "So we need a bit of `regex`'ing or regular-expression matching (all\n",
    "readily available from Scala via Java String types).\n",
    "\n",
    "We will cover the three things we want to do with a simple example from\n",
    "Middle Earth!\n",
    "\n",
    "-   replace all multiple whitespace characters with one white space\n",
    "    character \" \"\n",
    "-   replace all punction characters we specify within `[` and `]` such\n",
    "    as `[,?.!:;]` by the empty string `\"\"` (i.e., remove these\n",
    "    punctuation characters)\n",
    "-   convert everything to lower-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val example = \"Master, Master!   It's me, Sméagol... mhrhm*%* But they took away our precious, they wronged us. Gollum will protect us..., Master, it's me Sméagol.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example\n",
    "  .replaceAll(\"\\\\s+\", \" \") //replace multiple whitespace characters (including space, tab, new line, etc.) with one whitespace \" \"\n",
    "  .replaceAll(\"\"\"([,?.!:;])\"\"\", \"\") // replace the following punctions characters: , ? . ! : ; . with the empty string \"\"\n",
    "  .toLowerCase() // converting to lower-case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### More sophisticated word count\n",
    "\n",
    "We are now ready to do a word count of George Washington's SoU on\n",
    "January 8th 1790 as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val wordCount_sou17900108 = \n",
    " sou17900108\n",
    "    .flatMap(line => \n",
    "         line.replaceAll(\"\\\\s+\", \" \") //replace multiple whitespace characters (including space, tab, new line, etc.) with one whitespace \" \"\n",
    "             .replaceAll(\"\"\"([,?.!:;])\"\"\", \"\") // replace the following punctions characters: , ? . ! : ; . with the empty string \"\"\n",
    "             .toLowerCase() // converting to lower-case\n",
    "             .split(\" \"))\n",
    "    .map(x => (x, 1))\n",
    "    .reduceByKey(_+_)\n",
    "    \n",
    "wordCount_sou17900108.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val top10 = wordCount_sou17900108.sortBy(_._2, false).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### Doing it all together for George Washington and Barrack Obama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//sc.textFile(\"dbfs:/datasets/sou/17900108.txt\") // George Washington's first SoU\n",
    "sc.textFile(\"dbfs:/datasets/sou/20160112.txt\")   // Barrack Obama's second SoU\n",
    "    .flatMap(line => \n",
    "         line.replaceAll(\"\\\\s+\", \" \") //replace multiple whitespace characters (including space, tab, new line, etc.) with one whitespace \" \"\n",
    "             .replaceAll(\"\"\"([,?.!:;])\"\"\", \"\") // replace the following punctions characters: , ? . ! : ; . with the empty string \"\"\n",
    "             .toLowerCase() // converting to lower-case\n",
    "             .split(\" \"))\n",
    "    .map(x => (x,1))\n",
    "    .reduceByKey(_+_)\n",
    "    .sortBy(_._2, false)\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### Reading all SoUs at once using `wholetextFiles`\n",
    "\n",
    "Let us next read all text files (ending with `.txt`) in the directory\n",
    "`dbfs:/datasets/sou/` at once!\n",
    "\n",
    "`SparkContext.wholeTextFiles` lets you read a directory containing\n",
    "multiple small text files, and returns each of them as\n",
    "`(filename, content)` pairs of strings.\n",
    "\n",
    "This is in contrast with `textFile`, which would return one record per\n",
    "line in each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val souAll = sc.wholeTextFiles(\"dbfs:/datasets/sou/*.txt\") // Shift+Enter to read all text files in dbfs:/datasets/sou/\n",
    "souAll.cache() // let's cache this RDD for efficient reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "souAll.count() // Shift+enter to count the number of entries in RDD[(String,String)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "souAll.count() // Cntrl+Enter to count the number of entries in cached RDD[(String,String)] again (much faster!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Let's examine the first two elements of the RDD `souAll`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "souAll.take(2) // Cntr+Enter to see the first two elements of souAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Clearly, the elements are a pair of Strings, where the first String\n",
    "gives the filename and the second String gives the contents in the file.\n",
    "\n",
    "this can be very helpful to simply loop through the files and take an\n",
    "action, such as counting the number of words per address, as folows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// this just collects the file names which is the first element of the tuple given by \"._1\" \n",
    "souAll.map( fileContentsPair => fileContentsPair._1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Let us find the number of words in each of the SoU addresses next (we\n",
    "need to work with Strings inside the closure!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val wcs = souAll.map( fileContentsPair => \n",
    "  {\n",
    "    val wc = fileContentsPair._2\n",
    "                             .replaceAll(\"\\\\s+\", \" \") //replace multiple whitespace characters (including space, tab, new line, etc.) with one whitespace \" \"\n",
    "                             .replaceAll(\"\"\"([,?.!:;])\"\"\", \"\") // replace the following punctions characters: , ? . ! : ; . with the empty string \"\"\n",
    "                             .toLowerCase() // converting to lower-case\n",
    "                             .split(\" \") // split each word separated by white space\n",
    "                             .size // find the length of array\n",
    "    wc\n",
    "  }    \n",
    ")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcs.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "YouTry: HOMEWORK\n",
    "----------------\n",
    "\n",
    "-   HOWEWORK WordCount 1: `sortBy`\n",
    "-   HOMEWROK WordCount 2: `dbutils.fs`\n",
    "\n",
    "##### HOMEWORK WordCount 1. `sortBy`\n",
    "\n",
    "Let's understand `sortBy` a bit more carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val example = \"Master, Master!   It's me, Sméagol... mhrhm*%* But they took away our precious, they wronged us. Gollum will protect us..., Master, it's me Sméagol.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val words = example.replaceAll(\"\\\\s+\", \" \") //replace multiple whitespace characters (including space, tab, new line, etc.) with one whitespace \" \"\n",
    "       .replaceAll(\"\"\"([,?.!:;])\"\"\", \"\") // replace the following punctions characters: , ? . ! : ; . with the empty string \"\"\n",
    "       .toLowerCase() // converting to lower-case\n",
    "       .split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rddWords = sc.parallelize(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddWords.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val wordCounts = rddWords\n",
    "                  .map(x => (x,1))\n",
    "                  .reduceByKey(_+_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val top10 = wordCounts.sortBy(_._2, false).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Make your code easy to read for other developers ;)  \n",
    "Use 'case classes' with well defined variable names that everyone can\n",
    "understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val top10 = wordCounts.sortBy({\n",
    "  case (word, count) => count\n",
    "}, false).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "If you just want a total count of all words in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddWords.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "##### YoutTry: HOMEWORK WordCount 2: `dbutils.fs`\n",
    "\n",
    "Have a brief look at what other commands dbutils.fs supports. We will\n",
    "introduce them as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.help // some of these were used to ETL this data into dbfs:/datasets/sou "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Exercise 2: SouWordCount\n",
    "------------------------\n",
    "\n",
    "Count the number of each word across all the \"dbfs:/datasets/sou/\\*.txt\"\n",
    "files and output the result as an Array of (word,count) tuples from the\n",
    "most frequent to the least frequent word.\n",
    "\n",
    "This is the same as [Exercise 2 in the local\n",
    "environment](https://github.com/lamastex/scalable-data-science/tree/master/_sds/basics/infrastructure/onpremise/dockerCompose/programs/exercises/sparkSubmit#exercise-2-souwordcount)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// code in this cell the solution to the above exercise in the notebook environment\n",
    "//\n",
    "//"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
