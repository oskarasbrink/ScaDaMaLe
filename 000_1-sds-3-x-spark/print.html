<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title></title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x-spark/000_ScaDaMaLe.html">000_ScaDaMaLe</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x-spark/001_whySpark.html">001_whySpark</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x-spark/002_00_loginToDatabricks.html">002_00_loginToDatabricks</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x-spark/002_01_multiLingualNotebooks.html">002_01_multiLingualNotebooks</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x-spark/003_00_scalaCrashCourse.html">003_00_scalaCrashCourse</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x-spark/003_01_scalaCrashCourse.html">003_01_scalaCrashCourse</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x-spark/004_RDDsTransformationsActions.html">004_RDDsTransformationsActions</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x-spark/005_RDDsTransformationsActionsHOMEWORK.html">005_RDDsTransformationsActionsHOMEWORK</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x-spark/006_WordCount.html">006_WordCount</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x-spark/006a_PipedRDD.html">006a_PipedRDD</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title"></h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<ul>
<li><strong>Course Name:</strong> <em>Scalable Data Science and Distributed Machine Learning</em></li>
<li><strong>Course Acronym:</strong> <em>ScaDaMaLe</em> or <em>sds-3.x</em>.</li>
</ul>
<p>The course is given in several modules.</p>
<h2 id="expected-reference-readings"><a class="header" href="#expected-reference-readings">Expected Reference Readings</a></h2>
<p>Note that you need to be logged into your library with access to these publishers:</p>
<ul>
<li><a href="https://learning.oreilly.com/library/view/high-performance-spark/9781491943199/">https://learning.oreilly.com/library/view/high-performance-spark/9781491943199/</a></li>
<li><a href="https://learning.oreilly.com/library/view/spark-the-definitive/9781491912201/">https://learning.oreilly.com/library/view/spark-the-definitive/9781491912201/</a></li>
<li><a href="https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/">https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/</a></li>
<li>Introduction to Algorithms, Third Edition, Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein from
<ul>
<li><a href="https://ebookcentral.proquest.com/lib/uu/reader.action?docID=3339142">https://ebookcentral.proquest.com/lib/uu/reader.action?docID=3339142</a></li>
</ul>
</li>
<li><a href="https://github.com/lamastex/scalable-data-science/tree/master/read">Reading Materials Provided</a></li>
</ul>
<h2 id="course-contents"><a class="header" href="#course-contents">Course Contents</a></h2>
<p>The databricks notebooks will be made available as the course progresses in the : - course site at: - <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">https://lamastex.github.io/scalable-data-science/sds/3/x/</a> - and course book at: - <a href="https://lamastex.github.io/ScaDaMaLe/index.html">https://lamastex.github.io/ScaDaMaLe/index.html</a></p>
<ul>
<li>You may upload Course Content into Databricks Community Edition from:
<ul>
<li><a href="https://github.com/lamastex/scalable-data-science/tree/master/dbcArchives/latest/">2021 dbc ARCHIVES</a></li>
</ul>
</li>
</ul>
<h2 id="course-assessment"><a class="header" href="#course-assessment">Course Assessment</a></h2>
<p>There will be assigned readings, coding exercises and group projects. Course participation is encouraged.</p>
<p>Detailed instructions to pass each module will be given in the sequel.</p>
<h2 id="course-sponsors"><a class="header" href="#course-sponsors">Course Sponsors</a></h2>
<p>The course builds on contents developed since 2016 with support from New Zealand's Data Industry. The 2017-2019 versions were academically sponsored by Uppsala University's Inter-Faculty Course grant, Department of Mathematics and The Centre for Interdisciplinary Mathematics and industrially sponsored by <a href="https://databricks.com">databricks</a>, <a href="https://aws.amazon.com/">AWS</a> and Swedish data industry via <a href="https://combient.com">Combient AB</a>, <a href="https://seb.se/">SEB</a> and <a href="https://combient.com/mix">Combient Mix AB</a>. The 2021-2022 versions were/are academically sponsored by <a href="https://wasp-sweden.org/graduate-school/ai-graduate-school-courses/">AI-Track of the WASP Graduate School</a> and <a href="https://www.math.uu.se/research/cim/">Centre for Interdisciplinary Mathematics</a> and industrially sponsored by <a href="https://databricks.com">databricks</a> and <a href="https://aws.amazon.com/">AWS</a> via <em>databricks University Alliance</em> and <a href="https://combient.com/mix">Combient Mix AB</a> via industrial mentorships.</p>
<h2 id="course-instructor"><a class="header" href="#course-instructor">Course Instructor</a></h2>
<p>I, Raazesh Sainudiin or <strong>Raaz</strong>, will be an instructor for the course.</p>
<p>I have</p>
<ul>
<li>more than 16 years of academic research experience in applied mathematics and statistics and</li>
<li>over 3 and 5 years of full-time and part-time experience in the data industry.</li>
</ul>
<p>I currently (2022) have an effective joint appointment as:</p>
<ul>
<li><a href="https://katalog.uu.se/profile/?id=N17-214">Associate Professor of Mathematics with specialisation in Data Science</a> at <a href="https://www.math.uu.se/">Department of Mathematics</a>, <a href="https://www.uu.se/">Uppsala University</a>, Uppsala, Sweden and</li>
<li>Researching,Developing and Enabling Chair in Mathematical Data Engineering Sciences at <a href="https://combient.com/mix">Combient Mix AB</a>, Stockholm, Sweden</li>
</ul>
<p>Quick links on Raaz's background:</p>
<ul>
<li><a href="https://www.linkedin.com/in/raazesh-sainudiin-45955845/">https://www.linkedin.com/in/raazesh-sainudiin-45955845/</a></li>
<li><a href="https://lamastex.github.io/cv/">Raaz's academic CV</a></li>
<li><a href="https://lamastex.github.io/publications/">Raaz's publications list</a></li>
</ul>
</div>
<div class="cell markdown">
<h1 id="what-is-the-data-science-process"><a class="header" href="#what-is-the-data-science-process">What is the <a href="https://en.wikipedia.org/wiki/Data_science">Data Science Process</a></a></h1>
<p><strong>The Data Science Process in one picture</strong></p>
<p><img src="https://github.com/lamastex/scalable-data-science/raw/master/assets/images/sds.png" alt="what is sds?" title="sds" /></p>
<hr />
<h2 id="what-is-scalable-data-science-and-distributed-machine-learning"><a class="header" href="#what-is-scalable-data-science-and-distributed-machine-learning">What is scalable data science and distributed machine learning?</a></h2>
<p>Scalability merely refers to the ability of the data science process to scale to massive datasets (popularly known as <em>big data</em>).</p>
<p>For this we need <em>distributed fault-tolerant computing</em> typically over large clusters of commodity computers -- the core infrastructure in a public cloud today.</p>
<p><em>Distributed Machine Learning</em> allows the models in the data science process to be scalably trained and extract value from big data.</p>
</div>
<div class="cell markdown">
<h2 id="what-is-data-science"><a class="header" href="#what-is-data-science">What is Data Science?</a></h2>
<p>It is increasingly accepted that <a href="https://en.wikipedia.org/wiki/Data_science">Data Science</a></p>
<blockquote>
<p>is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data. Data science is related to data mining, machine learning and big data.</p>
</blockquote>
<blockquote>
<p>Data science is a &quot;concept to unify statistics, data analysis and their related methods&quot; in order to &quot;understand and analyze actual phenomena&quot; with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, domain knowledge and information science. Turing award winner Jim Gray imagined data science as a &quot;fourth paradigm&quot; of science (empirical, theoretical, computational and now data-driven) and asserted that &quot;everything about science is changing because of the impact of information technology&quot; and the data deluge.</p>
</blockquote>
<p>Now, let us look at two industrially-informed academic papers that influence the above quote on what is Data Science, but with a view towards the contents and syllabus of this course.</p>
<p>Source: <a href="https://dl.acm.org/citation.cfm?id=2500499">Vasant Dhar, Data Science and Prediction, Communications of the ACM, Vol. 56 (1). p. 64, DOI:10.1145/2500499</a></p>
<p><strong>key insights in the above paper</strong></p>
<ul>
<li>Data Science is the study of <em>the generalizabile extraction of knowledge from data</em>.</li>
<li>A common epistemic requirement in assessing whether new knowledge is actionable for decision making is its predictive power, not just its ability to explain the past.</li>
<li>A <em>data scientist requires an integrated skill set spanning</em>
<ul>
<li>mathematics,</li>
<li>machine learning,</li>
<li>artificial intelligence,</li>
<li>statistics,</li>
<li>databases, and</li>
<li>optimization,</li>
<li>along with a deep understanding of the craft of problem formulation to engineer effective solutions.</li>
</ul>
</li>
</ul>
<p>Source: <a href="https://www.science.org/doi/pdf/10.1126/science.aaa8415">Machine learning: Trends, perspectives, and prospects, M. I. Jordan, T. M. Mitchell, Science 17 Jul 2015: Vol. 349, Issue 6245, pp. 255-260, DOI: 10.1126/science.aaa8415</a></p>
<p><strong>key insights in the above paper</strong></p>
<ul>
<li>ML is concerned with the building of computers that improve automatically through experience</li>
<li>ML lies at the intersection of computer science and statistics and at the core of artificial intelligence and data science</li>
<li>Recent progress in ML is due to:
<ul>
<li>development of new algorithms and theory</li>
<li>ongoing explosion in the availability of online data</li>
<li>availability of low-cost computation (*through clusters of commodity hardware in the *cloud* )</li>
</ul>
</li>
<li>The adoption of data science and ML methods is leading to more evidence-based decision-making across:
<ul>
<li>health sciences (neuroscience research, )</li>
<li>manufacturing</li>
<li>robotics (autonomous vehicle)</li>
<li>vision, speech processing, natural language processing</li>
<li>education</li>
<li>financial modeling</li>
<li>policing</li>
<li>marketing</li>
</ul>
</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">//This allows easy embedding of publicly available information into any other notebook
//Example usage:
// displayHTML(frameIt(&quot;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Topics_in_LDA&quot;,250))
def frameIt( u:String, h:Int ) : String = {
      &quot;&quot;&quot;&lt;iframe 
 src=&quot;&quot;&quot;&quot;+ u+&quot;&quot;&quot;&quot;
 width=&quot;95%&quot; height=&quot;&quot;&quot;&quot; + h + &quot;&quot;&quot;&quot;
 sandbox&gt;
  &lt;p&gt;
    &lt;a href=&quot;http://spark.apache.org/docs/latest/index.html&quot;&gt;
      Fallback link for browsers that, unlikely, don't support frames
    &lt;/a&gt;
  &lt;/p&gt;
&lt;/iframe&gt;&quot;&quot;&quot;
   }
displayHTML(frameIt(&quot;https://en.wikipedia.org/wiki/Data_science&quot;,500))
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://en.wikipedia.org/wiki/Data_science"
 width="95%" height="500"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<p>But what is Data Engineering (including Machine Learning Engineering and Operations) and how does it relate to Data Science?</p>
</div>
<div class="cell markdown">
<h2 id="data-engineering"><a class="header" href="#data-engineering">Data Engineering</a></h2>
<p>There are several views on what a data engineer is supposed to do:</p>
<p>Some views are rather narrow and emphasise division of labour between data engineers and data scientists:</p>
<ul>
<li>https://www.oreilly.com/ideas/data-engineering-a-quick-and-simple-definition
<ul>
<li>Let's check out what skills a data engineer is expected to have according to the link above.</li>
</ul>
</li>
</ul>
<blockquote>
<p>&quot;Ian Buss, principal solutions architect at Cloudera, notes that data scientists focus on finding new insights from a data set, while data engineers are concerned with the production readiness of that data and all that comes with it: formats, scaling, resilience, security, and more.&quot;</p>
</blockquote>
<blockquote>
<p>What skills do data engineers need? Those “10-30 different big data technologies” Anderson references in “Data engineers vs. data scientists” can fall under numerous areas, such as file formats, &gt; ingestion engines, stream processing, batch processing, batch SQL, data storage, cluster management, transaction databases, web frameworks, data visualizations, and machine learning. And that’s just the tip of the iceberg.</p>
</blockquote>
<blockquote>
<p>Buss says data engineers should have the following skills and knowledge:</p>
</blockquote>
<blockquote>
<ul>
<li>They need to know Linux and they should be comfortable using the command line.</li>
<li>They should have experience programming in at least Python or Scala/Java.</li>
<li>They need to know SQL.</li>
<li>They need some understanding of distributed systems in general and how they are different from traditional storage and processing systems.</li>
<li>They need a deep understanding of the ecosystem, including ingestion (e.g. Kafka, Kinesis), processing frameworks (e.g. Spark, Flink) and storage engines (e.g. S3, HDFS, HBase, Kudu). They should know the strengths and weaknesses of each tool and what it's best used for.</li>
<li>They need to know how to access and process data.</li>
</ul>
</blockquote>
<p>Let's dive deeper into such highly compartmentalised views of data engineers and data scientists and the so-called &quot;machine learning engineers&quot; according the following view:</p>
<ul>
<li>https://www.oreilly.com/ideas/data-engineers-vs-data-scientists</li>
</ul>
<p>embedded below.</p>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://www.oreilly.com/ideas/data-engineers-vs-data-scientists&quot;,500))
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://www.oreilly.com/ideas/data-engineers-vs-data-scientists"
 width="95%" height="500"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<h2 id="the-data-engineering-scientist-as-the-middle-way"><a class="header" href="#the-data-engineering-scientist-as-the-middle-way">The Data Engineering Scientist as &quot;The Middle Way&quot;</a></h2>
<p>Here are some basic axioms that should be self-evident.</p>
<ul>
<li>Yes, there are differences in skillsets across humans
<ul>
<li>some humans will be better and have inclinations for engineering and others for pure mathematics by nature and nurture</li>
<li>one human cannot easily be a master of everything needed for innovating a new data-based product or service (very very rarely though this happens)</li>
</ul>
</li>
<li>Skills can be gained by any human who wants to learn to the extent s/he is able to expend time, energy, etc.</li>
</ul>
<p>For the <strong>Scalable Data Engineering Science Process:</strong> <em>towards Production-Ready and Productisable Prototyping</em> we need to allow each data engineer to be more of a data scientist and each data scientist to be more of a data engineer, up to each individual's <em>comfort zones</em> in technical and mathematical/conceptual and time-availability planes, but with some <strong>minimal expectations</strong> of mutual appreciation.</p>
<p>This course is designed to help you take the first minimal steps towards such a <strong>data engineering science</strong> process.</p>
<p>In the sequel it will become apparent <strong>why a team of data engineering scientists</strong> with skills across the conventional (2022) spectrum of data engineer versus data scientist <strong>is crucial</strong> for <strong>Production-Ready and Productisable Prototyping</strong>, whose outputs include standard AI products today.</p>
</div>
<div class="cell markdown">
<h1 id="a-brief-tour-of-data-science"><a class="header" href="#a-brief-tour-of-data-science">A Brief Tour of Data Science</a></h1>
<h2 id="history-of-data-analysis-and-where-does-big-data-come-from"><a class="header" href="#history-of-data-analysis-and-where-does-big-data-come-from">History of Data Analysis and Where Does &quot;Big Data&quot; Come From?</a></h2>
<ul>
<li>A Brief History and Timeline of Data Analysis and Big Data</li>
<li><a href="https://en.wikipedia.org/wiki/Big_data">https://en.wikipedia.org/wiki/Big_data</a></li>
<li><a href="https://whatis.techtarget.com/feature/A-history-and-timeline-of-big-data">https://whatis.techtarget.com/feature/A-history-and-timeline-of-big-data</a></li>
<li>Where does Data Come From?</li>
<li>Some of the sources of big data.
<ul>
<li>online click-streams (a lot of it is recorded but a tiny amount is analyzed):
<ul>
<li>record every click</li>
<li>every ad you view</li>
<li>every billing event,</li>
<li>every transaction, every network message, and every fault.</li>
</ul>
</li>
<li>User-generated content (on web and mobile devices):
<ul>
<li>every post that you make on Facebook</li>
<li>every picture sent on Instagram</li>
<li>every review you write for Yelp or TripAdvisor</li>
<li>every tweet you send on Twitter</li>
<li>every video that you post to YouTube.</li>
</ul>
</li>
<li>Science (for scientific computing):
<ul>
<li>data from various repositories for natural language processing:
<ul>
<li>Wikipedia,</li>
<li>the Library of Congress,</li>
<li>twitter firehose and google ngrams and digital archives,</li>
</ul>
</li>
<li>data from scientific instruments/sensors/computers:
<ul>
<li>the Large Hadron Collider (more data in a year than all the other data sources combined!)</li>
<li>genome sequencing data (sequencing cost is dropping much faster than Moore's Law!)</li>
<li>output of high-performance computers (super-computers) for data fusion, estimation/prediction and exploratory data analysis</li>
</ul>
</li>
</ul>
</li>
<li>Graphs are also an interesting source of big data (<em>network science</em>).
<ul>
<li>social networks (collaborations, followers, fb-friends or other relationships),</li>
<li>telecommunication networks,</li>
<li>computer networks,</li>
<li>road networks</li>
</ul>
</li>
<li>machine logs:
<ul>
<li>by servers around the internet (hundreds of millions of machines out there!)</li>
<li>internet of things.</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<h2 id="data-science-with-cloud-computing-and-whats-hard-about-it"><a class="header" href="#data-science-with-cloud-computing-and-whats-hard-about-it">Data Science with Cloud Computing and What's Hard about it?</a></h2>
<ul>
<li>See <a href="https://en.wikipedia.org/wiki/Cloud_computing">Cloud Computing</a> to understand the work-horse for analysing big data at <a href="https://en.wikipedia.org/wiki/Data_center">data centers</a></li>
</ul>
<blockquote>
<p>Cloud computing is the on-demand availability of computer system resources, especially data storage (cloud storage) and computing power, without direct active management by the user. Large clouds often have functions distributed over multiple locations, each location being a data center. Cloud computing relies on sharing of resources to achieve coherence and economies of scale, typically using a &quot;pay-as-you-go&quot; model which can help in reducing capital expenses but may also lead to unexpected operating expenses for unaware users.</p>
</blockquote>
<ul>
<li>
<p>In fact, if you are logged into <code>https://*.databricks.com/*</code> you are computing in the cloud! So the computations are actually running in an instance of the hardware available at a data center like the following:</p>
<ul>
<li><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/d7/CERN_Server_03.jpg/250px-CERN_Server_03.jpg" alt="" /></li>
</ul>
</li>
<li>
<p>Here is a data center used by CERN in 2010.</p>
<ul>
<li><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/98/Cern_datacenter.jpg/450px-Cern_datacenter.jpg" alt="" /></li>
</ul>
</li>
<li>
<p>What's hard about scalable data science in the cloud?</p>
<ul>
<li>To analyse datasets that are big, say more than a few TBs, we need to split the data and put it in several computers that are networked - <em>a typical cloud</em></li>
<li><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/de/Wikimedia_Foundation_Servers-8055_03_square.jpg/120px-Wikimedia_Foundation_Servers-8055_03_square.jpg" alt="" /></li>
<li>However, as the number of computer nodes in such a network increases, the probability of hardware failure or fault (say the hard-disk or memory or CPU or switch breaking down) also increases and can happen while the computation is being performed</li>
<li>Therefore for scalable data science, i.e., data science that can scale with the size of the input data by adding more computer nodes, we need <em>fault-tolerant computing and storage framework at the software level</em> to ensure the computations finish even if there are hardware faults.</li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<p>Here is a recommended light reading on <strong>What is &quot;Big Data&quot; -- Understanding the History</strong> (18 minutes): - <a href="https://towardsdatascience.com/what-is-big-data-understanding-the-history-32078f3b53ce">https://towardsdatascience.com/what-is-big-data-understanding-the-history-32078f3b53ce</a></p>
</div>
<div class="cell markdown">
<hr />
<hr />
</div>
<div class="cell markdown">
<h1 id="what-should-you-be-able-to-do-at-the-end-of-this-course"><a class="header" href="#what-should-you-be-able-to-do-at-the-end-of-this-course">What should <em>you</em> be able to do at the end of this course?</a></h1>
<p>By following these online interactions in the form of lab/lectures, asking questions, engaging in discussions, doing HOMEWORK assignments and completing the group project, you should be able to:</p>
<ul>
<li>Understand the principles of fault-tolerant scalable computing in Spark
<ul>
<li>in-memory and generic DAG extensions of Map-reduce</li>
<li>resilient distributed datasets for fault-tolerance</li>
<li>skills to process today's big data using state-of-the art techniques in Apache Spark 3.0, in terms of:
<ul>
<li>hands-on coding with realistic datasets</li>
<li>an intuitive understanding of the ideas behind the technology and methods</li>
<li>pointers to academic papers in the literature, technical blogs and video streams for <em>you to futher your theoretical understanding</em>.</li>
</ul>
</li>
</ul>
</li>
<li>More concretely, you will be able to:
<ul>
<li>Extract, Transform, Load, Interact, Explore and Analyze Data</li>
<li>Build Scalable Machine Learning Pipelines (or help build them) using Distributed Algorithms and Optimization</li>
</ul>
</li>
<li>How to keep up?
<ul>
<li>This is a fast-changing world.</li>
<li>Recent videos around Apache Spark are archived here (these videos are a great way to learn the latest happenings in industrial R&amp;D today!):
<ul>
<li><a href="https://databricks.com/dataaisummit">https://databricks.com/dataaisummit</a></li>
</ul>
</li>
</ul>
</li>
<li>What is mathematically stable in the world of 'big data'?
<ul>
<li>There is a growing body of work on the analysis of parallel and distributed algorithms, the work-horse of big data and AI.</li>
<li>We will see some of this in a theoretical module later, but the immediate focus here is on how to write programs and analyze data.</li>
</ul>
</li>
</ul>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="why-apache-spark"><a class="header" href="#why-apache-spark">Why Apache Spark?</a></h1>
<ul>
<li><a href="https://cacm.acm.org/magazines/2016/11/209116-apache-spark/fulltext">Apache Spark: A Unified Engine for Big Data Processing</a> By Matei Zaharia, Reynold S. Xin, Patrick Wendell, Tathagata Das, Michael Armbrust, Ankur Dave, Xiangrui Meng, Josh Rosen, Shivaram Venkataraman, Michael J. Franklin, Ali Ghodsi, Joseph Gonzalez, Scott Shenker, Ion Stoica Communications of the ACM, Vol. 59 No. 11, Pages 56-65 10.1145/2934664</li>
</ul>
<p><a href="https://player.vimeo.com/video/185645796"><img src="https://i.vimeocdn.com/video/597494216-6f494f2fb4efb90efe6bb3206f3892bec6b202352951512dbde44c976549dd87-d.jpg?mw=240&amp;q=255" alt="Apache Spark ACM Video" /></a></p>
<p>Right-click the above image-link, open in a new tab and watch the video (4 minutes) or read about it in the Communications of the ACM in the frame below or from the link above.</p>
<p>**Key Insights from <a href="https://cacm.acm.org/magazines/2016/11/209116-apache-spark/fulltext">Apache Spark: A Unified Engine for Big Data Processing</a> **</p>
<ul>
<li>A simple programming model can capture streaming, batch, and interactive workloads and enable new applications that combine them.</li>
<li>Apache Spark applications range from finance to scientific data processing and combine libraries for SQL, machine learning, and graphs.</li>
<li>In six years, Apache Spark has grown to 1,000 contributors and thousands of deployments.</li>
</ul>
<p><img src="https://dl.acm.org/cms/attachment/6f54b222-fe96-497a-8bfc-0e6ea250b05d/ins01.gif" alt="Key Insights" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">//This allows easy embedding of publicly available information into any other notebook
//Example usage:
// displayHTML(frameIt(&quot;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Topics_in_LDA&quot;,250))
def frameIt( u:String, h:Int ) : String = {
      &quot;&quot;&quot;&lt;iframe 
 src=&quot;&quot;&quot;&quot;+ u+&quot;&quot;&quot;&quot;
 width=&quot;95%&quot; height=&quot;&quot;&quot;&quot; + h + &quot;&quot;&quot;&quot;&gt;
  &lt;p&gt;
    &lt;a href=&quot;http://spark.apache.org/docs/latest/index.html&quot;&gt;
      Fallback link for browsers that, unlikely, don't support frames
    &lt;/a&gt;
  &lt;/p&gt;
&lt;/iframe&gt;&quot;&quot;&quot;
   }
displayHTML(frameIt(&quot;https://cacm.acm.org/magazines/2016/11/209116-apache-spark/fulltext&quot;,600))
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://cacm.acm.org/magazines/2016/11/209116-apache-spark/fulltext"
 width="95%" height="600">
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<p>Spark 3.0 is the latest version now (20200918) and it should be seen as the latest step in the evolution of tools in the big data ecosystem as summarized in <a href="https://towardsdatascience.com/what-is-big-data-understanding-the-history-32078f3b53ce">https://towardsdatascience.com/what-is-big-data-understanding-the-history-32078f3b53ce</a>:</p>
<p><img src="https://miro.medium.com/max/1200/1*0bWwqlOfjRqoUDqrH62GbQ.png" alt="Spark in context" /></p>
</div>
<div class="cell markdown">
<h2 id="alternatives-to-apache-spark"><a class="header" href="#alternatives-to-apache-spark">Alternatives to Apache Spark</a></h2>
<p>There are several alternatives to Apache Spark, but none of them have the penetration and community of Spark as of 2021.</p>
<p>For real-time streaming operations <a href="https://flink.apache.org/">Apache Flink</a> is competitive. See <a href="https://www.projectpro.io/article/apache-flink-vs-spark-will-one-overtake-the-other/282#toc-7">Apache Flink vs Spark – Will one overtake the other?</a> for a July 2021 comparison. Most scalable data science and engineering problems faced by several major industries in Sweden today are routinely solved using tools in the ecosystem around Apache Spark. Therefore, we will focus on Apache Spark here which still holds <a href="http://www.tpc.org/tpcds/results/tpcds_perf_results5.asp?spm=a2c65.11461447.0.0.626f184fy7PwOU&amp;resulttype=all">the world record for 10TB or 10,000 GB sort</a> by <a href="https://www.alibabacloud.com/blog/alibaba-cloud-e-mapreduce-sets-world-record-again-on-tpc-ds-benchmark_596195">Alibaba cloud</a> in 06/17/2020.</p>
<p>Several alternatives to Apache Spark exist. See the following for verious commercial options: - <a href="https://sourceforge.net/software/product/Apache-Spark/alternatives">https://sourceforge.net/software/product/Apache-Spark/alternatives</a></p>
<p>Read the following for a comparison of three popular frameworks in 2022 for distributed computing:</p>
<ul>
<li><a href="https://blog.dominodatalab.com/spark-dask-ray-choosing-the-right-framework">https://blog.dominodatalab.com/spark-dask-ray-choosing-the-right-framework</a></li>
</ul>
<p>Here, we will focus on Apache Spark as it is still a popular framework for distributed computing with a rich ecosystem around it.</p>
</div>
<div class="cell markdown">
<h2 id="the-big-data-problem"><a class="header" href="#the-big-data-problem">The big data problem</a></h2>
<p><strong>Hardware, distributing work, handling failed and slow machines</strong></p>
<p>Let us recall and appreciate the following:</p>
<ul>
<li>The Big Data Problem
<ul>
<li>Many routine problems today involve dealing with &quot;big data&quot;, operationally, this is a dataset that is larger than a few TBs and thus won't fit into a single commodity computer like a powerful desktop or laptop computer.</li>
</ul>
</li>
<li>Hardware for Big Data</li>
<li>The best single commodity computer can not handle big data as it has limited hard-disk and memory</li>
<li>Thus, we need to break the data up into lots of commodity computers that are networked together via cables to communicate instructions and data between them - this can be thought of as <em>a cloud</em></li>
<li>How to distribute work across a cluster of commodity machines?
<ul>
<li>We need a software-level framework for this.</li>
</ul>
</li>
<li>How to deal with failures or slow machines?
<ul>
<li>We also need a software-level framework for this.</li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<h2 id="key-papers"><a class="header" href="#key-papers">Key Papers</a></h2>
<ul>
<li>
<p>Key Historical Milestones</p>
<ul>
<li>1956-1979: <a href="https://en.wikipedia.org/wiki/Lisp_(programming_language)">Stanford, MIT, CMU, and other universities develop set/list operations in LISP, Prolog, and other languages for parallel processing</a></li>
<li>2004: <strong>READ</strong>: <a href="https://research.google/pubs/pub62/">Google's MapReduce: Simplified Data Processing on Large Clusters, by Jeffrey Dean and Sanjay Ghemawat</a></li>
<li>2006: <a href="https://en.wikipedia.org/wiki/Apache_Hadoop">Yahoo!'s Apache Hadoop, originating from the Yahoo!’s Nutch Project, Doug Cutting - wikipedia</a></li>
<li>2009: <a href="https://aws.amazon.com/emr/">Cloud computing with Amazon Web Services Elastic MapReduce</a>, a Hadoop version modified for Amazon Elastic Cloud Computing (EC2) and Amazon Simple Storage System (S3), including support for Apache Hive and Pig.</li>
<li>2010: <strong>READ</strong>: <a href="https://dx.doi.org/10.1109/MSST.2010.5496972">The Hadoop Distributed File System, by Konstantin Shvachko, Hairong Kuang, Sanjay Radia, and Robert Chansler. IEEE MSST</a></li>
</ul>
</li>
<li>
<p>Apache Spark Core Papers</p>
<ul>
<li>2012: <strong>READ</strong>: <a href="https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf">Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing, Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin Ma, Murphy McCauley, Michael J. Franklin, Scott Shenker and Ion Stoica. NSDI</a></li>
<li>2016: <a href="https://cacm.acm.org/magazines/2016/11/209116-apache-spark/fulltext">Apache Spark: A Unified Engine for Big Data Processing</a> By Matei Zaharia, Reynold S. Xin, Patrick Wendell, Tathagata Das, Michael Armbrust, Ankur Dave, Xiangrui Meng, Josh Rosen, Shivaram Venkataraman, Michael J. Franklin, Ali Ghodsi, Joseph Gonzalez, Scott Shenker, Ion Stoica , Communications of the ACM, Vol. 59 No. 11, Pages 56-65, 10.1145/2934664</li>
</ul>
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/week1/dbTrImg_BriefHistoryFuncProgBigData700x.png" alt="brief history of functional programming and big data by SparkCamp" /></p>
</li>
<li>
<p>A lot has happened since 2014 to improve efficiency of Spark and embed more into the big data ecosystem</p>
<ul>
<li>See <a href="https://www.youtube.com/watch?v=p4PkA2huzVc">Introducing Apache Spark 3.0 | Matei Zaharia and Brooke Wenig | Keynote Spark + AI Summit 2020</a>.</li>
</ul>
</li>
<li>
<p>More research papers on Spark are available from here:</p>
<ul>
<li><a href="https://databricks.com/resources?_sft_resource_type=research-papers">https://databricks.com/resources?<em>sft</em>resource_type=research-papers</a></li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<h2 id="mapreduce-and-apache-spark"><a class="header" href="#mapreduce-and-apache-spark">MapReduce and Apache Spark.</a></h2>
<p>MapReduce as we will see shortly in action is a framework for distributed fault-tolerant computing over a fault-tolerant distributed file-system, such as Google File System or open-source Hadoop for storage.</p>
<ul>
<li>Unfortunately, Map Reduce is bounded by Disk I/O and can be slow
<ul>
<li>especially when doing a sequence of MapReduce operations requirinr multiple Disk I/O operations</li>
</ul>
</li>
<li>Apache Spark can use Memory instead of Disk to speed-up MapReduce Operations
<ul>
<li>Spark Versus MapReduce - the speed-up is orders of magnitude faster</li>
</ul>
</li>
<li>SUMMARY
<ul>
<li>Spark uses memory instead of disk alone and is thus fater than Hadoop MapReduce</li>
<li>Spark's resilience abstraction is by RDD (resilient distributed dataset)</li>
<li>RDDs can be recovered upon failures from their <em>lineage graphs</em>, the recipes to make them starting from raw data</li>
<li>Spark supports a lot more than MapReduce, including streaming, interactive in-memory querying, etc.</li>
<li>Spark demonstrated an unprecedented sort of 1 petabyte (1,000 terabytes) worth of data in 234 minutes running on 190 Amazon EC2 instances (in 2015).</li>
<li>Spark expertise corresponds to the highest Median Salary in the US (~ 150K)</li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<hr />
<hr />
<p><strong>Next let us get everyone to login to databricks</strong> (or another Spark platform) to get our hands dirty with some Spark code!</p>
<hr />
<hr />
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="login-to-databricks"><a class="header" href="#login-to-databricks">Login to databricks</a></h1>
<p>We will use databricks community edition and later on the databricks project shard granted for this course under the <strong>databricks university alliance</strong> with cloud computing grants from databricks for waived DBU units and AWS.</p>
<p>Please go here for a relaxed and detailed-enough tour (later):</p>
<ul>
<li><a href="https://docs.databricks.com/index.html">https://docs.databricks.com/index.html</a></li>
</ul>
</div>
<div class="cell markdown">
<h2 id="databricks-community-edition"><a class="header" href="#databricks-community-edition">databricks community edition</a></h2>
<p>First obtain a free Obtain a databricks community edition account at <a href="https://community.cloud.databricks.com">https://community.cloud.databricks.com</a> by following these instructions: <a href="https://youtu.be/FH2KDhaFkZg">https://youtu.be/FH2KDhaFkZg</a>.</p>
<p>Let's get an overview of the databricks managed cloud for processing big data with Apache Spark.</p>
</div>
<div class="cell markdown">
<h2 id="dbc-essentials-what-is-databricks-cloud"><a class="header" href="#dbc-essentials-what-is-databricks-cloud">DBC Essentials: What is Databricks Cloud?</a></h2>
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/week1/dbTrImg_WorkspaceSparkPlatform700x.png" alt="DB workspace, spark, platform" /></p>
</div>
<div class="cell markdown">
<h2 id="dbc-essentials-shard-cluster-notebook-and-dashboard"><a class="header" href="#dbc-essentials-shard-cluster-notebook-and-dashboard">DBC Essentials: Shard, Cluster, Notebook and Dashboard</a></h2>
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/week1/dbTrImg_ShardClusterNotebookDashboard700x.png" alt="DB workspace, spark, platform" /></p>
</div>
<div class="cell markdown">
<p><strong>DBC Essentials: Team, State, Collaboration, Elastic Resources in one picture</strong></p>
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/week1/dbTrImg_TeamStateCollaborationElasticResources700x.png" alt="DB workspace, spark, platform" /></p>
</div>
<div class="cell markdown">
<p><strong>You Should All Have databricks community edition account by now!</strong> and have successfully logged in to it.</p>
</div>
<div class="cell markdown">
<h1 id="import-course-content-now"><a class="header" href="#import-course-content-now">Import Course Content Now!</a></h1>
<p>Two Steps:</p>
<ol>
<li>Create a folder named <code>scalable-data-science</code> in your <code>Workspace</code> (NO Typos due to hard-coding of paths in the sequel!)</li>
</ol>
<ul>
<li>Import the following <code>.dbc</code> archives from the following URL into <code>Workspace/scalable-data-science</code> folder you just created:
<ul>
<li><a href="https://github.com/lamastex/scalable-data-science/raw/master/dbcArchives/2021/">https://github.com/lamastex/scalable-data-science/raw/master/dbcArchives/2021/</a></li>
<li>start with the first file for now and import more as needed:
<ul>
<li><a href="https://github.com/lamastex/scalable-data-science/tree/master/dbcArchives/2021/000_1-sds-3-x">https://github.com/lamastex/scalable-data-science/tree/master/dbcArchives/2021/000_1-sds-3-x</a></li>
<li>...</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<h1 id="cloud-free-computing-environment"><a class="header" href="#cloud-free-computing-environment">Cloud-free Computing Environment</a></h1>
<h2 id="download-spark"><a class="header" href="#download-spark">download Spark</a></h2>
<p>Download Apache Spark from <a href="https://spark.apache.org/downloads.html">https://spark.apache.org/downloads.html</a> and prepare your laptop to run <code>spark-shell</code>. Note you may also have to install Java 8 or 11 in your system if you do not have it already and set your <code>JAVA_HOME</code> environment variable.</p>
<h2 id="docker"><a class="header" href="#docker">docker</a></h2>
<p>You can alternatively locally install and learn <a href="https://www.docker.com/">docker</a>; <a href="https://docs.docker.com/get-started/">https://docs.docker.com/get-started/</a>, first and then run Spark inside docker containers by following examples here:</p>
<ul>
<li><a href="https://github.com/lamastex/dockerDev">https://github.com/lamastex/dockerDev</a>.</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<p>Please go here for a relaxed and detailed-enough tour (later):</p>
<ul>
<li><a href="https://docs.databricks.com/index.html">https://docs.databricks.com/index.html</a></li>
</ul>
</div>
<div class="cell markdown">
<h1 id="multi-lingual-notebooks"><a class="header" href="#multi-lingual-notebooks">Multi-lingual Notebooks</a></h1>
<p>Write Spark code for processing your data in notebooks.</p>
<p>Note that there are several open-sourced notebook servers including:</p>
<ul>
<li><a href="https://zeppelin.apache.org/">zeppelin</a></li>
<li><a href="https://jupyter.org/">jupyter</a> and its variants</li>
<li>etc</li>
</ul>
<p>Here, we are mainly focused on using databricks notebooks due to its effeciently managed engineering layers over AWS (or Azure public clouds).</p>
<p><strong>NOTE</strong>: You should have already cloned this notebook and attached it to a cluster that you started in the Community Edition of databricks by now.</p>
<h2 id="databricks-notebook"><a class="header" href="#databricks-notebook">Databricks Notebook</a></h2>
<p>Next we delve into the mechanics of working with databricks notebooks. But many of the details also apply to other notebook environments with minor differences.</p>
</div>
<div class="cell markdown">
<h3 id="notebooks-can-be-written-in-python-scala-r-or-sql"><a class="header" href="#notebooks-can-be-written-in-python-scala-r-or-sql">Notebooks can be written in <strong>Python</strong>, <strong>Scala</strong>, <strong>R</strong>, or <strong>SQL</strong>.</a></h3>
<ul>
<li>This is a Scala notebook - which is indicated next to the title above by <code>(Scala)</code>.</li>
<li>One can choose the default language of the notebook when it is created.</li>
</ul>
</div>
<div class="cell markdown">
<h3 id="creating-a-new-notebook"><a class="header" href="#creating-a-new-notebook"><strong>Creating a new Notebook</strong></a></h3>
<p><img src="http://training.databricks.com/databricks_guide/Notebook/createNotebook.png" alt="Change Name" /></p>
<ul>
<li>Click the tiangle on the right side of a folder to open the folder menu.</li>
<li>Select <strong>Create &gt; Notebook</strong>.</li>
<li>Enter the name of the notebook, the language (Python, Scala, R or SQL) for the notebook, and a cluster to run it on.</li>
</ul>
</div>
<div class="cell markdown">
<h3 id="cloning-a-notebook"><a class="header" href="#cloning-a-notebook">Cloning a Notebook</a></h3>
<ul>
<li>You can clone a notebook to create a copy of it, for example if you want to edit or run an Example notebook like this one.</li>
<li>Click <strong>File &gt; Clone</strong> in the notebook context bar above.</li>
<li>Enter a new name and location for your notebook. If Access Control is enabled, you can only clone to folders that you have Manage permissions on.</li>
</ul>
</div>
<div class="cell markdown">
<h3 id="clone-or-import-this-notebook"><a class="header" href="#clone-or-import-this-notebook">Clone Or Import This Notebook</a></h3>
<ul>
<li>From the <strong>File</strong> menu at the top left of this notebook, choose <strong>Clone</strong> or click <strong>Import Notebook</strong> on the top right. This will allow you to interactively execute code cells as you proceed through the notebook.</li>
</ul>
<p><img src="http://training.databricks.com/databricks_guide/2.8/clone.png" alt="Menu Bar Clone Notebook" /> * Enter a name and a desired location for your cloned notebook (i.e. Perhaps clone to your own user directory or the &quot;Shared&quot; directory.) * Navigate to the location you selected (e.g. click Menu &gt; Workspace &gt; <code>Your cloned location</code>)</p>
</div>
<div class="cell markdown">
<h3 id="attach-the-notebook-to-a-cluster"><a class="header" href="#attach-the-notebook-to-a-cluster"><strong>Attach</strong> the Notebook to a <strong>cluster</strong></a></h3>
<ul>
<li>A <strong>Cluster</strong> is a group of machines which can run commands in cells.</li>
<li>Check the upper left corner of your notebook to see if it is <strong>Attached</strong> or <strong>Detached</strong>.</li>
<li>If <strong>Detached</strong>, click on the right arrow and select a cluster to attach your notebook to.
<ul>
<li>If there is no running cluster, create one as described in the <a href="contents/000_1-sds-3-x-spark//#workspace/databricks_guide/00%20Welcome%20to%20Databricks">Welcome to Databricks</a> guide.</li>
</ul>
</li>
</ul>
<p><img src="http://training.databricks.com/databricks_guide/2.8/detached.png" alt="Attach Notebook" /></p>
<h3 id="deep-dive-into-databricks-notebooks"><a class="header" href="#deep-dive-into-databricks-notebooks">Deep-dive into databricks notebooks</a></h3>
<p>Let's take a deeper dive into a databricks notebook next.</p>
</div>
<div class="cell markdown">
<hr />
<h4 id="cells-are-units-that-make-up-notebooks"><a class="header" href="#cells-are-units-that-make-up-notebooks"><img src="http://training.databricks.com/databricks_guide/icon_note3_s.png" alt="Quick Note" /> <strong>Cells</strong> are units that make up notebooks</a></h4>
<p><img src="http://training.databricks.com/databricks_guide/cell.png" alt="A Cell" /></p>
<p>Cells each have a type - including <strong>scala</strong>, <strong>python</strong>, <strong>sql</strong>, <strong>R</strong>, <strong>markdown</strong>, <strong>filesystem</strong>, and <strong>shell</strong>.</p>
<ul>
<li>While cells default to the type of the Notebook, other cell types are supported as well.</li>
<li>This cell is in <strong>markdown</strong> and is used for documentation. <a href="http://en.wikipedia.org/wiki/Markdown">Markdown</a> is a simple text formatting syntax.</li>
</ul>
<hr />
</div>
<div class="cell markdown">
<hr />
<h4 id="create-and-edit-a-new-markdown-cell-in-this-notebook"><a class="header" href="#create-and-edit-a-new-markdown-cell-in-this-notebook"><strong>Create</strong> and <strong>Edit</strong> a New Markdown Cell in this Notebook</a></h4>
<ul>
<li>When you mouse between cells, a + sign will pop up in the center that you can click on to create a new cell.</li>
</ul>
<p><img src="http://training.databricks.com/databricks_guide/create_new_cell.png" alt="New Cell" /> * Type <strong><code>%md Hello, world!</code></strong> into your new cell (<strong><code>%md</code></strong> indicates the cell is markdown).</p>
<ul>
<li>
<p>Click out of the cell to see the cell contents update.</p>
<p><img src="http://training.databricks.com/databricks_guide/run_cell.png" alt="Run cell" /></p>
</li>
</ul>
<hr />
</div>
<div class="cell markdown">
<p>Hello, world!</p>
</div>
<div class="cell markdown">
<h4 id="running-a-cell-in-your-notebook"><a class="header" href="#running-a-cell-in-your-notebook"><strong>Running a cell in your notebook.</strong></a></h4>
<ul>
<li>
<h4 id="press-shiftenter-when-in-the-cell-to-run-it-and-proceed-to-the-next-cell"><a class="header" href="#press-shiftenter-when-in-the-cell-to-run-it-and-proceed-to-the-next-cell">Press <strong>Shift+Enter</strong> when in the cell to <strong>run</strong> it and proceed to the next cell.</a></h4>
<ul>
<li>The cells contents should update. <img src="http://training.databricks.com/databricks_guide/run_cell.png" alt="Run cell" /></li>
</ul>
</li>
<li>
<p><strong>NOTE:</strong> Cells are not automatically run each time you open it.</p>
<ul>
<li>Instead, Previous results from running a cell are saved and displayed.</li>
</ul>
</li>
<li>
<h4 id="alternately-press-ctrlenter-when-in-a-cell-to-run-it-but-not-proceed-to-the-next-cell"><a class="header" href="#alternately-press-ctrlenter-when-in-a-cell-to-run-it-but-not-proceed-to-the-next-cell">Alternately, press <strong>Ctrl+Enter</strong> when in a cell to <strong>run</strong> it, but not proceed to the next cell.</a></h4>
</li>
</ul>
</div>
<div class="cell markdown">
<p><strong>You Try Now!</strong> Just double-click the cell below, modify the text following <code>%md</code> and press <strong>Ctrl+Enter</strong> to evaluate it and see it's mark-down'd output.</p>
<pre><code>&gt; %md Hello, world!
</code></pre>
</div>
<div class="cell markdown">
<p>Hello, world!</p>
</div>
<div class="cell markdown">
<hr />
<h4 id="markdown-cell-tips"><a class="header" href="#markdown-cell-tips"><img src="http://training.databricks.com/databricks_guide/icon_note3_s.png" alt="Quick Note" /> <strong>Markdown Cell Tips</strong></a></h4>
<ul>
<li>To change a non-markdown cell to markdown, add <strong>%md</strong> to very start of the cell.</li>
<li>After updating the contents of a markdown cell, click out of the cell to update the formatted contents of a markdown cell.</li>
<li>To edit an existing markdown cell, <strong>doubleclick</strong> the cell.</li>
</ul>
<p>Learn more about markdown:</p>
<ul>
<li><a href="https://guides.github.com/features/mastering-markdown/">https://guides.github.com/features/mastering-markdown/</a></li>
</ul>
<p>Note that there are flavours or minor variants and enhancements of markdown, including those specific to databricks, github, <a href="https://pandoc.org/MANUAL.html">pandoc</a>, etc.</p>
<p>It will be future-proof to remain in the syntactic zone of <em>pure markdown</em> (at the intersection of various flavours) as much as possible and go with <a href="https://pandoc.org/MANUAL.html">pandoc</a>-compatible style if choices are necessary. ***</p>
</div>
<div class="cell markdown">
<hr />
<h4 id="run-a-scala-cell"><a class="header" href="#run-a-scala-cell">Run a <strong>Scala Cell</strong></a></h4>
<ul>
<li>Run the following scala cell.</li>
<li>Note: There is no need for any special indicator (such as <code>%md</code>) necessary to create a Scala cell in a Scala notebook.</li>
<li>You know it is a scala notebook because of the <code>(Scala)</code> appended to the name of this notebook.</li>
<li>Make sure the cell contents updates before moving on.</li>
<li>Press <strong>Shift+Enter</strong> when in the cell to run it and proceed to the next cell.
<ul>
<li>The cells contents should update.</li>
<li>Alternately, press <strong>Ctrl+Enter</strong> when in a cell to <strong>run</strong> it, but not proceed to the next cell.</li>
</ul>
</li>
<li>characters following <code>//</code> are comments in scala. ***</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">1+1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res0: Int = 2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(System.currentTimeMillis) // press Ctrl+Enter to evaluate println that prints its argument as a line
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1647878280414
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">1+1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res2: Int = 2
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="spark-is-written-in-scala-but-"><a class="header" href="#spark-is-written-in-scala-but-">Spark is written in Scala, but ...</a></h3>
<p>For this reason Scala will be the primary language for this course is Scala.</p>
<p><strong>However, let us use the best language for the job!</strong> as each cell can be written in a specific language in the same notebook. Such multi-lingual notebooks are the norm in any realistic data science process today!</p>
<p>The beginning of each cells has a language type if it is not the default language of the notebook. Such cell-specific language types include the following with the prefix <code>%</code>:</p>
<ul>
<li>
<p><code>%scala</code> for <strong>Scala</strong>,</p>
</li>
<li>
<p><code>%py</code> for <strong>Python</strong>,</p>
</li>
<li>
<p><code>%r</code> for <strong>R</strong>,</p>
</li>
<li>
<p><code>%sql</code> for <strong>SQL</strong>,</p>
</li>
<li>
<p><code>%fs</code> for databricks' <strong>filesystem</strong>,</p>
</li>
<li>
<p><code>%sh</code> for <strong>BASH SHELL</strong> and</p>
</li>
<li>
<p><code>%md</code> for <strong>markdown</strong>.</p>
</li>
<li>
<p>While cells default to the language type of the Notebook (scala, python, r or sql), other cell types are supported as well in a cell-specific manner.</p>
</li>
<li>
<p>For example, Python Notebooks can contain python, sql, markdown, and even scala cells. This lets you write notebooks that do use multiple languages.</p>
</li>
<li>
<p>This cell is in <strong>markdown</strong> as it begins with <code>%md</code>and is used for documentation purposes.</p>
</li>
</ul>
</div>
<div class="cell markdown">
<p>Thus, <strong>all language-typed cells can be created in any notebook</strong>, regardless of the the default language of the notebook itself.</p>
</div>
<div class="cell markdown">
<p>Cross-language cells can be used to mix commands from other languages.</p>
<p><strong>Examples:</strong></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">print(&quot;For example, this is a scala notebook, but we can use %py to run python commands inline.&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-r">print(&quot;We can also access other languages such as R.&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// you can be explicit about the language even if the notebook's default language is the same
println(&quot;We can access Scala like this.&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>We can access Scala like this.
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Command line cells can be used to work with local files on the Spark driver node. * Start a cell with <code>%sh</code> to run a command line command</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh"># This is a command line cell. Commands you write here will be executed as if they were run on the command line.
# For example, in this cell we access the help pages for the bash shell.
ls
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">whoami
</code></pre>
</div>
<div class="cell markdown">
<p>Filesystem cells allow access to the Databricks File System (DBFS).</p>
<ul>
<li>Start a cell with <code>%fs</code> to run DBFS commands</li>
<li>Type <code>%fs help</code> for a list of commands</li>
</ul>
</div>
<div class="cell markdown">
<h4 id="notebooks-can-be-run-from-other-notebooks-using-run"><a class="header" href="#notebooks-can-be-run-from-other-notebooks-using-run">Notebooks can be run from other notebooks using <strong>%run</strong></a></h4>
<ul>
<li>Syntax: <code>%run /full/path/to/notebook</code></li>
<li>This is commonly used to import functions you defined in other notebooks.</li>
</ul>
</div>
<div class="cell markdown">
<h2 id="further-pointers"><a class="header" href="#further-pointers">Further Pointers</a></h2>
<p>Here are some useful links to bookmark as you will need to use them for Reference.</p>
<p>These links provide a relaxed and detailed-enough tour (that you are strongly encouraged to take later):</p>
<ul>
<li>databricks
<ul>
<li><a href="https://docs.databricks.com/index.html">https://docs.databricks.com/index.html</a></li>
</ul>
</li>
<li>scala
<ul>
<li><a href="https://docs.scala-lang.org/">https://docs.scala-lang.org/</a></li>
</ul>
</li>
</ul>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="scala-crash-course"><a class="header" href="#scala-crash-course">Scala Crash Course</a></h1>
<p>Here we take a minimalist approach to learning just enough Scala, the language that Apache Spark is written in, to be able to use Spark effectively.</p>
<p>In the sequel we can learn more Scala concepts as they arise. This learning can be done by chasing the pointers in this crash course for a detailed deeper dive on your own time.</p>
<p>There are two basic ways in which we can learn Scala:</p>
<p><strong>1. Learn Scala in a notebook environment</strong></p>
<p>For convenience we use databricks Scala notebooks like this one here.</p>
<p>You can learn Scala locally on your own computer using Scala REPL (and Spark using Spark-Shell).</p>
<p><strong>2. Learn Scala in your own computer</strong></p>
<p>The most easy way to get Scala locally is through sbt, the Scala Build Tool. You can also use an IDE that integrates sbt.</p>
<p>See: <a href="https://docs.scala-lang.org/getting-started/index.html">https://docs.scala-lang.org/getting-started/index.html</a> to set up Scala in your own computer.</p>
<p><strong>Software Engineering NOTE:</strong> You can use <code>docker pull lamastex/dockerdev:spark3x</code> and use the docker container for local development based on minimal pointers here: <a href="https://github.com/lamastex/dockerDev">https://github.com/lamastex/dockerDev</a>. Being able to modularise and reuse codes modularly requires the building of libraries which inturn requires building them locally with tools such as <code>sbt</code> or <code>mvn</code> for instance. You can also set-up moder IDEs like <a href="https://code.visualstudio.com/">https://code.visualstudio.com/</a> or <a href="https://www.jetbrains.com/idea/">https://www.jetbrains.com/idea/</a>, etc. for this.</p>
<h2 id="scala-resources"><a class="header" href="#scala-resources">Scala Resources</a></h2>
<p>You will not be learning scala systematically and thoroughly in this course. You will learn <em>to use</em> Scala by doing various Spark jobs.</p>
<p>If you are interested in learning scala properly, then there are various resources, including:</p>
<ul>
<li><a href="http://www.scala-lang.org/">scala-lang.org</a> is the <strong>core Scala resource</strong>. Bookmark the following three links:
<ul>
<li><a href="https://docs.scala-lang.org/tour/tour-of-scala.html">tour-of-scala</a> - Bite-sized introductions to core language features.
<ul>
<li>we will go through the tour in a hurry now as some Scala familiarity is needed immediately.</li>
</ul>
</li>
<li><a href="https://docs.scala-lang.org/overviews/scala-book/introduction.html">scala-book</a> - An online book introducing the main language features
<ul>
<li>you are expected to use this resource to figure out Scala as needed.</li>
</ul>
</li>
<li><a href="https://docs.scala-lang.org/cheatsheets/index.html">scala-cheatsheet</a> - A handy cheatsheet covering the basics of Scala syntax.</li>
<li><a href="https://superruzafa.github.io/visual-scala-reference/">visual-scala-reference</a> - This guide collects some of the most common functions of the Scala Programming Language and explain them conceptual and graphically in a simple way.</li>
</ul>
</li>
<li><a href="https://docs.scala-lang.org/learn.html">Online Resources</a>, including:
<ul>
<li><a href="https://www.coursera.org/course/progfun">courseera: Functional Programming Principles in Scala</a></li>
</ul>
</li>
<li><a href="http://www.scala-lang.org/documentation/books.html">Books</a>
<ul>
<li><a href="http://www.artima.com/pins1ed/">Programming in Scala, 1st Edition, Free Online Reading</a></li>
</ul>
</li>
</ul>
<p>The main sources for the following content are (you are encouraged to read them for more background):</p>
<ul>
<li><a href="https://www.scala-lang.org/old/sites/default/files/linuxsoft_archives/docu/files/ScalaByExample.pdf">Martin Oderski's Scala by example</a></li>
<li><a href="http://lintool.github.io/SparkTutorial/slides/day1_Scala_crash_course.pdf">Scala crash course by Holden Karau</a></li>
<li><a href="https://darrenjw.wordpress.com/2013/12/30/brief-introduction-to-scala-and-breeze-for-statistical-computing/">Darren's brief introduction to scala and breeze for statistical computing</a></li>
</ul>
<h2 id="what-is-scala"><a class="header" href="#what-is-scala">What is Scala?</a></h2>
<p>&quot;Scala smoothly integrates object-oriented and functional programming. It is designed to express common programming patterns in a concise, elegant, and type-safe way.&quot; by Matrin Odersky.</p>
<ul>
<li>High-level language for the Java Virtual Machine (JVM)</li>
<li>Object oriented + functional programming</li>
<li>Statically typed</li>
<li>Comparable in speed to Java</li>
<li>Type inference saves us from having to write explicit types most of the time Interoperates with Java</li>
<li>Can use any Java class (inherit from, etc.)</li>
<li>Can be called from Java code</li>
</ul>
<p>See a quick tour here:</p>
<ul>
<li><a href="https://docs.scala-lang.org/tour/tour-of-scala.html">https://docs.scala-lang.org/tour/tour-of-scala.html</a></li>
</ul>
<h2 id="why-scala"><a class="header" href="#why-scala">Why Scala?</a></h2>
<ul>
<li>Spark was originally written in Scala, which allows concise function syntax and interactive use</li>
<li>Spark APIs for other languages include:
<ul>
<li>Java API for standalone use</li>
<li>Python API added to reach a wider user community of programmes</li>
<li>R API added more recently to reach a wider community of data analyststs</li>
<li>Unfortunately, Python and R APIs are generally behind Spark's native Scala (for eg. GraphX is only available in Scala currently and datasets are only available in Scala as of 20200918).</li>
</ul>
</li>
<li>See Darren Wilkinson's 11 reasons for <a href="https://darrenjw.wordpress.com/2013/12/23/scala-as-a-platform-for-statistical-computing-and-data-science/">scala as a platform for statistical computing and data science</a>. It is embedded in-place below for your convenience.</li>
</ul>
</div>
<div class="cell markdown">
<h2 id="scala-versus-python-for-apache-spark"><a class="header" href="#scala-versus-python-for-apache-spark">Scala Versus Python for Apache Spark</a></h2>
<p>Read: <a href="https://www.projectpro.io/article/scala-vs-python-for-apache-spark/213">https://www.projectpro.io/article/scala-vs-python-for-apache-spark/213</a> for an interesting overview.</p>
</div>
<div class="cell markdown">
<h2 id="learn-scala-in-notebook-environment"><a class="header" href="#learn-scala-in-notebook-environment">Learn Scala in Notebook Environment</a></h2>
<hr />
<h3 id="run-a-scala-cell-1"><a class="header" href="#run-a-scala-cell-1">Run a <strong>Scala Cell</strong></a></h3>
<ul>
<li>Run the following scala cell.</li>
<li>Note: There is no need for any special indicator (such as <code>%md</code>) necessary to create a Scala cell in a Scala notebook.</li>
<li>You know it is a scala notebook because of the <code>(Scala)</code> appended to the name of this notebook.</li>
<li>Make sure the cell contents updates before moving on.</li>
<li>Press <strong>Shift+Enter</strong> when in the cell to run it and proceed to the next cell.
<ul>
<li>The cells contents should update.</li>
<li>Alternately, press <strong>Ctrl+Enter</strong> when in a cell to <strong>run</strong> it, but not proceed to the next cell.</li>
</ul>
</li>
<li>characters following <code>//</code> are comments in scala. ***</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">1+1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res0: Int = 2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(System.currentTimeMillis) // press Ctrl+Enter to evaluate println that prints its argument as a line
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1648132310663
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//%run &quot;/scalable-data-science/xtraResources/support/sdsFunctions&quot;
//This allows easy embedding of publicly available information into any other notebook
//when viewing in git-book just ignore this block - you may have to manually chase the URL in frameIt(&quot;URL&quot;).
//Example usage:
// displayHTML(frameIt(&quot;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Topics_in_LDA&quot;,250))
def frameIt( u:String, h:Int ) : String = {
      &quot;&quot;&quot;&lt;iframe 
 src=&quot;&quot;&quot;&quot;+ u+&quot;&quot;&quot;&quot;
 width=&quot;95%&quot; height=&quot;&quot;&quot;&quot; + h + &quot;&quot;&quot;&quot;
 sandbox&gt;
  &lt;p&gt;
    &lt;a href=&quot;http://spark.apache.org/docs/latest/index.html&quot;&gt;
      Fallback link for browsers that, unlikely, don't support frames
    &lt;/a&gt;
  &lt;/p&gt;
&lt;/iframe&gt;&quot;&quot;&quot;
   }
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>frameIt: (u: String, h: Int)String
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>See <a href="https://darrenjw.wordpress.com/2013/12/23/scala-as-a-platform-for-statistical-computing-and-data-science/">Scala as a platform for statistical computing and data science</a>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://darrenjw.wordpress.com/2013/12/23/scala-as-a-platform-for-statistical-computing-and-data-science/&quot;,500))
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://darrenjw.wordpress.com/2013/12/23/scala-as-a-platform-for-statistical-computing-and-data-science/"
 width="95%" height="500"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<h2 id="lets-get-our-hands-dirty-in-scala"><a class="header" href="#lets-get-our-hands-dirty-in-scala">Let's get our hands dirty in Scala</a></h2>
<p>We will go through the <strong>following</strong> programming concepts and tasks by building on <a href="https://docs.scala-lang.org/tour/basics.html">https://docs.scala-lang.org/tour/basics.html</a>.</p>
<ul>
<li><strong>Scala Types</strong></li>
<li><strong>Expressions and Printing</strong></li>
<li><strong>Naming and Assignments</strong></li>
<li><strong>Functions and Methods in Scala</strong></li>
<li><strong>Classes and Case Classes</strong></li>
<li><strong>Methods and Tab-completion</strong></li>
<li><strong>Objects and Traits</strong></li>
<li>Collections in Scala and Type Hierarchy</li>
<li>Functional Programming and MapReduce</li>
<li>Lazy Evaluations and Recursions</li>
</ul>
<p><strong>Remark</strong>: You need to take a computer science course (from CourseEra, for example) to properly learn Scala. Here, we will learn to use Scala by example to accomplish our data science tasks at hand. You can learn more Scala as needed from various sources pointed out above in <strong>Scala Resources</strong>.</p>
</div>
<div class="cell markdown">
<h3 id="scala-types"><a class="header" href="#scala-types">Scala Types</a></h3>
<p>In Scala, all values have a type, including numerical values and functions. The diagram below illustrates a subset of the type hierarchy.</p>
<p><img src="https://docs.scala-lang.org/resources/images/tour/unified-types-diagram.svg" alt="" /></p>
<p>For now, notice some common types we will be usinf including <code>Int</code>, <code>String</code>, <code>Double</code>, <code>Unit</code>, <code>Boolean</code>, <code>List</code>, etc. For more details see <a href="https://docs.scala-lang.org/tour/unified-types.html">https://docs.scala-lang.org/tour/unified-types.html</a>. We will return to this at the end of the notebook after seeing a brief tour of Scala now.</p>
</div>
<div class="cell markdown">
<h3 id="expressions"><a class="header" href="#expressions">Expressions</a></h3>
<p>Expressions are computable statements such as the <code>1+1</code> we have seen before.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">1+1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res3: Int = 2
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>We can print the output of a computed or evaluated expressions as a line using <code>println</code>:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(1+1) // printing 2
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(&quot;hej hej!&quot;) // printing a string
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>hej hej!
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="naming-and-assignments"><a class="header" href="#naming-and-assignments">Naming and Assignments</a></h3>
<p><strong>value and variable as <code>val</code> and <code>var</code></strong></p>
<p>You can name the results of expressions using keywords <code>val</code> and <code>var</code>.</p>
<p>Let us assign the integer value <code>5</code> to <code>x</code> as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x : Int = 5 // &lt;Ctrl+Enter&gt; to declare a value x to be integer 5. 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: Int = 5
</code></pre>
</div>
</div>
<div class="cell markdown">
<p><code>x</code> is a named result and it is a value since we used the keyword <code>val</code> when naming it.</p>
</div>
<div class="cell markdown">
<p>Scala is statically typed, but it uses built-in type inference machinery to automatically figure out that <code>x</code> is an integer or <code>Int</code> type as follows. Let's declare a value <code>x</code> to be <code>Int</code> 5 next without explictly using <code>Int</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = 5    // &lt;Ctrl+Enter&gt; to declare a value x as Int 5 (type automatically inferred)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: Int = 5
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let's declare <code>x</code> as a <code>Double</code> or double-precision floating-point type using decimal such as <code>5.0</code> (a digit has to follow the decimal point!)</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = 5.0   // &lt;Ctrl+Enter&gt; to declare a value x as Double 5
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: Double = 5.0
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Alternatively, we can assign <code>x</code> as a <code>Double</code> explicitly. Note that the decimal point is not needed in this case due to explicit typing as <code>Double</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x :  Double = 5    // &lt;Ctrl+Enter&gt; to declare a value x as Double 5 (type automatically inferred)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: Double = 5.0
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Next note that labels need to be declared on first use. We have declared <code>x</code> to be a <code>val</code> which is short for <em>value</em>. This makes <code>x</code> immutable (cannot be changed).</p>
<p>Thus, <code>x</code> cannot be just re-assigned, as the following code illustrates in the resulting error: <code>... error: reassignment to val</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//x = 10    //  uncomment and &lt;Ctrl+Enter&gt; to try to reassign val x to 10
</code></pre>
</div>
<div class="cell markdown">
<p>Scala allows declaration of mutable variables as well using <code>var</code>, as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">var y = 2    // &lt;Shift+Enter&gt; to declare a variable y to be integer 2 and go to next cell
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>y: Int = 2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">y = 3    // &lt;Shift+Enter&gt; to change the value of y to 3
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>y: Int = 3
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">y = y+1 // adds 1 to y
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>y: Int = 4
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">y += 2 // adds 2 to y
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(y) // the var y is 6 now
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>6
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="blocks"><a class="header" href="#blocks">Blocks</a></h3>
<p>Just combine expressions by surrounding them with <code>{</code> and <code>}</code> called a block.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println({
  val x = 1+1
  x+2 // expression in last line is returned for the block
})// prints 4
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>4
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println({ val x=22; x+2})
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>24
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="functions"><a class="header" href="#functions">Functions</a></h3>
<p>Functions are expressions that have parameters. A function takes arguments as input and returns expressions as output.</p>
<p>A function can be nameless or <em>anonymous</em> and simply return an output from a given input. For example, the following annymous function returns the square of the input integer.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">(x: Int) =&gt; x*x
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res11: Int =&gt; Int = $Lambda$4875/223024816@e360af5
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>On the left of <code>=&gt;</code> is a list of parameters with name and type. On the right is an expression involving the parameters.</p>
</div>
<div class="cell markdown">
<p>You can also name functions:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val multiplyByItself = (x: Int) =&gt; x*x
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>multiplyByItself: Int =&gt; Int = $Lambda$4876/785598721@598b0d0
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(multiplyByItself(10))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>100
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>A function can have no parameters:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val howManyAmI = () =&gt; 1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>howManyAmI: () =&gt; Int = $Lambda$4879/491587432@ee9b630
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(howManyAmI()) // 1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>A function can have more than one parameter:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val multiplyTheseTwoIntegers = (a: Int, b: Int) =&gt; a*b
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>multiplyTheseTwoIntegers: (Int, Int) =&gt; Int = $Lambda$4880/1755560048@781f5a73
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(multiplyTheseTwoIntegers(2,4)) // 8
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>8
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="methods"><a class="header" href="#methods">Methods</a></h3>
<p>Methods are very similar to functions, but a few key differences exist.</p>
<p>Methods use the <code>def</code> keyword followed by a name, parameter list(s), a return type, and a body.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def square(x: Int): Int = x*x    // &lt;Shitf+Enter&gt; to define a function named square
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>square: (x: Int)Int
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Note that the return type <code>Int</code> is specified after the parameter list and a <code>:</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">square(5)    // &lt;Shitf+Enter&gt; to call this function on argument 5
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res15: Int = 25
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val y = 3    // &lt;Shitf+Enter&gt; make val y as Int 3
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>y: Int = 3
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">square(y) // &lt;Shitf+Enter&gt; to call the function on val y of the right argument type Int
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res16: Int = 9
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = 5.0     // let x be Double 5.0
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: Double = 5.0
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//square(x) // &lt;Shift+Enter&gt; to call the function on val x of type Double will give type mismatch error
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def square(x: Int): Int = { // &lt;Shitf+Enter&gt; to declare function in a block
  val answer = x*x
  answer // the last line of the function block is returned
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>square: (x: Int)Int
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">square(5000)    // &lt;Shift+Enter&gt; to call the function
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res19: Int = 25000000
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Shift+Enter&gt; to define function with input and output type as String
def announceAndEmit(text: String): String = 
{
  println(text)
  text // the last line of the function block is returned
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>announceAndEmit: (text: String)String
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Scala has a <code>return</code> keyword but it is rarely used as the expression in the last line of the multi-line block is the method's return value.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Ctrl+Enter&gt; to call function which prints as line and returns as String
announceAndEmit(&quot;roger  roger&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>roger  roger
res20: String = roger  roger
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>A method can have output expressions involving multiple parameter lists:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def multiplyAndTranslate(x: Int, y: Int)(translateBy: Int): Int = (x * y) + translateBy
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>multiplyAndTranslate: (x: Int, y: Int)(translateBy: Int)Int
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(multiplyAndTranslate(2, 3)(4))  // (2*3)+4 = 10
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>10
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>A method can have no parameter lists at all:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def time: Long = System.currentTimeMillis
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>time: Long
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(&quot;Current time in milliseconds is &quot; + time)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Current time in milliseconds is 1648133577033
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(&quot;Current time in milliseconds is &quot; + time)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Current time in milliseconds is 1647862244746
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="classes"><a class="header" href="#classes">Classes</a></h3>
<p>The <code>class</code> keyword followed by the name and constructor parameters is used to define a class.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">class Box(h: Int, w: Int, d: Int) {
  def printVolume(): Unit = println(h*w*d)
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined class Box
</code></pre>
</div>
</div>
<div class="cell markdown">
<ul>
<li>The return type of the method <code>printVolume</code> is <code>Unit</code>.</li>
<li>When the return type is <code>Unit</code> it indicates that there is nothing meaningful to return, similar to <code>void</code> in Java and C, but with a difference.</li>
<li>Because every Scala expression must have some value, there is actually a singleton value of type <code>Unit</code>, written <code>()</code> and carrying no information.</li>
</ul>
</div>
<div class="cell markdown">
<p>We can make an instance of the class with the <code>new</code> keyword.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val my1Cube = new Box(1,1,1)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>my1Cube: Box = Box@a2caab0
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>And call the method on the instance.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">my1Cube.printVolume() // 1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Our named instance <code>my1Cube</code> of the <code>Box</code> class is immutable due to <code>val</code>.</p>
<p>You can have mutable instances of the class using <code>var</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">var myVaryingCuboid = new Box(1,3,2)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>myVaryingCuboid: Box = Box@3dee8585
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">myVaryingCuboid.printVolume()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>6
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">myVaryingCuboid = new Box(1,1,1)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>myVaryingCuboid: Box = Box@35035270
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">myVaryingCuboid.printVolume()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>See <a href="https://docs.scala-lang.org/tour/classes.html">https://docs.scala-lang.org/tour/classes.html</a> for more details as needed.</p>
</div>
<div class="cell markdown">
<h3 id="case-classes"><a class="header" href="#case-classes">Case Classes</a></h3>
<p>Scala has a special type of class called a <em>case class</em> that can be defined with the <code>case class</code> keyword.</p>
<p>Unlike classes, whose instances are compared by reference, instances of case classes are immutable by default and compared by value. This makes them useful for defining rows of typed values in Spark.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">case class Point(x: Int, y: Int, z: Int)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined class Point
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Case classes can be instantiated without the <code>new</code> keyword.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val point = Point(1, 2, 3)
val anotherPoint = Point(1, 2, 3)
val yetAnotherPoint = Point(2, 2, 2)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>point: Point = Point(1,2,3)
anotherPoint: Point = Point(1,2,3)
yetAnotherPoint: Point = Point(2,2,2)
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Instances of case classes are compared by value and not by reference.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">if (point == anotherPoint) {
  println(point + &quot; and &quot; + anotherPoint + &quot; are the same.&quot;)
} else {
  println(point + &quot; and &quot; + anotherPoint + &quot; are different.&quot;)
} // Point(1,2,3) and Point(1,2,3) are the same.

if (point == yetAnotherPoint) {
  println(point + &quot; and &quot; + yetAnotherPoint + &quot; are the same.&quot;)
} else {
  println(point + &quot; and &quot; + yetAnotherPoint + &quot; are different.&quot;)
} // Point(1,2,3) and Point(2,2,2) are different.
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Point(1,2,3) and Point(1,2,3) are the same.
Point(1,2,3) and Point(2,2,2) are different.
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>By contrast, instances of classes are compared by reference.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">myVaryingCuboid.printVolume() // should be 1 x 1 x 1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">my1Cube.printVolume()  // should be 1 x 1 x 1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">if (myVaryingCuboid == my1Cube) {
  println(&quot;myVaryingCuboid and my1Cube are the same.&quot;)
} else {
  println(&quot;myVaryingCuboid and my1Cube are different.&quot;)
} // they are compared by reference and are not the same.
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>myVaryingCuboid and my1Cube are different.
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>More about case classes here: <a href="https://docs.scala-lang.org/tour/case-classes.html">https://docs.scala-lang.org/tour/case-classes.html</a>.</p>
</div>
<div class="cell markdown">
<h3 id="methods-and-tab-completion"><a class="header" href="#methods-and-tab-completion">Methods and Tab-completion</a></h3>
<p>Many methods of a class can be accessed by <code>.</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val s  = &quot;hi&quot;    // &lt;Ctrl+Enter&gt; to declare val s to String &quot;hi&quot;
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>s: String = hi
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>You can place the cursor after <code>.</code> following a declared object and find out the methods available for it as shown in the image below.</p>
<p><img src="https://github.com/raazesh-sainudiin/scalable-data-science/raw/master/images/week1/tabCompletionAfterSDot.png" alt="tabCompletionAfterSDot PNG image" /></p>
<p><strong>You Try</strong> doing this next.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//s.  // place cursor after the '.' and press Tab to see all available methods for s 
</code></pre>
</div>
<div class="cell markdown">
<p>For example,</p>
<ul>
<li>scroll down to <code>contains</code> and double-click on it.</li>
<li>This should lead to <code>s.contains</code> in your cell.</li>
<li>Now add an argument String to see if <code>s</code> contains the argument, for example, try:
<ul>
<li><code>s.contains(&quot;f&quot;)</code></li>
<li><code>s.contains(&quot;&quot;)</code> and</li>
<li><code>s.contains(&quot;i&quot;)</code></li>
</ul>
</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//s    // &lt;Shift-Enter&gt; recall the value of String s
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">s.contains(&quot;f&quot;)     // &lt;Shift-Enter&gt; returns Boolean false since s does not contain the string &quot;f&quot;
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res30: Boolean = false
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">s.contains(&quot;&quot;)    // &lt;Shift-Enter&gt; returns Boolean true since s contains the empty string &quot;&quot;
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res31: Boolean = true
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">s.contains(&quot;i&quot;)    // &lt;Ctrl+Enter&gt; returns Boolean true since s contains the string &quot;i&quot;
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res32: Boolean = true
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="objects"><a class="header" href="#objects">Objects</a></h3>
<p>Objects are single instances of their own definitions using the <code>object</code> keyword. You can think of them as singletons of their own classes.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">object IdGenerator {
  private var currentId = 0
  def make(): Int = {
    currentId += 1
    currentId
  }
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined object IdGenerator
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>You can access an object through its name:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val newId: Int = IdGenerator.make()
val newerId: Int = IdGenerator.make()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>newId: Int = 1
newerId: Int = 2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(newId) // 1
println(newerId) // 2
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1
2
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>For details see <a href="https://docs.scala-lang.org/tour/singleton-objects.html">https://docs.scala-lang.org/tour/singleton-objects.html</a></p>
</div>
<div class="cell markdown">
<h3 id="traits"><a class="header" href="#traits">Traits</a></h3>
<p>Traits are abstract data types containing certain fields and methods. They can be defined using the <code>trait</code> keyword.</p>
<p>In Scala inheritance, a class can only extend one other class, but it can extend multiple traits.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">trait Greeter {
  def greet(name: String): Unit
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined trait Greeter
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Traits can have default implementations also.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">trait Greeter {
  def greet(name: String): Unit =
    println(&quot;Hello, &quot; + name + &quot;!&quot;)
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined trait Greeter
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>You can extend traits with the <code>extends</code> keyword and override an implementation with the <code>override</code> keyword:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">class DefaultGreeter extends Greeter

class SwedishGreeter extends Greeter {
  override def greet(name: String): Unit = {
    println(&quot;Hej hej, &quot; + name + &quot;!&quot;)
  }
}

class CustomizableGreeter(prefix: String, postfix: String) extends Greeter {
  override def greet(name: String): Unit = {
    println(prefix + name + postfix)
  }
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined class DefaultGreeter
defined class SwedishGreeter
defined class CustomizableGreeter
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Instantiate the classes.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val greeter = new DefaultGreeter()
val swedishGreeter = new SwedishGreeter()
val customGreeter = new CustomizableGreeter(&quot;How are you, &quot;, &quot;?&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>greeter: DefaultGreeter = DefaultGreeter@22688208
swedishGreeter: SwedishGreeter = SwedishGreeter@3474f6c4
customGreeter: CustomizableGreeter = CustomizableGreeter@44acef6d
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Call the <code>greet</code> method in each case.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">greeter.greet(&quot;Scala developer&quot;) // Hello, Scala developer!
swedishGreeter.greet(&quot;Scala developer&quot;) // Hej hej, Scala developer!
customGreeter.greet(&quot;Scala developer&quot;) // How are you, Scala developer?
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Hello, Scala developer!
Hej hej, Scala developer!
How are you, Scala developer?
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>A class can also be made to extend multiple traits.</p>
<p>For more details see: <a href="https://docs.scala-lang.org/tour/traits.html">https://docs.scala-lang.org/tour/traits.html</a>.</p>
</div>
<div class="cell markdown">
<h3 id="main-method"><a class="header" href="#main-method">Main Method</a></h3>
<p>The main method is the entry point of a Scala program.</p>
<p>The Java Virtual Machine requires a main method, named <code>main</code>, that takes an array of strings as its only argument.</p>
<p>Using an object, you can define the main method as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">object Main {
  def main(args: Array[String]): Unit =
    println(&quot;Hello, Scala developer!&quot;)
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined object Main
</code></pre>
</div>
</div>
<div class="cell markdown">
<p><strong>What I try not do while learning a new language?</strong></p>
<ol>
<li>I don't immediately try to ask questions like: <em>how can I do this particular variation of some small thing I just learned so I can use patterns I am used to from another language I am hooked-on right now?</em></li>
<li>first go through the detailed Scala Tour on your own and then through the 50 odd lessons in the Scala Book</li>
<li>then return to 1. and ask detailed cross-language comparison questions by diving deep as needed with the source and scala docs as needed (google or duck-duck-go search!).</li>
</ol>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="scala-crash-course-continued"><a class="header" href="#scala-crash-course-continued">Scala Crash Course Continued</a></h1>
<p>Recall!</p>
<h2 id="scala-resources-1"><a class="header" href="#scala-resources-1">Scala Resources</a></h2>
<p>You will not be learning scala systematically and thoroughly in this course. You will learn <em>to use</em> Scala by doing various Spark jobs.</p>
<p>If you are interested in learning scala properly, then there are various resources, including:</p>
<ul>
<li><a href="http://www.scala-lang.org/">scala-lang.org</a> is the <strong>core Scala resource</strong>. Bookmark the following three links:
<ul>
<li><a href="https://docs.scala-lang.org/tour/tour-of-scala.html">tour-of-scala</a> - Bite-sized introductions to core language features.
<ul>
<li>we will go through the tour in a hurry now as some Scala familiarity is needed immediately.</li>
</ul>
</li>
<li><a href="https://docs.scala-lang.org/overviews/scala-book/introduction.html">scala-book</a> - An online book introducing the main language features
<ul>
<li>you are expected to use this resource to figure out Scala as needed.</li>
</ul>
</li>
<li><a href="https://docs.scala-lang.org/cheatsheets/index.html">scala-cheatsheet</a> - A handy cheatsheet covering the basics of Scala syntax.</li>
<li><a href="https://superruzafa.github.io/visual-scala-reference/">visual-scala-reference</a> - This guide collects some of the most common functions of the Scala Programming Language and explain them conceptual and graphically in a simple way.</li>
</ul>
</li>
<li><a href="https://docs.scala-lang.org/learn.html">Online Resources</a>, including:
<ul>
<li><a href="https://www.coursera.org/course/progfun">courseera: Functional Programming Principles in Scala</a></li>
</ul>
</li>
<li><a href="http://www.scala-lang.org/documentation/books.html">Books</a>
<ul>
<li><a href="http://www.artima.com/pins1ed/">Programming in Scala, 1st Edition, Free Online Reading</a></li>
</ul>
</li>
</ul>
<p>The main sources for the following content are (you are encouraged to read them for more background):</p>
<ul>
<li><a href="https://www.scala-lang.org/old/sites/default/files/linuxsoft_archives/docu/files/ScalaByExample.pdf">Martin Oderski's Scala by example</a></li>
<li><a href="http://lintool.github.io/SparkTutorial/slides/day1_Scala_crash_course.pdf">Scala crash course by Holden Karau</a></li>
<li><a href="https://darrenjw.wordpress.com/2013/12/30/brief-introduction-to-scala-and-breeze-for-statistical-computing/">Darren's brief introduction to scala and breeze for statistical computing</a></li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//%run &quot;/scalable-data-science/xtraResources/support/sdsFunctions&quot;
//This allows easy embedding of publicly available information into any other notebook
//when viewing in git-book just ignore this block - you may have to manually chase the URL in frameIt(&quot;URL&quot;).
//Example usage:
// displayHTML(frameIt(&quot;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Topics_in_LDA&quot;,250))
def frameIt( u:String, h:Int ) : String = {
      &quot;&quot;&quot;&lt;iframe 
 src=&quot;&quot;&quot;&quot;+ u+&quot;&quot;&quot;&quot;
 width=&quot;95%&quot; height=&quot;&quot;&quot;&quot; + h + &quot;&quot;&quot;&quot;
 sandbox&gt;
  &lt;p&gt;
    &lt;a href=&quot;http://spark.apache.org/docs/latest/index.html&quot;&gt;
      Fallback link for browsers that, unlikely, don't support frames
    &lt;/a&gt;
  &lt;/p&gt;
&lt;/iframe&gt;&quot;&quot;&quot;
   }
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>frameIt: (u: String, h: Int)String
</code></pre>
</div>
</div>
<div class="cell markdown">
<h2 id="lets-continue-getting-our-hands-dirty-in-scala"><a class="header" href="#lets-continue-getting-our-hands-dirty-in-scala">Let's continue getting our hands dirty in Scala</a></h2>
<p>We will go through the <strong>remaining</strong> programming concepts and tasks by building on <a href="https://docs.scala-lang.org/tour/basics.html">https://docs.scala-lang.org/tour/basics.html</a>.</p>
<ul>
<li>Scala Types</li>
<li>Expressions and Printing</li>
<li>Naming and Assignments</li>
<li>Functions and Methods in Scala</li>
<li>Classes and Case Classes</li>
<li>Methods and Tab-completion</li>
<li>Objects and Traits</li>
<li><strong>Collections in Scala and Type Hierarchy</strong></li>
<li><strong>Functional Programming and MapReduce</strong></li>
<li><strong>Lazy Evaluations and Recursions</strong></li>
</ul>
<p><strong>Remark</strong>: You need to take a computer science course (from CourseEra, for example) to properly learn Scala. Here, we will learn to use Scala by example to accomplish our data science tasks at hand. You can learn more Scala as needed from various sources pointed out above in <strong>Scala Resources</strong>.</p>
</div>
<div class="cell markdown">
<h3 id="scala-type-hierarchy"><a class="header" href="#scala-type-hierarchy">Scala Type Hierarchy</a></h3>
<p>In Scala, all values have a type, including numerical values and functions. The diagram below illustrates a subset of the type hierarchy.</p>
<p><img src="https://docs.scala-lang.org/resources/images/tour/unified-types-diagram.svg" alt="" /></p>
<p>For now, notice some common types we will be usinf including <code>Int</code>, <code>String</code>, <code>Double</code>, <code>Unit</code>, <code>Boolean</code>, <code>List</code>, etc. For more details see <a href="https://docs.scala-lang.org/tour/unified-types.html">https://docs.scala-lang.org/tour/unified-types.html</a>.</p>
<p>Let us take a closer look at Scala Type Hierarchy now.</p>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://docs.scala-lang.org/tour/unified-types.html&quot; , 550))
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://docs.scala-lang.org/tour/unified-types.html"
 width="95%" height="550"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<h3 id="scala-collections"><a class="header" href="#scala-collections">Scala Collections</a></h3>
<p>Familiarize yourself with the main Scala collections classes here:</p>
<ul>
<li><a href="https://docs.scala-lang.org/overviews/scala-book/collections-101.html">https://docs.scala-lang.org/overviews/scala-book/collections-101.html</a></li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://docs.scala-lang.org/overviews/scala-book/collections-101.html&quot; , 550))
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://docs.scala-lang.org/overviews/scala-book/collections-101.html"
 width="95%" height="550"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<h4 id="list"><a class="header" href="#list">List</a></h4>
<p>Lists are one of the most basic data structures.</p>
<p>There are several other Scala collections and we will introduce them as needed. The other most common ones are <code>Vector</code>, <code>Array</code> and <code>Seq</code> and the <code>ArrayBuffer</code>.</p>
<p>For details on list see: - <a href="https://docs.scala-lang.org/overviews/scala-book/list-class.html">https://docs.scala-lang.org/overviews/scala-book/list-class.html</a></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Ctrl+Enter&gt; to declare (an immutable) val lst as List of Int's 1,2,3
val lst = List(1, 2, 3)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>lst: List[Int] = List(1, 2, 3)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell markdown">
<h4 id="vectors"><a class="header" href="#vectors">Vectors</a></h4>
<blockquote>
<p>The Vector class is an indexed, immutable sequence. The “indexed” part of the description means that you can access Vector elements very rapidly by their index value, such as accessing listOfPeople(999999).</p>
</blockquote>
<p>In general, except for the difference that Vector is indexed and List is not, the two classes work the same, so we’ll run through these examples quickly.</p>
<p>For details see: - <a href="https://docs.scala-lang.org/overviews/scala-book/vector-class.html">https://docs.scala-lang.org/overviews/scala-book/vector-class.html</a></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val vec = Vector(1,2,3)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>vec: scala.collection.immutable.Vector[Int] = Vector(1, 2, 3)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">vec(0) // access first element with index 0
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res0: Int = 1
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="arrays-sequences-and-tuples"><a class="header" href="#arrays-sequences-and-tuples">Arrays, Sequences and Tuples</a></h4>
<p>See <a href="https://www.scala-lang.org/api/current/scala/collection/index.html">https://www.scala-lang.org/api/current/scala/collection/index.html</a> for docs.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val arr = Array(1,2,3) // &lt;Shift-Enter&gt; to declare an Array
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>arr: Array[Int] = Array(1, 2, 3)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val seq = Seq(1,2,3)    // &lt;Shift-Enter&gt; to declare a Seq
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>seq: Seq[Int] = List(1, 2, 3)
</code></pre>
</div>
</div>
<div class="cell markdown">
<blockquote>
<p>A tuple is a neat class that gives you a simple way to store heterogeneous (different) items in the same container. We will use tuples for key-value pairs in Spark.</p>
</blockquote>
<p>See <a href="https://docs.scala-lang.org/overviews/scala-book/tuples.html">https://docs.scala-lang.org/overviews/scala-book/tuples.html</a></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val myTuple = ('a',1) // a 2-tuple
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>myTuple: (Char, Int) = (a,1)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">myTuple._1 // accessing the first element of the tuple. NOTE index starts at 1 not 0 for tuples
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res1: Char = a
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">myTuple._2 // accessing the second element of the tuple
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res2: Int = 1
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="functional-programming-and-mapreduce"><a class="header" href="#functional-programming-and-mapreduce">Functional Programming and MapReduce</a></h3>
<p><em>&quot;Functional programming is a style of programming that emphasizes writing applications using only pure functions and immutable values. As Alvin Alexander wrote in Functional Programming, Simplified, rather than using that description, it can be helpful to say that functional programmers have an extremely strong desire to see their code as math — to see the combination of their functions as a series of algebraic equations. In that regard, you could say that functional programmers like to think of themselves as mathematicians. That’s the driving desire that leads them to use only pure functions and immutable values, because that’s what you use in algebra and other forms of math.&quot;</em></p>
<p>See <a href="https://docs.scala-lang.org/overviews/scala-book/functional-programming.html">https://docs.scala-lang.org/overviews/scala-book/functional-programming.html</a> for short lessons in functional programming.</p>
<p>We will apply functions for processing elements of a scala collection to quickly demonstrate functional programming.</p>
<h4 id="five-ways-of-adding-1"><a class="header" href="#five-ways-of-adding-1">Five ways of adding 1</a></h4>
<p>The first four use anonymous functions and the last one uses a named method.</p>
<ol>
<li>explicit version:</li>
</ol>
<pre><code class="language-%scala">(x: Int) =&gt; x + 1  
</code></pre>
<ol>
<li>type-inferred more intuitive version:</li>
</ol>
<pre><code class="language-%scala">x =&gt; x + 1   
</code></pre>
<ol>
<li>placeholder syntax (each argument must be used exactly once):</li>
</ol>
<pre><code class="language-%scala">_ + 1 
</code></pre>
<ol>
<li>type-inferred more intuitive version with code-block for larger function body:</li>
</ol>
<pre><code class="language-%scala">x =&gt; { 
      // body is a block of code
      val integerToAdd = 1
      x + integerToAdd
}
</code></pre>
<ol>
<li>as methods using <code>def</code>:</li>
</ol>
<pre><code class="language-%scala">def addOne(x: Int): Int = x + 1
</code></pre>
</div>
<div class="cell markdown">
<p>Now, let's do some functional programming over scala collection (<code>List</code>) using some of their methods: <code>map</code>, <code>filter</code> and <code>reduce</code>. In the end we will write our first mapReduce program!</p>
<p>For more details see:</p>
<ul>
<li><a href="https://docs.scala-lang.org/overviews/scala-book/collections-methods.html">https://docs.scala-lang.org/overviews/scala-book/collections-methods.html</a></li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://superruzafa.github.io/visual-scala-reference/map/&quot; , 500))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Shift+Enter&gt; to map each value x of lst with x+10 to return a new List(11, 12, 13)
lst.map(x =&gt; x + 10)  
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res3: List[Int] = List(11, 12, 13)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Shift+Enter&gt; for the same as above using place-holder syntax
lst.map( _ + 10)  
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res4: List[Int] = List(11, 12, 13)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://superruzafa.github.io/visual-scala-reference/map/&quot; , 600))
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://superruzafa.github.io/visual-scala-reference/map/"
 width="95%" height="600"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Shift+Enter&gt; to return a new List(1, 3) after filtering x's from lst if (x % 2 == 1) is true
lst.filter(x =&gt; (x % 2 == 1) )
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res5: List[Int] = List(1, 3)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Shift+Enter&gt; for the same as above using place-holder syntax
lst.filter( _ % 2 == 1 )
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res6: List[Int] = List(1, 3)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://superruzafa.github.io/visual-scala-reference/reduce/&quot; , 600))
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://superruzafa.github.io/visual-scala-reference/reduce/"
 width="95%" height="600"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Shift+Enter&gt; to use reduce to add elements of lst two at a time to return Int 6
lst.reduce( (x, y) =&gt; x + y )
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res7: Int = 6
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Ctrl+Enter&gt; for the same as above but using place-holder syntax
lst.reduce( _ + _ )
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res8: Int = 6
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let's combine <code>map</code> and <code>reduce</code> programs above to find the sum of after 10 has been added to every element of the original List <code>lst</code> as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">lst.map(x =&gt; x+10)
   .reduce((x,y) =&gt; x+y) // &lt;Ctrl-Enter&gt; to get Int 36 = sum(1+10,2+10,3+10)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res9: Int = 36
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="exercise-in-functional-programming"><a class="header" href="#exercise-in-functional-programming">Exercise in Functional Programming</a></h4>
<p>You should spend an hour or so going through the Functional Programming Section of the Scala Book:</p>
<ul>
<li><a href="https://docs.scala-lang.org/overviews/scala-book/functional-programming.html">https://docs.scala-lang.org/overviews/scala-book/functional-programming.html</a></li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://docs.scala-lang.org/overviews/scala-book/functional-programming.html&quot; , 700))
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://docs.scala-lang.org/overviews/scala-book/functional-programming.html"
 width="95%" height="700"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<p>There are lots of methods in Scala Collections. And much more in this <em>scalable language</em>. See for example <a href="http://docs.scala-lang.org/cheatsheets/index.html">http://docs.scala-lang.org/cheatsheets/index.html</a>.</p>
</div>
<div class="cell markdown">
<h3 id="lazy-evaluation"><a class="header" href="#lazy-evaluation">Lazy Evaluation</a></h3>
<p>Another powerful programming concept we will need is <em>lazy evaluation</em> -- a form of delayed evaluation. So the value of an expression that is lazily evaluated is only available when it is actually needed.</p>
<p>This is to be contrasted with <em>eager evaluation</em> that we have seen so far -- an expression is immediately evaluated.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val eagerImmutableInt = 1 // eagerly evaluated as 1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>eagerImmutableInt: Int = 1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">var eagerMutableInt = 2 // eagerly evaluated as 2
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>eagerMutableInt: Int = 2
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let's demonstrate lazy evaluation using a <code>getTime</code> method and the keyword <code>lazy</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import java.util.Calendar
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import java.util.Calendar
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">lazy val lazyImmutableTime = Calendar.getInstance.getTime // lazily defined and not evaluated immediately
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>lazyImmutableTime: java.util.Date = &lt;lazy&gt;
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val eagerImmutableTime = Calendar.getInstance.getTime // egaerly evaluated immediately
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>eagerImmutableTime: java.util.Date = Thu Mar 24 15:38:05 UTC 2022
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(lazyImmutableTime) // evaluated when actully needed by println
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Thu Mar 24 15:38:16 UTC 2022
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(eagerImmutableTime) // prints what was already evaluated eagerly
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Thu Mar 24 15:38:05 UTC 2022
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def lazyDefinedInt = 5 // you can also use method to lazily define 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>lazyDefinedInt: Int
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">lazyDefinedInt // only evaluated now
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res12: Int = 5
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>for more details including the following example with <code>StringBuilder</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val builder = new StringBuilder //built-in
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>builder: StringBuilder =
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">builder.result()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res14: String = &quot;&quot;
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = { builder += 'x'; 1 } // eagerly evaluates x as 1 after appending 'x' to builder. NOTE: ';' is used to separate multiple expressions on the same line
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: Int = 1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">builder.result()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res15: String = x
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">x
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res16: Int = 1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">builder.result() // calling x again should not append x again to builder
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res17: String = x
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">lazy val y = { builder += 'y'; 2 } // lazily evaluate y later when it is called
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>y: Int = &lt;lazy&gt;
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">builder.result() // builder should remain unchanged
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res18: String = x
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def z = { builder += 'z'; 3 } // lazily evaluate z later when the method is called
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>z: Int
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">builder.result() // builder should remain unchanged
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res19: String = x
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>What should <code>builder.result()</code> be after the following arithmetic expression involving <code>x</code>,<code>y</code> and <code>z</code> is evaluated?</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">z + y + x + z + y + x
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res20: Int = 12
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="lazy-evaluation-exercise---you-try-now"><a class="header" href="#lazy-evaluation-exercise---you-try-now">Lazy Evaluation Exercise - You try Now!</a></h4>
<p>Understand why the output above is what it is!</p>
<ul>
<li>Why is <code>z</code> different in its appearance in the final <code>builder</code> string when compared to <code>x</code> and <code>y</code> as we evaluated?</li>
</ul>
<!-- -->
<pre><code>z + y + x + z + y + x
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// putting it all together
val builder = new StringBuilder

val x = { builder += 'x'; 1 }
lazy val y = { builder += 'y'; 2 }
def z = { builder += 'z'; 3 }

// comment next line after different summands to understand difference between val, lazy val and def
z + y + x  + z + y + x 

builder.result()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>builder: StringBuilder = xzyz
x: Int = 1
y: Int = &lt;lazy&gt;
z: Int
res21: String = xzyz
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="why-lazy"><a class="header" href="#why-lazy">Why Lazy?</a></h3>
<p>Imagine a more complex expression involving the evaluation of millions of values. Lazy evaluation will allow us to actually compute with big data when it may become impossible to hold all the values in memory. This is exactly what Apache Spark does as we will see.</p>
</div>
<div class="cell markdown">
<h3 id="recursions"><a class="header" href="#recursions">Recursions</a></h3>
<p>Recursion is a powerful framework when a function calls another function, including itself, until some terminal condition is reached.</p>
<p>Here we want to distinguish between two ways of implementing a recursion using a simple example of factorial.</p>
<p>Recall that for any natural number \(n\), its factorial is denoted and defined as follows:</p>
<p>\[
n!  :=  n \times (n-1) \times (n-2) \times \cdots \times 2 \times 1 
\]</p>
<p>which has the following recursive expression:</p>
<p>\[
n! = n*(n-1)! , , \qquad  0! = 1
\]</p>
<p>Let us implement it using two approaches: a naive approach that can run out of memory and another tail-recursive approach that uses constant memory. Read <a href="https://www.scala-exercises.org/scala_tutorial/tail_recursion">https://www.scala-exercises.org/scala<em>tutorial/tail</em>recursion</a> for details.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def factorialNaive(n: Int): Int =
  if (n == 0) 1 else n * factorialNaive(n - 1)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>factorialNaive: (n: Int)Int
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">factorialNaive(4)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res22: Int = 24
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>When <code>factorialNaive(4)</code> was evaluated above the following steps were actually done:</p>
<pre><code>factorial(4)
if (4 == 0) 1 else 4 * factorial(4 - 1)
4 * factorial(3)
4 * (3 * factorial(2))
4 * (3 * (2 * factorial(1)))
4 * (3 * (2 * (1 * factorial(0)))
4 * (3 * (2 * (1 * 1)))
24
</code></pre>
</div>
<div class="cell markdown">
<p>Notice how we add one more element to our expression at each recursive call. Our expressions becomes bigger and bigger until we end by reducing it to the final value. So the final expression given by a directed acyclic graph (DAG) of the pairwise multiplications given by the right-branching binary tree, whose leaves are input integers and internal nodes are the bianry <code>*</code> operator, can get very large when the input <code>n</code> is large.</p>
<p><em>Tail recursion</em> is a sophisticated way of implementing certain recursions so that memory requirements can be kept constant, as opposed to naive recursions.</p>
<blockquote>
<p><a href="https://www.scala-exercises.org/scala_tutorial/tail_recursion">Tail Recursion</a></p>
</blockquote>
<blockquote>
<p>That difference in the rewriting rules actually translates directly to a difference in the actual execution on a computer. In fact, it turns out that <strong>if you have a recursive function that calls itself as its last action, then you can reuse the stack frame of that function. This is called tail recursion.</strong></p>
</blockquote>
<blockquote>
<p>And by applying that trick, a tail recursive function can execute in constant stack space, so it's really just another formulation of an iterative process. We could say a tail recursive function is the functional form of a loop, and it executes just as efficiently as a loop.</p>
</blockquote>
<p>Implementation of tail recursion in the Exercise below uses Scala <a href="https://docs.scala-lang.org/tour/annotations.html">annotation</a>, which is a way to associate meta-information with definitions. In our case, the annotation <code>@tailrec</code> ensures that a method is indeed <a href="https://en.wikipedia.org/wiki/Tail_call">tail-recursive</a>. See the last link to understand how memory requirements can be kept constant in tail recursions.</p>
<p>We mainly want you to know that tail recursions are an important functional programming concept.</p>
</div>
<div class="cell markdown">
<h4 id="tail-recursion-exercise---you-try-now"><a class="header" href="#tail-recursion-exercise---you-try-now">Tail Recursion Exercise - You Try Now</a></h4>
<p>Replace <code>???</code> with the correct values to make this a tail recursion for factorial.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import scala.annotation.tailrec

// replace ??? with the right values to make this a tail recursion for factorial
def factorialTail(n: Int): Int = {
  @tailrec
  def iter(x: Int, result: Int): Int =
    if ( x == ???) result
    else iter(x - 1, result * x)

  iter( n, ??? )
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import scala.annotation.tailrec
factorialTail: (n: Int)Int
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">factorialTail(3) //shouldBe 6
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res23: Int = 6
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">factorialTail(4) //shouldBe 24
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res24: Int = 24
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Functional Programming is a vast subject and we are merely covering the fewest core ideas to get started with Apache Spark asap.</p>
<p>We will return to more concepts as we need them in the sequel.</p>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="introduction-to-spark"><a class="header" href="#introduction-to-spark">Introduction to Spark</a></h1>
<h2 id="spark-essentials-rdds-transformations-and-actions"><a class="header" href="#spark-essentials-rdds-transformations-and-actions">Spark Essentials: RDDs, Transformations and Actions</a></h2>
<ul>
<li>This introductory notebook describes how to get started running Spark (Scala) code in Notebooks.</li>
<li>Working with Spark's Resilient Distributed Datasets (RDDs)
<ul>
<li>creating RDDs</li>
<li>performing basic transformations on RDDs</li>
<li>performing basic actions on RDDs</li>
</ul>
</li>
</ul>
<p><strong>RECOLLECT</strong> from <code>001_WhySpark</code> notebook and AJ's videos that <em>Spark does fault-tolerant, distributed, in-memory computing</em></p>
<p><strong>THEORY CAVEAT</strong> This module is focused on getting you to quickly write Spark programs with a high-level appreciation of the underlying concepts.</p>
<p>In the last module, we will spend more time on analyzing the core algorithms in parallel and distributed setting of a typical Spark cluster today -- where several multi-core parallel computers (Spark workers) are networked together to provide a fault-tolerant distributed computing platform.</p>
</div>
<div class="cell markdown">
<h2 id="spark-cluster-overview"><a class="header" href="#spark-cluster-overview">Spark Cluster Overview:</a></h2>
<p><strong>Driver Program, Cluster Manager and Worker Nodes</strong></p>
<p>The <em>driver</em> does the following:</p>
<ol>
<li>connects to a <em>cluster manager</em> to allocate resources across applications</li>
</ol>
<ul>
<li>acquire <em>executors</em> on cluster nodes
<ul>
<li>executor processs run compute tasks and cache data in memory or disk on a <em>worker node</em></li>
</ul>
</li>
<li>sends <em>application</em> (user program built on Spark) to the executors</li>
<li>sends <em>tasks</em> for the executors to run
<ul>
<li>task is a unit of work that will be sent to one executor</li>
</ul>
</li>
</ul>
<p><img src="http://spark.apache.org/docs/latest/img/cluster-overview.png" alt="" /></p>
<p>See <a href="http://spark.apache.org/docs/latest/cluster-overview.html">http://spark.apache.org/docs/latest/cluster-overview.html</a> for an overview of the spark cluster.</p>
</div>
<div class="cell markdown">
<h2 id="the-abstraction-of-resilient-distributed-dataset-rdd"><a class="header" href="#the-abstraction-of-resilient-distributed-dataset-rdd">The Abstraction of Resilient Distributed Dataset (RDD)</a></h2>
<p><strong>RDD is a fault-tolerant collection of elements that can be operated on in parallel.</strong></p>
<h3 id="key-points-to-note"><a class="header" href="#key-points-to-note">Key Points to Note</a></h3>
<ul>
<li>Resilient distributed datasets (RDDs) are the primary abstraction in Spark.</li>
<li>RDDs are immutable once created:
<ul>
<li>can transform it.</li>
<li>can perform actions on it.</li>
<li>but cannot change an RDD once you construct it.</li>
</ul>
</li>
<li>Spark tracks each RDD's lineage information or recipe to enable its efficient recomputation if a machine fails.</li>
<li>RDDs enable operations on collections of elements in parallel.</li>
<li>We can construct RDDs by:
<ul>
<li>parallelizing Scala collections such as lists or arrays</li>
<li>by transforming an existing RDD,</li>
<li>from files in distributed file systems such as (HDFS, S3, etc.).</li>
</ul>
</li>
<li>We can specify the number of partitions for an RDD</li>
<li>The more partitions in an RDD, the more opportunities for parallelism</li>
<li>There are <strong>two types of operations</strong> you can perform on an RDD:
<ul>
<li><strong>transformations</strong> (are lazily evaluated)
<ul>
<li>map</li>
<li>flatMap</li>
<li>filter</li>
<li>distinct</li>
<li>...</li>
</ul>
</li>
<li><strong>actions</strong> (actual evaluation happens)
<ul>
<li>count</li>
<li>reduce</li>
<li>take</li>
<li>collect</li>
<li>takeOrdered</li>
<li>...</li>
</ul>
</li>
</ul>
</li>
<li>Spark transformations enable us to create new RDDs from an existing RDD.</li>
<li>RDD transformations are lazy evaluations (results are not computed right away)</li>
<li>Spark remembers the set of transformations that are applied to a base data set (this is the lineage graph of RDD)</li>
<li>The allows Spark to automatically recover RDDs from failures and slow workers.</li>
<li>The lineage graph is a recipe for creating a result and it can be optimized before execution.</li>
<li>A transformed RDD is executed only when an action runs on it.</li>
<li>You can also persist, or cache, RDDs in memory or on disk (this speeds up iterative ML algorithms that transforms the initial RDD iteratively).</li>
<li>Here is a great reference URL for programming guides for Spark that one should try to cover first
<ul>
<li><a href="http://spark.apache.org/docs/latest/programming-guide.html">http://spark.apache.org/docs/latest/programming-guide.html</a>.</li>
<li>and specifically for RDDs: <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html">https://spark.apache.org/docs/latest/rdd-programming-guide.html</a></li>
</ul>
</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">//This allows easy embedding of publicly available information into any other notebook
//Example usage:
// displayHTML(frameIt(&quot;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Topics_in_LDA&quot;,250))
def frameIt( u:String, h:Int ) : String = {
      &quot;&quot;&quot;&lt;iframe 
 src=&quot;&quot;&quot;&quot;+ u+&quot;&quot;&quot;&quot;
 width=&quot;95%&quot; height=&quot;&quot;&quot;&quot; + h + &quot;&quot;&quot;&quot;
 sandbox&gt;
  &lt;p&gt;
    &lt;a href=&quot;http://spark.apache.org/docs/latest/index.html&quot;&gt;
      Fallback link for browsers that, unlikely, don't support frames
    &lt;/a&gt;
  &lt;/p&gt;
&lt;/iframe&gt;&quot;&quot;&quot;
   }
displayHTML(frameIt(&quot;https://spark.apache.org/docs/latest/rdd-programming-guide.html&quot;,700))
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://spark.apache.org/docs/latest/rdd-programming-guide.html"
 width="95%" height="700"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<h2 id="lets-get-our-hands-dirty-in-spark"><a class="header" href="#lets-get-our-hands-dirty-in-spark">Let's get our hands dirty in Spark!</a></h2>
<p><strong>DO NOW!</strong></p>
<p>In your databricks community edition:</p>
<ol>
<li>In your <code>WorkSpace</code> create a Folder named <code>scalable-data-science</code></li>
<li><em>Import</em> the databricks archive file at the following URL:
<ul>
<li><a href="https://github.com/lamastex/scalable-data-science/raw/master/dbcArchives/2017/parts/xtraResources.dbc">https://github.com/lamastex/scalable-data-science/raw/master/dbcArchives/2017/parts/xtraResources.dbc</a></li>
</ul>
</li>
<li>This should open a structure of directories in with path: <code>/Workspace/scalable-data-science/xtraResources/</code></li>
</ol>
</div>
<div class="cell markdown">
<p><strong>Let us look at the legend and overview of the visual RDD Api by doing the following first:</strong></p>
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-1.png" alt="" /></p>
</div>
<div class="cell markdown">
<h3 id="running-spark"><a class="header" href="#running-spark">Running <strong>Spark</strong></a></h3>
<p>The variable <strong>sc</strong> allows you to access a Spark Context to run your Spark programs. Recall <code>SparkContext</code> is in the Driver Program.</p>
<p><img src="http://spark.apache.org/docs/latest/img/cluster-overview.png" alt="" /></p>
<p>**NOTE: Do not create the <em>sc</em> variable - it is already initialized for you in spark-shell REPL, that includes notebook environments like databricks, Jupyter, zeppelin, etc. **</p>
</div>
<div class="cell markdown">
<h3 id="we-will-do-the-following-next"><a class="header" href="#we-will-do-the-following-next">We will do the following next:</a></h3>
<ol>
<li>Create an RDD using <code>sc.parallelize</code></li>
</ol>
<ul>
<li>Perform the <code>collect</code> action on the RDD and find the number of partitions it is made of using <code>getNumPartitions</code> action</li>
<li>Perform the <code>take</code> action on the RDD</li>
<li>Transform the RDD by <code>map</code> to make another RDD</li>
<li>Transform the RDD by <code>filter</code> to make another RDD</li>
<li>Perform the <code>reduce</code> action on the RDD</li>
<li>Transform the RDD by <code>flatMap</code> to make another RDD</li>
<li>Create a Pair RDD</li>
<li>Perform some transformations on a Pair RDD</li>
<li>Where in the cluster is your computation running?</li>
<li>Shipping Closures, Broadcast Variables and Accumulator Variables</li>
<li>Spark Essentials: Summary</li>
<li>HOMEWORK</li>
<li>Importing Standard Scala and Java libraries</li>
</ul>
</div>
<div class="cell markdown">
<h4 id="entry-point"><a class="header" href="#entry-point">Entry Point</a></h4>
<p>Now we are ready to start programming in Spark!</p>
<p>Our entry point for Spark applications is the class <code>SparkSession</code>. An instance of this object is already instantiated for us which can be easily demonstrated by running the next cell</p>
<p>We will need these docs!</p>
<ul>
<li><a href="https://spark.apache.org/docs/3.0.1/api/scala/org/apache/spark/rdd/RDD.html">RDD Scala Docs</a></li>
<li><a href="https://spark.apache.org/docs/3.0.1/api/scala/org/apache/spark/sql/Dataset.html">Dataset Scala Docs</a></li>
<li><a href="https://spark.apache.org/docs/3.0.1/api/scala/index.html">https://spark.apache.org/docs/3.0.1/api/scala/index.html</a> you can simply search for other Spark classes, methods, etc here</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(spark) // spark is already created for us in databricks or in spark-shell
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>org.apache.spark.sql.SparkSession@2afef3ad
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>NOTE that since Spark 2.0 <code>SparkSession</code> is a replacement for the other entry points: * <code>SparkContext</code>, available in our notebook as <strong>sc</strong>. * <code>SQLContext</code>, or more specifically its subclass <code>HiveContext</code>, available in our notebook as <strong>sqlContext</strong>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(sc)
println(sqlContext)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>org.apache.spark.SparkContext@6d040faa
org.apache.spark.sql.hive.HiveContext@238333d0
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>We will be using the pre-made SparkContext <code>sc</code> when learning about RDDs.</p>
</div>
<div class="cell markdown">
<h4 id="1-create-an-rdd-using-scparallelize"><a class="header" href="#1-create-an-rdd-using-scparallelize">1. Create an RDD using <code>sc.parallelize</code></a></h4>
<p>First, let us create an RDD of three elements (of integer type <code>Int</code>) from a Scala <code>Seq</code> (or <code>List</code> or <code>Array</code>) with two partitions by using the <code>parallelize</code> method of the available Spark Context <code>sc</code> as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = sc.parallelize(Array(1, 2, 3), 2)    // &lt;Ctrl+Enter&gt; to evaluate this cell (using 2 partitions)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[651] at parallelize at command-4088905069026221:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//x.  // place the cursor after 'x.' and hit Tab to see the methods available for the RDD x we created
</code></pre>
</div>
<div class="cell markdown">
<h4 id="2-perform-the-collect-action-on-the-rdd-and-find-the-number-of-partitions-in-it-using-getnumpartitions-action"><a class="header" href="#2-perform-the-collect-action-on-the-rdd-and-find-the-number-of-partitions-in-it-using-getnumpartitions-action">2. Perform the <code>collect</code> action on the RDD and find the number of partitions in it using <code>getNumPartitions</code> action</a></h4>
<p>No action has been taken by <code>sc.parallelize</code> above. To see what is &quot;cooked&quot; by the recipe for RDD <code>x</code> we need to take an action.</p>
<p>The simplest is the <code>collect</code> action which returns all of the elements of the RDD as an <code>Array</code> to the driver program and displays it.</p>
<p><em>So you have to make sure that all of that data will fit in the driver program if you call <code>collect</code> action!</em></p>
</div>
<div class="cell markdown">
<h5 id="let-us-look-at-the-collect-action-in-detail-and-return-here-to-try-out-the-example-codes"><a class="header" href="#let-us-look-at-the-collect-action-in-detail-and-return-here-to-try-out-the-example-codes">Let us look at the <a href="contents/000_1-sds-3-x-spark//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/actions/collect">collect action in detail</a> and return here to try out the example codes.</a></h5>
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-90.png" alt="" /></p>
</div>
<div class="cell markdown">
<p>Let us perform a <code>collect</code> action on RDD <code>x</code> as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">x.collect()    // &lt;Ctrl+Enter&gt; to collect (action) elements of rdd; should be (1, 2, 3)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res2: Array[Int] = Array(1, 2, 3)
</code></pre>
</div>
</div>
<div class="cell markdown">
<p><em>CAUTION:</em> <code>collect</code> can crash the driver when called upon an RDD with massively many elements.
So, it is better to use other diplaying actions like <code>take</code> or <code>takeOrdered</code> as follows:</p>
</div>
<div class="cell markdown">
<h5 id="let-us-look-at-the-getnumpartitions-action-in-detail-and-return-here-to-try-out-the-example-codes"><a class="header" href="#let-us-look-at-the-getnumpartitions-action-in-detail-and-return-here-to-try-out-the-example-codes">Let us look at the <a href="contents/000_1-sds-3-x-spark//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/actions/getNumPartitions">getNumPartitions action in detail</a> and return here to try out the example codes.</a></h5>
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-88.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Ctrl+Enter&gt; to evaluate this cell and find the number of partitions in RDD x
x.getNumPartitions 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res3: Int = 2
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>We can see which elements of the RDD are in which parition by calling <code>glom()</code> before <code>collect()</code>.</p>
<p><code>glom()</code> flattens elements of the same partition into an <code>Array</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">x.glom().collect() // glom() flattens elements on the same partition
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res4: Array[Array[Int]] = Array(Array(1), Array(2, 3))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val a = x.glom().collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>a: Array[Array[Int]] = Array(Array(1), Array(2, 3))
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Thus from the output above, <code>Array[Array[Int]] = Array(Array(1), Array(2, 3))</code>, we know that <code>1</code> is in one partition while <code>2</code> and <code>3</code> are in another partition.</p>
</div>
<div class="cell markdown">
<h5 id="you-try"><a class="header" href="#you-try">You Try!</a></h5>
<p>Crate an RDD <code>x</code> with three elements, 1,2,3, and this time do not specifiy the number of partitions. Then the default number of partitions will be used. Find out what this is for the cluster you are attached to.</p>
<p>The default number of partitions for an RDD depends on the cluster this notebook is attached to among others - see <a href="http://spark.apache.org/docs/latest/programming-guide.html">programming-guide</a>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = sc.parallelize(Seq(1, 2, 3))    // &lt;Shift+Enter&gt; to evaluate this cell (using default number of partitions)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[654] at parallelize at command-4088905069026235:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">x.getNumPartitions // &lt;Shift+Enter&gt; to evaluate this cell
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res5: Int = 2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">x.glom().collect() // &lt;Ctrl+Enter&gt; to evaluate this cell
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res6: Array[Array[Int]] = Array(Array(1), Array(2, 3))
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="3-perform-the-take-action-on-the-rdd"><a class="header" href="#3-perform-the-take-action-on-the-rdd">3. Perform the <code>take</code> action on the RDD</a></h4>
<p>The <code>.take(n)</code> action returns an array with the first <code>n</code> elements of the RDD.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">x.take(2) // Ctrl+Enter to take two elements from the RDD x
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res7: Array[Int] = Array(1, 2)
</code></pre>
</div>
</div>
<div class="cell markdown">
<h5 id="you-try-1"><a class="header" href="#you-try-1">You Try!</a></h5>
<p>Fill in the parenthes <code>( )</code> below in order to <code>take</code> just one element from RDD <code>x</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//x.take(1) // uncomment by removing '//' before x in the cell and fill in the parenthesis to take just one element from RDD x and Cntrl+Enter
</code></pre>
</div>
<div class="cell markdown">
<hr />
<h4 id="4-transform-the-rdd-by-map-to-make-another-rdd"><a class="header" href="#4-transform-the-rdd-by-map-to-make-another-rdd">4. Transform the RDD by <code>map</code> to make another RDD</a></h4>
<p>The <code>map</code> transformation returns a new RDD that's formed by passing each element of the source RDD through a function (closure). The closure is automatically passed on to the workers for evaluation (when an action is called later).</p>
</div>
<div class="cell markdown">
<h5 id="let-us-look-at-the-map-transformation-in-detail-and-return-here-to-try-out-the-example-codes"><a class="header" href="#let-us-look-at-the-map-transformation-in-detail-and-return-here-to-try-out-the-example-codes">Let us look at the <a href="contents/000_1-sds-3-x-spark//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/map">map transformation in detail</a> and return here to try out the example codes.</a></h5>
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-18.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Shift+Enter to make RDD x and RDD y that is mapped from x
val x = sc.parallelize(Array(&quot;b&quot;, &quot;a&quot;, &quot;c&quot;)) // make RDD x: [b, a, c]
val y = x.map(z =&gt; (z,1))                    // map x into RDD y: [(b, 1), (a, 1), (c, 1)]
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[656] at parallelize at command-4088905069026244:2
y: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[657] at map at command-4088905069026244:3
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Cntrl+Enter to collect and print the two RDDs
println(x.collect().mkString(&quot;, &quot;))
println(y.collect().mkString(&quot;, &quot;))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>b, a, c
(b,1), (a,1), (c,1)
</code></pre>
</div>
</div>
<div class="cell markdown">
<hr />
<h4 id="5-transform-the-rdd-by-filter-to-make-another-rdd"><a class="header" href="#5-transform-the-rdd-by-filter-to-make-another-rdd">5. Transform the RDD by <code>filter</code> to make another RDD</a></h4>
<p>The <code>filter</code> transformation returns a new RDD that's formed by selecting those elements of the source RDD on which the function returns <code>true</code>.</p>
</div>
<div class="cell markdown">
<h5 id="let-us-look-at-the-filter-transformation-in-detail-and-return-here-to-try-out-the-example-codes"><a class="header" href="#let-us-look-at-the-filter-transformation-in-detail-and-return-here-to-try-out-the-example-codes">Let us look at the <a href="contents/000_1-sds-3-x-spark//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/filter">filter transformation in detail</a> and return here to try out the example codes.</a></h5>
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-24.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//Shift+Enter to make RDD x and filter it by (n =&gt; n%2 == 1) to make RDD y
val x = sc.parallelize(Array(1,2,3))
// the closure (n =&gt; n%2 == 1) in the filter will 
// return True if element n in RDD x has remainder 1 when divided by 2 (i.e., if n is odd)
val y = x.filter(n =&gt; n%2 == 1) 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[658] at parallelize at command-4088905069026248:2
y: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[659] at filter at command-4088905069026248:5
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Cntrl+Enter to collect and print the two RDDs
println(x.collect().mkString(&quot;, &quot;))
println(y.collect().mkString(&quot;, &quot;))
//y.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1, 2, 3
1, 3
</code></pre>
</div>
</div>
<div class="cell markdown">
<hr />
<h4 id="6-perform-the-reduce-action-on-the-rdd"><a class="header" href="#6-perform-the-reduce-action-on-the-rdd">6. Perform the <code>reduce</code> action on the RDD</a></h4>
<p>Reduce aggregates a data set element using a function (closure). This function takes two arguments and returns one and can often be seen as a binary operator. This operator has to be commutative and associative so that it can be computed correctly in parallel (where we have little control over the order of the operations!).</p>
</div>
<div class="cell markdown">
<h5 id="let-us-look-at-the-reduce-action-in-detail-and-return-here-to-try-out-the-example-codes"><a class="header" href="#let-us-look-at-the-reduce-action-in-detail-and-return-here-to-try-out-the-example-codes">Let us look at the <a href="contents/000_1-sds-3-x-spark//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/actions/reduce">reduce action in detail</a> and return here to try out the example codes.</a></h5>
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-94.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//Shift+Enter to make RDD x of inteegrs 1,2,3,4 and reduce it to sum
val x = sc.parallelize(Array(1,2,3,4))
val y = x.reduce((a,b) =&gt; a+b)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[660] at parallelize at command-4088905069026252:2
y: Int = 10
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//Cntrl+Enter to collect and print RDD x and the Int y, sum of x
println(x.collect.mkString(&quot;, &quot;))
println(y)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1, 2, 3, 4
10
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="7-transform-an-rdd-by-flatmap-to-make-another-rdd"><a class="header" href="#7-transform-an-rdd-by-flatmap-to-make-another-rdd">7. Transform an RDD by <code>flatMap</code> to make another RDD</a></h4>
<p><code>flatMap</code> is similar to <code>map</code> but each element from input RDD can be mapped to zero or more output elements. Therefore your function should return a sequential collection such as an <code>Array</code> rather than a single element as shown below.</p>
</div>
<div class="cell markdown">
<h5 id="let-us-look-at-the-flatmap-transformation-in-detail-and-return-here-to-try-out-the-example-codes"><a class="header" href="#let-us-look-at-the-flatmap-transformation-in-detail-and-return-here-to-try-out-the-example-codes">Let us look at the <a href="contents/000_1-sds-3-x-spark//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/flatMap">flatMap transformation in detail</a> and return here to try out the example codes.</a></h5>
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-31.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//Shift+Enter to make RDD x and flatMap it into RDD by closure (n =&gt; Array(n, n*100, 42))
val x = sc.parallelize(Array(1,2,3))
val y = x.flatMap(n =&gt; Array(n, n*100, 42))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[661] at parallelize at command-4088905069026256:2
y: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[662] at flatMap at command-4088905069026256:3
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//Cntrl+Enter to collect and print RDDs x and y
println(x.collect().mkString(&quot;, &quot;))
println(y.collect().mkString(&quot;, &quot;))
sc.parallelize(Array(1,2,3)).map(n =&gt; Array(n,n*100,42)).collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1, 2, 3
1, 100, 42, 2, 200, 42, 3, 300, 42
res10: Array[Array[Int]] = Array(Array(1, 100, 42), Array(2, 200, 42), Array(3, 300, 42))
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="8-create-a-pair-rdd"><a class="header" href="#8-create-a-pair-rdd">8. Create a Pair RDD</a></h4>
<p>Let's next work with RDD of <code>(key,value)</code> pairs called a <em>Pair RDD</em> or <em>Key-Value RDD</em>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Cntrl+Enter to make RDD words and display it by collect
val words = sc.parallelize(Array(&quot;a&quot;, &quot;b&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;))
words.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>words: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[665] at parallelize at command-4088905069026259:2
res11: Array[String] = Array(a, b, a, a, b, b, a, a, a, b, b)
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let's make a Pair RDD called <code>wordCountPairRDD</code> that is made of (key,value) pairs with key=word and value=1 in order to encode each occurrence of each word in the RDD <code>words</code>, as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Cntrl+Enter to make and collect Pair RDD wordCountPairRDD
val wordCountPairRDD = words.map(s =&gt; (s, 1))
wordCountPairRDD.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>wordCountPairRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[666] at map at command-4088905069026261:2
res12: Array[(String, Int)] = Array((a,1), (b,1), (a,1), (a,1), (b,1), (b,1), (a,1), (a,1), (a,1), (b,1), (b,1))
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="wide-transformations-and-shuffles"><a class="header" href="#wide-transformations-and-shuffles">Wide Transformations and Shuffles</a></h4>
<p>So far we have seen transformations that are <strong>narrow</strong> -- with no data transfer between partitions. Think of <code>map</code>.</p>
<p><code>ReduceByKey</code> and <code>GroupByKey</code> are <strong>wide</strong> transformations as data has to be shuffled across the partitions in different executors -- this is generally very expensive operation.</p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-40.png" alt="" /></p>
</div>
<div class="cell markdown">
<p>READ the <strong>Background</strong> about Shuffles in the programming guide below.</p>
<blockquote>
<p>In Spark, data is generally not distributed across partitions to be in the necessary place for a specific operation. During computations, a single task will operate on a single partition - thus, to organize all the data for a single reduceByKey reduce task to execute, Spark needs to perform an all-to-all operation. It must read from all partitions to find all the values for all keys, and then bring together values across partitions to compute the final result for each key - this is called the shuffle</p>
</blockquote>
<p>READ the <strong>Performance Impact</strong> about Shuffles in the programming guide below.</p>
<blockquote>
<p>The Shuffle is an expensive operation since it involves disk I/O, data serialization, and network I/O. To organize data for the shuffle, Spark generates sets of tasks - map tasks to organize the data, and a set of reduce tasks to aggregate it. This nomenclature comes from MapReduce and does not directly relate to Spark’s map and reduce operations.</p>
</blockquote>
<blockquote>
<p>Internally, results from individual map tasks are kept in memory until they can’t fit. Then, these are sorted based on the target partition and written to a single file. On the reduce side, tasks read the relevant sorted blocks.</p>
</blockquote>
<p><a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations">https://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations</a></p>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations&quot;,500))
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations"
 width="95%" height="500"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<h4 id="9-perform-some-transformations-on-a-pair-rdd"><a class="header" href="#9-perform-some-transformations-on-a-pair-rdd">9. Perform some transformations on a Pair RDD</a></h4>
<p>Let's next work with RDD of <code>(key,value)</code> pairs called a <em>Pair RDD</em> or <em>Key-Value RDD</em>.</p>
<p>Now some of the Key-Value transformations that we could perform include the following.</p>
<ul>
<li><strong><code>reduceByKey</code> transformation</strong>
<ul>
<li>which takes an RDD and returns a new RDD of key-value pairs, such that:
<ul>
<li>the values for each key are aggregated using the given reduced function</li>
<li>and the reduce function has to be of the type that takes two values and returns one value.</li>
</ul>
</li>
</ul>
</li>
<li><strong><code>sortByKey</code> transformation</strong>
<ul>
<li>this returns a new RDD of key-value pairs that's sorted by keys in ascending order</li>
</ul>
</li>
<li><strong><code>groupByKey</code> transformation</strong>
<ul>
<li>this returns a new RDD consisting of key and iterable-valued pairs.</li>
</ul>
</li>
</ul>
<p>Let's see some concrete examples next.</p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-44.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Cntrl+Enter to reduceByKey and collect wordcounts RDD
//val wordcounts = wordCountPairRDD.reduceByKey( _ + _ )
val wordcounts = wordCountPairRDD.reduceByKey( (value1, value2) =&gt; value1 + value2 )
wordcounts.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>wordcounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[668] at reduceByKey at command-4088905069026268:3
res14: Array[(String, Int)] = Array((b,5), (a,6))
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Now, let us do just the crucial steps and avoid collecting intermediate RDDs (something we should avoid for large datasets anyways, as they may not fit in the driver program).</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//Cntrl+Enter to make words RDD and do the word count in two lines
val words = sc.parallelize(Array(&quot;a&quot;, &quot;b&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;))
val wordcounts = words
                    .map(s =&gt; (s, 1))
                    .reduceByKey(_ + _)
                    .collect() 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>words: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[669] at parallelize at command-4088905069026270:2
wordcounts: Array[(String, Int)] = Array((b,5), (a,6))
</code></pre>
</div>
</div>
<div class="cell markdown">
<h5 id="you-try-2"><a class="header" href="#you-try-2">You Try!</a></h5>
<p>You try evaluating <code>sortByKey()</code> which will make a new RDD that consists of the elements of the original pair RDD that are sorted by Keys.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Shift+Enter and comprehend code
val words = sc.parallelize(Array(&quot;a&quot;, &quot;b&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;))
val wordCountPairRDD = words.map(s =&gt; (s, 1))
val wordCountPairRDDSortedByKey = wordCountPairRDD.sortByKey()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>words: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[672] at parallelize at command-4088905069026272:2
wordCountPairRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[673] at map at command-4088905069026272:3
wordCountPairRDDSortedByKey: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[676] at sortByKey at command-4088905069026272:4
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">wordCountPairRDD.collect() // Shift+Enter and comprehend code
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res15: Array[(String, Int)] = Array((a,1), (b,1), (a,1), (a,1), (b,1), (b,1), (a,1), (a,1), (a,1), (b,1), (b,1))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">wordCountPairRDDSortedByKey.collect() // Cntrl+Enter and comprehend code
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res16: Array[(String, Int)] = Array((a,1), (a,1), (a,1), (a,1), (a,1), (a,1), (b,1), (b,1), (b,1), (b,1), (b,1))
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>The next key value transformation we will see is <code>groupByKey</code></p>
<p>When we apply the <code>groupByKey</code> transformation to <code>wordCountPairRDD</code> we end up with a new RDD that contains two elements. The first element is the tuple <code>b</code> and an iterable <code>CompactBuffer(1,1,1,1,1)</code> obtained by grouping the value <code>1</code> for each of the five key value pairs <code>(b,1)</code>. Similarly the second element is the key <code>a</code> and an iterable <code>CompactBuffer(1,1,1,1,1,1)</code> obtained by grouping the value <code>1</code> for each of the six key value pairs <code>(a,1)</code>.</p>
<p><em>CAUTION</em>: <code>groupByKey</code> can cause a large amount of data movement across the network. It also can create very large iterables at a worker. Imagine you have an RDD where you have 1 billion pairs that have the key <code>a</code>. All of the values will have to fit in a single worker if you use group by key. So instead of a group by key, consider using reduced by key.</p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-45.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val wordCountPairRDDGroupByKey = wordCountPairRDD.groupByKey() // &lt;Shift+Enter&gt; CAUTION: this transformation can be very wide!
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>wordCountPairRDDGroupByKey: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[677] at groupByKey at command-4088905069026277:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">wordCountPairRDDGroupByKey.collect()  // Cntrl+Enter
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res17: Array[(String, Iterable[Int])] = Array((b,CompactBuffer(1, 1, 1, 1, 1)), (a,CompactBuffer(1, 1, 1, 1, 1, 1)))
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="10-understanding-closures---where-in-the-cluster-is-your-computation-running"><a class="header" href="#10-understanding-closures---where-in-the-cluster-is-your-computation-running">10. Understanding Closures - Where in the cluster is your computation running?</a></h4>
<blockquote>
<p>One of the harder things about Spark is understanding the scope and life cycle of variables and methods when executing code across a cluster. RDD operations that modify variables outside of their scope can be a frequent source of confusion. In the example below we’ll look at code that uses <code>foreach()</code> to increment a counter, but similar issues can occur for other operations as well.</p>
</blockquote>
<p><a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#understanding-closures-">https://spark.apache.org/docs/latest/rdd-programming-guide.html#understanding-closures-</a></p>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://spark.apache.org/docs/latest/rdd-programming-guide.html#understanding-closures-&quot;,500))
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://spark.apache.org/docs/latest/rdd-programming-guide.html#understanding-closures-"
 width="95%" height="500"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val data = Array(1, 2, 3, 4, 5)
var counter = 0
var rdd = sc.parallelize(data)

// Wrong: Don't do this!!
rdd.foreach(x =&gt; counter += x)

println(&quot;Counter value: &quot; + counter)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Counter value: 0
data: Array[Int] = Array(1, 2, 3, 4, 5)
counter: Int = 0
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[678] at parallelize at command-4088905069026281:3
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>From RDD programming guide:</p>
<blockquote>
<p>The behavior of the above code is undefined, and may not work as intended. To execute jobs, Spark breaks up the processing of RDD operations into tasks, each of which is executed by an executor. Prior to execution, Spark computes the task’s closure. The closure is those variables and methods which must be visible for the executor to perform its computations on the RDD (in this case foreach()). This closure is serialized and sent to each executor.</p>
</blockquote>
<blockquote>
<p>The variables within the closure sent to each executor are now copies and thus, when counter is referenced within the foreach function, it’s no longer the counter on the driver node. There is still a counter in the memory of the driver node but this is no longer visible to the executors! The executors only see the copy from the serialized closure. Thus, the final value of counter will still be zero since all operations on counter were referencing the value within the serialized closure.</p>
</blockquote>
</div>
<div class="cell markdown">
<h4 id="11-shipping-closures-broadcast-variables-and-accumulator-variables"><a class="header" href="#11-shipping-closures-broadcast-variables-and-accumulator-variables">11. Shipping Closures, Broadcast Variables and Accumulator Variables</a></h4>
<h5 id="closures-broadcast-and-accumulator-variables"><a class="header" href="#closures-broadcast-and-accumulator-variables">Closures, Broadcast and Accumulator Variables</a></h5>
<p>Spark automatically creates closures</p>
<ul>
<li>for functions that run on RDDs at workers,</li>
<li>and for any global variables that are used by those workers</li>
<li>one closure per worker is sent with every task</li>
<li>and there's no communication between workers</li>
<li>closures are one way from the driver to the worker</li>
<li>any changes that you make to the global variables at the workers
<ul>
<li>are not sent to the driver or</li>
<li>are not sent to other workers.</li>
</ul>
</li>
</ul>
<p>The problem we have is that these closures</p>
<ul>
<li>are automatically created are sent or re-sent with every job</li>
<li>with a large global variable it gets inefficient to send/resend lots of data to each worker</li>
<li>we cannot communicate that back to the driver</li>
</ul>
<p>To do this, Spark provides shared variables in two different types.</p>
<ul>
<li><strong>broadcast variables</strong>
<ul>
<li>lets us to efficiently send large read-only values to all of the workers</li>
<li>these are saved at the workers for use in one or more Spark operations.</li>
</ul>
</li>
<li><strong>accumulator variables</strong>
<ul>
<li>These allow us to aggregate values from workers back to the driver.</li>
<li>only the driver can access the value of the accumulator</li>
<li>for the tasks, the accumulators are basically write-only</li>
</ul>
</li>
</ul>
<hr />
</div>
<div class="cell markdown">
<h5 id="accumulators"><a class="header" href="#accumulators">Accumulators</a></h5>
<blockquote>
<p>Accumulators are variables that are only “added” to through an associative and commutative operation and can therefore be efficiently supported in parallel. They can be used to implement counters (as in MapReduce) or sums. Spark natively supports accumulators of numeric types, and programmers can add support for new types.</p>
</blockquote>
<p>Read: <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#accumulators">https://spark.apache.org/docs/latest/rdd-programming-guide.html#accumulators</a>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://spark.apache.org/docs/latest/rdd-programming-guide.html#accumulators&quot;,500))
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://spark.apache.org/docs/latest/rdd-programming-guide.html#accumulators"
 width="95%" height="500"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<blockquote>
<p>A numeric accumulator can be created by calling SparkContext.longAccumulator() or SparkContext.doubleAccumulator() to accumulate values of type Long or Double, respectively. Tasks running on a cluster can then add to it using the add method. However, they cannot read its value. Only the driver program can read the accumulator’s value, using its value method.</p>
</blockquote>
<blockquote>
<p>The code below shows an accumulator being used to add up the elements of an array:</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val accum = sc.longAccumulator(&quot;My Accumulator&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>accum: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 9875, name: Some(My Accumulator), value: 0)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sc.parallelize(Array(1, 2, 3, 4)).foreach(x =&gt; accum.add(x))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">accum.value
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res20: Long = 10
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">spark.range(1, 100000).foreach(x =&gt; accum.add(x)) // bigger example
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">accum.value
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res22: Long = 4999950010
</code></pre>
</div>
</div>
<div class="cell markdown">
<h5 id="broadcast-variables"><a class="header" href="#broadcast-variables">Broadcast Variables</a></h5>
<p>From <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#broadcast-variables">https://spark.apache.org/docs/latest/rdd-programming-guide.html#broadcast-variables</a>:</p>
<blockquote>
<p>Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. They can be used, for example, to give every node a copy of a large input dataset in an efficient manner. Spark also attempts to distribute broadcast variables using efficient broadcast algorithms to reduce communication cost.</p>
</blockquote>
<blockquote>
<p>Spark actions are executed through a set of stages, separated by distributed “shuffle” operations. Spark automatically broadcasts the common data needed by tasks within each stage. The data broadcasted this way is cached in serialized form and deserialized before running each task. This means that explicitly creating broadcast variables is only useful when tasks across multiple stages need the same data or when caching the data in deserialized form is important.</p>
</blockquote>
<blockquote>
<p>Broadcast variables are created from a variable v by calling SparkContext.broadcast(v). The broadcast variable is a wrapper around v, and its value can be accessed by calling the value method. The code below shows this in action.</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://spark.apache.org/docs/latest/rdd-programming-guide.html#broadcast-variables&quot;,500))
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://spark.apache.org/docs/latest/rdd-programming-guide.html#broadcast-variables"
 width="95%" height="500"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val broadcastVar = sc.broadcast(Array(1, 2, 3))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>broadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(327)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">broadcastVar.value
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res23: Array[Int] = Array(1, 2, 3)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">broadcastVar.value(0)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res24: Int = 1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rdd = sc.parallelize(1 to 10)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[686] at parallelize at command-4088905069026297:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd.collect
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res25: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd.map(x =&gt; x%3).collect
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res26: Array[Int] = Array(1, 2, 0, 1, 2, 0, 1, 2, 0, 1)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd.map(x =&gt; x+broadcastVar.value(x%3)).collect
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res27: Array[Int] = Array(3, 5, 4, 6, 8, 7, 9, 11, 10, 12)
</code></pre>
</div>
</div>
<div class="cell markdown">
<blockquote>
<p>After the broadcast variable is created, it should be used instead of the value v in any functions run on the cluster so that v is not shipped to the nodes more than once. In addition, the object v should not be modified after it is broadcast in order to ensure that all nodes get the same value of the broadcast variable (e.g. if the variable is shipped to a new node later).</p>
</blockquote>
<blockquote>
<p>To release the resources that the broadcast variable copied onto executors, call .unpersist(). If the broadcast is used again afterwards, it will be re-broadcast. To permanently release all resources used by the broadcast variable, call .destroy(). The broadcast variable can’t be used after that. Note that these methods do not block by default. To block until resources are freed, specify blocking=true when calling them.</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">broadcastVar.unpersist()
</code></pre>
</div>
<div class="cell markdown">
<h5 id="a-more-interesting-example-of-broadcast-variable"><a class="header" href="#a-more-interesting-example-of-broadcast-variable">A more interesting example of broadcast variable</a></h5>
<p>Let us broadcast maps and use them to lookup the values at each executor. This example is taken from: - <a href="https://sparkbyexamples.com/spark/spark-broadcast-variables/">https://sparkbyexamples.com/spark/spark-broadcast-variables/</a></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val states = Map((&quot;NY&quot;,&quot;New York&quot;),(&quot;CA&quot;,&quot;California&quot;),(&quot;FL&quot;,&quot;Florida&quot;))
val countries = Map((&quot;USA&quot;,&quot;United States of America&quot;),(&quot;IN&quot;,&quot;India&quot;))

val broadcastStates = spark.sparkContext.broadcast(states) // same as sc.broadcast
val broadcastCountries = spark.sparkContext.broadcast(countries)

val data = Seq((&quot;James&quot;,&quot;Smith&quot;,&quot;USA&quot;,&quot;CA&quot;),
    (&quot;Michael&quot;,&quot;Rose&quot;,&quot;USA&quot;,&quot;NY&quot;),
    (&quot;Robert&quot;,&quot;Williams&quot;,&quot;USA&quot;,&quot;CA&quot;),
    (&quot;Maria&quot;,&quot;Jones&quot;,&quot;USA&quot;,&quot;FL&quot;))

val rdd = spark.sparkContext.parallelize(data) // spark.sparkContext is the same as sc.parallelize in spark-shell/notebook

  val rdd2 = rdd.map(f =&gt; {
    val country = f._3
    val state = f._4
    val fullCountry = broadcastCountries.value.get(country).get
    val fullState = broadcastStates.value.get(state).get
    (f._1,f._2,fullCountry,fullState)
  })
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>states: scala.collection.immutable.Map[String,String] = Map(NY -&gt; New York, CA -&gt; California, FL -&gt; Florida)
countries: scala.collection.immutable.Map[String,String] = Map(USA -&gt; United States of America, IN -&gt; India)
broadcastStates: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Map[String,String]] = Broadcast(331)
broadcastCountries: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Map[String,String]] = Broadcast(332)
data: Seq[(String, String, String, String)] = List((James,Smith,USA,CA), (Michael,Rose,USA,NY), (Robert,Williams,USA,CA), (Maria,Jones,USA,FL))
rdd: org.apache.spark.rdd.RDD[(String, String, String, String)] = ParallelCollectionRDD[689] at parallelize at command-4088905069026304:12
rdd2: org.apache.spark.rdd.RDD[(String, String, String, String)] = MapPartitionsRDD[690] at map at command-4088905069026304:14
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(rdd2.collect().mkString(&quot;\n&quot;))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>(James,Smith,United States of America,California)
(Michael,Rose,United States of America,New York)
(Robert,Williams,United States of America,California)
(Maria,Jones,United States of America,Florida)
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="13-homework"><a class="header" href="#13-homework">13. HOMEWORK</a></h4>
<p>See the notebook in this folder named <code>005_RDDsTransformationsActionsHOMEWORK</code>. This notebook will give you more examples of the operations above as well as others we will be using later, including:</p>
<ul>
<li>Perform the <code>takeOrdered</code> action on the RDD</li>
<li>Transform the RDD by <code>distinct</code> to make another RDD and</li>
<li>Doing a bunch of transformations to our RDD and performing an action in a single cell.</li>
</ul>
</div>
<div class="cell markdown">
<hr />
<hr />
<h4 id="14-importing-standard-scala-and-java-libraries"><a class="header" href="#14-importing-standard-scala-and-java-libraries">14. Importing Standard Scala and Java libraries</a></h4>
<ul>
<li>For other libraries that are not available by default, you can upload other libraries to the Workspace.</li>
<li>Refer to the <strong><a href="https://docs.databricks.com/user-guide/libraries.html">Libraries</a></strong> guide for more details.</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import scala.math._
val x = min(1, 10)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import scala.math._
x: Int = 1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import java.util.HashMap
val map = new HashMap[String, Int]()
map.put(&quot;a&quot;, 1)
map.put(&quot;b&quot;, 2)
map.put(&quot;c&quot;, 3)
map.put(&quot;d&quot;, 4)
map.put(&quot;e&quot;, 5)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import java.util.HashMap
map: java.util.HashMap[String,Int] = {a=1, b=2, c=3, d=4, e=5}
res30: Int = 0
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="homework-on-rdd-transformations-and-actions"><a class="header" href="#homework-on-rdd-transformations-and-actions">HOMEWORK on RDD Transformations and Actions</a></h1>
<p>Just go through the notebook and familiarize yourself with these transformations and actions.</p>
</div>
<div class="cell markdown">
<ol>
<li>Perform the <code>takeOrdered</code> action on the RDD</li>
</ol>
<hr />
<p>To illustrate <code>take</code> and <code>takeOrdered</code> actions, let's create a bigger RDD named <code>rdd0_1000000</code> that is made up of a million integers from 0 to 1000000.
We will <code>sc.parallelize</code> the <code>Seq</code> Scala collection by using its <code>.range(startInteger,stopInteger)</code> method.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rdd0_1000000 = sc.parallelize(Seq.range(0, 1000000)) // &lt;Shift+Enter&gt; to create an RDD of million integers: 0,1,2,...,10^6
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rdd0_1000000: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[184] at parallelize at command-2971213210277291:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd0_1000000.take(5) // &lt;Ctrl+Enter&gt; gives the first 5 elements of the RDD, (0, 1, 2, 3, 4)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res0: Array[Int] = Array(0, 1, 2, 3, 4)
</code></pre>
</div>
</div>
<div class="cell markdown">
<p><code>takeordered(n)</code> returns <code>n</code> elements ordered in ascending order (by default) or as specified by the optional key function, as shown below.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd0_1000000.takeOrdered(5) // &lt;Shift+Enter&gt; is same as rdd0_1000000.take(5) 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res1: Array[Int] = Array(0, 1, 2, 3, 4)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd0_1000000.takeOrdered(5)(Ordering[Int].reverse) // &lt;Ctrl+Enter&gt; to get the last 5 elements of the RDD 999999, 999998, ..., 999995
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res2: Array[Int] = Array(999999, 999998, 999997, 999996, 999995)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// HOMEWORK: edit the numbers below to get the last 20 elements of an RDD made of a sequence of integers from 669966 to 969696
sc.parallelize(Seq.range(0, 10)).takeOrdered(5)(Ordering[Int].reverse) // &lt;Ctrl+Enter&gt; evaluate this cell after editing it for the right answer
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res3: Array[Int] = Array(9, 8, 7, 6, 5)
</code></pre>
</div>
</div>
<div class="cell markdown">
<ol start="2">
<li>More examples of <code>map</code></li>
</ol>
<hr />
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rdd = sc.parallelize(Seq(1, 2, 3, 4))    // &lt;Shift+Enter&gt; to evaluate this cell (using default number of partitions)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[186] at parallelize at command-2971213210277298:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd.map( x =&gt; x*2) // &lt;Ctrl+Enter&gt; to transform rdd by map that doubles each element
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res4: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[187] at map at command-2971213210277299:1
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>To see what's in the transformed RDD, let's perform the actions of <code>count</code> and <code>collect</code> on the <code>rdd.map( x =&gt; x*2)</code>, the transformation of <code>rdd</code> by the <code>map</code> given by the closure <code>x =&gt; x*2</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd.map( x =&gt; x*2).count()    // &lt;Shift+Enter&gt; to perform count (action) the element of the RDD = 4
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res5: Long = 4
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd.map( x =&gt; x*2).collect()    // &lt;Shift+Enter&gt; to perform collect (action) to show 2, 4, 6, 8
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res6: Array[Int] = Array(2, 4, 6, 8)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// HOMEWORK: uncomment the last line in this cell and modify the '&lt;Fill-In-Here&gt;' in the code below to collect and display the square (x*x) of each element of the RDD
// the answer should be Array[Int] = Array(1, 4, 9, 16) Press &lt;Cntrl+Enter&gt; to evaluate the cell after modifying '???'

//sc.parallelize(Seq(1, 2, 3, 4)).map( x =&gt; &lt;Fill-In-Here&gt; ).collect()
</code></pre>
</div>
<div class="cell markdown">
<ol start="3">
<li>More examples of <code>filter</code></li>
</ol>
<hr />
<p>Let's declare another <code>val</code> RDD named <code>rddFiltered</code> by transforming our first RDD named <code>rdd</code> via the <code>filter</code> transformation <code>x%2==0</code> (of being even).</p>
<p>This filter transformation based on the closure <code>x =&gt; x%2==0</code> will return <code>true</code> if the element, modulo two, equals zero. The closure is automatically passed on to the workers for evaluation (when an action is called later). So this will take our RDD of (1,2,3,4) and return RDD of (2, 4).</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rddFiltered = rdd.filter( x =&gt; x%2==0 )    // &lt;Ctrl+Enter&gt; to declare rddFiltered from transforming rdd
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rddFiltered: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[190] at filter at command-2971213210277305:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rddFiltered.collect()    // &lt;Ctrl+Enter&gt; to collect (action) elements of rddFiltered; should be (2, 4)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res8: Array[Int] = Array(2, 4)
</code></pre>
</div>
</div>
<div class="cell markdown">
<ol start="4">
<li>More examples of <code>reduce</code></li>
</ol>
<hr />
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rdd = sc.parallelize(Array(1,2,3,4,5))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[191] at parallelize at command-2971213210277308:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd.reduce( (x,y)=&gt;x+y ) // &lt;Shift+Enter&gt; to do reduce (action) to sum and return Int = 15
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res9: Int = 15
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd.reduce( _ + _ )    // &lt;Shift+Enter&gt; to do same sum as above and return Int = 15 (undescore syntax)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res10: Int = 15
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd.reduce( (x,y)=&gt;x*y ) // &lt;Shift+Enter&gt; to do reduce (action) to multiply and return Int = 120
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res11: Int = 120
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rdd0_1000000 = sc.parallelize(Seq.range(0, 1000000)) // &lt;Shift+Enter&gt; to create an RDD of million integers: 0,1,2,...,10^6
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rdd0_1000000: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[192] at parallelize at command-2971213210277312:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd0_1000000.reduce( (x,y)=&gt;x+y ) // &lt;Ctrl+Enter&gt; to do reduce (action) to sum and return Int 1783293664
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res12: Int = 1783293664
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// the following correctly returns Int = 0 although for wrong reason 
// we have flowed out of Int's numeric limits!!! (but got lucky with 0*x=0 for any Int x)
// &lt;Shift+Enter&gt; to do reduce (action) to multiply and return Int = 0
rdd0_1000000.reduce( (x,y)=&gt;x*y ) 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res13: Int = 0
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Ctrl+Enter&gt; to do reduce (action) to multiply 1*2*...*9*10 and return correct answer Int = 3628800
sc.parallelize(Seq.range(1, 11)).reduce( (x,y)=&gt;x*y ) 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res14: Int = 3628800
</code></pre>
</div>
</div>
<div class="cell markdown">
<p><strong>CAUTION: Know the limits of your numeric types!</strong></p>
</div>
<div class="cell markdown">
<p>The minimum and maximum value of <code>Int</code> and <code>Long</code> types are as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">(Int.MinValue , Int.MaxValue)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res15: (Int, Int) = (-2147483648,2147483647)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">(Long.MinValue, Long.MaxValue)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res16: (Long, Long) = (-9223372036854775808,9223372036854775807)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Ctrl+Enter&gt; to do reduce (action) to multiply 1*2*...*20 and return wrong answer as Int = -2102132736
//  we have overflowed out of Int's in a circle back to negative Ints!!! (rigorous distributed numerics, anyone?)
sc.parallelize(Seq.range(1, 21)).reduce( (x,y)=&gt;x*y ) 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res17: Int = -2102132736
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//&lt;Ctrl+Enter&gt; we can accomplish the multiplication using Long Integer types 
// by adding 'L' ro integer values, Scala infers that it is type Long
sc.parallelize(Seq.range(1L, 21L)).reduce( (x,y)=&gt;x*y ) 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res18: Long = 2432902008176640000
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>As the following products over Long Integers indicate, they are limited too!</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala"> // &lt;Shift+Enter&gt; for wrong answer Long = -8718968878589280256 (due to Long's numeric limits)
sc.parallelize(Seq.range(1L, 61L)).reduce( (x,y)=&gt;x*y )
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res19: Long = -8718968878589280256
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Cntrl+Enter&gt; for wrong answer Long = 0 (due to Long's numeric limits)
sc.parallelize(Seq.range(1L, 100L)).reduce( (x,y)=&gt;x*y ) 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res20: Long = 0
</code></pre>
</div>
</div>
<div class="cell markdown">
<hr />
<ol start="5">
<li>Let us do a bunch of transformations to our RDD and perform an action</li>
</ol>
<hr />
<ul>
<li>start from a Scala <code>Seq</code>,</li>
<li><code>sc.parallelize</code> the list to create an RDD,</li>
<li><code>filter</code> that RDD, creating a new filtered RDD,</li>
<li>do a <code>map</code> transformation that maps that RDD to a new mapped RDD,</li>
<li>and finally, perform a <code>reduce</code> action to sum the elements in the RDD.</li>
</ul>
<p>This last <code>reduce</code> action causes the <code>parallelize</code>, the <code>filter</code>, and the <code>map</code> transformations to actually be executed, and return a result back to the driver machine.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sc.parallelize(Seq(1, 2, 3, 4))    // &lt;Ctrl+Enter&gt; will return Array(4, 8)
  .filter(x =&gt; x%2==0)             // (2, 4) is the filtered RDD
  .map(x =&gt; x*2)                   // (4, 8) is the mapped RDD
  .reduce(_+_)                     // 4+8=12 is the final result from reduce
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res21: Int = 12
</code></pre>
</div>
</div>
<div class="cell markdown">
<ol start="6">
<li>Transform the RDD by <code>distinct</code> to make another RDD</li>
</ol>
<hr />
<p>Let's declare another RDD named <code>rdd2</code> that has some repeated elements to apply the <code>distinct</code> transformation to it. That would give us a new RDD that only contains the distinct elements of the input RDD.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rdd2 = sc.parallelize(Seq(4, 1, 3, 2, 2, 2, 3, 4))    // &lt;Ctrl+Enter&gt; to declare rdd2
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[201] at parallelize at command-2971213210277328:1
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let's apply the <code>distinct</code> transformation to <code>rdd2</code> and have it return a new RDD named <code>rdd2Distinct</code> that contains the distinct elements of the source RDD <code>rdd2</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rdd2Distinct = rdd2.distinct() // &lt;Ctrl+Enter&gt; transformation: distinct gives distinct elements of rdd2
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rdd2Distinct: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[204] at distinct at command-2971213210277330:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd2Distinct.collect()    // &lt;Ctrl+Enter&gt; to collect (action) as Array(4, 2, 1, 3)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res22: Array[Int] = Array(4, 2, 1, 3)
</code></pre>
</div>
</div>
<div class="cell markdown">
<ol start="7">
<li>more flatMap</li>
</ol>
<hr />
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rdd = sc. parallelize(Array(1,2,3)) // &lt;Shift+Enter&gt; to create an RDD of three Int elements 1,2,3
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[205] at parallelize at command-2971213210277333:1
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let us pass the <code>rdd</code> above to a map with a closure that will take in each element <code>x</code> and return <code>Array(x, x+5)</code>. So each element of the mapped RDD named <code>rddOfArrays</code> is an <code>Array[Int]</code>, an array of integers.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Shift+Enter&gt; to make RDD of Arrays, i.e., RDD[Array[int]]
val rddOfArrays = rdd.map( x =&gt; Array(x, x+5) ) 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rddOfArrays: org.apache.spark.rdd.RDD[Array[Int]] = MapPartitionsRDD[206] at map at command-2971213210277335:2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rddOfArrays.collect() // &lt;Ctrl+Enter&gt; to see it is RDD[Array[int]] = (Array(1, 6), Array(2, 7), Array(3, 8))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res23: Array[Array[Int]] = Array(Array(1, 6), Array(2, 7), Array(3, 8))
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Now let's observer what happens when we use <code>flatMap</code> to transform the same <code>rdd</code> and create another RDD called <code>rddfM</code>.</p>
<p>Interestingly, <code>flatMap</code> <em>flattens</em> our <code>rdd</code> by taking each <code>Array</code> (or sequence in general) and truning it into individual elements.</p>
<p>Thus, we end up with the RDD <code>rddfM</code> consisting of the elements (1, 6, 2, 7, 3, 8) as shown from the output of <code>rddfM.collect</code> below.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rddfM = rdd.flatMap(x =&gt; Array(x, x+5))    // &lt;Shift+Enter&gt; to flatMap the rdd using closure (x =&gt; Array(x, x+5))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rddfM: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[207] at flatMap at command-2971213210277338:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rddfM.collect    // &lt;Ctrl+Enter&gt; to collect rddfM = (1, 6, 2, 7, 3, 8)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res24: Array[Int] = Array(1, 6, 2, 7, 3, 8)
</code></pre>
</div>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="word-count-on-us-state-of-the-union-sou-addresses"><a class="header" href="#word-count-on-us-state-of-the-union-sou-addresses">Word Count on US State of the Union (SoU) Addresses</a></h1>
<ul>
<li>Word Count in big data is the equivalent of <code>Hello World</code> in programming</li>
<li>We count the number of occurences of each word in the first and last (2016) SoU addresses.</li>
</ul>
<p><strong>prerequisite</strong> see <strong>DO NOW</strong> below. You should have loaded data as instructed in <code>scalable-data-science/xtraResources/sdsDatasets</code>.</p>
<h4 id="do-now-if-not-done-already"><a class="header" href="#do-now-if-not-done-already">DO NOW (if not done already)</a></h4>
<p>In your databricks community edition:</p>
<ol>
<li>In your <code>WorkSpace</code> create a Folder named <code>scalable-data-science</code></li>
<li><code>Import</code> the databricks archive file at the following URL:
<ul>
<li><a href="https://github.com/lamastex/scalable-data-science/raw/master/dbcArchives/2017/parts/xtraResources.dbc">https://github.com/lamastex/scalable-data-science/raw/master/dbcArchives/2017/parts/xtraResources.dbc</a></li>
</ul>
</li>
<li>This should open a structure of directories in with path: <code>/Workspace/scalable-data-science/xtraResources/</code></li>
</ol>
</div>
<div class="cell markdown">
<p>An interesting analysis of the textual content of the <em>State of the Union (SoU)</em> addresses by all US presidents was done in:</p>
<ul>
<li><a href="http://www.pnas.org/content/112/35/10837.full">Alix Rule, Jean-Philippe Cointet, and Peter S. Bearman, Lexical shifts, substantive changes, and continuity in State of the Union discourse, 1790–2014, PNAS 2015 112 (35) 10837-10844; doi:10.1073/pnas.1512221112</a>.</li>
</ul>
<p><img src="https://www.pnas.org/cms/10.1073/pnas.1512221112/asset/52ec0970-5ebe-4119-a1d1-f6b4e83be604/assets/graphic/pnas.1512221112fig05.jpeg" alt="" /></p>
<p><a href="http://www.pnas.org/content/112/35/10837.full">Fig. 5</a>. A river network captures the flow across history of US political discourse, as perceived by contemporaries. Time moves along the x axis. Clusters on semantic networks of 300 most frequent terms for each of 10 historical periods are displayed as vertical bars. Relations between clusters of adjacent periods are indexed by gray flows, whose density reflects their degree of connection. Streams that connect at any point in history may be considered to be part of the same system, indicated with a single color.</p>
<h2 id="let-us-investigate-this-dataset-ourselves"><a class="header" href="#let-us-investigate-this-dataset-ourselves">Let us investigate this dataset ourselves!</a></h2>
<ol>
<li>We first get the source text data by scraping and parsing from <a href="http://stateoftheunion.onetwothree.net/texts/index.html">http://stateoftheunion.onetwothree.net/texts/index.html</a> as explained in <a href="contents/000_1-sds-3-x-spark//#workspace/scalable-data-science/xtraResources/sdsDatasets/scraperUSStateofUnionAddresses">scraping and parsing SoU addresses</a>.</li>
</ol>
<ul>
<li>This data is already made available in DBFS, our distributed file system.</li>
<li>We only do the simplest word count with this data in this notebook and will do more sophisticated analyses in the sequel (including topic modeling, etc).</li>
</ul>
</div>
<div class="cell markdown">
<h2 id="key-data-management-concepts"><a class="header" href="#key-data-management-concepts">Key Data Management Concepts</a></h2>
<h3 id="the-structure-spectrum"><a class="header" href="#the-structure-spectrum">The Structure Spectrum</a></h3>
<p>Let's peruse through the following blog from IBM to get an idea of structured, unstructured and semi-structured data:</p>
<ul>
<li><a href="https://www.ibm.com/cloud/blog/structured-vs-unstructured-data">https://www.ibm.com/cloud/blog/structured-vs-unstructured-data</a></li>
</ul>
<p>In this notebook we will be working with <strong>unstructured</strong> or <strong>schema-never</strong> data (plain text files).</p>
<p>Later on we will be working with structured or tabular data as well as semi-structured data.</p>
<hr />
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">//This allows easy embedding of publicly available information into any other notebook
//Example usage:
// displayHTML(frameIt(&quot;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Topics_in_LDA&quot;,250))
def frameIt( u:String, h:Int ) : String = {
      &quot;&quot;&quot;&lt;iframe 
 src=&quot;&quot;&quot;&quot;+ u+&quot;&quot;&quot;&quot;
 width=&quot;95%&quot; height=&quot;&quot;&quot;&quot; + h + &quot;&quot;&quot;&quot;
 sandbox&gt;
  &lt;p&gt;
    &lt;a href=&quot;http://spark.apache.org/docs/latest/index.html&quot;&gt;
      Fallback link for browsers that, unlikely, don't support frames
    &lt;/a&gt;
  &lt;/p&gt;
&lt;/iframe&gt;&quot;&quot;&quot;
   }
displayHTML(frameIt(&quot;https://www.ibm.com/cloud/blog/structured-vs-unstructured-data&quot;,700))
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://www.ibm.com/cloud/blog/structured-vs-unstructured-data"
 width="95%" height="700"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<h3 id="dbfs-and-dbutils---where-is-this-dataset-in-our-distributed-file-system"><a class="header" href="#dbfs-and-dbutils---where-is-this-dataset-in-our-distributed-file-system">DBFS and dbutils - where is this dataset in our distributed file system?</a></h3>
<ul>
<li>Since we are on the databricks cloud, it has a file system called DBFS</li>
<li>DBFS is similar to HDFS, the Hadoop distributed file system</li>
<li>dbutils allows us to interact with dbfs.</li>
<li>The 'display' command displays the list of files in a given directory in the file system.</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-fs">ls dbfs:/
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>path</th>
<th>name</th>
<th>size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dbfs:/.../</td>
<td>.../</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/FileStore/</td>
<td>FileStore/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/_checkpoint/</td>
<td>_checkpoint/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/databricks-datasets/</td>
<td>databricks-datasets/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/databricks-results/</td>
<td>databricks-results/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/</td>
<td>datasets/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/dbfs/</td>
<td>dbfs/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/files/</td>
<td>files/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/graphXcheckpointdir/</td>
<td>graphXcheckpointdir/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/local_disk0/</td>
<td>local_disk0/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/tmp/</td>
<td>tmp/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/trendcalculus/</td>
<td>trendcalculus/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/user/</td>
<td>user/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/uu-DS-projects/</td>
<td>uu-DS-projects/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/{DBFSPath}/</td>
<td>{DBFSPath}/</td>
<td>0.0</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-fs">ls dbfs:/datasets/sou
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>path</th>
<th>name</th>
<th>size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dbfs:/datasets/sou/17900108.txt</td>
<td>17900108.txt</td>
<td>6725.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/17901208.txt</td>
<td>17901208.txt</td>
<td>8427.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/17911025.txt</td>
<td>17911025.txt</td>
<td>14175.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/17921106.txt</td>
<td>17921106.txt</td>
<td>12736.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/17931203.txt</td>
<td>17931203.txt</td>
<td>11668.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/17941119.txt</td>
<td>17941119.txt</td>
<td>17615.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/17951208.txt</td>
<td>17951208.txt</td>
<td>12296.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/17961207.txt</td>
<td>17961207.txt</td>
<td>17340.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/17971122.txt</td>
<td>17971122.txt</td>
<td>12473.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/17981208.txt</td>
<td>17981208.txt</td>
<td>13394.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/17991203.txt</td>
<td>17991203.txt</td>
<td>9236.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18001111.txt</td>
<td>18001111.txt</td>
<td>8382.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18011208.txt</td>
<td>18011208.txt</td>
<td>19342.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18021215.txt</td>
<td>18021215.txt</td>
<td>13003.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18031017.txt</td>
<td>18031017.txt</td>
<td>14022.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18041108.txt</td>
<td>18041108.txt</td>
<td>12652.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18051203.txt</td>
<td>18051203.txt</td>
<td>17190.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18061202.txt</td>
<td>18061202.txt</td>
<td>17135.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18071027.txt</td>
<td>18071027.txt</td>
<td>14334.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18081108.txt</td>
<td>18081108.txt</td>
<td>16225.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18091129.txt</td>
<td>18091129.txt</td>
<td>11050.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18101205.txt</td>
<td>18101205.txt</td>
<td>15028.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18111105.txt</td>
<td>18111105.txt</td>
<td>13941.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18121104.txt</td>
<td>18121104.txt</td>
<td>19615.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18131207.txt</td>
<td>18131207.txt</td>
<td>19532.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18140920.txt</td>
<td>18140920.txt</td>
<td>12632.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18151205.txt</td>
<td>18151205.txt</td>
<td>19398.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18161203.txt</td>
<td>18161203.txt</td>
<td>20331.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18171212.txt</td>
<td>18171212.txt</td>
<td>26236.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18181116.txt</td>
<td>18181116.txt</td>
<td>26445.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18191207.txt</td>
<td>18191207.txt</td>
<td>27880.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18201114.txt</td>
<td>18201114.txt</td>
<td>20503.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18211203.txt</td>
<td>18211203.txt</td>
<td>34364.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18221203.txt</td>
<td>18221203.txt</td>
<td>28154.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18231202.txt</td>
<td>18231202.txt</td>
<td>38329.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18241207.txt</td>
<td>18241207.txt</td>
<td>49869.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18251206.txt</td>
<td>18251206.txt</td>
<td>53992.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18261205.txt</td>
<td>18261205.txt</td>
<td>46482.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18271204.txt</td>
<td>18271204.txt</td>
<td>42481.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18281202.txt</td>
<td>18281202.txt</td>
<td>44202.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18291208.txt</td>
<td>18291208.txt</td>
<td>62923.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18301206.txt</td>
<td>18301206.txt</td>
<td>90641.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18311206.txt</td>
<td>18311206.txt</td>
<td>42902.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18321204.txt</td>
<td>18321204.txt</td>
<td>46879.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18331203.txt</td>
<td>18331203.txt</td>
<td>46991.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18341201.txt</td>
<td>18341201.txt</td>
<td>80364.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18351207.txt</td>
<td>18351207.txt</td>
<td>64395.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18361205.txt</td>
<td>18361205.txt</td>
<td>73306.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18371205.txt</td>
<td>18371205.txt</td>
<td>68927.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18381203.txt</td>
<td>18381203.txt</td>
<td>69880.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18391202.txt</td>
<td>18391202.txt</td>
<td>80147.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18401205.txt</td>
<td>18401205.txt</td>
<td>55025.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18411207.txt</td>
<td>18411207.txt</td>
<td>48792.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18421206.txt</td>
<td>18421206.txt</td>
<td>49788.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18431206.txt</td>
<td>18431206.txt</td>
<td>47670.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18441203.txt</td>
<td>18441203.txt</td>
<td>55494.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18451202.txt</td>
<td>18451202.txt</td>
<td>95894.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18461208.txt</td>
<td>18461208.txt</td>
<td>107852.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18471207.txt</td>
<td>18471207.txt</td>
<td>96912.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18481205.txt</td>
<td>18481205.txt</td>
<td>127557.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18491204.txt</td>
<td>18491204.txt</td>
<td>46003.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18501202.txt</td>
<td>18501202.txt</td>
<td>49823.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18511202.txt</td>
<td>18511202.txt</td>
<td>79335.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18521206.txt</td>
<td>18521206.txt</td>
<td>59438.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18531205.txt</td>
<td>18531205.txt</td>
<td>58031.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18541204.txt</td>
<td>18541204.txt</td>
<td>61917.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18551231.txt</td>
<td>18551231.txt</td>
<td>70459.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18561202.txt</td>
<td>18561202.txt</td>
<td>63906.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18571208.txt</td>
<td>18571208.txt</td>
<td>82051.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18581206.txt</td>
<td>18581206.txt</td>
<td>98523.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18591219.txt</td>
<td>18591219.txt</td>
<td>74089.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18601203.txt</td>
<td>18601203.txt</td>
<td>84283.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18611203.txt</td>
<td>18611203.txt</td>
<td>41587.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18621201.txt</td>
<td>18621201.txt</td>
<td>50008.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18631208.txt</td>
<td>18631208.txt</td>
<td>37109.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18641206.txt</td>
<td>18641206.txt</td>
<td>36201.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18651204.txt</td>
<td>18651204.txt</td>
<td>54781.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18661203.txt</td>
<td>18661203.txt</td>
<td>44152.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18671203.txt</td>
<td>18671203.txt</td>
<td>71650.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18681209.txt</td>
<td>18681209.txt</td>
<td>60650.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18691206.txt</td>
<td>18691206.txt</td>
<td>46099.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18701205.txt</td>
<td>18701205.txt</td>
<td>52113.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18711204.txt</td>
<td>18711204.txt</td>
<td>38805.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18721202.txt</td>
<td>18721202.txt</td>
<td>23984.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18731201.txt</td>
<td>18731201.txt</td>
<td>60406.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18741207.txt</td>
<td>18741207.txt</td>
<td>55136.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18751207.txt</td>
<td>18751207.txt</td>
<td>73272.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18761205.txt</td>
<td>18761205.txt</td>
<td>40873.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18771203.txt</td>
<td>18771203.txt</td>
<td>48620.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18781202.txt</td>
<td>18781202.txt</td>
<td>48552.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18791201.txt</td>
<td>18791201.txt</td>
<td>71149.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18801206.txt</td>
<td>18801206.txt</td>
<td>41294.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18811206.txt</td>
<td>18811206.txt</td>
<td>24189.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18821204.txt</td>
<td>18821204.txt</td>
<td>19065.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18831204.txt</td>
<td>18831204.txt</td>
<td>23860.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18841201.txt</td>
<td>18841201.txt</td>
<td>55230.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18851208.txt</td>
<td>18851208.txt</td>
<td>121030.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18861206.txt</td>
<td>18861206.txt</td>
<td>92873.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18871206.txt</td>
<td>18871206.txt</td>
<td>31685.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18881203.txt</td>
<td>18881203.txt</td>
<td>55460.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18891203.txt</td>
<td>18891203.txt</td>
<td>77944.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18901201.txt</td>
<td>18901201.txt</td>
<td>69588.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18911209.txt</td>
<td>18911209.txt</td>
<td>96894.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18921206.txt</td>
<td>18921206.txt</td>
<td>81825.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18931203.txt</td>
<td>18931203.txt</td>
<td>76786.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18941202.txt</td>
<td>18941202.txt</td>
<td>97793.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18951207.txt</td>
<td>18951207.txt</td>
<td>89791.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18961204.txt</td>
<td>18961204.txt</td>
<td>94943.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18971206.txt</td>
<td>18971206.txt</td>
<td>72748.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18981205.txt</td>
<td>18981205.txt</td>
<td>123819.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18991205.txt</td>
<td>18991205.txt</td>
<td>93175.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19001203.txt</td>
<td>19001203.txt</td>
<td>118487.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19011203.txt</td>
<td>19011203.txt</td>
<td>115838.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19021202.txt</td>
<td>19021202.txt</td>
<td>57671.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19031207.txt</td>
<td>19031207.txt</td>
<td>90262.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19041206.txt</td>
<td>19041206.txt</td>
<td>104031.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19051205.txt</td>
<td>19051205.txt</td>
<td>147449.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19061203.txt</td>
<td>19061203.txt</td>
<td>138165.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19071203.txt</td>
<td>19071203.txt</td>
<td>161983.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19081208.txt</td>
<td>19081208.txt</td>
<td>115609.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19091207.txt</td>
<td>19091207.txt</td>
<td>84749.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19101206.txt</td>
<td>19101206.txt</td>
<td>42598.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19111205.txt</td>
<td>19111205.txt</td>
<td>143491.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19121203.txt</td>
<td>19121203.txt</td>
<td>153124.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19131202.txt</td>
<td>19131202.txt</td>
<td>20536.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19141208.txt</td>
<td>19141208.txt</td>
<td>25441.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19151207.txt</td>
<td>19151207.txt</td>
<td>44773.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19161205.txt</td>
<td>19161205.txt</td>
<td>12773.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19171204.txt</td>
<td>19171204.txt</td>
<td>22077.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19181202.txt</td>
<td>19181202.txt</td>
<td>31400.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19191202.txt</td>
<td>19191202.txt</td>
<td>28511.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19201207.txt</td>
<td>19201207.txt</td>
<td>16119.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19211206.txt</td>
<td>19211206.txt</td>
<td>34334.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19221208.txt</td>
<td>19221208.txt</td>
<td>35419.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19231206.txt</td>
<td>19231206.txt</td>
<td>41144.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19241203.txt</td>
<td>19241203.txt</td>
<td>42503.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19251208.txt</td>
<td>19251208.txt</td>
<td>66289.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19261207.txt</td>
<td>19261207.txt</td>
<td>62608.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19271206.txt</td>
<td>19271206.txt</td>
<td>54125.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19281204.txt</td>
<td>19281204.txt</td>
<td>50110.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19291203.txt</td>
<td>19291203.txt</td>
<td>68959.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19301202.txt</td>
<td>19301202.txt</td>
<td>29041.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19311208.txt</td>
<td>19311208.txt</td>
<td>36649.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19321206.txt</td>
<td>19321206.txt</td>
<td>26421.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19340103.txt</td>
<td>19340103.txt</td>
<td>13545.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19350104.txt</td>
<td>19350104.txt</td>
<td>21221.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19360103.txt</td>
<td>19360103.txt</td>
<td>22300.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19370106.txt</td>
<td>19370106.txt</td>
<td>16738.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19380103.txt</td>
<td>19380103.txt</td>
<td>28069.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19390104.txt</td>
<td>19390104.txt</td>
<td>22563.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19400103.txt</td>
<td>19400103.txt</td>
<td>18722.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19410106.txt</td>
<td>19410106.txt</td>
<td>19386.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19420106.txt</td>
<td>19420106.txt</td>
<td>19911.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19430107.txt</td>
<td>19430107.txt</td>
<td>26314.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19440111.txt</td>
<td>19440111.txt</td>
<td>22151.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19450106.txt</td>
<td>19450106.txt</td>
<td>48891.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19460121.txt</td>
<td>19460121.txt</td>
<td>174651.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19470106.txt</td>
<td>19470106.txt</td>
<td>37406.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19480107.txt</td>
<td>19480107.txt</td>
<td>30550.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19490105.txt</td>
<td>19490105.txt</td>
<td>20792.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19500104.txt</td>
<td>19500104.txt</td>
<td>30423.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19510108.txt</td>
<td>19510108.txt</td>
<td>22924.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19520109.txt</td>
<td>19520109.txt</td>
<td>30228.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19530107.txt</td>
<td>19530107.txt</td>
<td>56767.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19530202.txt</td>
<td>19530202.txt</td>
<td>43620.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19540107.txt</td>
<td>19540107.txt</td>
<td>37843.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19550106.txt</td>
<td>19550106.txt</td>
<td>46532.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19560105.txt</td>
<td>19560105.txt</td>
<td>52138.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19570110.txt</td>
<td>19570110.txt</td>
<td>25846.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19580109.txt</td>
<td>19580109.txt</td>
<td>30344.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19590109.txt</td>
<td>19590109.txt</td>
<td>30145.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19600107.txt</td>
<td>19600107.txt</td>
<td>35099.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19610112.txt</td>
<td>19610112.txt</td>
<td>40396.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19610130.txt</td>
<td>19610130.txt</td>
<td>31641.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19620111.txt</td>
<td>19620111.txt</td>
<td>39488.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19630114.txt</td>
<td>19630114.txt</td>
<td>31666.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19640108.txt</td>
<td>19640108.txt</td>
<td>18659.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19650104.txt</td>
<td>19650104.txt</td>
<td>25389.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19660112.txt</td>
<td>19660112.txt</td>
<td>30570.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19670110.txt</td>
<td>19670110.txt</td>
<td>41668.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19680117.txt</td>
<td>19680117.txt</td>
<td>28834.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19690114.txt</td>
<td>19690114.txt</td>
<td>23634.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19700122.txt</td>
<td>19700122.txt</td>
<td>25408.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19710122.txt</td>
<td>19710122.txt</td>
<td>25793.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19720120.txt</td>
<td>19720120.txt</td>
<td>23099.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19730202.txt</td>
<td>19730202.txt</td>
<td>9844.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19740130.txt</td>
<td>19740130.txt</td>
<td>29231.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19750115.txt</td>
<td>19750115.txt</td>
<td>24801.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19760119.txt</td>
<td>19760119.txt</td>
<td>29731.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19770112.txt</td>
<td>19770112.txt</td>
<td>27923.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19780119.txt</td>
<td>19780119.txt</td>
<td>26564.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19790125.txt</td>
<td>19790125.txt</td>
<td>19544.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19800121.txt</td>
<td>19800121.txt</td>
<td>20124.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19810116.txt</td>
<td>19810116.txt</td>
<td>217980.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19820126.txt</td>
<td>19820126.txt</td>
<td>31166.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19830125.txt</td>
<td>19830125.txt</td>
<td>33255.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19840125.txt</td>
<td>19840125.txt</td>
<td>29705.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19850206.txt</td>
<td>19850206.txt</td>
<td>25364.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19860204.txt</td>
<td>19860204.txt</td>
<td>20449.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19870127.txt</td>
<td>19870127.txt</td>
<td>22334.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19880125.txt</td>
<td>19880125.txt</td>
<td>28565.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19890209.txt</td>
<td>19890209.txt</td>
<td>27855.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19900131.txt</td>
<td>19900131.txt</td>
<td>21434.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19910129.txt</td>
<td>19910129.txt</td>
<td>22433.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19920128.txt</td>
<td>19920128.txt</td>
<td>26644.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19930217.txt</td>
<td>19930217.txt</td>
<td>39255.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19940125.txt</td>
<td>19940125.txt</td>
<td>42320.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19950124.txt</td>
<td>19950124.txt</td>
<td>51325.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19960123.txt</td>
<td>19960123.txt</td>
<td>36386.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19970204.txt</td>
<td>19970204.txt</td>
<td>39038.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19980127.txt</td>
<td>19980127.txt</td>
<td>42255.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19990119.txt</td>
<td>19990119.txt</td>
<td>43592.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/20000127.txt</td>
<td>20000127.txt</td>
<td>44244.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/20010227.txt</td>
<td>20010227.txt</td>
<td>25330.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/20010920.txt</td>
<td>20010920.txt</td>
<td>17383.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/20020129.txt</td>
<td>20020129.txt</td>
<td>22653.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/20030128.txt</td>
<td>20030128.txt</td>
<td>31878.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/20040120.txt</td>
<td>20040120.txt</td>
<td>30611.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/20050202.txt</td>
<td>20050202.txt</td>
<td>29875.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/20060131.txt</td>
<td>20060131.txt</td>
<td>31449.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/20070123.txt</td>
<td>20070123.txt</td>
<td>31998.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/20080128.txt</td>
<td>20080128.txt</td>
<td>33830.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/20090224.txt</td>
<td>20090224.txt</td>
<td>33640.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/20100127.txt</td>
<td>20100127.txt</td>
<td>40980.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/20110125.txt</td>
<td>20110125.txt</td>
<td>39582.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/20120124.txt</td>
<td>20120124.txt</td>
<td>40338.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/20130212.txt</td>
<td>20130212.txt</td>
<td>37815.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/20140128.txt</td>
<td>20140128.txt</td>
<td>39625.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/20150120.txt</td>
<td>20150120.txt</td>
<td>38528.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/20160112.txt</td>
<td>20160112.txt</td>
<td>31083.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/20170228.txt</td>
<td>20170228.txt</td>
<td>29323.0</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell markdown">
<p>Let us display the <em>head</em> or the first few lines of the file <code>dbfs:/datasets/sou/17900108.txt</code> to see what it contains using <code>dbutils.fs.head</code> method.
<code>head(file: String, maxBytes: int = 65536): String</code> -&gt; Returns up to the first 'maxBytes' bytes of the given file as a String encoded in UTF-8 as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-fs">head --maxBytes=673 dbfs:/datasets/sou/17900108.txt 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>[Truncated to first 673 bytes]
George Washington 

January 8, 1790 
Fellow-Citizens of the Senate and House of Representatives: 
I embrace with great satisfaction the opportunity which now presents itself of congratulating you on the present favorable prospects of our public affairs. The recent accession of the important state of North Carolina to the Constitution of the United States (of which official information has been received), the rising credit and respectability of our country, the general and increasing good will toward the government of the Union, and the concord, peace, and plenty with which we are blessed are circumstances auspicious in an eminent degree to our national prosperity. 
</code></pre>
</div>
</div>
<div class="cell markdown">
<h5 id="you-try-3"><a class="header" href="#you-try-3">You Try!</a></h5>
<p>Uncomment and modify <code>xxxx</code> in the cell below to read the first 1000 bytes from the file.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//%fs
//head --maxBytes=xxxx dbfs:/datasets/sou/17900108.txt
</code></pre>
</div>
<div class="cell markdown">
<h3 id="read-the-file-into-spark-context-as-an-rdd-of-strings"><a class="header" href="#read-the-file-into-spark-context-as-an-rdd-of-strings">Read the file into Spark Context as an RDD of Strings</a></h3>
<ul>
<li>The <code>textFile</code> method on the available <code>SparkContext</code> <code>sc</code> can read the text file <code>dbfs:/datasets/sou/17900108.txt</code> into Spark and create an RDD of Strings
<ul>
<li>but this is done lazily until an action is taken on the RDD <code>sou17900108</code>!</li>
</ul>
</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val sou17900108 = sc.textFile(&quot;dbfs:/datasets/sou/17900108.txt&quot;) // Cntrl+Enter to read in the textfile as RDD[String]
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>sou17900108: org.apache.spark.rdd.RDD[String] = dbfs:/datasets/sou/17900108.txt MapPartitionsRDD[744] at textFile at command-4088905069026012:1
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="perform-some-actions-on-the-rdd"><a class="header" href="#perform-some-actions-on-the-rdd">Perform some actions on the RDD</a></h3>
<ul>
<li>Each String in the RDD <code>sou17900108</code> represents one line of data from the file and can be made to perform one of the following actions:
<ul>
<li>count the number of elements in the RDD <code>sou17900108</code> (i.e., the number of lines in the text file <code>dbfs:/datasets/sou/17900108.txt</code>) using <code>sou17900108.count()</code></li>
<li>display the contents of the RDD using <code>take</code> or <code>collect</code>.</li>
</ul>
</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou17900108.count() // &lt;Shift+Enter&gt; to count the number of elements in the RDD
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res3: Long = 23
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou17900108.take(5) // &lt;Shift+Enter&gt; to display the first 5 elements of RDD
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res4: Array[String] = Array(&quot;George Washington &quot;, &quot;&quot;, &quot;January 8, 1790 &quot;, &quot;Fellow-Citizens of the Senate and House of Representatives: &quot;, &quot;I embrace with great satisfaction the opportunity which now presents itself of congratulating you on the present favorable prospects of our public affairs. The recent accession of the important state of North Carolina to the Constitution of the United States (of which official information has been received), the rising credit and respectability of our country, the general and increasing good will toward the government of the Union, and the concord, peace, and plenty with which we are blessed are circumstances auspicious in an eminent degree to our national prosperity. &quot;)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou17900108.take(5).foreach(println) // &lt;Shift+Enter&gt; to display the first 5 elements of RDD line by line
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>George Washington 

January 8, 1790 
Fellow-Citizens of the Senate and House of Representatives: 
I embrace with great satisfaction the opportunity which now presents itself of congratulating you on the present favorable prospects of our public affairs. The recent accession of the important state of North Carolina to the Constitution of the United States (of which official information has been received), the rising credit and respectability of our country, the general and increasing good will toward the government of the Union, and the concord, peace, and plenty with which we are blessed are circumstances auspicious in an eminent degree to our national prosperity. 
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou17900108.collect // &lt;Cntrl+Enter&gt; to display all the elements of RDD
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res6: Array[String] = Array(&quot;George Washington &quot;, &quot;&quot;, &quot;January 8, 1790 &quot;, &quot;Fellow-Citizens of the Senate and House of Representatives: &quot;, &quot;I embrace with great satisfaction the opportunity which now presents itself of congratulating you on the present favorable prospects of our public affairs. The recent accession of the important state of North Carolina to the Constitution of the United States (of which official information has been received), the rising credit and respectability of our country, the general and increasing good will toward the government of the Union, and the concord, peace, and plenty with which we are blessed are circumstances auspicious in an eminent degree to our national prosperity. &quot;, &quot;In resuming your consultations for the general good you can not but derive encouragement from the reflection that the measures of the last session have been as satisfactory to your constituents as the novelty and difficulty of the work allowed you to hope. Still further to realize their expectations and to secure the blessings which a gracious Providence has placed within our reach will in the course of the present important session call for the cool and deliberate exertion of your patriotism, firmness, and wisdom. &quot;, &quot;Among the many interesting objects which will engage your attention that of providing for the common defense will merit particular regard. To be prepared for war is one of the most effectual means of preserving peace. &quot;, &quot;A free people ought not only to be armed, but disciplined; to which end a uniform and well-digested plan is requisite; and their safety and interest require that they should promote such manufactories as tend to render them independent of others for essential, particularly military, supplies. &quot;, &quot;The proper establishment of the troops which may be deemed indispensable will be entitled to mature consideration. In the arrangements which may be made respecting it it will be of importance to conciliate the comfortable support of the officers and soldiers with a due regard to economy. &quot;, &quot;There was reason to hope that the pacific measures adopted with regard to certain hostile tribes of Indians would have relieved the inhabitants of our southern and western frontiers from their depredations, but you will perceive from the information contained in the papers which I shall direct to be laid before you (comprehending a communication from the Commonwealth of Virginia) that we ought to be prepared to afford protection to those parts of the Union, and, if necessary, to punish aggressors. &quot;, &quot;The interests of the United States require that our intercourse with other nations should be facilitated by such provisions as will enable me to fulfill my duty in that respect in the manner which circumstances may render most conducive to the public good, and to this end that the compensation to be made to the persons who may be employed should, according to the nature of their appointments, be defined by law, and a competent fund designated for defraying the expenses incident to the conduct of foreign affairs. &quot;, &quot;Various considerations also render it expedient that the terms on which foreigners may be admitted to the rights of citizens should be speedily ascertained by a uniform rule of naturalization. &quot;, &quot;Uniformity in the currency, weights, and measures of the United States is an object of great importance, and will, I am persuaded, be duly attended to. &quot;, &quot;The advancement of agriculture, commerce, and manufactures by all proper means will not, I trust, need recommendation; but I can not forbear intimating to you the expediency of giving effectual encouragement as well to the introduction of new and useful inventions from abroad as to the exertions of skill and genius in producing them at home, and of facilitating the intercourse between the distant parts of our country by a due attention to the post-office and post-roads. &quot;, &quot;Nor am I less persuaded that you will agree with me in opinion that there is nothing which can better deserve your patronage than the promotion of science and literature. Knowledge is in every country the surest basis of public happiness. In one in which the measures of government receive their impressions so immediately from the sense of the community as in ours it is proportionably essential. &quot;, &quot;To the security of a free constitution it contributes in various ways--by convincing those who are intrusted with the public administration that every valuable end of government is best answered by the enlightened confidence of the people, and by teaching the people themselves to know and to value their own rights; to discern and provide against invasions of them; to distinguish between oppression and the necessary exercise of lawful authority; between burthens proceeding from a disregard to their convenience and those resulting from the inevitable exigencies of society; to discriminate the spirit of liberty from that of licentiousness-- cherishing the first, avoiding the last--and uniting a speedy but temperate vigilance against encroachments, with an inviolable respect to the laws. &quot;, &quot;Whether this desirable object will be best promoted by affording aids to seminaries of learning already established, by the institution of a national university, or by any other expedients will be well worthy of a place in the deliberations of the legislature. &quot;, &quot;Gentlemen of the House of Representatives: &quot;, &quot;I saw with peculiar pleasure at the close of the last session the resolution entered into by you expressive of your opinion that an adequate provision for the support of the public credit is a matter of high importance to the national honor and prosperity. In this sentiment I entirely concur; and to a perfect confidence in your best endeavors to devise such a provision as will be truly with the end I add an equal reliance on the cheerful cooperation of the other branch of the legislature. &quot;, &quot;It would be superfluous to specify inducements to a measure in which the character and interests of the United States are so obviously so deeply concerned, and which has received so explicit a sanction from your declaration. &quot;, &quot;Gentlemen of the Senate and House of Representatives: &quot;, &quot;I have directed the proper officers to lay before you, respectively, such papers and estimates as regard the affairs particularly recommended to your consideration, and necessary to convey to you that information of the state of the Union which it is my duty to afford. &quot;, The welfare of our country is the great object to which our cares and efforts ought to be directed, and I shall derive great satisfaction from a cooperation with you in the pleasing though arduous task of insuring to our fellow citizens the blessings which they have a right to expect from a free, efficient, and equal government.)
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="cache-the-rdd-in-distributed-memory-to-avoid-recreating-it-for-each-action"><a class="header" href="#cache-the-rdd-in-distributed-memory-to-avoid-recreating-it-for-each-action">Cache the RDD in (distributed) memory to avoid recreating it for each action</a></h3>
<ul>
<li>Above, every time we took an action on the same RDD, the RDD was reconstructed from the textfile.
<ul>
<li>Spark's advantage compared to Hadoop MapReduce is the ability to cache or store the RDD in distributed memory across the nodes.</li>
</ul>
</li>
<li>Let's use <code>.cache()</code> after creating an RDD so that it is in memory after the first action (and thus avoid reconstruction for subsequent actions).
<ul>
<li>count the number of elements in the RDD <code>sou17900108</code> (i.e., the number of lines in the text file <code>dbfs:/datasets/sou/17900108.txt</code>) using <code>sou17900108.count()</code></li>
<li>display the contents of the RDD using <code>take</code> or <code>collect</code>.</li>
</ul>
</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Shift+Enter to read in the textfile as RDD[String] and cache it in distributed memory
val sou17900108 = sc.textFile(&quot;dbfs:/datasets/sou/17900108.txt&quot;)
sou17900108.cache() // cache the RDD in memory
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>sou17900108: org.apache.spark.rdd.RDD[String] = dbfs:/datasets/sou/17900108.txt MapPartitionsRDD[750] at textFile at command-4088905069026019:2
res7: sou17900108.type = dbfs:/datasets/sou/17900108.txt MapPartitionsRDD[750] at textFile at command-4088905069026019:2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou17900108.count() // Shift+Enter during this count action the RDD is constructed from texfile and cached
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res8: Long = 23
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou17900108.count() // Shift+Enter during this count action the cached RDD is used (notice less time taken by the same command)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res9: Long = 23
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou17900108.take(5) // &lt;Cntrl+Enter&gt; to display the first 5 elements of the cached RDD
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res10: Array[String] = Array(&quot;George Washington &quot;, &quot;&quot;, &quot;January 8, 1790 &quot;, &quot;Fellow-Citizens of the Senate and House of Representatives: &quot;, &quot;I embrace with great satisfaction the opportunity which now presents itself of congratulating you on the present favorable prospects of our public affairs. The recent accession of the important state of North Carolina to the Constitution of the United States (of which official information has been received), the rising credit and respectability of our country, the general and increasing good will toward the government of the Union, and the concord, peace, and plenty with which we are blessed are circumstances auspicious in an eminent degree to our national prosperity. &quot;)
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="lifecycle-of-a-spark-program---summary"><a class="header" href="#lifecycle-of-a-spark-program---summary">Lifecycle of a Spark Program - Summary</a></h4>
<ul>
<li>create RDDs from:
<ul>
<li>some external data source (such as a distributed file system)</li>
<li>parallelized collection in your driver program</li>
</ul>
</li>
<li>lazily transform these RDDs into new RDDs</li>
<li>cache some of those RDDs for future reuse</li>
<li>you perform actions to execute parallel computation to produce results</li>
</ul>
</div>
<div class="cell markdown">
<h3 id="transform-lines-to-words"><a class="header" href="#transform-lines-to-words">Transform lines to words</a></h3>
<ul>
<li>We need to loop through each line and split the line into words</li>
<li>For now, let us split using whitespace</li>
<li>More sophisticated regular expressions can be used to split the line (as we will see soon)</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou17900108
.flatMap(line =&gt; line.split(&quot; &quot;))
.take(100)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res12: Array[String] = Array(George, Washington, &quot;&quot;, January, 8,, 1790, Fellow-Citizens, of, the, Senate, and, House, of, Representatives:, I, embrace, with, great, satisfaction, the, opportunity, which, now, presents, itself, of, congratulating, you, on, the, present, favorable, prospects, of, our, public, affairs., The, recent, accession, of, the, important, state, of, North, Carolina, to, the, Constitution, of, the, United, States, (of, which, official, information, has, been, received),, the, rising, credit, and, respectability, of, our, country,, the, general, and, increasing, good, will, toward, the, government, of, the, Union,, and, the, concord,, peace,, and, plenty, with, which, we, are, blessed, are, circumstances, auspicious, in, an, eminent, degree, to)
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="naive-word-count"><a class="header" href="#naive-word-count">Naive word count</a></h3>
<p>At a first glace, to do a word count of George Washingtons SoU address, we are templed to do the following:</p>
<ul>
<li>just break each line by the whitespace character &quot; &quot; and find the words using a <code>flatMap</code></li>
<li>then do the <code>map</code> with the closure <code>word =&gt; (word, 1)</code> to initialize each <code>word</code> with a integer count of <code>1</code>
<ul>
<li>ie., transform each word to a <em>(key, value)</em> pair or <code>Tuple</code> such as <code>(word, 1)</code></li>
</ul>
</li>
<li>then count all <em>value</em>s with the same <em>key</em> (<code>word</code> is the Key in our case) by doing a
<ul>
<li><code>reduceByKey(_+_)</code></li>
</ul>
</li>
<li>and finally <code>collect()</code> to display the results.</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou17900108
.flatMap( line =&gt; line.split(&quot; &quot;) )
.map( word =&gt; (word, 1) )
.reduceByKey(_+_)
.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res13: Array[(String, Int)] = Array((call,1), (country,3), (House,3), (promoted,1), (admitted,1), (agree,1), (accession,1), (exertion,1), (plenty,1), (have,4), (incident,1), (consideration,,1), (session,3), (national,3), (equal,2), (we,2), (intimating,1), (been,2), (who,2), (eminent,1), (any,1), (immediately,1), (essential.,1), (western,1), (speedy,1), (institution,1), (respect,2), (me,2), (peace.,1), (frontiers,1), (free,2), (parts,2), (are,4), (blessings,2), (8,,1), (authority;,1), (presents,1), (affairs,1), (discriminate,1), (expressive,1), (administration,1), (introduction,1), (comfortable,1), (our,10), (as,9), (intrusted,1), (circumstances,2), (peace,,1), (respectability,1), (contributes,1), (branch,1), (better,1), (them,2), (independent,1), (proceeding,1), (duty,2), (law,,1), (foreigners,1), (satisfactory,1), (is,10), (convey,1), (appointments,,1), (favorable,1), (Senate,2), (am,2), (certain,1), (shall,2), (Commonwealth,1), (Virginia),1), (proper,3), (States,4), (recommendation;,1), (impressions,1), (sense,1), (they,2), (new,1), (my,2), (rising,1), (expedient,1), (hope.,1), (uniting,1), (oppression,1), (free,,1), (now,1), (due,2), (has,3), (university,,1), (deserve,1), (licentiousness--,1), (safety,1), (degree,1), (persons,1), (giving,1), (learning,1), (depredations,,1), (Washington,1), (conducive,1), (according,1), (need,1), (manufactures,1), (render,3), (invasions,1), (honor,1), (fulfill,1), (Still,1), (directed,1), (basis,1), (southern,1), (conduct,1), (exigencies,1), (well-digested,1), (objects,1), (Indians,1), (truly,1), (cares,1), (foreign,1), (welfare,1), (consultations,1), (resolution,1), (means,2), (cherishing,1), (this,3), (convincing,1), (deemed,1), (right,1), (There,1), (themselves,1), (general,2), (entirely,1), (explicit,1), (defense,1), (only,1), (importance,,1), (opinion,2), (security,1), (exercise,1), (Knowledge,1), (already,1), (established,,1), (particularly,2), (satisfaction,2), (realize,1), (afford.,1), (rule,1), (cheerful,1), (nations,1), (measure,1), (congratulating,1), (hope,1), (can,3), (resuming,1), (relieved,1), (country,,1), (communication,1), (will,,1), (aggressors.,1), (into,1), (there,1), (science,1), (hostile,1), (rights;,1), (trust,,1), (discern,1), (lay,1), (own,1), (reason,1), (Among,1), (directed,,1), (declaration.,1), (essential,,1), (patriotism,,1), (high,1), (mature,1), (laid,1), (compensation,1), (surest,1), (advancement,1), (respecting,1), (consideration.,1), (one,2), (with,11), (obviously,1), (first,,1), (January,1), (best,3), (importance,2), (interesting,1), (seminaries,1), (post-roads.,1), (proportionably,1), (duly,1), (attention,2), (promote,1), (economy.,1), (afford,1), (Representatives:,3), (from,12), (other,3), (interest,1), (affairs.,2), (well,2), (close,1), (further,1), (received),,1), (facilitated,1), (requisite;,1), (affording,1), (allowed,1), (their,7), (concord,,1), (adequate,1), (last,2), (expediency,1), (between,3), (will,13), (information,3), (useful,1), (valuable,1), (&quot;&quot;,1), (confidence,2), (war,1), (provisions,1), (designated,1), (providing,1), (important,2), (encroachments,,1), (uniform,2), (vigilance,1), (so,4), (devise,1), (blessed,1), (Uniformity,1), (reliance,1), (it,6), (The,5), (than,1), (others,1), (attended,1), (deeply,1), (troops,1), (fund,1), (embrace,1), (protection,1), (secure,1), (desirable,1), (engage,1), (received,1), (such,4), (literature.,1), (add,1), (recommended,1), (papers,2), (burthens,1), (common,1), (end,4), (preserving,1), (Whether,1), (to.,1), (deliberations,1), (resulting,1), (place,1), (ways--by,1), (supplies.,1), (derive,2), (To,2), (laws.,1), (great,4), ((of,1), (establishment,1), (commerce,,1), (task,1), (armed,,1), (reflection,1), (less,1), (currency,,1), (lawful,1), (last--and,1), (inevitable,1), (expedients,1), (speedily,1), (the,92), (sentiment,1), (not,3), (nothing,1), (enable,1), (manufactories,1), (most,2), (if,1), (considerations,1), (be,20), (punish,1), (all,1), (contained,1), (though,1), (legislature.,2), (toward,1), (credit,2), (superfluous,1), (disregard,1), (rights,1), (regard,3), (but,5), (official,1), (deliberate,1), (skill,1), (persuaded,1), (itself,1), (increasing,1), (distinguish,1), (necessary,2), (Nor,1), (George,1), (on,3), (distant,1), (against,2), (would,2), (perfect,1), (before,2), (at,2), (object,3), (estimates,1), (them;,1), (should,,1), (interests,2), (Union,,2), (may,5), (government,3), (ascertained,1), (good,,1), (gracious,1), (or,1), (insuring,1), (I,11), (aids,1), (intercourse,2), (Union,1), (of,68), (respectively,,1), (fellow,1), (reach,1), (Various,1), (1790,1), (saw,1), (answered,1), (producing,1), (encouragement,2), (Carolina,1), (particular,1), (Fellow-Citizens,1), (inducements,1), (auspicious,1), (arrangements,1), (difficulty,1), (pacific,1), (opportunity,1), (prosperity.,2), (patronage,1), (A,1), (plan,1), (which,18), (cooperation,2), (you,,1), (also,1), (inhabitants,1), (competent,1), (require,2), (should,3), (tend,1), (genius,1), (naturalization.,1), (character,1), (promotion,1), (for,7), (Gentlemen,2), (teaching,1), (worthy,1), (placed,1), (effectual,2), (present,2), (entitled,1), (your,9), (inventions,1), (terms,1), (North,1), (cool,1), (happiness.,1), (officers,2), (Providence,1), (people,2), (abroad,1), (pleasure,1), (expect,1), (facilitating,1), (was,1), (merit,1), (community,1), (endeavors,1), (arduous,1), (exertions,1), (peculiar,1), (society;,1), (firmness,,1), (pleasing,1), (by,11), (expectations,1), (tribes,1), (efforts,1), (defined,1), (inviolable,1), (It,1), (value,1), (an,5), (soldiers,1), (temperate,1), (sanction,1), (disciplined;,1), (recent,1), (provision,2), (conciliate,1), (made,2), (constitution,1), (agriculture,,1), (concerned,,1), (enlightened,1), (novelty,1), (people,,1), (adopted,1), (efficient,,1), (defraying,1), (wisdom.,1), (employed,1), (convenience,1), (ought,3), (in,16), (provide,1), (weights,,1), (In,4), (good,2), (those,3), (necessary,,1), (support,2), (manner,1), (public,5), (course,1), (and,,1), (entered,1), (within,1), (ours,1), (receive,1), (prospects,1), (liberty,1), (every,2), (matter,1), (nature,1), (you,10), ((comprehending,1), (prepared,2), (various,1), (avoiding,1), (that,15), (a,20), (many,1), (spirit,1), (expenses,1), (not,,1), (work,1), (state,2), (government.,1), (concur;,1), (to,53), (know,1), (military,,1), (persuaded,,1), (post-office,1), (perceive,1), (Constitution,1), (specify,1), (regard.,1), (and,39), (indispensable,1), (constituents,1), (home,,1), (forbear,1), (United,4), (direct,1), (citizens,2), (measures,4))
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Unfortunately, as you can see from the <code>collect</code> above:</p>
<ul>
<li>the words have punctuations at the end which means that the same words are being counted as different words. Eg: importance</li>
<li>empty words are being counted</li>
</ul>
<p>So we need a bit of <code>regex</code>'ing or regular-expression matching (all readily available from Scala via Java String types).</p>
<p>We will cover the three things we want to do with a simple example from Middle Earth!</p>
<ul>
<li>replace all multiple whitespace characters with one white space character &quot; &quot;</li>
<li>replace all punction characters we specify within <code>[</code> and <code>]</code> such as <code>[,?.!:;]</code> by the empty string <code>&quot;&quot;</code> (i.e., remove these punctuation characters)</li>
<li>convert everything to lower-case.</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val example = &quot;Master, Master!   It's me, Sméagol... mhrhm*%* But they took away our precious, they wronged us. Gollum will protect us..., Master, it's me Sméagol.&quot;
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>example: String = Master, Master!   It's me, Sméagol... mhrhm*%* But they took away our precious, they wronged us. Gollum will protect us..., Master, it's me Sméagol.
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">example
  .replaceAll(&quot;\\s+&quot;, &quot; &quot;) //replace multiple whitespace characters (including space, tab, new line, etc.) with one whitespace &quot; &quot;
  .replaceAll(&quot;&quot;&quot;([,?.!:;])&quot;&quot;&quot;, &quot;&quot;) // replace the following punctions characters: , ? . ! : ; . with the empty string &quot;&quot;
  .toLowerCase() // converting to lower-case
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res15: String = master master it's me sméagol mhrhm*%* but they took away our precious they wronged us gollum will protect us master it's me sméagol
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="more-sophisticated-word-count"><a class="header" href="#more-sophisticated-word-count">More sophisticated word count</a></h3>
<p>We are now ready to do a word count of George Washington's SoU on January 8th 1790 as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val wordCount_sou17900108 = 
 sou17900108
    .flatMap(line =&gt; 
         line.replaceAll(&quot;\\s+&quot;, &quot; &quot;) //replace multiple whitespace characters (including space, tab, new line, etc.) with one whitespace &quot; &quot;
             .replaceAll(&quot;&quot;&quot;([,?.!:;])&quot;&quot;&quot;, &quot;&quot;) // replace the following punctions characters: , ? . ! : ; . with the empty string &quot;&quot;
             .toLowerCase() // converting to lower-case
             .split(&quot; &quot;))
    .map(x =&gt; (x, 1))
    .reduceByKey(_+_)
    
wordCount_sou17900108.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>wordCount_sou17900108: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[785] at reduceByKey at command-4088905069026032:9
res16: Array[(String, Int)] = Array((university,1), (call,1), (country,4), (promoted,1), (agree,1), (admitted,1), (accession,1), (exertion,1), (plenty,1), (have,4), (incident,1), (session,3), (national,3), (equal,2), (we,2), (intimating,1), (been,2), (who,2), (eminent,1), (any,1), (consideration,2), (immediately,1), (western,1), (speedy,1), (institution,1), (respect,2), (me,2), (discriminate,1), (frontiers,1), (free,3), (affairs,3), (are,4), (economy,1), (administration,1), (parts,2), (presents,1), (blessings,2), (expressive,1), (introduction,1), (comfortable,1), (our,10), (as,9), (intrusted,1), (circumstances,2), (respectability,1), (branch,1), (contributes,1), (better,1), (them,3), (independent,1), (proceeding,1), (received),1), (duty,2), (foreigners,1), (satisfactory,1), (is,10), (convey,1), (commonwealth,1), (favorable,1), (am,2), (certain,1), (january,1), (shall,2), (sense,1), (proper,3), (impressions,1), (disciplined,1), (they,2), (new,1), (my,2), (rising,1), (expedient,1), (uniting,1), (oppression,1), (now,1), (due,2), (has,3), (deserve,1), (licentiousness--,1), (safety,1), (degree,1), (persons,1), (8,1), (giving,1), (concur,1), (learning,1), (conducive,1), (according,1), (need,1), (manufactures,1), (render,3), (invasions,1), (honor,1), (fulfill,1), (directed,2), (basis,1), (southern,1), (conduct,1), (law,1), (well-digested,1), (exigencies,1), (senate,2), (objects,1), (truly,1), (cares,1), (knowledge,1), (concord,1), (foreign,1), (welfare,1), (consultations,1), (resolution,1), (means,2), (cherishing,1), (this,3), (convincing,1), (deemed,1), (right,1), (post-roads,1), (themselves,1), (general,2), (entirely,1), (explicit,1), (defense,1), (only,1), (washington,1), (established,1), (house,3), (first,1), (still,1), (opinion,2), (security,1), (exercise,1), (already,1), (particularly,2), (satisfaction,2), (essential,2), (patriotism,1), (realize,1), (rule,1), (cheerful,1), (nations,1), (measure,1), (congratulating,1), (hope,2), (can,3), (resuming,1), (peace,2), (encroachments,1), (relieved,1), (communication,1), (society,1), (into,1), (there,2), (science,1), (hostile,1), (representatives,3), (discern,1), (lay,1), (own,1), (reason,1), (high,1), (mature,1), (laid,1), (states,4), (compensation,1), (surest,1), (advancement,1), (trust,1), (respecting,1), (declaration,1), (one,2), (with,11), (uniformity,1), (obviously,1), (best,3), (importance,3), (interesting,1), (proportionably,1), (duly,1), (seminaries,1), (aggressors,1), (attention,2), (promote,1), (afford,2), (close,1), (depredations,1), (from,12), (other,3), (well,2), (interest,1), (further,1), (among,1), (virginia),1), (facilitated,1), (appointments,1), (affording,1), (allowed,1), (their,7), (adequate,1), (last,2), (expediency,1), (between,3), (indians,1), (will,14), (information,3), (useful,1), (valuable,1), (&quot;&quot;,1), (confidence,2), (war,1), (provisions,1), (designated,1), (providing,1), (important,2), (uniform,2), (so,4), (vigilance,1), (devise,1), (blessed,1), (reliance,1), (it,7), (agriculture,1), (than,1), (others,1), (attended,1), (deeply,1), (troops,1), (fund,1), (embrace,1), (protection,1), (secure,1), (desirable,1), (engage,1), (received,1), (such,4), (nothing,1), (naturalization,1), (add,1), (recommended,1), (papers,2), (burthens,1), (common,1), (end,4), (preserving,1), (weights,1), (deliberations,1), (resulting,1), (place,1), (ways--by,1), (derive,2), (great,4), ((of,1), (establishment,1), (expedients,1), (task,1), (reflection,1), (less,1), (last--and,1), (inevitable,1), (lawful,1), (speedily,1), (the,97), (sentiment,1), (not,4), (enable,1), (manufactories,1), (most,2), (if,1), (considerations,1), (providence,1), (be,20), (home,1), (punish,1), (all,1), (contained,1), (persuaded,2), (though,1), (laws,1), (toward,1), (credit,2), (gentlemen,2), (efficient,1), (superfluous,1), (disregard,1), (rights,2), (regard,4), (but,5), (official,1), (deliberate,1), (skill,1), (itself,1), (increasing,1), (nor,1), (distinguish,1), (necessary,3), (on,3), (distant,1), (against,2), (would,2), (perfect,1), (before,2), (at,2), (object,3), (estimates,1), (united,4), (union,3), (fellow-citizens,1), (interests,2), (may,5), (government,4), (ascertained,1), (armed,1), (gracious,1), (requisite,1), (or,1), (insuring,1), (aids,1), (intercourse,2), (of,68), (fellow,1), (reach,1), (saw,1), (1790,1), (answered,1), (encouragement,2), (producing,1), (prosperity,2), (particular,1), (currency,1), (inducements,1), (auspicious,1), (arrangements,1), (difficulty,1), (pacific,1), (i,11), (opportunity,1), (plan,1), (patronage,1), (military,1), (north,1), (which,18), (cooperation,2), (also,1), (inhabitants,1), (competent,1), (require,2), (should,4), (tend,1), (genius,1), (for,7), (whether,1), (promotion,1), (character,1), (teaching,1), (worthy,1), (placed,1), (effectual,2), (present,2), (entitled,1), (your,9), (inventions,1), (terms,1), (cool,1), (authority,1), (pleasing,1), (officers,2), (literature,1), (people,3), (abroad,1), (pleasure,1), (expect,1), (facilitating,1), (was,1), (merit,1), (community,1), (endeavors,1), (arduous,1), (exertions,1), (peculiar,1), (sanction,1), (by,11), (concerned,1), (expectations,1), (tribes,1), (efforts,1), (defined,1), (inviolable,1), (value,1), (an,5), (soldiers,1), (supplies,1), (temperate,1), (recent,1), (provision,2), (conciliate,1), (happiness,1), (made,2), (constitution,2), (enlightened,1), (novelty,1), (adopted,1), (defraying,1), (carolina,1), (employed,1), (george,1), (convenience,1), (ought,3), (in,20), (provide,1), (commerce,1), (good,3), (those,3), (recommendation,1), (support,2), (manner,1), (public,5), (course,1), (receive,1), (entered,1), (within,1), (ours,1), (prospects,1), (liberty,1), (every,2), (matter,1), (nature,1), (you,11), ((comprehending,1), (prepared,2), (various,2), (avoiding,1), (that,15), (legislature,2), (a,21), (many,1), (spirit,1), (expenses,1), (work,1), (state,2), (to,56), (know,1), (wisdom,1), (post-office,1), (perceive,1), (respectively,1), (specify,1), (firmness,1), (and,40), (forbear,1), (indispensable,1), (constituents,1), (direct,1), (measures,4), (citizens,2))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val top10 = wordCount_sou17900108.sortBy(_._2, false).collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>top10: Array[(String, Int)] = Array((the,97), (of,68), (to,56), (and,40), (a,21), (be,20), (in,20), (which,18), (that,15), (will,14), (from,12), (with,11), (i,11), (by,11), (you,11), (our,10), (is,10), (as,9), (your,9), (their,7), (it,7), (for,7), (but,5), (may,5), (an,5), (public,5), (country,4), (have,4), (are,4), (states,4), (so,4), (such,4), (end,4), (great,4), (not,4), (regard,4), (united,4), (government,4), (should,4), (measures,4), (session,3), (national,3), (free,3), (affairs,3), (them,3), (proper,3), (has,3), (render,3), (this,3), (house,3), (can,3), (representatives,3), (best,3), (importance,3), (other,3), (between,3), (information,3), (necessary,3), (on,3), (object,3), (union,3), (people,3), (ought,3), (good,3), (those,3), (equal,2), (we,2), (been,2), (who,2), (consideration,2), (respect,2), (me,2), (parts,2), (blessings,2), (circumstances,2), (duty,2), (am,2), (shall,2), (they,2), (my,2), (due,2), (directed,2), (senate,2), (means,2), (general,2), (opinion,2), (particularly,2), (satisfaction,2), (essential,2), (hope,2), (peace,2), (there,2), (one,2), (attention,2), (afford,2), (well,2), (last,2), (confidence,2), (important,2), (uniform,2), (papers,2), (derive,2), (most,2), (persuaded,2), (credit,2), (gentlemen,2), (rights,2), (against,2), (would,2), (before,2), (at,2), (interests,2), (intercourse,2), (encouragement,2), (prosperity,2), (cooperation,2), (require,2), (effectual,2), (present,2), (officers,2), (provision,2), (made,2), (constitution,2), (support,2), (every,2), (prepared,2), (various,2), (legislature,2), (state,2), (citizens,2), (university,1), (call,1), (promoted,1), (agree,1), (admitted,1), (accession,1), (exertion,1), (plenty,1), (incident,1), (intimating,1), (eminent,1), (any,1), (immediately,1), (western,1), (speedy,1), (institution,1), (discriminate,1), (frontiers,1), (economy,1), (administration,1), (presents,1), (expressive,1), (introduction,1), (comfortable,1), (intrusted,1), (respectability,1), (branch,1), (contributes,1), (better,1), (independent,1), (proceeding,1), (received),1), (foreigners,1), (satisfactory,1), (convey,1), (commonwealth,1), (favorable,1), (certain,1), (january,1), (sense,1), (impressions,1), (disciplined,1), (new,1), (rising,1), (expedient,1), (uniting,1), (oppression,1), (now,1), (deserve,1), (licentiousness--,1), (safety,1), (degree,1), (persons,1), (8,1), (giving,1), (concur,1), (learning,1), (conducive,1), (according,1), (need,1), (manufactures,1), (invasions,1), (honor,1), (fulfill,1), (basis,1), (southern,1), (conduct,1), (law,1), (well-digested,1), (exigencies,1), (objects,1), (truly,1), (cares,1), (knowledge,1), (concord,1), (foreign,1), (welfare,1), (consultations,1), (resolution,1), (cherishing,1), (convincing,1), (deemed,1), (right,1), (post-roads,1), (themselves,1), (entirely,1), (explicit,1), (defense,1), (only,1), (washington,1), (established,1), (first,1), (still,1), (security,1), (exercise,1), (already,1), (patriotism,1), (realize,1), (rule,1), (cheerful,1), (nations,1), (measure,1), (congratulating,1), (resuming,1), (encroachments,1), (relieved,1), (communication,1), (society,1), (into,1), (science,1), (hostile,1), (discern,1), (lay,1), (own,1), (reason,1), (high,1), (mature,1), (laid,1), (compensation,1), (surest,1), (advancement,1), (trust,1), (respecting,1), (declaration,1), (uniformity,1), (obviously,1), (interesting,1), (proportionably,1), (duly,1), (seminaries,1), (aggressors,1), (promote,1), (close,1), (depredations,1), (interest,1), (further,1), (among,1), (virginia),1), (facilitated,1), (appointments,1), (affording,1), (allowed,1), (adequate,1), (expediency,1), (indians,1), (useful,1), (valuable,1), (&quot;&quot;,1), (war,1), (provisions,1), (designated,1), (providing,1), (vigilance,1), (devise,1), (blessed,1), (reliance,1), (agriculture,1), (than,1), (others,1), (attended,1), (deeply,1), (troops,1), (fund,1), (embrace,1), (protection,1), (secure,1), (desirable,1), (engage,1), (received,1), (nothing,1), (naturalization,1), (add,1), (recommended,1), (burthens,1), (common,1), (preserving,1), (weights,1), (deliberations,1), (resulting,1), (place,1), (ways--by,1), ((of,1), (establishment,1), (expedients,1), (task,1), (reflection,1), (less,1), (last--and,1), (inevitable,1), (lawful,1), (speedily,1), (sentiment,1), (enable,1), (manufactories,1), (if,1), (considerations,1), (providence,1), (home,1), (punish,1), (all,1), (contained,1), (though,1), (laws,1), (toward,1), (efficient,1), (superfluous,1), (disregard,1), (official,1), (deliberate,1), (skill,1), (itself,1), (increasing,1), (nor,1), (distinguish,1), (distant,1), (perfect,1), (estimates,1), (fellow-citizens,1), (ascertained,1), (armed,1), (gracious,1), (requisite,1), (or,1), (insuring,1), (aids,1), (fellow,1), (reach,1), (saw,1), (1790,1), (answered,1), (producing,1), (particular,1), (currency,1), (inducements,1), (auspicious,1), (arrangements,1), (difficulty,1), (pacific,1), (opportunity,1), (plan,1), (patronage,1), (military,1), (north,1), (also,1), (inhabitants,1), (competent,1), (tend,1), (genius,1), (whether,1), (promotion,1), (character,1), (teaching,1), (worthy,1), (placed,1), (entitled,1), (inventions,1), (terms,1), (cool,1), (authority,1), (pleasing,1), (literature,1), (abroad,1), (pleasure,1), (expect,1), (facilitating,1), (was,1), (merit,1), (community,1), (endeavors,1), (arduous,1), (exertions,1), (peculiar,1), (sanction,1), (concerned,1), (expectations,1), (tribes,1), (efforts,1), (defined,1), (inviolable,1), (value,1), (soldiers,1), (supplies,1), (temperate,1), (recent,1), (conciliate,1), (happiness,1), (enlightened,1), (novelty,1), (adopted,1), (defraying,1), (carolina,1), (employed,1), (george,1), (convenience,1), (provide,1), (commerce,1), (recommendation,1), (manner,1), (course,1), (receive,1), (entered,1), (within,1), (ours,1), (prospects,1), (liberty,1), (matter,1), (nature,1), ((comprehending,1), (avoiding,1), (many,1), (spirit,1), (expenses,1), (work,1), (know,1), (wisdom,1), (post-office,1), (perceive,1), (respectively,1), (specify,1), (firmness,1), (forbear,1), (indispensable,1), (constituents,1), (direct,1))
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="doing-it-all-together-for-george-washington-and-barrack-obama"><a class="header" href="#doing-it-all-together-for-george-washington-and-barrack-obama">Doing it all together for George Washington and Barrack Obama</a></h3>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//sc.textFile(&quot;dbfs:/datasets/sou/17900108.txt&quot;) // George Washington's first SoU
sc.textFile(&quot;dbfs:/datasets/sou/20160112.txt&quot;)   // Barrack Obama's second SoU
    .flatMap(line =&gt; 
         line.replaceAll(&quot;\\s+&quot;, &quot; &quot;) //replace multiple whitespace characters (including space, tab, new line, etc.) with one whitespace &quot; &quot;
             .replaceAll(&quot;&quot;&quot;([,?.!:;])&quot;&quot;&quot;, &quot;&quot;) // replace the following punctions characters: , ? . ! : ; . with the empty string &quot;&quot;
             .toLowerCase() // converting to lower-case
             .split(&quot; &quot;))
    .map(x =&gt; (x,1))
    .reduceByKey(_+_)
    .sortBy(_._2, false)
    .collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res17: Array[(String, Int)] = Array((the,264), (and,189), (to,188), (of,135), (we,116), (a,100), (that,100), (in,90), (our,86), (for,59), (is,51), (or,49), (it,46), (i,41), (on,40), (this,36), (who,35), (but,34), (us,33), (have,32), (are,30), (as,30), (that's,30), (not,27), (their,26), (world,24), (with,24), (more,24), (they,23), (it's,23), (will,22), (be,22), (america,21), (when,21), (can,20), (people,20), (all,20), (work,20), (you,19), (years,18), (so,18), (do,18), (up,18), (make,17), (year,17), (should,17), (american,16), (new,16), (just,16), (if,16), (by,16), (economy,15), (from,15), (change,15), (because,15), (now,15), (americans,15), (want,14), (has,14), (how,14), (there,14), (we've,14), (every,14), (need,13), (out,13), (over,13), (even,13), (better,13), (see,13), (than,13), (country,12), (what,12), (get,12), (future,12), (only,11), (at,11), (about,11), (job,11), (like,11), (security,10), (why,10), (most,10), (those,10), (back,10), (way,10), (them,9), (right,9), (some,9), (one,9), (still,9), (his,9), (keep,9), (know,9), (these,9), (time,9), (no,9), (workers,9), (we're,8), (jobs,8), (him,8), (doesn't,8), (other,8), (past,8), (isil,8), (leadership,8), (also,8), (he,8), (system,8), (voices,8), (next,8), (an,8), (going,7), (my,7), (big,7), (energy,7), (war,7), (together,7), (give,7), (i'm,7), (made,7), (everything,7), (politics,7), (care,7), (power,7), (agree,6), (been,6), (don't,6), (families,6), (without,6), (cut,6), (its,6), (same,6), (states,6), (after,6), (best,6), (was,6), (tonight,6), (economic,6), (democracy,6), (where,6), (opportunity,6), (doing,6), (help,6), (many,6), (can't,6), (national,5), (here,5), (go,5), (into,5), (congress,5), (business,5), (last,5), (president,5), (happen,5), (got,5), (believe,5), (countries,5), (planet,5), (me,5), (come,5), (america's,5), (vote,5), (start,5), (health,5), (love,5), (basic,5), (own,5), (kids,5), (seven,5), (nearly,5), (spirit,5), (needs,5), (we'll,5), (sure,5), (political,5), (may,5), (each,5), (matter,5), (especially,5), (military,5), (progress,5), (lot,5), (take,5), (away,4), (long,4), (foreign,4), (hardworking,4), (allies,4), (oil,4), (travel,4), (done,4), (science,4), (down,4), (important,4), (any,4), (i'll,4), (final,4), (pay,4), (day,4), (single,4), (first,4), (elected,4), (harder,4), (businesses,4), (education,4), (between,4), (almost,4), (answer,4), (history,4), (international,4), (talk,4), (ways,4), (stand,4), (money,4), (there's,4), (approach,4), (told,4), (place,4), (makes,4), (less,4), (pass,4), (everyone,4), (things,4), (comes,4), (changes,4), (look,4), (had,4), (before,4), (fellow,4), (climate,4), (much,4), (communities,4), (save,4), (around,4), (safe,4), (hard,4), (retirement,4), (working,4), (did,4), (words,4), (nation,4), (terrorists,4), (everybody,4), (government,4), (student,4), (won't,4), (what's,3), (tougher,3), (true,3), (agreement,3), (too,3), (nations,3), (second,3), (focus,3), (serious,3), (quiet,3), (open,3), (lead,3), (office,3), (line,3), (chamber,3), (equal,3), (program,3), (fair,3), (live,3), (benefits,3), (didn't,3), (reason,3), (fighting,3), (training,3), (life,3), (she,3), (citizen,3), (two,3), (getting,3), (set,3), (shouldn't,3), (feel,3), (means,3), (play,3), (ourselves,3), (terrorist,3), (thing,3), (kind,3), (qaeda,3), (build,3), (syria,3), (issues,3), (built,3), (family,3), (fact,3), (push,3), (century,3), (young,3), (end,3), (strength,3), (middle,3), (speaker,3), (control,3), (resources,3), (community,3), (might,3), (good,3), (troops,3), (such,3), (let,3), (mr,3), (cuts,3), (interests,3), (he's,3), (say,3), (incredible,3), (strongest,3), (al,3), (isn't,3), (global,3), (support,3), (medical,3), (citizens,3), (others,3), (through,3), (fall,3), (else,3), (justice,3), (rules,3), (protect,3), (strong,3), (great,3), (average,3), (rights,3), (show,3), (against,3), (would,3), (united,3), (face,3), (use,3), (college,3), (commitment,3), (your,3), (put,3), (wants,3), (crisis,3), (reflect,3), (state,3), (generations,3), (biggest,3), (shot,2), (behind,2), (wasn't,2), (month,2), (technology,2), (favor,2), (son,2), (party,2), (promises,2), (rest,2), (stop,2), (21st,2), (washington,2), (handful,2), (society,2), (leaders,2), (something,2), (intend,2), (everywhere,2), (recognize,2), (bills,2), (enemies,2), (taking,2), (vulnerable,2), (respect,2), (voice,2), (wage,2), (immigrant,2), (asia,2), (fight,2), (earth,2), (overnight,2), (muster,2), (sector,2), (tens,2), (question,2), (200,2), (you'll,2), (discovery,2), (forces,2), (powerful,2), (trends,2), (argue,2), (coverage,2), (until,2), (africa,2), (chance,2), (manufacturing,2), (recruit,2), (threat,2), (send,2), (they've,2), (few,2), (optimism,2), (none,2), (freedom,2), (priorities,2), (speak,2), (affordable,2), (school,2), (confidence,2), (created,2), (call,2), (strengths,2), (improve,2), (plenty,2), (low-income,2), (begins,2), (challenges,2), (parties,2), (instead,2), (million,2), (parts,2), (fighters,2), (rigged,2), (law,2), (think,2), (soldier,2), (already,2), (cancer,2), (act,2), (real,2), (works,2), (depend,2), (word,2), (isolating,2), (iran,2), (interest,2), (fear,2), (students,2), (very,2), (whatever,2), (pushing,2), (under,2), (ever,2), (collective,2), (unarmed,2), (space,2), (insurance,2), (gives,2), (stronger,2), (learn,2), (areas,2), (air,2), (doubt,2), (worked,2), (civilians,2), (easy,2), (enough,2), (god,2), (choices,2), (since,2), (coalition,2), (saving,2), (beat,2), (attention,2), (understand,2), (providing,2), (assembly,2), (iowa,2), (decline,2), (plotting,2), (lift,2), (careers,2), (gotten,2), (ukraine,2), (shape,2), (alone,2), (different,2), (brings,2), (pull,2), (ten,2), (nor,2), (development,2), (find,2), (networks,2), (fields,2), (broken,2), (iraq,2), (percent,2), (folks,2), (truth,2), (transition,2), (third,2), (pace,2), (east,2), (cure,2), (i've,2), (early,2), (unemployment,2), (higher,2), (commerce,2), (stamp,2), (public,2), (decisions,2), (anew,2), (tax,2), (finally,2), (coal,2), (internet,2), (accelerate,2), (clean,2), (rooted,2), (voting,2), (ask,2), (tools,2), (bless,2), (saw,2), (innovation,2), (protecting,2), (unconditional,2), (retrain,2), (whether,2), (problem,2), (haven't,2), (both,2), (red,2), (turning,2), (sixty,2), (politicians,2), (wages,2), (try,2), (easier,2), (companies,2), (lose,2), (course,2), (ago,2), (another,2), (could,2), (result,2), (steps,2), (research,2), (budget,2), (diversity,2), (vice,2), (person,2), (violence,2), (record,2), (lincoln,2), (reach,2), (trying,2), (walking,2), (conflicts,2), (helping,2), (unique,2), (today's,2), (cost,2), (respects,2), (attacks,2), (relations,2), (started,2), (far,2), (mean,2), (congressional,2), (computer,2), (reform,2), (taken,2), (worst,2), (choice,2), (poverty,2), (income,2), (solve,2), (force,2), (spread,2), (entrepreneur,2), (either,2), (bipartisan,2), (brothers,2), (gone,2), (possible,2), (let's,2), (union,2), (willingness,2), (prison,2), (immigrants,2), (achieve,2), (must,2), (taxes,2), (corporate,2), (constructive,2), (her,2), (you're,2), (pose,2), (then,2), (today,2), (path,2), (promise,2), (depends,2), (stopped,2), (growing,2), (online,2), (always,2), (hold,2), (someone,1), (goals,1), (greater,1), (urging,1), (graduation,1), (reality,1), (order,1), (hidden,1), (minimum,1), (conflict,1), (offering,1), (knows,1), (prefer,1), (hubs,1), (partnership,1), (shut,1), (arsenal,1), (vietnam,1), (element,1), (encourages,1), (african,1), (republicans,1), (raise,1), (cheaper,1), (regrets,1), (pursue,1), (namely,1), (fourth,1), (veterans,1), (regardless,1), (unnecessary,1), (straight,1), (sources,1), (boardrooms,1), (sick,1), (rolled,1), (trans-pacific,1), (giving,1), (avoided,1), (math,1), (betrays,1), (childhood,1), (places,1), (point,1), (low,1), (marathon,1), (six,1), (three,1), (breaks,1), (stake,1), (blow,1), (depression,1), (often,1), (spilling,1), (grace,1), (blood,1), (remote,1), (results,1), (earlier,1), (reflects,1), (welcome,1), (defend,1), (extending,1), (capacity,1), (someday,1), (precious,1), (enormous,1), (fiction,1), (finds,1), (accounts,1), (rational,1), (scourge,1), (recipe,1), (muslims,1), (protester,1), (accept,1), (casts,1), (disruptions,1), (create,1), (immigration,1), (bound,1), (high,1), (reinvented,1), (hire,1), (slowed,1), (graduates,1), (who've,1), (classes,1), (teacher,1), (winning,1), (increased,1), (next-generation,1), (standing,1), (despite,1), (decades-long,1), (largest,1), (afford,1), (building,1), (date,1), (well,1), (compromise,1), (weight,1), (souls,1), (rallying,1), (dispute,1), (stays,1), (envy,1), (funds,1), (report,1), (brochure,1), (&quot;&quot;,1), (tape,1), (maybe,1), (cold,1), (ocean,1), (industry,1), (demands,1), (hands,1), (plots,1), (finance,1), (weakens,1), (branches,1), (package,1), (priority,1), (disagree,1), (policy,1), (diminished,1), (lie,1), (simple,1), (doctors,1), (rooftops,1), (transportation,1), (trucks,1), (dangerous,1), (movements,1), (becoming,1), (late,1), (inside,1), (three-quarters,1), (yemen,1), (russians,1), (shores,1), (matters,1), (promising,1), (conventional,1), (ends,1), (boosted,1), (products,1), (ii,1), (benghazi,1), (wise,1), (elevated,1), (failing,1), (brave,1), (arguments,1), (loved,1), (diplomatic,1), (outward,1), (asian,1), (existence,1), (level,1), (bigger,1), (claiming,1), (loan,1), (vision,1), (tv,1), (territory,1), (produce,1), (presidency,1), (perpetrator,1), (lesson,1), (nuclear-armed,1), (hemisphere,1), (feeding,1), (uses,1), (colombia,1), (locate,1), (prescription,1), (hunted,1), (cures,1), (ensure,1), (&quot;we,1), (religions,1), (partnering,1), (hear,1), (starting,1), (bucks,1), (obama,1), (expanding,1), (region,1), (extra,1), (wind,1), (senator,1), (longest,1), (gun,1), (although,1), (faith,1), (past&quot;,1), (i'd,1), (ruin,1), (afghanistan,1), (deal,1), (concern,1), (60,1), (treating,1), (orbit,1), (deny,1), (thank,1), (active,1), (austin,1), (somebody,1), (borrower's,1), (worse,1), (generation,1), (restore,1), (inspired,1), (vicious,1), (abuse,1), (helped,1), (protected,1), (replace,1), (marks,1), (later,1), (appreciate,1), (died,1), (grit,1), (pouring,1), (expand,1), (rely,1), (expected,1), (promote,1), (close,1), (able,1), (nuclear,1), (understanding,1), (permanent,1), (stage,1), (role,1), (diplomacy,1), (garages,1), (influx,1), (worth,1), (sell,1), (protects,1), (sputnik,1), (francis,1), (sanctions,1), (tends,1), (turned,1), (list,1), (dreamer,1), ('til,1), (congressman,1), (barack,1), (guarantee,1), (tea,1), (grows,1), (uphold,1), (worker,1), (partiers,1), (extraordinary,1), (campaign,1), (laden,1), (serves,1), (medicare,1), (wealthiest,1), (evil,1), (application,1), (fossil,1), (gained,1), (offend,1), (2015,1), (mat,1), (uniquely,1), (paying,1), (thomas,1), (practices,1), (joe,1), (status,1), (tpp,1), (falls,1), (expense,1), (anybody,1), (trapped,1), (biden,1), (contradicts,1), (ultimately,1), (rather,1), (january,1), (outdated,1), (flow,1), (spend,1), (kill,1), (ryan,1), (automated,1), (pray,1), (responsible,1), (rising,1), (courage,1), (meanwhile,1), (four,1), (charity,1), (bomb,1), (body,1), (selves,1), (threatened,1), (effort,1), (twisted,1), (anyone,1), (slipping,1), (bite,1), (good-paying,1), (requires,1), (withers,1), (vandalized,1), (poison,1), (bonds,1), (over-the-top,1), (dr,1), (pretty,1), (controls,1), (unpatriotic,1), (epidemic,1), (announcing,1), (cell,1), (vital,1), (rate,1), (upon,1), (patients,1), (helps,1), (camps,1), (struggling,1), (polls,1), (figured,1), (hatred,1), (brakes,1), (entrenched,1), (arizona,1), (world's,1), (example,1), (measure,1), (peace,1), (streak,1), (assistance,1), (strengthen,1), (extreme,1), (overcame,1), (stuck,1), (action,1), (hands-on,1), (pope,1), (attack,1), (reforms,1), (trust,1), (ideology,1), (reduced,1), (dirty,1), (disease,1), (concentrated,1), (join,1), (sitting,1), (delivered,1), (famine,1), (avoids,1), (forty,1), (allowed,1), (believed,1), (restored,1), (founders,1), (peril,1), (advance,1), (child,1), (door,1), (president's,1), (costs,1), (reduce,1), (insist,1), (edison,1), (solving,1), (briefing,1), (offshore,1), (superpower,1), (debating,1), (willing,1), (big-hearted,1), (setting,1), (nation-build,1), (problems,1), (targets,1), (private-sector,1), (loyalty,1), (preserve,1), (moscow,1), (cutting,1), (fifties,1), (stockpile,1), (address,1), (prevent,1), (mosque,1), (cop,1), (whose,1))
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="reading-all-sous-at-once-using-wholetextfiles"><a class="header" href="#reading-all-sous-at-once-using-wholetextfiles">Reading all SoUs at once using <code>wholetextFiles</code></a></h3>
<p>Let us next read all text files (ending with <code>.txt</code>) in the directory <code>dbfs:/datasets/sou/</code> at once!</p>
<p><code>SparkContext.wholeTextFiles</code> lets you read a directory containing multiple small text files, and returns each of them as <code>(filename, content)</code> pairs of strings.</p>
<p>This is in contrast with <code>textFile</code>, which would return one record per line in each file.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val souAll = sc.wholeTextFiles(&quot;dbfs:/datasets/sou/*.txt&quot;) // Shift+Enter to read all text files in dbfs:/datasets/sou/
souAll.cache() // let's cache this RDD for efficient reuse
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>souAll: org.apache.spark.rdd.RDD[(String, String)] = dbfs:/datasets/sou/*.txt MapPartitionsRDD[802] at wholeTextFiles at command-4088905069026037:1
res18: souAll.type = dbfs:/datasets/sou/*.txt MapPartitionsRDD[802] at wholeTextFiles at command-4088905069026037:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">souAll.count() // Shift+enter to count the number of entries in RDD[(String,String)]
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res19: Long = 231
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">souAll.count() // Cntrl+Enter to count the number of entries in cached RDD[(String,String)] again (much faster!)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res20: Long = 231
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let's examine the first two elements of the RDD <code>souAll</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-scala">souAll.take(2) // Cntr+Enter to see the first two elements of souAll
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res21: Array[(String, String)] =
Array((dbfs:/datasets/sou/17900108.txt,&quot;George Washington

January 8, 1790
Fellow-Citizens of the Senate and House of Representatives:
I embrace with great satisfaction the opportunity which now presents itself of congratulating you on the present favorable prospects of our public affairs. The recent accession of the important state of North Carolina to the Constitution of the United States (of which official information has been received), the rising credit and respectability of our country, the general and increasing good will toward the government of the Union, and the concord, peace, and plenty with which we are blessed are circumstances auspicious in an eminent degree to our national prosperity.
In resuming your consultations for the general good you can not but derive encouragement from the reflection that the measures of the last session have been as satisfactory to your constituents as the novelty and difficulty of the work allowed you to hope. Still further to realize their expectations and to secure the blessings which a gracious Providence has placed within our reach will in the course of the present important session call for the cool and deliberate exertion of your patriotism, firmness, and wisdom.
Among the many interesting objects which will engage your attention that of providing for the common defense will merit particular regard. To be prepared for war is one of the most effectual means of preserving peace.
A free people ought not only to be armed, but disciplined; to which end a uniform and well-digested plan is requisite; and their safety and interest require that they should promote such manufactories as tend to render them independent of others for essential, particularly military, supplies.
The proper establishment of the troops which may be deemed indispensable will be entitled to mature consideration. In the arrangements which may be made respecting it it will be of importance to conciliate the comfortable support of the officers and soldiers with a due regard to economy.
There was reason to hope that the pacific measures adopted with regard to certain hostile tribes of Indians would have relieved the inhabitants of our southern and western frontiers from their depredations, but you will perceive from the information contained in the papers which I shall direct to be laid before you (comprehending a communication from the Commonwealth of Virginia) that we ought to be prepared to afford protection to those parts of the Union, and, if necessary, to punish aggressors.
The interests of the United States require that our intercourse with other nations should be facilitated by such provisions as will enable me to fulfill my duty in that respect in the manner which circumstances may render most conducive to the public good, and to this end that the compensation to be made to the persons who may be employed should, according to the nature of their appointments, be defined by law, and a competent fund designated for defraying the expenses incident to the conduct of foreign affairs.
Various considerations also render it expedient that the terms on which foreigners may be admitted to the rights of citizens should be speedily ascertained by a uniform rule of naturalization.
Uniformity in the currency, weights, and measures of the United States is an object of great importance, and will, I am persuaded, be duly attended to.
The advancement of agriculture, commerce, and manufactures by all proper means will not, I trust, need recommendation; but I can not forbear intimating to you the expediency of giving effectual encouragement as well to the introduction of new and useful inventions from abroad as to the exertions of skill and genius in producing them at home, and of facilitating the intercourse between the distant parts of our country by a due attention to the post-office and post-roads.
Nor am I less persuaded that you will agree with me in opinion that there is nothing which can better deserve your patronage than the promotion of science and literature. Knowledge is in every country the surest basis of public happiness. In one in which the measures of government receive their impressions so immediately from the sense of the community as in ours it is proportionably essential.
To the security of a free constitution it contributes in various ways--by convincing those who are intrusted with the public administration that every valuable end of government is best answered by the enlightened confidence of the people, and by teaching the people themselves to know and to value their own rights; to discern and provide against invasions of them; to distinguish between oppression and the necessary exercise of lawful authority; between burthens proceeding from a disregard to their convenience and those resulting from the inevitable exigencies of society; to discriminate the spirit of liberty from that of licentiousness-- cherishing the first, avoiding the last--and uniting a speedy but temperate vigilance against encroachments, with an inviolable respect to the laws.
Whether this desirable object will be best promoted by affording aids to seminaries of learning already established, by the institution of a national university, or by any other expedients will be well worthy of a place in the deliberations of the legislature.
Gentlemen of the House of Representatives:
I saw with peculiar pleasure at the close of the last session the resolution entered into by you expressive of your opinion that an adequate provision for the support of the public credit is a matter of high importance to the national honor and prosperity. In this sentiment I entirely concur; and to a perfect confidence in your best endeavors to devise such a provision as will be truly with the end I add an equal reliance on the cheerful cooperation of the other branch of the legislature.
It would be superfluous to specify inducements to a measure in which the character and interests of the United States are so obviously so deeply concerned, and which has received so explicit a sanction from your declaration.
Gentlemen of the Senate and House of Representatives:
I have directed the proper officers to lay before you, respectively, such papers and estimates as regard the affairs particularly recommended to your consideration, and necessary to convey to you that information of the state of the Union which it is my duty to afford.
The welfare of our country is the great object to which our cares and efforts ought to be directed, and I shall derive great satisfaction from a cooperation with you in the pleasing though arduous task of insuring to our fellow citizens the blessings which they have a right to expect from a free, efficient, and equal government.
&quot;), (dbfs:/datasets/sou/17901208.txt,&quot;George Washington

December 8, 1790
Fellow-Citizens of the Senate and House of Representatives:
In meeting you again I feel much satisfaction in being able to repeat my congratulations on the favorable prospects which continue to distinguish our public affairs. The abundant fruits of another year have blessed our country with plenty and with the means of a flourishing commerce.
The progress of public credit is witnessed by a considerable rise of American stock abroad as well as at home, and the revenues allotted for this and other national purposes have been productive beyond the calculations by which they were regulated. This latter circumstance is the more pleasing, as it is not only a proof of the fertility of our resources, but as it assures us of a further increase of the national respectability and credit, and, let me add, as it bears an honorable testimony to the patriotism and integrity of the mercantile and marine part of our citizens. The punctuality of the former in discharging their engagements has been exemplary.
In conformity to the powers vested in me by acts of the last session, a loan of 3,000,000 florins, toward which some provisional measures had previously taken place, has been completed in Holland. As well the celerity with which it has been filled as the nature of the terms (considering the more than ordinary demand for borrowing created by the situation of Europe) give a reasonable hope that the further execution of those powers may proceed with advantage and success. The Secretary of the Treasury has my directions to communicate such further particulars as may be requisite for more precise information.
Since your last sessions I have received communications by which it appears that the district of Kentucky, at present a part of Virginia, has concurred in certain propositions contained in a law of that State, in consequence of which the district is to become a distinct member of the Union, in case the requisite sanction of Congress be added. For this sanction application is now made. I shall cause the papers on this very transaction to be laid before you.
The liberality and harmony with which it has been conducted will be found to do great honor to both the parties, and the sentiments of warm attachment to the Union and its present Government expressed by our fellow citizens of Kentucky can not fail to add an affectionate concern for their particular welfare to the great national impressions under which you will decide on the case submitted to you.
It has been heretofore known to Congress that frequent incursions have been made on our frontier settlements by certain banditti of Indians from the northwest side of the Ohio. These, with some of the tribes dwelling on and near the Wabash, have of late been particularly active in their depredations, and being emboldened by the impunity of their crimes and aided by such parts of the neighboring tribes as could be seduced to join in their hostilities or afford them a retreat for their prisoners and plunder, they have, instead of listening to the humane invitations and overtures made on the part of the United States, renewed their violences with fresh alacrity and greater effect. The lives of a number of valuable citizens have thus been sacrificed, and some of them under circumstances peculiarly shocking, whilst others have been carried into a deplorable captivity.
These aggravated provocations rendered it essential to the safety of the Western settlements that the aggressors should be made sensible that the Government of the Union is not less capable of punishing their crimes than it is disposed to respect their rights and reward their attachments. As this object could not be effected by defensive measures, it became necessary to put in force the act which empowers the President to call out the militia for the protection of the frontiers, and I have accordingly authorized an expedition in which the regular troops in that quarter are combined with such drafts of militia as were deemed sufficient. The event of the measure is yet unknown to me. The Secretary of War is directed to lay before you a statement of the information on which it is founded, as well as an estimate of the expense with which it will be attended.
The disturbed situation of Europe, and particularly the critical posture of the great maritime powers, whilst it ought to make us the more thankful for the general peace and security enjoyed by the United States, reminds us at the same time of the circumspection with which it becomes us to preserve these blessings. It requires also that we should not overlook the tendency of a war, and even of preparations for a war, among the nations most concerned in active commerce with this country to abridge the means, and thereby at least enhance the price, of transporting its valuable productions to their markets. I recommend it to your serious reflections how far and in what mode it may be expedient to guard against embarrassments from these contingencies by such encouragements to our own navigation as will render our commerce and agriculture less dependent on foreign bottoms, which may fail us in the very moments most interesting to both of these great objects. Our fisheries and the transportation of our own produce offer us abundant means for guarding ourselves against this evil.
Your attention seems to be not less due to that particular branch of our trade which belongs to the Mediterranean. So many circumstances unite in rendering the present state of it distressful to us that you will not think any deliberations misemployed which may lead to its relief and protection.
The laws you have already passed for the establishment of a judiciary system have opened the doors of justice to all descriptions of persons. You will consider in your wisdom whether improvements in that system may yet be made, and particularly whether an uniform process of execution on sentences issuing from the Federal courts be not desirable through all the States.
The patronage of our commerce, of our merchants and sea men, has called for the appointment of consuls in foreign countries. It seems expedient to regulate by law the exercise of that jurisdiction and those functions which are permitted them, either by express convention or by a friendly indulgence, in the places of their residence. The consular convention, too, with His Most Christian Majesty has stipulated in certain cases the aid of the national authority to his consuls established here. Some legislative provision is requisite to carry these stipulations into full effect.
The establishment of the militia, of a mint, of standards of weights and measures, of the post office and post roads are subjects which I presume you will resume of course, and which are abundantly urged by their own importance.
Gentlemen of the House of Representatives:
The sufficiency of the revenues you have established for the objects to which they are appropriated leaves no doubt that the residuary provisions will be commensurate to the other objects for which the public faith stands now pledged. Allow me, moreover, to hope that it will be a favorite policy with you, not merely to secure a payment of the interest of the debt funded, but as far and as fast as the growing resources of the country will permit to exonerate it of the principal itself. The appropriation you have made of the Western land explains your dispositions on this subject, and I am persuaded that the sooner that valuable fund can be made to contribute, along with the other means, to the actual reduction of the public debt the more salutary will the measure be to every public interest, as well as the more satisfactory to our constituents.
Gentlemen of the Senate and House of Representatives:
In pursuing the various and weighty business of the present session I indulge the fullest persuasion that your consultation will be equally marked with wisdom and animated by the love of your country. In whatever belongs to my duty you shall have all the cooperation which an undiminished zeal for its welfare can inspire. It will be happy for us both, and our best reward, if, by a successful administration of our respective trusts, we can make the established Government more and more instrumental in promoting the good of our fellow citizens, and more and more the object of their attachment and confidence.
GO. WASHINGTON
&quot;))
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Clearly, the elements are a pair of Strings, where the first String gives the filename and the second String gives the contents in the file.</p>
<p>this can be very helpful to simply loop through the files and take an action, such as counting the number of words per address, as folows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// this just collects the file names which is the first element of the tuple given by &quot;._1&quot; 
souAll.map( fileContentsPair =&gt; fileContentsPair._1).collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res22: Array[String] = Array(dbfs:/datasets/sou/17900108.txt, dbfs:/datasets/sou/17901208.txt, dbfs:/datasets/sou/17911025.txt, dbfs:/datasets/sou/17921106.txt, dbfs:/datasets/sou/17931203.txt, dbfs:/datasets/sou/17941119.txt, dbfs:/datasets/sou/17951208.txt, dbfs:/datasets/sou/17961207.txt, dbfs:/datasets/sou/17971122.txt, dbfs:/datasets/sou/17981208.txt, dbfs:/datasets/sou/17991203.txt, dbfs:/datasets/sou/18001111.txt, dbfs:/datasets/sou/18011208.txt, dbfs:/datasets/sou/18021215.txt, dbfs:/datasets/sou/18031017.txt, dbfs:/datasets/sou/18041108.txt, dbfs:/datasets/sou/18051203.txt, dbfs:/datasets/sou/18061202.txt, dbfs:/datasets/sou/18071027.txt, dbfs:/datasets/sou/18081108.txt, dbfs:/datasets/sou/18091129.txt, dbfs:/datasets/sou/18101205.txt, dbfs:/datasets/sou/18111105.txt, dbfs:/datasets/sou/18121104.txt, dbfs:/datasets/sou/18131207.txt, dbfs:/datasets/sou/18140920.txt, dbfs:/datasets/sou/18151205.txt, dbfs:/datasets/sou/18161203.txt, dbfs:/datasets/sou/18171212.txt, dbfs:/datasets/sou/18181116.txt, dbfs:/datasets/sou/18191207.txt, dbfs:/datasets/sou/18201114.txt, dbfs:/datasets/sou/18211203.txt, dbfs:/datasets/sou/18221203.txt, dbfs:/datasets/sou/18231202.txt, dbfs:/datasets/sou/18241207.txt, dbfs:/datasets/sou/18251206.txt, dbfs:/datasets/sou/18261205.txt, dbfs:/datasets/sou/18271204.txt, dbfs:/datasets/sou/18281202.txt, dbfs:/datasets/sou/18291208.txt, dbfs:/datasets/sou/18301206.txt, dbfs:/datasets/sou/18311206.txt, dbfs:/datasets/sou/18321204.txt, dbfs:/datasets/sou/18331203.txt, dbfs:/datasets/sou/18341201.txt, dbfs:/datasets/sou/18351207.txt, dbfs:/datasets/sou/18361205.txt, dbfs:/datasets/sou/18371205.txt, dbfs:/datasets/sou/18381203.txt, dbfs:/datasets/sou/18391202.txt, dbfs:/datasets/sou/18401205.txt, dbfs:/datasets/sou/18411207.txt, dbfs:/datasets/sou/18421206.txt, dbfs:/datasets/sou/18431206.txt, dbfs:/datasets/sou/18441203.txt, dbfs:/datasets/sou/18451202.txt, dbfs:/datasets/sou/18461208.txt, dbfs:/datasets/sou/18471207.txt, dbfs:/datasets/sou/18481205.txt, dbfs:/datasets/sou/18491204.txt, dbfs:/datasets/sou/18501202.txt, dbfs:/datasets/sou/18511202.txt, dbfs:/datasets/sou/18521206.txt, dbfs:/datasets/sou/18531205.txt, dbfs:/datasets/sou/18541204.txt, dbfs:/datasets/sou/18551231.txt, dbfs:/datasets/sou/18561202.txt, dbfs:/datasets/sou/18571208.txt, dbfs:/datasets/sou/18581206.txt, dbfs:/datasets/sou/18591219.txt, dbfs:/datasets/sou/18601203.txt, dbfs:/datasets/sou/18611203.txt, dbfs:/datasets/sou/18621201.txt, dbfs:/datasets/sou/18631208.txt, dbfs:/datasets/sou/18641206.txt, dbfs:/datasets/sou/18651204.txt, dbfs:/datasets/sou/18661203.txt, dbfs:/datasets/sou/18671203.txt, dbfs:/datasets/sou/18681209.txt, dbfs:/datasets/sou/18691206.txt, dbfs:/datasets/sou/18701205.txt, dbfs:/datasets/sou/18711204.txt, dbfs:/datasets/sou/18721202.txt, dbfs:/datasets/sou/18731201.txt, dbfs:/datasets/sou/18741207.txt, dbfs:/datasets/sou/18751207.txt, dbfs:/datasets/sou/18761205.txt, dbfs:/datasets/sou/18771203.txt, dbfs:/datasets/sou/18781202.txt, dbfs:/datasets/sou/18791201.txt, dbfs:/datasets/sou/18801206.txt, dbfs:/datasets/sou/18811206.txt, dbfs:/datasets/sou/18821204.txt, dbfs:/datasets/sou/18831204.txt, dbfs:/datasets/sou/18841201.txt, dbfs:/datasets/sou/18851208.txt, dbfs:/datasets/sou/18861206.txt, dbfs:/datasets/sou/18871206.txt, dbfs:/datasets/sou/18881203.txt, dbfs:/datasets/sou/18891203.txt, dbfs:/datasets/sou/18901201.txt, dbfs:/datasets/sou/18911209.txt, dbfs:/datasets/sou/18921206.txt, dbfs:/datasets/sou/18931203.txt, dbfs:/datasets/sou/18941202.txt, dbfs:/datasets/sou/18951207.txt, dbfs:/datasets/sou/18961204.txt, dbfs:/datasets/sou/18971206.txt, dbfs:/datasets/sou/18981205.txt, dbfs:/datasets/sou/18991205.txt, dbfs:/datasets/sou/19001203.txt, dbfs:/datasets/sou/19011203.txt, dbfs:/datasets/sou/19021202.txt, dbfs:/datasets/sou/19031207.txt, dbfs:/datasets/sou/19041206.txt, dbfs:/datasets/sou/19051205.txt, dbfs:/datasets/sou/19061203.txt, dbfs:/datasets/sou/19071203.txt, dbfs:/datasets/sou/19081208.txt, dbfs:/datasets/sou/19091207.txt, dbfs:/datasets/sou/19101206.txt, dbfs:/datasets/sou/19111205.txt, dbfs:/datasets/sou/19121203.txt, dbfs:/datasets/sou/19131202.txt, dbfs:/datasets/sou/19141208.txt, dbfs:/datasets/sou/19151207.txt, dbfs:/datasets/sou/19161205.txt, dbfs:/datasets/sou/19171204.txt, dbfs:/datasets/sou/19181202.txt, dbfs:/datasets/sou/19191202.txt, dbfs:/datasets/sou/19201207.txt, dbfs:/datasets/sou/19211206.txt, dbfs:/datasets/sou/19221208.txt, dbfs:/datasets/sou/19231206.txt, dbfs:/datasets/sou/19241203.txt, dbfs:/datasets/sou/19251208.txt, dbfs:/datasets/sou/19261207.txt, dbfs:/datasets/sou/19271206.txt, dbfs:/datasets/sou/19281204.txt, dbfs:/datasets/sou/19291203.txt, dbfs:/datasets/sou/19301202.txt, dbfs:/datasets/sou/19311208.txt, dbfs:/datasets/sou/19321206.txt, dbfs:/datasets/sou/19340103.txt, dbfs:/datasets/sou/19350104.txt, dbfs:/datasets/sou/19360103.txt, dbfs:/datasets/sou/19370106.txt, dbfs:/datasets/sou/19380103.txt, dbfs:/datasets/sou/19390104.txt, dbfs:/datasets/sou/19400103.txt, dbfs:/datasets/sou/19410106.txt, dbfs:/datasets/sou/19420106.txt, dbfs:/datasets/sou/19430107.txt, dbfs:/datasets/sou/19440111.txt, dbfs:/datasets/sou/19450106.txt, dbfs:/datasets/sou/19460121.txt, dbfs:/datasets/sou/19470106.txt, dbfs:/datasets/sou/19480107.txt, dbfs:/datasets/sou/19490105.txt, dbfs:/datasets/sou/19500104.txt, dbfs:/datasets/sou/19510108.txt, dbfs:/datasets/sou/19520109.txt, dbfs:/datasets/sou/19530107.txt, dbfs:/datasets/sou/19530202.txt, dbfs:/datasets/sou/19540107.txt, dbfs:/datasets/sou/19550106.txt, dbfs:/datasets/sou/19560105.txt, dbfs:/datasets/sou/19570110.txt, dbfs:/datasets/sou/19580109.txt, dbfs:/datasets/sou/19590109.txt, dbfs:/datasets/sou/19600107.txt, dbfs:/datasets/sou/19610112.txt, dbfs:/datasets/sou/19610130.txt, dbfs:/datasets/sou/19620111.txt, dbfs:/datasets/sou/19630114.txt, dbfs:/datasets/sou/19640108.txt, dbfs:/datasets/sou/19650104.txt, dbfs:/datasets/sou/19660112.txt, dbfs:/datasets/sou/19670110.txt, dbfs:/datasets/sou/19680117.txt, dbfs:/datasets/sou/19690114.txt, dbfs:/datasets/sou/19700122.txt, dbfs:/datasets/sou/19710122.txt, dbfs:/datasets/sou/19720120.txt, dbfs:/datasets/sou/19730202.txt, dbfs:/datasets/sou/19740130.txt, dbfs:/datasets/sou/19750115.txt, dbfs:/datasets/sou/19760119.txt, dbfs:/datasets/sou/19770112.txt, dbfs:/datasets/sou/19780119.txt, dbfs:/datasets/sou/19790125.txt, dbfs:/datasets/sou/19800121.txt, dbfs:/datasets/sou/19810116.txt, dbfs:/datasets/sou/19820126.txt, dbfs:/datasets/sou/19830125.txt, dbfs:/datasets/sou/19840125.txt, dbfs:/datasets/sou/19850206.txt, dbfs:/datasets/sou/19860204.txt, dbfs:/datasets/sou/19870127.txt, dbfs:/datasets/sou/19880125.txt, dbfs:/datasets/sou/19890209.txt, dbfs:/datasets/sou/19900131.txt, dbfs:/datasets/sou/19910129.txt, dbfs:/datasets/sou/19920128.txt, dbfs:/datasets/sou/19930217.txt, dbfs:/datasets/sou/19940125.txt, dbfs:/datasets/sou/19950124.txt, dbfs:/datasets/sou/19960123.txt, dbfs:/datasets/sou/19970204.txt, dbfs:/datasets/sou/19980127.txt, dbfs:/datasets/sou/19990119.txt, dbfs:/datasets/sou/20000127.txt, dbfs:/datasets/sou/20010227.txt, dbfs:/datasets/sou/20010920.txt, dbfs:/datasets/sou/20020129.txt, dbfs:/datasets/sou/20030128.txt, dbfs:/datasets/sou/20040120.txt, dbfs:/datasets/sou/20050202.txt, dbfs:/datasets/sou/20060131.txt, dbfs:/datasets/sou/20070123.txt, dbfs:/datasets/sou/20080128.txt, dbfs:/datasets/sou/20090224.txt, dbfs:/datasets/sou/20100127.txt, dbfs:/datasets/sou/20110125.txt, dbfs:/datasets/sou/20120124.txt, dbfs:/datasets/sou/20130212.txt, dbfs:/datasets/sou/20140128.txt, dbfs:/datasets/sou/20150120.txt, dbfs:/datasets/sou/20160112.txt, dbfs:/datasets/sou/20170228.txt)
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let us find the number of words in each of the SoU addresses next (we need to work with Strings inside the closure!).</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val wcs = souAll.map( fileContentsPair =&gt; 
  {
    val wc = fileContentsPair._2
                             .replaceAll(&quot;\\s+&quot;, &quot; &quot;) //replace multiple whitespace characters (including space, tab, new line, etc.) with one whitespace &quot; &quot;
                             .replaceAll(&quot;&quot;&quot;([,?.!:;])&quot;&quot;&quot;, &quot;&quot;) // replace the following punctions characters: , ? . ! : ; . with the empty string &quot;&quot;
                             .toLowerCase() // converting to lower-case
                             .split(&quot; &quot;) // split each word separated by white space
                             .size // find the length of array
    wc
  }    
)      
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>wcs: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[804] at map at command-4088905069026045:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">wcs.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res24: Array[Int] = Array(1088, 1408, 2308, 2103, 1970, 2920, 1991, 2872, 2062, 2223, 1510, 1377, 3229, 2206, 2284, 2106, 2948, 2873, 2399, 2693, 1836, 2451, 2280, 3247, 3263, 2119, 3159, 3372, 4432, 4382, 4716, 3446, 5828, 4739, 6387, 8423, 9009, 7754, 6993, 7319, 10528, 15077, 7194, 7880, 7906, 13451, 10814, 12360, 11444, 11484, 13425, 8985, 8245, 8406, 8031, 9314, 16117, 18226, 16416, 21295, 7624, 8323, 13248, 9927, 9592, 10139, 11615, 10478, 13656, 16352, 12341, 14030, 6981, 8397, 6116, 5994, 9223, 7132, 11986, 9822, 7708, 8742, 6464, 3995, 10028, 9200, 12209, 6799, 8023, 7887, 11638, 6703, 3828, 3073, 3790, 8953, 19751, 15140, 5294, 9025, 13006, 11527, 16297, 13683, 12288, 15897, 14676, 15448, 12115, 20213, 15138, 19139, 19600, 9761, 14881, 17411, 25038, 23580, 27387, 19388, 13895, 6769, 23710, 25156, 3557, 4538, 7691, 2122, 3918, 5470, 4760, 2710, 5609, 5749, 6706, 6969, 10848, 10313, 8782, 8065, 10998, 4551, 5691, 4215, 2229, 3521, 3816, 2736, 4683, 3742, 3176, 3294, 3476, 4539, 3761, 8130, 27728, 6039, 5097, 3403, 5132, 3994, 5340, 9605, 6947, 5980, 7240, 8251, 4135, 4917, 4875, 5634, 6194, 5171, 6442, 5332, 3193, 4400, 5261, 7117, 4861, 4113, 4460, 4480, 3972, 1660, 5159, 4109, 4957, 4646, 4585, 3248, 3401, 33569, 5200, 5577, 4978, 4255, 3512, 3808, 4860, 4827, 3776, 3771, 4724, 7013, 7406, 9149, 6289, 6748, 7289, 7478, 7393, 4401, 2972, 3830, 5378, 5177, 5055, 5327, 5604, 5758, 5945, 7111, 6945, 6992, 6472, 6783, 6732, 5443, 5042)
</code></pre>
</div>
</div>
<div class="cell markdown">
<h2 id="youtry-homework"><a class="header" href="#youtry-homework">YouTry: HOMEWORK</a></h2>
<ul>
<li>HOWEWORK WordCount 1: <code>sortBy</code></li>
<li>HOMEWROK WordCount 2: <code>dbutils.fs</code></li>
</ul>
</div>
<div class="cell markdown">
<h5 id="homework-wordcount-1-sortby"><a class="header" href="#homework-wordcount-1-sortby">HOMEWORK WordCount 1. <code>sortBy</code></a></h5>
<p>Let's understand <code>sortBy</code> a bit more carefully.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val example = &quot;Master, Master!   It's me, Sméagol... mhrhm*%* But they took away our precious, they wronged us. Gollum will protect us..., Master, it's me Sméagol.&quot;
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>example: String = Master, Master!   It's me, Sméagol... mhrhm*%* But they took away our precious, they wronged us. Gollum will protect us..., Master, it's me Sméagol.
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val sou17900108 = sc.textFile(&quot;dbfs:/datasets/sou/17900108.txt&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>sou17900108: org.apache.spark.rdd.RDD[String] = dbfs:/datasets/sou/17900108.txt MapPartitionsRDD[169] at textFile at command-2971213210278065:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rddWords = sou17900108
    .flatMap(line =&gt; 
         line.replaceAll(&quot;\\s+&quot;, &quot; &quot;) //replace multiple whitespace characters (including space, tab, new line, etc.) with one whitespace &quot; &quot;
             .replaceAll(&quot;&quot;&quot;([,?.!:;])&quot;&quot;&quot;, &quot;&quot;) // replace the following punctions characters: , ? . ! : ; . with the empty string &quot;&quot;
             .toLowerCase() // converting to lower-case
             .split(&quot; &quot;))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rddWords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[171] at flatMap at command-2971213210278066:2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rddWords.take(10)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res25: Array[String] = Array(george, washington, &quot;&quot;, january, 8, 1790, fellow-citizens, of, the, senate)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val wordCounts = rddWords
                  .map(x =&gt; (x,1))
                  .reduceByKey(_+_)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>wordCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[173] at reduceByKey at command-2971213210278068:3
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val top10 = wordCounts.sortBy(_._2, false).take(10)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>top10: Array[(String, Int)] = Array((the,97), (of,68), (to,56), (and,40), (a,21), (be,20), (in,20), (which,18), (that,15), (will,14))
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Make your code easy to read for other developers ;)
Use 'case classes' with well defined variable names that everyone can understand</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val top10 = wordCounts.sortBy({
  case (word, count) =&gt; count
}, false).take(10)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>top10: Array[(String, Int)] = Array((the,97), (of,68), (to,56), (and,40), (a,21), (be,20), (in,20), (which,18), (that,15), (will,14))
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>If you just want a total count of all words in the file</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rddWords.count
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res26: Long = 1089
</code></pre>
</div>
</div>
<div class="cell markdown">
<h5 id="youttry-homework-wordcount-2-dbutilsfs"><a class="header" href="#youttry-homework-wordcount-2-dbutilsfs">YoutTry: HOMEWORK WordCount 2: <code>dbutils.fs</code></a></h5>
<p>Have a brief look at what other commands dbutils.fs supports. We will introduce them as needed.</p>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">dbutils.fs.help // some of these were used to ETL this data into dbfs:/datasets/sou 
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<div class = "ansiout"><b>dbutils.fs</b> provides utilities for working with FileSystems. Most methods in
this package can take either a DBFS path (e.g., "/foo" or "dbfs:/foo"), or
another FileSystem URI.
<p>For more info about a method, use <b>dbutils.fs.help(&quot;methodName&quot;)</b>.</p>
<p>In notebooks, you can also use the %fs shorthand to access DBFS. The %fs shorthand maps
straightforwardly onto dbutils calls. For example, &quot;%fs head --maxBytes=10000 /file/path&quot;
translates into &quot;dbutils.fs.head(&quot;/file/path&quot;, maxBytes = 10000)&quot;.
<h3 id="fsutils"><a class="header" href="#fsutils">fsutils</a></h3><b>cp(from: String, to: String, recurse: boolean = false): boolean</b> -&gt; Copies a file or directory, possibly across FileSystems<br /><b>head(file: String, maxBytes: int = 65536): String</b> -&gt; Returns up to the first 'maxBytes' bytes of the given file as a String encoded in UTF-8<br /><b>ls(dir: String): Seq</b> -&gt; Lists the contents of a directory<br /><b>mkdirs(dir: String): boolean</b> -&gt; Creates the given directory if it does not exist, also creating any necessary parent directories<br /><b>mv(from: String, to: String, recurse: boolean = false): boolean</b> -&gt; Moves a file or directory, possibly across FileSystems<br /><b>put(file: String, contents: String, overwrite: boolean = false): boolean</b> -&gt; Writes the given String out to a file, encoded in UTF-8<br /><b>rm(dir: String, recurse: boolean = false): boolean</b> -&gt; Removes a file or directory<br /><br /><h3 id="mount"><a class="header" href="#mount">mount</a></h3><b>mount(source: String, mountPoint: String, encryptionType: String = &quot;&quot;, owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -&gt; Mounts the given source directory into DBFS at the given mount point<br /><b>mounts: Seq</b> -&gt; Displays information about what is mounted within DBFS<br /><b>refreshMounts: boolean</b> -&gt; Forces all machines in this cluster to refresh their mount cache, ensuring they receive the most recent information<br /><b>unmount(mountPoint: String): boolean</b> -&gt; Deletes a DBFS mount point<br /><br /></div></p>
</div>
</div>
<div class="cell markdown">
<h2 id="exercise-2-souwordcount"><a class="header" href="#exercise-2-souwordcount">Exercise 2: SouWordCount</a></h2>
<p>Count the number of each word across all the &quot;dbfs:/datasets/sou/*.txt&quot; files and output the result as an Array of (word,count) tuples from the most frequent to the least frequent word.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// code in this cell the solution to the above exercise in the notebook environment
//
//
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="piped-rdds-and-bayesian-ab-testing"><a class="header" href="#piped-rdds-and-bayesian-ab-testing">Piped RDDs and Bayesian AB Testing</a></h1>
<p>Advanced Topic.</p>
</div>
<div class="cell markdown">
<p>Here we will first take excerpts with minor modifications from the end of <strong>Chapter 12. Resilient Distributed Datasets (RDDs)</strong> of <em>Spark: The Definitive Guide</em>:</p>
<ul>
<li>https://learning.oreilly.com/library/view/spark-the-definitive/9781491912201/ch12.html</li>
</ul>
<p>Next, we will do Bayesian AB Testing using PipedRDDs.</p>
</div>
<div class="cell markdown">
<p>First, we create the toy RDDs as in <em>The Definitive Guide</em>:</p>
<blockquote>
<p><strong>From a Local Collection</strong></p>
</blockquote>
<blockquote>
<p>To create an RDD from a collection, you will need to use the parallelize method on a SparkContext (within a SparkSession). This turns a single node collection into a parallel collection. When creating this parallel collection, you can also explicitly state the number of partitions into which you would like to distribute this array. In this case, we are creating two partitions:</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// in Scala
val myCollection = &quot;Spark The Definitive Guide : Big Data Processing Made Simple&quot;  .split(&quot; &quot;)
val words = spark.sparkContext.parallelize(myCollection, 2)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>myCollection: Array[String] = Array(Spark, The, Definitive, Guide, :, Big, Data, Processing, Made, Simple)
words: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[805] at parallelize at command-4088905069025460:3
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># in Python
myCollection = &quot;Spark The Definitive Guide : Big Data Processing Made Simple&quot;\
  .split(&quot; &quot;)
words = spark.sparkContext.parallelize(myCollection, 2)
words
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<p><strong>glom</strong> from <em>The Definitive Guide</em></p>
</blockquote>
<blockquote>
<p><code>glom</code> is an interesting function that takes every partition in your dataset and converts them to arrays. This can be useful if you’re going to collect the data to the driver and want to have an array for each partition. However, this can cause serious stability issues because if you have large partitions or a large number of partitions, it’s simple to crash the driver.</p>
</blockquote>
</div>
<div class="cell markdown">
<p>Let's use <code>glom</code> to see how our <code>words</code> are distributed among the two partitions we used explicitly.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">words.glom.collect 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res0: Array[Array[String]] = Array(Array(Spark, The, Definitive, Guide, :), Array(Big, Data, Processing, Made, Simple))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">words.glom().collect()
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<p><strong>Checkpointing</strong> from <em>The Definitive Guide</em></p>
</blockquote>
<blockquote>
<p>One feature not available in the DataFrame API is the concept of checkpointing. Checkpointing is the act of saving an RDD to disk so that future references to this RDD point to those intermediate partitions on disk rather than recomputing the RDD from its original source. This is similar to caching except that it’s not stored in memory, only disk. This can be helpful when performing iterative computation, similar to the use cases for caching:</p>
</blockquote>
<p>Let's create a directory in <code>dbfs:///</code> for checkpointing of RDDs in the sequel. The following <code>%fs mkdirs /path_to_dir</code> is a shortcut to create a directory in <code>dbfs:///</code></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-fs">mkdirs /datasets/ScaDaMaLe/checkpointing/
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res1: Boolean = true
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">spark.sparkContext.setCheckpointDir(&quot;dbfs:///datasets/ScaDaMaLe/checkpointing&quot;)
words.checkpoint()
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<p>Now, when we reference this RDD, it will derive from the checkpoint instead of the source data. This can be a helpful optimization.</p>
</blockquote>
</div>
<div class="cell markdown">
<h2 id="youtry"><a class="header" href="#youtry">YouTry</a></h2>
<p>Just some more words in <code>haha_words</code> with <code>\n</code>, the End-Of-Line (EOL) characters, in-place.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val haha_words = sc.parallelize(Seq(&quot;ha\nha&quot;, &quot;he\nhe\nhe&quot;, &quot;ho\nho\nho\nho&quot;),3)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>haha_words: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[809] at parallelize at command-4088905069025471:1
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let's use <code>glom</code> to see how our <code>haha_words</code> are distributed among the partitions</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">haha_words.glom.collect
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res3: Array[Array[String]] =
Array(Array(ha
ha), Array(he
he
he), Array(ho
ho
ho
ho))
</code></pre>
</div>
</div>
<div class="cell markdown">
<blockquote>
<h1 id="pipe-rdds-to-system-commands"><a class="header" href="#pipe-rdds-to-system-commands">Pipe RDDs to System Commands</a></h1>
</blockquote>
<blockquote>
<p>The pipe method is probably one of Spark’s more interesting methods. With pipe, you can return an RDD created by piping elements to a forked external process. The resulting RDD is computed by executing the given process once per partition. All elements of each input partition are written to a process’s stdin as lines of input separated by a newline. The resulting partition consists of the process’s stdout output, with each line of stdout resulting in one element of the output partition. A process is invoked even for empty partitions.</p>
</blockquote>
<blockquote>
<p>The print behavior can be customized by providing two functions.</p>
</blockquote>
<p>We can use a simple example and pipe each partition to the command wc. Each row will be passed in as a new line, so if we perform a line count, we will get the number of lines, one per partition:</p>
</div>
<div class="cell markdown">
<p>The following produces a <code>PipedRDD</code>:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val wc_l_PipedRDD = words.pipe(&quot;wc -l&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>wc_l_PipedRDD: org.apache.spark.rdd.RDD[String] = PipedRDD[811] at pipe at command-4088905069025476:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">wc_l_PipedRDD = words.pipe(&quot;wc -l&quot;)
wc_l_PipedRDD
</code></pre>
</div>
<div class="cell markdown">
<p>Now, we take an action via <code>collect</code> to bring the results to the Driver.</p>
<p>NOTE: Be careful what you collect! You can always write the output to parquet of binary files in <code>dbfs:///</code> if the returned output is large.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">wc_l_PipedRDD.collect
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res4: Array[String] = Array(5, 5)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">wc_l_PipedRDD.collect()
</code></pre>
</div>
<div class="cell markdown">
<p>In this case, we got the number of lines returned by <code>wc -l</code> per partition.</p>
</div>
<div class="cell markdown">
<h2 id="youtry-1"><a class="header" href="#youtry-1">YouTry</a></h2>
<p>Try to make sense of the next few cells where we do NOT specifiy the number of partitions explicitly and let Spark decide on the number of partitions automatically.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val haha_words = sc.parallelize(Seq(&quot;ha\nha&quot;, &quot;he\nhe\nhe&quot;, &quot;ho\nho\nho\nho&quot;),3)
haha_words.glom.collect
val wc_l_PipedRDD_haha_words = haha_words.pipe(&quot;wc -l&quot;)
wc_l_PipedRDD_haha_words.collect()
</code></pre>
</div>
<div class="cell markdown">
<p>Do you understand why the above <code>collect</code> statement returns what it does?</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val haha_words_again = sc.parallelize(Seq(&quot;ha\nha&quot;, &quot;he\nhe\nhe&quot;, &quot;ho\nho\nho\nho&quot;))
haha_words_again.glom.collect
val wc_l_PipedRDD_haha_words_again = haha_words_again.pipe(&quot;wc -l&quot;)
wc_l_PipedRDD_haha_words_again.collect()
</code></pre>
</div>
<div class="cell markdown">
<p>Did you understand why some of the results are <code>0</code> in the last <code>collect</code> statement?</p>
</div>
<div class="cell markdown">
<blockquote>
<h1 id="mappartitions"><a class="header" href="#mappartitions">mapPartitions</a></h1>
</blockquote>
<blockquote>
<p>The previous command revealed that Spark operates on a per-partition basis when it comes to actually executing code. You also might have noticed earlier that the return signature of a map function on an RDD is actually <code>MapPartitionsRDD</code>.</p>
</blockquote>
<p>Or <code>ParallelCollectionRDD</code> in our case.</p>
<blockquote>
<p>This is because map is just a row-wise alias for <code>mapPartitions</code>, which makes it possible for you to map an individual partition (represented as an iterator). That’s because physically on the cluster we operate on each partition individually (and not a specific row). A simple example creates the value “1” for every partition in our data, and the sum of the following expression will count the number of partitions we have:</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// in Scala
words.mapPartitions(part =&gt; Iterator[Int](1)).sum() // 2.0
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># in Python
words.mapPartitions(lambda part: [1]).sum() # 2
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<p>Naturally, this means that we operate on a per-partition basis and therefore it allows us to perform an operation on that <em>entire</em> partition. This is valuable for performing something on an entire subdataset of your RDD. You can gather all values of a partition class or group into one partition and then operate on that entire group using arbitrary functions and controls. An example use case of this would be that you could pipe this through some custom machine learning algorithm and train an individual model for that company’s portion of the dataset. A Facebook engineer has an interesting demonstration of their particular implementation of the pipe operator with a similar use case demonstrated at <a href="https://spark-summit.org/east-2017/events/experiences-with-sparks-rdd-apis-for-complex-custom-applications/">Spark Summit East 2017</a>.</p>
</blockquote>
<blockquote>
<p>Other functions similar to <code>mapPartitions</code> include <code>mapPartitionsWithIndex</code>. With this you specify a function that accepts an index (within the partition) and an iterator that goes through all items within the partition. The partition index is the partition number in your RDD, which identifies where each record in our dataset sits (and potentially allows you to debug). You might use this to test whether your map functions are behaving correctly:</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// in Scala
def indexedFunc(partitionIndex:Int, withinPartIterator: Iterator[String]) = {  withinPartIterator.toList.map(    
  value =&gt; s&quot;Partition: $partitionIndex =&gt; $value&quot;).iterator
                                                                            }
words.mapPartitionsWithIndex(indexedFunc).collect()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># in Python
def indexedFunc(partitionIndex, withinPartIterator):  
  return [&quot;partition: {} =&gt; {}&quot;.format(partitionIndex,    x) for x in withinPartIterator]
words.mapPartitionsWithIndex(indexedFunc).collect()
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<h1 id="foreachpartition"><a class="header" href="#foreachpartition">foreachPartition</a></h1>
</blockquote>
<blockquote>
<p>Although <code>mapPartitions</code> needs a return value to work properly, this next function does not. <code>foreachPartition</code> simply iterates over all the partitions of the data. The difference is that the function has no return value. This makes it great for doing something with each partition like writing it out to a database. In fact, this is how many data source connectors are written. You can create</p>
</blockquote>
<p>your</p>
<blockquote>
<p>own text file source if you want by specifying outputs to the temp directory with a random ID:</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">words.foreachPartition { iter =&gt;  
  import java.io._  
  import scala.util.Random  
  val randomFileName = new Random().nextInt()  
  val pw = new PrintWriter(new File(s&quot;/tmp/random-file-${randomFileName}.txt&quot;))  
  while (iter.hasNext) {
    pw.write(iter.next())  
  }  
  pw.close()
}
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<p>You’ll find these two files if you scan your /tmp directory.</p>
</blockquote>
<p>You need to scan for the file across all the nodes. As the file may not be in the Driver node's <code>/tmp/</code> directory but in those of the executors that hosted the partition.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">pwd
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">ls /tmp/random-file-*.txt
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell markdown">
<h1 id="numerically-rigorous-bayesian-ab-testing"><a class="header" href="#numerically-rigorous-bayesian-ab-testing">Numerically Rigorous Bayesian AB Testing</a></h1>
<p>This is an example of Bayesian AB Testing with computer-aided proofs for the posterior samples.</p>
<p>The main learning goal for you is to use pipedRDDs to distribute, in an embarassingly paralle way, across all the worker nodes in the Spark cluster an executible <code>IsIt1or2Coins</code>.</p>
<h3 id="what-does-isit1or2coins-do"><a class="header" href="#what-does-isit1or2coins-do">What does <code>IsIt1or2Coins</code> do?</a></h3>
<p>At a very high-level, to understand what <code>IsIt1or2Coins</code> does, imagine the following simple experiment.</p>
<p>We are given</p>
<ul>
<li>the number of heads that result from a first sequence of independent and identical tosses of a coin and then</li>
<li>we are given the number of heads that result from a second sequence of independent and identical tosses of a coin</li>
</ul>
<p>Our decision problem is to do help shed light on whether both sequence of tosses came from the same coin or not (whatever the bias may be).</p>
<p><code>IsIt1or2Coins</code> tries to help us decide if the two sequence of coin-tosses are based on one coin with an unknown bias or two coins with different biases.</p>
<p>If you are curious about details feel free to see:</p>
<ul>
<li>Exact Bayesian A/B testing using distributed fault-tolerant Moore rejection sampler, Benny Avelin and Raazesh Sainudiin, Extended Abstract, 2 pages, 2018 <a href="http://lamastex.org/preprints/20180507_ABTestingViaDistributedMRS.pdf">(PDF 104KB)</a>.</li>
<li>which builds on: An auto-validating, trans-dimensional, universal rejection sampler for locally Lipschitz arithmetical expressions, Raazesh Sainudiin and Thomas York, <a href="http://interval.louisiana.edu/reliable-computing-journal/volume-18/reliable-computing-18-pp-015-054.pdf">Reliable Computing, vol.18, pp.15-54, 2013</a> (<a href="http://lamastex.org/preprints/avs_rc_2013.pdf">preprint: PDF 2612KB</a>)</li>
</ul>
<p><strong>See first about <code>PipedRDDs</code> excerpt from <em>Spark The Definitive Guide</em> earlier.</strong></p>
<h3 id="getting-the-executible-isit1or2coins-into-our-spark-cluster"><a class="header" href="#getting-the-executible-isit1or2coins-into-our-spark-cluster">Getting the executible <code>IsIt1or2Coins</code> into our Spark Cluster</a></h3>
<p><strong>This has already been done in the project-shard. You need not do it again for this executible!</strong></p>
<p>You need to upload the C++ executible <code>IsIt1or2Coins</code> from: - https://github.com/lamastex/mrs2</p>
<p>Here, suppose you have an executible for linux x86 64 bit processor with all dependencies pre-compiled into one executibe.</p>
<p>Say this executible is <code>IsIt10r2Coins</code>.</p>
<p>This executible comes from the following dockerised build:</p>
<ul>
<li>https://github.com/lamastex/mrs2/tree/master/docker</li>
<li>by statically compiling inside the docerised environment for mrs2:
<ul>
<li>https://github.com/lamastex/mrs2/tree/master/mrs-2.0/examples/MooreRejSam/IsIt1or2Coins</li>
</ul>
</li>
</ul>
<p>You can replace the executible with any other executible with appropriate I/O to it.</p>
<p>Then you upload the executible to databricks' <code>FileStore</code>.</p>
</div>
<div class="cell markdown">
<p>Just note the path to the file and DO NOT click <code>Create Table</code> or other buttons!</p>
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/images/2020/ScaDaMaLe/screenShotOfUploadingStaticExecutibleIsIt1or2CoinsViaFileStore.png" alt="creenShotOfUploadingStaticExecutibleIsIt1or2CoinsViaFileStore" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-fs">ls &quot;/FileStore/tables/IsIt1or2Coins&quot;
</code></pre>
</div>
<div class="cell markdown">
<p>Now copy the file from <code>dbfs://FileStore</code> that you just uploaded into the local file system of the Driver.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">dbutils.fs.cp(&quot;dbfs:/FileStore/tables/IsIt1or2Coins&quot;, &quot;file:/tmp/IsIt1or2Coins&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">ls -al /tmp/IsIt1or2Coins
</code></pre>
</div>
<div class="cell markdown">
<p>Note it is a big static executible with all dependencies inbuilt (it uses GNU Scientific Library and a specialized C++ Library called C-XSC or C Extended for Scientific Computing to do hard-ware optimized rigorous numerical proofs using Interval-Extended Hessian Differentiation Arithmetics over Rounding-Controlled Hardware-Specified Machine Intervals).</p>
<p>Just note it is over 6.5MB. Also we need to change the permissions so it is indeed executible.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">chmod +x /tmp/IsIt1or2Coins
</code></pre>
</div>
<div class="cell markdown">
<h1 id="usage-instructions-for-isit1or2coins"><a class="header" href="#usage-instructions-for-isit1or2coins">Usage instructions for IsIt1or2Coins</a></h1>
<p><code>./IsIt1or2Coins numboxes numiter seed numtosses1 heads1 numtosses2 heads2 logScale</code> - numboxes = Number of boxes for Moore Rejection Sampling (Rigorous von Neumann Rejection Sampler) - numiter = Number of samples drawn from posterior distribution to estimate the model probabilities - seed = a random number seed - numtosses1 = number of tosses for the first coin - heads1 = number of heads shown up on the first coin - numtosses2 = number of tosses for the second coin - heads2 = number of heads shown up on the second coin - logscale = True/False as Int</p>
<p>Don't worry about the details of what the executible <code>IsIt1or2Coins</code> is doing for now. Just realise that this executible takes some input on command-line and gives some output.</p>
</div>
<div class="cell markdown">
<p>Let's make sure the executible takes input and returns output string on the Driver node.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">/tmp/IsIt1or2Coins 1000 100 234565432 1000 500 1200 600 1
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh"># You can also do it like this

/dbfs/FileStore/tables/IsIt1or2Coins 1000 100 234565432 1000 500 1200 600 1
</code></pre>
</div>
<div class="cell markdown">
<h2 id="moving-the-executables-to-the-worker-nodes"><a class="header" href="#moving-the-executables-to-the-worker-nodes">Moving the executables to the worker nodes</a></h2>
</div>
<div class="cell markdown">
<p>To copy the executible from <code>dbfs</code> to the local drive of each executor you can use the following helper function.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import scala.sys.process._
import scala.concurrent.duration._
// from Ivan Sadikov

def copyFile(): Unit = {
  &quot;mkdir -p /tmp/executor/bin&quot;.!!
  &quot;cp /dbfs/FileStore/tables/IsIt1or2Coins /tmp/executor/bin/&quot;.!!
}

sc.runOnEachExecutor(copyFile, new FiniteDuration(1, HOURS))
</code></pre>
</div>
<div class="cell markdown">
<p>Now, let us use piped RDDs via <code>bash</code> to execute the given command in each partition as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val input = Seq(&quot;/tmp/executor/bin/IsIt1or2Coins 1000 100 234565432 1000 500 1200 600 1&quot;, &quot;/tmp/executor/bin/IsIt1or2Coins 1000 100 234565432 1000 500 1200 600 1&quot;)

val output = sc
  .parallelize(input)
  .repartition(2)
  .pipe(&quot;bash&quot;)
  .collect()
</code></pre>
</div>
<div class="cell markdown">
<p>In fact, you can just use <code>DBFS FUSE</code> to run the commands without any file copy in databricks-provisioned Spark clusters we are on here:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val isIt1or2StaticExecutible = &quot;/dbfs/FileStore/tables/IsIt1or2Coins&quot;
val same_input = Seq(s&quot;$isIt1or2StaticExecutible 1000 100 234565432 1000 500 1200 600 1&quot;, 
                     s&quot;$isIt1or2StaticExecutible 1000 100 234565432 1000 500 1200 600 1&quot;)

val same_output = sc
  .parallelize(same_input)
  .repartition(2)
  .pipe(&quot;bash&quot;)
  .collect()
</code></pre>
</div>
<div class="cell markdown">
<p>Thus by mixing several different executibles that are statically compiled for linux 64 bit machine, we can mix and match multiple executibles with appropriate inputs.</p>
<p>The resulting outputs can themselves be re-processed in Spark to feed into toher pipedRDDs or normal RDDs or DataFrames and DataSets.</p>
</div>
<div class="cell markdown">
<p>Finally, we can have more than one command per partition and then use <code>mapPartitions</code> to send all the executible commands within the input partition that is to be run by the executor in which that partition resides as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val isIt1or2StaticExecutible = &quot;/dbfs/FileStore/tables/IsIt1or2Coins&quot;

// let us make 2 commands in each of the 2 input partitions
val same_input_mp = Seq(s&quot;$isIt1or2StaticExecutible 1000 100 234565432 1000 500 1200 600 1&quot;, 
                        s&quot;$isIt1or2StaticExecutible 1000 100 123456789 1000 500 1200 600 1&quot;,
                        s&quot;$isIt1or2StaticExecutible 1000 100 123456789 1000 500 1200 600 1&quot;,
                        s&quot;$isIt1or2StaticExecutible 1000 100 234565432 1000 500 1200 600 1&quot;)

val same_output_mp = sc
  .parallelize(same_input)
  .repartition(2)
  .pipe(&quot;bash&quot;)
  .mapPartitions(x =&gt; Seq(x.mkString(&quot;\n&quot;)).iterator)
  .collect()
</code></pre>
</div>
<div class="cell markdown">
<p>allCatch is a useful tool to use as a filtering function when testing if a command will work without error.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import scala.util.control.Exception.allCatch
(allCatch opt &quot; 12 &quot;.trim.toLong).isDefined
</code></pre>
</div>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->

        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </body>
</html>
