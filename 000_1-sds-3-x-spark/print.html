<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>sds-3.x/ScaDaMaLe</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="scroll-mdbook-outputs.css">

        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x-spark/000_ScaDaMaLe.html">000_ScaDaMaLe</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x-spark/001_whySpark.html">001_whySpark</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x-spark/002_00_loginToDatabricks.html">002_00_loginToDatabricks</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x-spark/002_01_multiLingualNotebooks.html">002_01_multiLingualNotebooks</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x-spark/002_02_dbcCEdataLoader.html">002_02_dbcCEdataLoader</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x-spark/003_00_scalaCrashCourse.html">003_00_scalaCrashCourse</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x-spark/003_01_scalaCrashCourse.html">003_01_scalaCrashCourse</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x-spark/004_RDDsTransformationsActions.html">004_RDDsTransformationsActions</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x-spark/005_RDDsTransformationsActionsHOMEWORK.html">005_RDDsTransformationsActionsHOMEWORK</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x-spark/006_WordCount.html">006_WordCount</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x-spark/006a_PipedRDD.html">006a_PipedRDD</a></li><li class="chapter-item expanded affix "><a href="editors.html">Editors</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">sds-3.x/ScaDaMaLe</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<ul>
<li><strong>Course Name:</strong> <em>Scalable Data Science and Distributed Machine Learning</em></li>
<li><strong>Course Acronym:</strong> <em>ScaDaMaLe</em> or <em>sds-3.x</em>.</li>
</ul>
<p>The course is given in several modules.</p>
<h2 id="expected-reference-readings"><a class="header" href="#expected-reference-readings">Expected Reference Readings</a></h2>
<p>Note that you need to be logged into your library with access to these publishers:</p>
<ul>
<li><a href="https://learning.oreilly.com/library/view/high-performance-spark/9781491943199/">https://learning.oreilly.com/library/view/high-performance-spark/9781491943199/</a></li>
<li><a href="https://learning.oreilly.com/library/view/spark-the-definitive/9781491912201/">https://learning.oreilly.com/library/view/spark-the-definitive/9781491912201/</a></li>
<li><a href="https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/">https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/</a></li>
<li>Introduction to Algorithms, Third Edition, Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein from
<ul>
<li><a href="https://ebookcentral.proquest.com/lib/uu/reader.action?docID=3339142">https://ebookcentral.proquest.com/lib/uu/reader.action?docID=3339142</a></li>
</ul>
</li>
<li><a href="https://github.com/lamastex/scalable-data-science/tree/master/read">Reading Materials Provided</a></li>
</ul>
<h2 id="course-sponsors"><a class="header" href="#course-sponsors">Course Sponsors</a></h2>
<p>The course builds on contents developed since 2016 with support from New Zealand's Data Industry. The 2017-2019 versions were academically sponsored by Uppsala University's Inter-Faculty Course grant, Department of Mathematics and The Centre for Interdisciplinary Mathematics and industrially sponsored by <a href="https://databricks.com">databricks</a>, <a href="https://aws.amazon.com/">AWS</a> and Swedish data industry via <a href="https://combient.com">Combient AB</a>, <a href="https://seb.se/">SEB</a> and <a href="https://combient.com/mix">Combient Mix AB</a>. The 2021-2023 versions were/are academically sponsored by <a href="https://wasp-sweden.org/graduate-school/">WASP Graduate School</a> and <a href="https://www.math.uu.se/research/cim/">Centre for Interdisciplinary Mathematics</a>, and industrially sponsored by <a href="https://databricks.com">databricks</a> and <a href="https://aws.amazon.com/">AWS</a> via <em>databricks University Alliance</em> and <a href="https://combient.com/mix">Combient Mix AB</a> via industrial mentorships and internships.</p>
<h2 id="course-instructor"><a class="header" href="#course-instructor">Course Instructor</a></h2>
<p>I, Raazesh Sainudiin or <strong>Raaz</strong>, will be an instructor for the course.</p>
<p>I have</p>
<ul>
<li>more than 16 years of academic research experience in applied mathematics and statistics and</li>
<li>over 8 years of experience in the data industry.</li>
</ul>
<p>I currently (2022) have an effective joint appointment as:</p>
<ul>
<li><a href="https://katalog.uu.se/profile/?id=N17-214">Associate Professor of Mathematics with specialisation in Data Science</a> at <a href="https://www.math.uu.se/">Department of Mathematics</a>, <a href="https://www.uu.se/">Uppsala University</a>, Uppsala, Sweden and</li>
<li>Researching,Developing and Enabling Chair in Mathematical Data Engineering Sciences at <a href="https://combient.com/mix">Combient Mix AB</a>, Stockholm, Sweden</li>
</ul>
<p>Quick links on Raaz's background:</p>
<ul>
<li><a href="https://www.linkedin.com/in/raazesh-sainudiin-45955845/">https://www.linkedin.com/in/raazesh-sainudiin-45955845/</a></li>
<li><a href="https://lamastex.github.io/cv/">Raaz's academic CV</a></li>
<li><a href="https://lamastex.github.io/publications/">Raaz's publications list</a></li>
</ul>
</div>
<div class="cell markdown">
<h1 id="what-is-the-data-science-process"><a class="header" href="#what-is-the-data-science-process">What is the <a href="https://en.wikipedia.org/wiki/Data_science">Data Science Process</a></a></h1>
<p><strong>The Data Science Process in one picture</strong></p>
<p><img src="https://github.com/lamastex/scalable-data-science/raw/master/assets/images/sds.png" alt="what is sds?" title="sds" /></p>
<hr />
<h2 id="data-science-process-under-the-algorithms-machines-peoples-planet-framework"><a class="header" href="#data-science-process-under-the-algorithms-machines-peoples-planet-framework">Data Science Process under the Algorithms-Machines-Peoples-Planet Framework</a></h2>
<p>Note that the <strong>Data Product</strong> that is typically a desired <em>outcome</em> of the <em>Data Science Process</em> can be anything that has commercial value (to help make a livig, colloquially speaking), including a software product, hardware product, personalized medicine for a specific individual, or pasture-raised chicken based on intensive data collection from field experiments in regenerative agriculture, among others.</p>
<p>It is extremely important to be aware of the underlying actual costs and benefits in any Data Science Process under the <strong>Algorithms-Machines-Peoples-Planet Framework</strong>. We will see this in the sequel at a high level.</p>
<h2 id="what-is-scalable-data-science-and-distributed-machine-learning"><a class="header" href="#what-is-scalable-data-science-and-distributed-machine-learning">What is scalable data science and distributed machine learning?</a></h2>
<p>Scalability merely refers to the ability of the data science process to scale to massive datasets (popularly known as <em>big data</em>).</p>
<p>For this we need <em>distributed fault-tolerant computing</em> typically over large clusters of commodity computers -- the core infrastructure in a public cloud today.</p>
<p><em>Distributed Machine Learning</em> allows the models in the data science process to be scalably trained and extract value from big data.</p>
</div>
<div class="cell markdown">
<h2 id="what-is-data-science"><a class="header" href="#what-is-data-science">What is Data Science?</a></h2>
<p>It is increasingly accepted that <a href="https://en.wikipedia.org/wiki/Data_science">Data Science</a></p>
<blockquote>
<p>is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data. Data science is related to data mining, machine learning and big data.</p>
</blockquote>
<blockquote>
<p>Data science is a &quot;concept to unify statistics, data analysis and their related methods&quot; in order to &quot;understand and analyze actual phenomena&quot; with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, domain knowledge and information science. Turing award winner Jim Gray imagined data science as a &quot;fourth paradigm&quot; of science (empirical, theoretical, computational and now data-driven) and asserted that &quot;everything about science is changing because of the impact of information technology&quot; and the data deluge.</p>
</blockquote>
<p>Now, let us look at two industrially-informed academic papers that influence the above quote on what is Data Science, but with a view towards the contents and syllabus of this course.</p>
<p>Source: <a href="https://dl.acm.org/citation.cfm?id=2500499">Vasant Dhar, Data Science and Prediction, Communications of the ACM, Vol. 56 (1). p. 64, DOI:10.1145/2500499</a></p>
<p><strong>key insights in the above paper</strong></p>
<ul>
<li>Data Science is the study of <em>the generalizabile extraction of knowledge from data</em>.</li>
<li>A common epistemic requirement in assessing whether new knowledge is actionable for decision making is its predictive power, not just its ability to explain the past.</li>
<li>A <em>data scientist requires an integrated skill set spanning</em>
<ul>
<li>mathematics,</li>
<li>machine learning,</li>
<li>artificial intelligence,</li>
<li>statistics,</li>
<li>databases, and</li>
<li>optimization,</li>
<li>along with a deep understanding of the craft of problem formulation to engineer effective solutions.</li>
</ul>
</li>
</ul>
<p>Source: <a href="https://www.science.org/doi/pdf/10.1126/science.aaa8415">Machine learning: Trends, perspectives, and prospects, M. I. Jordan, T. M. Mitchell, Science 17 Jul 2015: Vol. 349, Issue 6245, pp. 255-260, DOI: 10.1126/science.aaa8415</a></p>
<p><strong>key insights in the above paper</strong></p>
<ul>
<li>ML is concerned with the building of computers that improve automatically through experience</li>
<li>ML lies at the intersection of computer science and statistics and at the core of artificial intelligence and data science</li>
<li>Recent progress in ML is due to:
<ul>
<li>development of new algorithms and theory</li>
<li>ongoing explosion in the availability of online data</li>
<li>availability of low-cost computation (through clusters of commodity hardware in the <em>cloud</em> )</li>
</ul>
</li>
<li>The adoption of data science and ML methods is leading to more evidence-based decision-making across:
<ul>
<li>life sciences (neuroscience, genomics, agriculture, etc. )</li>
<li>manufacturing</li>
<li>robotics (autonomous vehicle)</li>
<li>vision, speech processing, natural language processing</li>
<li>education</li>
<li>financial modeling</li>
<li>policing</li>
<li>marketing</li>
</ul>
</li>
</ul>
<h2 id="us-cias-cto-gus-hunts-view-on-big-data-and-data-science"><a class="header" href="#us-cias-cto-gus-hunts-view-on-big-data-and-data-science">US CIA's CTO Gus Hunt's View on Big Data and Data Science</a></h2>
<p>This is recommended viewing for historical insights into Big Data and Data Science skills - thanks to Snowden's [Permanent Record](https://en.wikipedia.org/wiki/Permanent<em>Record</em>(autobiography) for this pointer. At least watch for a few minutes from about 23 minutes to see what Gus Hunt, the then CTO of the American CIA, thinks about the combination of skills needed for Data Science. Watch the whole talk by Gus Hunt titled <em>CIA's Chief Tech Officer on Big Data: We Try to Collect Everything and Hang Onto It Forever</em> if you have 20 minutes or so to get a better framework for this first lecture on the <em>data science process</em>.</p>
<p><a href="https://youtu.be/GUPd2uMiXXg"><img src="https://img.youtube.com/vi/GUPd2uMiXXg/0.jpg" alt="Gus Hunt on Big Data and Data Science" /></a></p>
<p>Apache Spark actually grew out of Obama era Big Data straetigic grants to UC Berkeley's AMP Lab (the academic origins of Apark and databricks).</p>
<p>The Gus Hunt's distinction proposed here between <strong>enumeration</strong> versus <strong>modeling</strong> is mathematically fundamental. The latter is within the realm of classical <em>probabilistic learning theory</em> (eg. <a href="https://link.springer.com/book/10.1007/978-1-4612-0711-5">Probabilistic Theory of Pattern Recognition, Devroye, Luc; Györfi, László; Lugosi, Gábor, 1996</a>), including, Deep Learning, Reinforcement Learning, etc.), while the former is partly frameable within a wider mathematical setting known as <em>predicting individual sequences</em> (eg. <a href="https://doi.org/10.1017/CBO9780511546921">Prediction, Learning and Games, Cesa-Bianchi, N., &amp; Lugosi, G., 2006</a>).</p>
</div>
<div class="cell markdown">
<p>But what is Data Engineering (including Machine Learning Engineering and Operations) and how does it relate to Data Science?</p>
</div>
<div class="cell markdown">
<h2 id="data-engineering"><a class="header" href="#data-engineering">Data Engineering</a></h2>
<p>There are several views on what a data engineer is supposed to do:</p>
<p>Some views are rather narrow and emphasise division of labour between data engineers and data scientists:</p>
<ul>
<li>https://www.oreilly.com/ideas/data-engineering-a-quick-and-simple-definition
<ul>
<li>Let's check out what skills a data engineer is expected to have according to the link above.</li>
</ul>
</li>
</ul>
<blockquote>
<p>&quot;Ian Buss, principal solutions architect at Cloudera, notes that data scientists focus on finding new insights from a data set, while data engineers are concerned with the production readiness of that data and all that comes with it: formats, scaling, resilience, security, and more.&quot;</p>
</blockquote>
<blockquote>
<p>What skills do data engineers need? Those “10-30 different big data technologies” Anderson references in “Data engineers vs. data scientists” can fall under numerous areas, such as file formats, &gt; ingestion engines, stream processing, batch processing, batch SQL, data storage, cluster management, transaction databases, web frameworks, data visualizations, and machine learning. And that’s just the tip of the iceberg.</p>
</blockquote>
<blockquote>
<p>Buss says data engineers should have the following skills and knowledge:</p>
</blockquote>
<blockquote>
<ul>
<li>They need to know Linux and they should be comfortable using the command line.</li>
<li>They should have experience programming in at least Python or Scala/Java.</li>
<li>They need to know SQL.</li>
<li>They need some understanding of distributed systems in general and how they are different from traditional storage and processing systems.</li>
<li>They need a deep understanding of the ecosystem, including ingestion (e.g. Kafka, Kinesis), processing frameworks (e.g. Spark, Flink) and storage engines (e.g. S3, HDFS, HBase, Kudu). They should know the strengths and weaknesses of each tool and what it's best used for.</li>
<li>They need to know how to access and process data.</li>
</ul>
</blockquote>
<p>Let's dive deeper into such highly compartmentalised views of data engineers and data scientists and the so-called &quot;machine learning engineers&quot; according the following view:</p>
<ul>
<li>https://www.oreilly.com/ideas/data-engineers-vs-data-scientists</li>
</ul>
<p>Industry will keep evolving, so expect new titles in the future to get new types of jobs done.</p>
</div>
<div class="cell markdown">
<h2 id="the-data-engineering-scientist-as-the-middle-way"><a class="header" href="#the-data-engineering-scientist-as-the-middle-way">The Data Engineering Scientist as &quot;The Middle Way&quot;</a></h2>
<p>Here are some basic axioms that should be self-evident.</p>
<ul>
<li>Yes, there are differences in skillsets across humans
<ul>
<li>some humans will be better and have inclinations for engineering and others for pure mathematics by nature and nurture</li>
<li>one human cannot easily be a master of everything needed for innovating a new data-based product or service (very very rarely though this happens)</li>
</ul>
</li>
<li>Skills can be gained by any human who wants to learn to the extent s/he is able to expend time, energy, etc.</li>
</ul>
<p>For the <strong>Scalable Data Engineering Science Process:</strong> <em>towards Production-Ready and Productisable Prototyping</em> we need to allow each data engineer to be more of a data scientist and each data scientist to be more of a data engineer, up to each individual's <em>comfort zones</em> in technical and mathematical/conceptual and time-availability planes, but with some <strong>minimal expectations</strong> of mutual appreciation.</p>
<p>This course is designed to help you take the first minimal steps towards such a <strong>data engineering science</strong> process.</p>
<p>In the sequel it will become apparent <strong>why a team of data engineering scientists</strong> with skills across the conventional (2022) spectrum of data engineer versus data scientist <strong>is crucial</strong> for <strong>Production-Ready and Productisable Prototyping</strong>, whose outputs include standard AI products today.</p>
</div>
<div class="cell markdown">
<h1 id="a-brief-tour-of-data-science"><a class="header" href="#a-brief-tour-of-data-science">A Brief Tour of Data Science</a></h1>
<h2 id="history-of-data-analysis-and-where-does-big-data-come-from"><a class="header" href="#history-of-data-analysis-and-where-does-big-data-come-from">History of Data Analysis and Where Does &quot;Big Data&quot; Come From?</a></h2>
<ul>
<li>A Brief History and Timeline of Data Analysis and Big Data</li>
<li><a href="https://en.wikipedia.org/wiki/Big_data">https://en.wikipedia.org/wiki/Big_data</a></li>
<li><a href="https://whatis.techtarget.com/feature/A-history-and-timeline-of-big-data">https://whatis.techtarget.com/feature/A-history-and-timeline-of-big-data</a></li>
<li>Where does Data Come From?</li>
<li>Some of the sources of big data.
<ul>
<li>online click-streams (a lot of it is recorded but a tiny amount is analyzed):
<ul>
<li>record every click</li>
<li>every ad you view</li>
<li>every billing event,</li>
<li>every transaction, every network message, and every fault.</li>
</ul>
</li>
<li>User-generated content (on web and mobile devices):
<ul>
<li>every post that you make on Facebook</li>
<li>every picture sent on Instagram</li>
<li>every review you write for Yelp or TripAdvisor</li>
<li>every tweet you send on Twitter</li>
<li>every video that you post to YouTube.</li>
</ul>
</li>
<li>Science (for scientific computing):
<ul>
<li>data from various repositories for natural language processing:
<ul>
<li>Wikipedia,</li>
<li>the Library of Congress,</li>
<li>twitter firehose and google ngrams and digital archives,</li>
</ul>
</li>
<li>data from scientific instruments/sensors/computers:
<ul>
<li>the Large Hadron Collider (more data in a year than all the other data sources combined!)</li>
<li>genome sequencing data (sequencing cost is dropping much faster than Moore's Law!)</li>
<li>output of high-performance computers (super-computers) for data fusion, estimation/prediction and exploratory data analysis</li>
</ul>
</li>
</ul>
</li>
<li>Graphs are also an interesting source of big data (<em>network science</em>).
<ul>
<li>social networks (collaborations, followers, fb-friends or other relationships),</li>
<li>telecommunication networks,</li>
<li>computer networks,</li>
<li>road networks</li>
</ul>
</li>
<li>machine logs:
<ul>
<li>by servers around the internet (hundreds of millions of machines out there!)</li>
<li>internet of things.</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<h2 id="data-science-with-cloud-computing-and-whats-hard-about-it"><a class="header" href="#data-science-with-cloud-computing-and-whats-hard-about-it">Data Science with Cloud Computing and What's Hard about it?</a></h2>
<ul>
<li>See <a href="https://en.wikipedia.org/wiki/Cloud_computing">Cloud Computing</a> to understand the work-horse for analysing big data at <a href="https://en.wikipedia.org/wiki/Data_center">data centers</a></li>
</ul>
<blockquote>
<p>Cloud computing is the on-demand availability of computer system resources, especially data storage (cloud storage) and computing power, without direct active management by the user. Large clouds often have functions distributed over multiple locations, each location being a data center. Cloud computing relies on sharing of resources to achieve coherence and economies of scale, typically using a &quot;pay-as-you-go&quot; model which can help in reducing capital expenses but may also lead to unexpected operating expenses for unaware users.</p>
</blockquote>
<ul>
<li>
<p>In fact, if you are logged into <code>https://*.databricks.com/*</code> you are computing in the cloud! So the computations are actually running in an instance of the hardware available at a data center like the following:</p>
<ul>
<li><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/d7/CERN_Server_03.jpg/250px-CERN_Server_03.jpg" alt="" /></li>
</ul>
</li>
<li>
<p>Here is a data center used by CERN in 2010.</p>
<ul>
<li><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/98/Cern_datacenter.jpg/450px-Cern_datacenter.jpg" alt="" /></li>
</ul>
</li>
<li>
<p>What's hard about scalable data science in the cloud?</p>
<ul>
<li>To analyse datasets that are big, say more than a few TBs, we need to split the data and put it in several computers that are networked - <em>a typical cloud</em></li>
<li><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/de/Wikimedia_Foundation_Servers-8055_03_square.jpg/120px-Wikimedia_Foundation_Servers-8055_03_square.jpg" alt="" /></li>
<li>However, as the number of computer nodes in such a network increases, the probability of hardware failure or fault (say the hard-disk or memory or CPU or switch breaking down) also increases and can happen while the computation is being performed</li>
<li>Therefore for scalable data science, i.e., data science that can scale with the size of the input data by adding more computer nodes, we need <em>fault-tolerant computing and storage framework at the software level</em> to ensure the computations finish even if there are hardware faults.</li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<p>Here is a recommended light reading on <strong>What is &quot;Big Data&quot; -- Understanding the History</strong> (18 minutes): - <a href="https://towardsdatascience.com/what-is-big-data-understanding-the-history-32078f3b53ce">https://towardsdatascience.com/what-is-big-data-understanding-the-history-32078f3b53ce</a></p>
</div>
<div class="cell markdown">
<hr />
<hr />
</div>
<div class="cell markdown">
<h1 id="what-should-you-be-able-to-do-at-the-end-of-this-course"><a class="header" href="#what-should-you-be-able-to-do-at-the-end-of-this-course">What should <em>you</em> be able to do at the end of this course?</a></h1>
<p>By following these online interactions in the form of lab/lectures, asking questions, engaging in discussions, doing HOMEWORK assignments and completing the group project, you should be able to:</p>
<ul>
<li>Understand the principles of fault-tolerant scalable computing in Spark
<ul>
<li>in-memory and generic DAG extensions of Map-reduce</li>
<li>resilient distributed datasets for fault-tolerance</li>
<li>skills to process today's big data using state-of-the art techniques in Apache Spark 3.0, in terms of:
<ul>
<li>hands-on coding with realistic datasets</li>
<li>an intuitive understanding of the ideas behind the technology and methods</li>
<li>pointers to academic papers in the literature, technical blogs and video streams for <em>you to futher your theoretical understanding</em>.</li>
</ul>
</li>
</ul>
</li>
<li>More concretely, you will be able to:
<ul>
<li>Extract, Transform, Load, Interact, Explore and Analyze Data</li>
<li>Build Scalable Machine Learning Pipelines (or help build them) using Distributed Algorithms and Optimization</li>
</ul>
</li>
<li>How to keep up?
<ul>
<li>This is a fast-changing world.</li>
<li>Recent videos around Apache Spark are archived here (these videos are a great way to learn the latest happenings in industrial R&amp;D today!):
<ul>
<li><a href="https://databricks.com/dataaisummit">https://databricks.com/dataaisummit</a></li>
</ul>
</li>
</ul>
</li>
<li>What is mathematically stable in the world of 'big data'?
<ul>
<li>There is a growing body of work on the analysis of parallel and distributed algorithms, the work-horse of big data and AI.</li>
<li>We will see the core of this in the theoretical material from Reza Zadeh's course on <em>Distributed Algorithms and Optimisation</em>.</li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<h2 id="data-science-process-under-the-algorithms-machines-peoples-planet-framework-1"><a class="header" href="#data-science-process-under-the-algorithms-machines-peoples-planet-framework-1">Data Science Process under the Algorithms-Machines-Peoples-Planet Framework</a></h2>
<p><span style="font-size: 10pt;">Preparatory perusal at some distance, without necessarily associating oneself with any particular "philosophy", in order to intuitively understand the <em>Data Science Process under the Algorithms-Machines-Peoples-Planet (AMPP) Framework</em>, as formal Decision Problems and Decision Procedures (including any AI/ML algorithm) for consideration before Action using Mathematical Decision Theory.</span></p> <ol> <li><span style="font-size: 10pt;">Kate Crawford and Vladan Joler, “Anatomy of an AI System: The Amazon Echo As An Anatomical Map of Human Labor, Data and Planetary Resources,” AI Now Institute and Share Lab, (September 7, 2018)</span> <ul> <li><span style="font-size: 10pt;">Browse <a class="inline_disabled" href="https://anatomyof.ai/" target="_blank" rel="noopener">anatomyof.ai</a>.</span></li> <li><span style="font-size: 10pt;">View <a class="inline_disabled" href="http://www.anatomyof.ai/img/ai-anatomy-map.pdf" target="_blank" rel="noopener">full-scale map as PDF</a></span></li> <li><span style="font-size: 10pt;">Read <a class="inline_disabled" href="http://www.anatomyof.ai/img/ai-anatomy-publication.pdf" target="_blank" rel="noopener">Map + Essay as PDF in A3 format</a> (expected time is about 1.5 hours for thorough comprehension)</span></li> </ul> </li> <li><span style="font-size: 10pt;">MANIFESTO ON THE FUTURE OF SEEDS. Produced by The International Commission on the Future of Food and Agriculture, 36 pages. (2006). Disseminated as PDF from <a class="inline_disabled" href="http://lamastex.org/JOE/ManifestoOnFutureOfSeeds_2006.pdf" target="_blank" rel="noopener">http://lamastex.org/JOE/ManifestoOnFutureOfSeeds*2006.pdf</a>.</span></li> <li><span style="font-size: 10pt;">The Joint Operating Environment (JOE) \[limited to United States' perspective\]: </span> <ul> <li><span style="font-size: 10pt;">Read at least page 3 on "About this Study" <a class="inline_disabled" href="https://www.jcs.mil/Portals/36/Documents/Doctrine/concepts/joe_2008.pdf" target="_blank" rel="noopener">https://www.jcs.mil/Portals/36/Documents/Doctrine/concepts/joe*2008.pdf</a></span><br /> <ul> <li><span style="font-size: 10pt;">Do a deeper dive if interested at <a class="inline_disabled" href="https://www.jcs.mil/Doctrine/Joint-Concepts/JOE/" target="_blank" rel="noopener">https://www.jcs.mil/Doctrine/Joint-Concepts/JOE/</a> following through <a href="https://www.jcs.mil/Portals/36/Documents/Doctrine/concepts/joe_2010.pdf?ver=2017-12-30-132036-843" target="_blank" rel="noopener">Joint Operating Environment, 2010</a> and <a href="https://www.jcs.mil/Portals/36/Documents/Doctrine/concepts/joe_2035_july16.pdf?ver=2017-12-28-162059-917" target="_blank" rel="noopener">Joint Operating Environment 2035, 14 July 2016</a></span></li> </ul> </li> </ul> </li> <li><span style="font-size: 10pt;">Know the doctrine of Mutual Assured Destruction (MAD). Peruse: <a class="inline_disabled" href="https://en.wikipedia.org/wiki/Mutual_assured_destruction" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Mutual*assured*destruction</a> </span></li> <li><span style="font-size: 10pt;">Peruse <a class="inline_disabled" href="https://www.oneearth.org/our-mission/" target="_blank" rel="noopener">https://www.oneearth.org/our-mission/</a>, and more specifically inform yourself by further perusing</span> <ol> <li><span style="font-size: 10pt;">Energy Transition: <a class="inline_disabled" href="https://www.oneearth.org/science/energy/" target="_blank" rel="noopener">https://www.oneearth.org/science/energy/</a></span></li> <li><span style="font-size: 10pt;">Nature Conservation: <a class="inline_disabled" href="https://www.oneearth.org/science/nature/" target="_blank" rel="noopener">https://www.oneearth.org/science/nature/</a></span></li> <li><span style="font-size: 10pt;">Regenerative Agriculture: <a href="https://www.oneearth.org/science/agriculture/" target="_blank" rel="noopener">https://www.oneearth.org/science/agriculture/</a> </span></li> </ol> </li> </ol>
</div>
<div class="cell markdown">
<h2 id="interactions"><a class="header" href="#interactions">Interactions</a></h2>
<p>When <em>you are involved</em> in a <em>data science process</em> (to &quot;make a living&quot;, say) under the AMPP framework, your <strong>Algorithms</strong> implemented on <strong>Machines</strong> can have <em>different effects</em> on <strong>Peoples</strong> (meaning, any living populations of any species, including different human sub-populations, plants, animals, microbes, etc.) and our <strong>Planet</strong> (soils, climates, oceans, etc.) as long as the <em>Joint Operating Environment</em> is stable to avoid <em>Mutual Assured Destruction</em>.</p>
<p><strong>Discussion 0</strong></p>
<ul>
<li>Is it important to be aware of such <em>different effects</em> when building of a data product in some data science process?
<ul>
<li>We will limit the discussions on the concrete matter of &quot;Alexa&quot;, AWS's voice assistant.
<ul>
<li>what are your thoughts on <a href="https://anatomyof.ai/">https://anatomyof.ai/</a>?
<ul>
<li>view <a href="https://youtu.be/uM7gqPnmDDc">https://youtu.be/uM7gqPnmDDc</a></li>
<li>view <a href="https://youtu.be/i_EZwV6wGP0">https://youtu.be/i_EZwV6wGP0</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>This is a primer for our industrial guest speakers from <a href="https://www.trase.earth/">https://www.trase.earth/</a> who will talk about supply chains.</p>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="why-apache-spark"><a class="header" href="#why-apache-spark">Why Apache Spark?</a></h1>
<ul>
<li><a href="https://cacm.acm.org/magazines/2016/11/209116-apache-spark/fulltext/">Apache Spark: A Unified Engine for Big Data Processing</a> By Matei Zaharia, Reynold S. Xin, Patrick Wendell, Tathagata Das, Michael Armbrust, Ankur Dave, Xiangrui Meng, Josh Rosen, Shivaram Venkataraman, Michael J. Franklin, Ali Ghodsi, Joseph Gonzalez, Scott Shenker, Ion Stoica Communications of the ACM, Vol. 59 No. 11, Pages 56-65 10.1145/2934664</li>
</ul>
<p><a href="https://player.vimeo.com/video/185645796"><img src="https://i.vimeocdn.com/video/597494216-6f494f2fb4efb90efe6bb3206f3892bec6b202352951512dbde44c976549dd87-d.jpg?mw=240&amp;q=255" alt="Apache Spark ACM Video" /></a></p>
<p>Right-click the above image-link, open in a new tab and watch the video (4 minutes) or read about it in the Communications of the ACM in the frame below or from the link above.</p>
<p>Key Insights from <a href="https://cacm.acm.org/magazines/2016/11/209116-apache-spark/fulltext">Apache Spark: A Unified Engine for Big Data Processing</a></p>
<ul>
<li>A simple programming model can capture streaming, batch, and interactive workloads and enable new applications that combine them.</li>
<li>Apache Spark applications range from finance to scientific data processing and combine libraries for SQL, machine learning, and graphs.</li>
<li>In six years, Apache Spark has grown to 1,000 contributors and thousands of deployments.</li>
</ul>
<p><img src="https://dl.acm.org/cms/attachment/6f54b222-fe96-497a-8bfc-0e6ea250b05d/ins01.gif" alt="Key Insights" /></p>
</div>
<div class="cell markdown">
<p>Spark 3.0 is the latest version now (20200918) and it should be seen as the latest step in the evolution of tools in the big data ecosystem as summarized in <a href="https://towardsdatascience.com/what-is-big-data-understanding-the-history-32078f3b53ce">https://towardsdatascience.com/what-is-big-data-understanding-the-history-32078f3b53ce</a>:</p>
<p><img src="https://miro.medium.com/max/1200/1*0bWwqlOfjRqoUDqrH62GbQ.png" alt="Spark in context" /></p>
</div>
<div class="cell markdown">
<h2 id="alternatives-to-apache-spark"><a class="header" href="#alternatives-to-apache-spark">Alternatives to Apache Spark</a></h2>
<p>There are several alternatives to Apache Spark, but none of them have the penetration and community of Spark as of 2021.</p>
<p>For real-time streaming operations <a href="https://flink.apache.org/">Apache Flink</a> is competitive. See <a href="https://www.projectpro.io/article/apache-flink-vs-spark-will-one-overtake-the-other/282#toc-7">Apache Flink vs Spark – Will one overtake the other?</a> for a July 2021 comparison. Most scalable data science and engineering problems faced by several major industries in Sweden today are routinely solved using tools in the ecosystem around Apache Spark. Therefore, we will focus on Apache Spark here which still holds <a href="https://www.tpc.org/tpcds/results/tpcds_perf_results5.asp?spm=a2c65.11461447.0.0.626f184fy7PwOU&amp;resulttype=all">the world record for 10TB or 10,000 GB sort</a> by <a href="https://www.alibabacloud.com/blog/alibaba-cloud-e-mapreduce-sets-world-record-again-on-tpc-ds-benchmark_596195">Alibaba cloud</a> in 06/17/2020.</p>
<p>Several alternatives to Apache Spark exist. See the following for verious commercial options: - <a href="https://sourceforge.net/software/product/Apache-Spark/alternatives">https://sourceforge.net/software/product/Apache-Spark/alternatives</a></p>
<p>Read the following for a comparison of three popular frameworks in 2022 for distributed computing:</p>
<ul>
<li><a href="https://www.dominodatalab.com/blog/spark-dask-ray-choosing-the-right-framework">https://www.dominodatalab.com/blog/spark-dask-ray-choosing-the-right-framework</a></li>
</ul>
<p>Here, we will focus on Apache Spark as it is still a popular framework for distributed computing with a rich ecosystem around it.</p>
</div>
<div class="cell markdown">
<h2 id="the-big-data-problem"><a class="header" href="#the-big-data-problem">The big data problem</a></h2>
<p><strong>Hardware, distributing work, handling failed and slow machines</strong></p>
<p>Let us recall and appreciate the following:</p>
<ul>
<li>The Big Data Problem
<ul>
<li>Many routine problems today involve dealing with &quot;big data&quot;, operationally, this is a dataset that is larger than a few TBs and thus won't fit into a single commodity computer like a powerful desktop or laptop computer.</li>
</ul>
</li>
<li>Hardware for Big Data</li>
<li>The best single commodity computer can not handle big data as it has limited hard-disk and memory</li>
<li>Thus, we need to break the data up into lots of commodity computers that are networked together via cables to communicate instructions and data between them - this can be thought of as <em>a cloud</em></li>
<li>How to distribute work across a cluster of commodity machines?
<ul>
<li>We need a software-level framework for this.</li>
</ul>
</li>
<li>How to deal with failures or slow machines?
<ul>
<li>We also need a software-level framework for this.</li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<h2 id="key-papers"><a class="header" href="#key-papers">Key Papers</a></h2>
<ul>
<li>Key Historical Milestones
<ul>
<li>1956-1979: <a href="https://en.wikipedia.org/wiki/Lisp_(programming_language)">Stanford, MIT, CMU, and other universities develop set/list operations in LISP, Prolog, and other languages for parallel processing</a></li>
<li>2004: <strong>READ</strong>: <a href="https://research.google/pubs/pub62/">Google's MapReduce: Simplified Data Processing on Large Clusters, by Jeffrey Dean and Sanjay Ghemawat</a></li>
<li>2006: <a href="https://en.wikipedia.org/wiki/Apache_Hadoop">Yahoo!'s Apache Hadoop, originating from the Yahoo!’s Nutch Project, Doug Cutting - wikipedia</a></li>
<li>2009: <a href="https://aws.amazon.com/emr/">Cloud computing with Amazon Web Services Elastic MapReduce</a>, a Hadoop version modified for Amazon Elastic Cloud Computing (EC2) and Amazon Simple Storage System (S3), including support for Apache Hive and Pig.</li>
<li>2010: <strong>READ</strong>: <a href="http://ieeexplore.ieee.org/document/5496972/">The Hadoop Distributed File System, by Konstantin Shvachko, Hairong Kuang, Sanjay Radia, and Robert Chansler. IEEE MSST</a></li>
</ul>
</li>
<li>Apache Spark Core Papers
<ul>
<li>2012: <strong>READ</strong>: <a href="https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf">Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing, Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin Ma, Murphy McCauley, Michael J. Franklin, Scott Shenker and Ion Stoica. NSDI</a></li>
<li>2016: <a href="https://cacm.acm.org/magazines/2016/11/209116-apache-spark/fulltext">Apache Spark: A Unified Engine for Big Data Processing</a> By Matei Zaharia, Reynold S. Xin, Patrick Wendell, Tathagata Das, Michael Armbrust, Ankur Dave, Xiangrui Meng, Josh Rosen, Shivaram Venkataraman, Michael J. Franklin, Ali Ghodsi, Joseph Gonzalez, Scott Shenker, Ion Stoica , Communications of the ACM, Vol. 59 No. 11, Pages 56-65, 10.1145/2934664</li>
</ul>
</li>
<li>A lot has happened since 2016 to improve efficiency of Spark and embed more into the big data ecosystem
<ul>
<li>See <a href="https://www.youtube.com/watch?v=p4PkA2huzVc">Introducing Apache Spark 3.0 | Matei Zaharia and Brooke Wenig | Keynote Spark + AI Summit 2020</a>.</li>
</ul>
</li>
<li>More research papers on Spark are available from here and we will refer to them in the sequel as needed:
<ul>
<li><a href="https://databricks.com/resources?_sft_resource_type=research-papers">https://databricks.com/resources?<em>sft</em>resource_type=research-papers</a></li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<h2 id="mapreduce-and-apache-spark"><a class="header" href="#mapreduce-and-apache-spark">MapReduce and Apache Spark.</a></h2>
<p>MapReduce as we will see shortly in action is a framework for distributed fault-tolerant computing over a fault-tolerant distributed file-system, such as Google File System or open-source Hadoop for storage.</p>
<ul>
<li>Unfortunately, Map Reduce is bounded by Disk I/O and can be slow
<ul>
<li>especially when doing a sequence of MapReduce operations requirinr multiple Disk I/O operations</li>
</ul>
</li>
<li>Apache Spark can use Memory instead of Disk to speed-up MapReduce Operations
<ul>
<li>Spark Versus MapReduce - the speed-up is orders of magnitude faster</li>
</ul>
</li>
<li>SUMMARY
<ul>
<li>Spark uses memory instead of disk alone and is thus fater than Hadoop MapReduce</li>
<li>Spark's resilience abstraction is by RDD (resilient distributed dataset)</li>
<li>RDDs can be recovered upon failures from their <em>lineage graphs</em>, the recipes to make them starting from raw data</li>
<li>Spark supports a lot more than MapReduce, including streaming, interactive in-memory querying, etc.</li>
<li>Spark demonstrated an unprecedented sort of 1 petabyte (1,000 terabytes) worth of data in 234 minutes running on 190 Amazon EC2 instances (in 2015).</li>
<li>Spark expertise corresponds to the highest Median Salary in the US (~ 150K)</li>
</ul>
</li>
</ul>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="login-to-databricks"><a class="header" href="#login-to-databricks">Login to databricks</a></h1>
<p>We will use databricks community edition and later on the databricks project shard granted for this course under the <strong>databricks university alliance</strong> with cloud computing grants from databricks for waived DBU units and AWS.</p>
<p>Please go here for a relaxed and detailed-enough tour (later):</p>
<ul>
<li><a href="https://docs.databricks.com/index.html">https://docs.databricks.com/index.html</a></li>
</ul>
</div>
<div class="cell markdown">
<h2 id="databricks-community-edition"><a class="header" href="#databricks-community-edition">databricks community edition</a></h2>
<p>First obtain a free Obtain a databricks community edition account at <a href="https://community.cloud.databricks.com">https://community.cloud.databricks.com</a> by following these instructions: <a href="https://youtu.be/FH2KDhaFkZg">https://youtu.be/FH2KDhaFkZg</a>.</p>
<p>Let's get an overview of the databricks managed cloud for processing big data with Apache Spark.</p>
</div>
<div class="cell markdown">
<h2 id="dbc-essentials-what-is-databricks-cloud"><a class="header" href="#dbc-essentials-what-is-databricks-cloud">DBC Essentials: What is Databricks Cloud?</a></h2>
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/week1/dbTrImg_WorkspaceSparkPlatform700x.png" alt="DB workspace, spark, platform" /></p>
</div>
<div class="cell markdown">
<h2 id="dbc-essentials-shard-cluster-notebook-and-dashboard"><a class="header" href="#dbc-essentials-shard-cluster-notebook-and-dashboard">DBC Essentials: Shard, Cluster, Notebook and Dashboard</a></h2>
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/week1/dbTrImg_ShardClusterNotebookDashboard700x.png" alt="DB workspace, spark, platform" /></p>
</div>
<div class="cell markdown">
<p><strong>DBC Essentials: Team, State, Collaboration, Elastic Resources in one picture</strong></p>
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/week1/dbTrImg_TeamStateCollaborationElasticResources700x.png" alt="DB workspace, spark, platform" /></p>
</div>
<div class="cell markdown">
<p><strong>You Should All Have databricks community edition account by now!</strong> and have successfully logged in to it.</p>
</div>
<div class="cell markdown">
<h1 id="import-course-content-now"><a class="header" href="#import-course-content-now">Import Course Content Now!</a></h1>
<p>Two Steps:</p>
<ol>
<li>Create a folder named <code>scalable-data-science</code> in your <code>Workspace</code> (NO Typos due to hard-coding of paths in the sequel!)</li>
</ol>
<ul>
<li>Import the following <code>.dbc</code> archives from the following URL into <code>Workspace/scalable-data-science</code> folder you just created:
<ul>
<li><a href="https://github.com/lamastex/scalable-data-science/raw/master/dbcArchives/latest/">https://github.com/lamastex/scalable-data-science/raw/master/dbcArchives/latest/</a></li>
<li>start with the first file for now and import more as needed:
<ul>
<li><a href="https://github.com/lamastex/scalable-data-science/raw/master/dbcArchives/latest/000_1-sds-3-x-spark.dbc">https://github.com/lamastex/scalable-data-science/raw/master/dbcArchives/latest/000_1-sds-3-x-spark.dbc</a></li>
<li>...</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<h1 id="open-source-computing-environment"><a class="header" href="#open-source-computing-environment">Open-Source Computing Environment</a></h1>
<p>We will mainly use docker for local development on our own infrastructure, such as a laptop.</p>
<h2 id="docker"><a class="header" href="#docker">docker</a></h2>
<p>You can install and learn <a href="https://www.docker.com/">docker</a> on your laptop; <a href="https://docs.docker.com/get-started/">https://docs.docker.com/get-started/</a>.</p>
<p>In a Terminal, after installing docker, type:</p>
<pre><code>$ docker run --rm -it lamastex/dockerdev:latest
</code></pre>
<p>The above docker container has already downloaded the needed sources for a very basic Spark developer environment using this <a href="https://github.com/lamastex/dockerDev/blob/master/Dockerfile">Dockerfile</a> which builds on this <a href="https://github.com/lamastex/dockerDev/blob/master/spark3x.Dockerfile">spark3x.Dockerfile</a>. Having the source for a dockerfile is helpful when one needs to adapt it for the future.</p>
<p>You can test your docker installation by launching the <code>spark-shell</code> inside the docker container, compute <code>1+1</code> and <code>:quit</code> from spark-shell and <code>exit</code> from docker.</p>
<pre><code>root@7715366c86a4:~# spark-shell
22/08/18 12:24:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to &quot;WARN&quot;.
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://7715366c86a4:4040
Spark context available as 'sc' (master = local[*], app id = local-1660825477089).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.1.2
      /_/
         
Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_312)
Type in expressions to have them evaluated.
Type :help for more information.

scala&gt; 1+1
res0: Int = 2

scala&gt; :quit
root@c6ced3348c97:~# exit
</code></pre>
<h2 id="zeppelin"><a class="header" href="#zeppelin">zeppelin</a></h2>
<p>You may also use the open source project <a href="https://zeppelin.apache.org/">https://zeppelin.apache.org/</a> through the provided zeppelin spark docker container <a href="https://hub.docker.com/r/lamastex/zeppelin-spark">lamastex/zeppelin-spark</a> after successfully installing docker in your laptop.</p>
<p>Several companies use databricks notebooks but several others use zeppelin or jupyter or other notebook servers. Therefore, it is good to be familiar with a couple notebook servers and formats.</p>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<p>Please go here for a relaxed and detailed-enough tour (later):</p>
<ul>
<li><a href="https://docs.databricks.com/index.html">https://docs.databricks.com/index.html</a></li>
</ul>
</div>
<div class="cell markdown">
<h1 id="multi-lingual-notebooks"><a class="header" href="#multi-lingual-notebooks">Multi-lingual Notebooks</a></h1>
<p>Write Spark code for processing your data in notebooks.</p>
<p>Note that there are several open-sourced notebook servers including:</p>
<ul>
<li><a href="https://zeppelin.apache.org/">zeppelin</a></li>
<li><a href="https://jupyter.org/">jupyter</a> and its variants</li>
<li><a href="https://polynote.org/latest/">polynote</a></li>
<li>for a list of notebook alternatives to databricks see:
<ul>
<li><a href="https://datasciencenotebook.org/alternatives/databricks">https://datasciencenotebook.org/alternatives/databricks</a></li>
</ul>
</li>
</ul>
<p>For comparison between pairs of notebook servers see:</p>
<ul>
<li><a href="https://datasciencenotebook.org/compare/zeppelin/databricks">https://datasciencenotebook.org/compare/zeppelin/databricks</a></li>
<li><a href="https://datasciencenotebook.org/compare/jupyter/databricks">https://datasciencenotebook.org/compare/jupyter/databricks</a></li>
</ul>
<p>Here, we are mainly focused on using databricks notebooks due to its effeciently managed engineering layers over AWS (or Azure or GPC public clouds).</p>
<p>This allows us to focus on the concepts and models and get to implement codes with ease.</p>
<p><strong>NOTE</strong>: By now you should have already opened this notebook and attached it to a cluster:</p>
<ul>
<li>that you started in the Community Edition of databricks</li>
<li>or that you started in the zeppelin spark docker container similar to <a href="https://hub.docker.com/r/lamastex/zeppelin-spark">lamastex/zeppelin-spark</a></li>
</ul>
<h2 id="databricks-notebook"><a class="header" href="#databricks-notebook">Databricks Notebook</a></h2>
<p>Next we delve into the mechanics of working with databricks notebooks.</p>
<ul>
<li>But many of the details also apply to other notebook environments such as zeppelin with minor differences.</li>
</ul>
<h3 id="some-zeppelin-caveats"><a class="header" href="#some-zeppelin-caveats">Some Zeppelin caveats</a></h3>
<ul>
<li>Since many companies use other notebook servers we will make remarks on minor differences between databricks and zeppelin notebook formats, in case you want to try zeppelin server on your laptop or your own infrastructure.</li>
<li>Note that one can run Jupyter within Zeppelin. We use Zeppelin as opposed to Jupyter here because Zeppelin comes with a much broader collection of default interpreters for today's big data ecosystem and we wanted to only have one other open-source notebook server to complement learning in the currently (2022) closed-source notebook server of databricks.</li>
</ul>
</div>
<div class="cell markdown">
<h3 id="notebooks-can-be-written-in-python-scala-r-or-sql"><a class="header" href="#notebooks-can-be-written-in-python-scala-r-or-sql">Notebooks can be written in <strong>Python</strong>, <strong>Scala</strong>, <strong>R</strong>, or <strong>SQL</strong>.</a></h3>
<ul>
<li>This is a Scala notebook - which is indicated next to the title above by <code>(Scala)</code>.</li>
<li>One can choose the default language of the notebook when it is created.</li>
</ul>
</div>
<div class="cell markdown">
<h3 id="creating-a-new-notebook"><a class="header" href="#creating-a-new-notebook"><strong>Creating a new Notebook</strong></a></h3>
<ul>
<li>Select <strong>Create &gt; Notebook</strong>.</li>
<li>Enter the name of the notebook, the language (Python, Scala, R or SQL) for the notebook, and a cluster to run it on.</li>
<li>From Left Menu you can go to 'Workspace' and then Click the tiangle on the right side of a folder to open the folder menu and create notebook in the folder.</li>
</ul>
</div>
<div class="cell markdown">
<h3 id="cloning-a-notebook"><a class="header" href="#cloning-a-notebook">Cloning a Notebook</a></h3>
<ul>
<li>You can clone a notebook to create a copy of it, for example if you want to edit or run an Example notebook like this one.</li>
<li>Click <strong>File &gt; Clone</strong> in the notebook context bar above.</li>
<li>Enter a new name and location for your notebook. If Access Control is enabled, you can only clone to folders that you have Manage permissions on.</li>
</ul>
</div>
<div class="cell markdown">
<h3 id="clone-or-import-this-notebook"><a class="header" href="#clone-or-import-this-notebook">Clone Or Import This Notebook</a></h3>
<ul>
<li>
<p>From the <strong>File</strong> menu at the top left of this notebook, choose <strong>Clone</strong> or click <strong>Import Notebook</strong> on the top right. This will allow you to interactively execute code cells as you proceed through the notebook.</p>
</li>
<li>
<p>Enter a name and a desired location for your cloned notebook (i.e. Perhaps clone to your own user directory or the &quot;Shared&quot; directory.)</p>
</li>
<li>
<p>Navigate to the location you selected (e.g. click Menu &gt; Workspace &gt; <code>Your cloned location</code>)</p>
</li>
</ul>
</div>
<div class="cell markdown">
<h3 id="attach-the-notebook-to-a-cluster"><a class="header" href="#attach-the-notebook-to-a-cluster"><strong>Attach</strong> the Notebook to a <strong>cluster</strong></a></h3>
<ul>
<li>A <strong>Cluster</strong> is a group of machines which can run commands in cells.</li>
<li>Check the upper left corner of your notebook to see if it is <strong>Attached</strong> or <strong>Detached</strong>.</li>
<li>If <strong>Detached</strong>, click on the right arrow and select a cluster to attach your notebook to.
<ul>
<li>If there is no running cluster, create one.</li>
</ul>
</li>
</ul>
<h3 id="deep-dive-into-databricks-notebooks"><a class="header" href="#deep-dive-into-databricks-notebooks">Deep-dive into databricks notebooks</a></h3>
<p>Let's take a deeper dive into a databricks notebook next.</p>
</div>
<div class="cell markdown">
<hr />
<h4 id="cells-are-units-that-make-up-notebooks"><a class="header" href="#cells-are-units-that-make-up-notebooks"><strong>Cells</strong> are units that make up notebooks</a></h4>
<p>Cells each have a type - including <strong>scala</strong>, <strong>python</strong>, <strong>sql</strong>, <strong>R</strong>, <strong>markdown</strong>, <strong>filesystem</strong>, and <strong>shell</strong>.</p>
<ul>
<li>While cells default to the type of the Notebook, other cell types are supported as well.</li>
<li>This cell is in <strong>markdown</strong> and is used for documentation. <a href="http://en.wikipedia.org/wiki/Markdown">Markdown</a> is a simple text formatting syntax.</li>
</ul>
<h4 id="beware-of-minor-differences-across-notebook-formats"><a class="header" href="#beware-of-minor-differences-across-notebook-formats">Beware of minor differences across notebook formats</a></h4>
<ul>
<li>Different notebook formats use different tags for the same language interpreter. But they are quite easy to remember.
<ul>
<li>For example in zeppelin <code>%spark</code> and <code>%pyspark</code> are used instead of databricks' <code>%scala</code> and <code>%python</code> to denote spark and pyspark cells.</li>
<li>However <code>%md</code> and <code>%sh</code> are used for markdown and shell in both notebook formats. ***</li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<hr />
<h4 id="create-and-edit-a-new-markdown-cell-in-this-notebook"><a class="header" href="#create-and-edit-a-new-markdown-cell-in-this-notebook"><strong>Create</strong> and <strong>Edit</strong> a New Markdown Cell in this Notebook</a></h4>
<p><strong>NOTE:</strong> You will be writing your group project report as databricks notebooks, therefore it is important to use markdown effectively.</p>
<ul>
<li>
<p>When you mouse between cells, a + sign will pop up in the center that you can click on to create a new cell.</p>
</li>
<li>
<p>Type <strong><code>%md Hello, world!</code></strong> into your new cell (<strong><code>%md</code></strong> indicates the cell is markdown).</p>
</li>
<li>
<p>Click out of the cell or double-click to see the cell contents update.</p>
</li>
</ul>
<hr />
</div>
<div class="cell markdown">
<h4 id="running-a-cell-in-your-notebook"><a class="header" href="#running-a-cell-in-your-notebook">Running a cell in your notebook.</a></h4>
<ul>
<li>Press <strong>Shift+Enter</strong> when in the cell to <strong>run</strong> it and proceed to the next cell.
<ul>
<li>The cells contents should update.</li>
</ul>
</li>
<li><strong>NOTE:</strong> Cells are not automatically run each time you open it.
<ul>
<li>Instead, Previous results from running a cell are saved and displayed.</li>
</ul>
</li>
<li>Alternately, press <strong>Ctrl+Enter</strong> when in a cell to <strong>run</strong> it, but not proceed to the next cell.</li>
</ul>
</div>
<div class="cell markdown">
<p><strong>You Try Now!</strong> Just double-click the cell below, modify the text following <code>%md</code> and press <strong>Ctrl+Enter</strong> to evaluate it and see it's mark-down'd output.</p>
<pre><code>&gt; %md Hello, world!
</code></pre>
</div>
<div class="cell markdown">
<p>Hello, world!</p>
</div>
<div class="cell markdown">
<h4 id="markdown-cell-tips"><a class="header" href="#markdown-cell-tips">Markdown Cell Tips</a></h4>
<ul>
<li>To change a non-markdown cell to markdown, add <code>%md</code> to very start of the cell.</li>
<li>After updating the contents of a markdown cell, click out of the cell to update the formatted contents of a markdown cell.</li>
<li>To edit an existing markdown cell, <strong>doubleclick</strong> the cell.</li>
</ul>
<p>Learn more about markdown:</p>
<ul>
<li><a href="https://guides.github.com/features/mastering-markdown/">https://guides.github.com/features/mastering-markdown/</a></li>
</ul>
<p>Note that there are flavours or minor variants and enhancements of markdown, including those specific to databricks, github, <a href="https://pandoc.org/MANUAL.html">pandoc</a>, etc.</p>
<p>It will be future-proof to remain in the syntactic zone of <em>pure markdown</em> (at the intersection of various flavours) as much as possible and go with <a href="https://pandoc.org/MANUAL.html">pandoc</a>-compatible style if choices are necessary. ***</p>
</div>
<div class="cell markdown">
<hr />
<h4 id="run-a-scala-cell"><a class="header" href="#run-a-scala-cell">Run a <strong>Scala Cell</strong></a></h4>
<ul>
<li>Run the following scala cell.</li>
<li>Note: There is no need for any special indicator (such as <code>%md</code>) necessary to create a Scala cell in a Scala notebook.</li>
<li>You know it is a scala notebook because of the <code>(Scala)</code> appended to the name of this notebook.</li>
<li>Make sure the cell contents updates before moving on.</li>
<li>Press <strong>Shift+Enter</strong> when in the cell to run it and proceed to the next cell.
<ul>
<li>The cells contents should update.</li>
<li>Alternately, press <strong>Ctrl+Enter</strong> when in a cell to <strong>run</strong> it, but not proceed to the next cell.</li>
</ul>
</li>
<li>characters following <code>//</code> are comments in scala.</li>
<li><strong>RECALL:</strong> In zeppelin <code>%spark</code> is used instead of <code>%scala</code>. ***</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">1+1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res0: Int = 2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(System.currentTimeMillis) // press Ctrl+Enter to evaluate println that prints its argument as a line
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1664271875320
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">1+1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res2: Int = 2
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="spark-is-written-in-scala-but-"><a class="header" href="#spark-is-written-in-scala-but-">Spark is written in Scala, but ...</a></h3>
<p>For this reason Scala will be the primary language for this course is Scala.</p>
<p><strong>However, let us use the best language for the job!</strong> as each cell can be written in a specific language in the same notebook. Such multi-lingual notebooks are the norm in any realistic data science process today!</p>
<p>The beginning of each cells has a language type if it is not the default language of the notebook. Such cell-specific language types include the following with the prefix <code>%</code>:</p>
<ul>
<li>
<p><code>%scala</code> for <strong>Spark Scala</strong>,</p>
<ul>
<li><code>%spark</code> in zeppelin</li>
</ul>
</li>
<li>
<p><code>%py</code> for <strong>PySpark</strong>,</p>
<ul>
<li><code>%pyspark</code> in zeppelin</li>
</ul>
</li>
<li>
<p><code>%r</code> for <strong>R</strong>,</p>
</li>
<li>
<p><code>%sql</code> for <strong>SQL</strong>,</p>
</li>
<li>
<p><code>%sh</code> for <strong>BASH SHELL</strong> and</p>
</li>
<li>
<p><code>%md</code> for <strong>markdown</strong>.</p>
</li>
<li>
<p>While cells default to the language type of the Notebook (scala, python, r or sql), other cell types are supported as well in a cell-specific manner.</p>
</li>
<li>
<p>For example, Python Notebooks can contain python, sql, markdown, and even scala cells. This lets you write notebooks that do use multiple languages.</p>
</li>
<li>
<p>This cell is in <strong>markdown</strong> as it begins with <code>%md</code>and is used for documentation purposes.</p>
</li>
</ul>
</div>
<div class="cell markdown">
<p>Thus, <strong>all language-typed cells can be created in any notebook</strong>, regardless of the the default language of the notebook itself, provided interpreter support exists for the languages.</p>
</div>
<div class="cell markdown">
<p>Cross-language cells can be used to mix commands from other languages.</p>
<p><strong>Examples:</strong></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">print(&quot;For example, this is a scala notebook, but we can use %py to run python commands inline.&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// you can be explicit about the language even if the notebook's default language is the same
println(&quot;We can access Scala like this.&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>We can access Scala like this.
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Command line cells can be used to work with local files on the Spark driver node. * Start a cell with <code>%sh</code> to run a command line command</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh"># This is a command line cell. Commands you write here will be executed as if they were run on the command line.
# For example, in this cell we access the help pages for the bash shell.
ls
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">whoami
</code></pre>
</div>
<div class="cell markdown">
<h4 id="notebooks-can-be-run-from-other-notebooks-using-run"><a class="header" href="#notebooks-can-be-run-from-other-notebooks-using-run">Notebooks can be run from other notebooks using <strong>%run</strong></a></h4>
<p>This is commonly used to import functions you defined in other notebooks.</p>
<ul>
<li>Syntax in databricks: <code>%run /full/path/to/notebook</code></li>
<li>Syntax in zeppelin: <code>z.runNote(&quot;id&quot;)</code></li>
</ul>
<h4 id="displaying-in-notebooks"><a class="header" href="#displaying-in-notebooks">Displaying in notebooks</a></h4>
<p>Notebooks are great for displaying data in the input cell immediately in the result or output cell. This is often an effective web-based REPL environment to prototype for the data science process.</p>
<p>To display a dataframe or tabular data named <code>myDataFrame</code>:</p>
<ul>
<li>Notebook-agnostic syntax in Spark for textual display: <code>myDataFrame.show()</code></li>
<li>Syntax in databricks: <code>display(myDataFrame)</code></li>
<li>Syntax in zeppelin: <code>z.show(myDataFrame)</code></li>
</ul>
<p>One can also display more sophisticated outputs involving HTML and D3 using <code>display_HTML</code> in databricks or <code>%html</code> in zeppelin. See respective documentation for details. Zeppelin is extremely flexible via <a href="https://zeppelin.apache.org/docs/0.10.1/development/helium/overview.html">helium</a> when it comes to creating interactive visualisations of results from a data science process pipeline.</p>
</div>
<div class="cell markdown">
<h2 id="further-pointers"><a class="header" href="#further-pointers">Further Pointers</a></h2>
<p>Here are some useful links to bookmark as you will need to use them for Reference.</p>
<p>These links provide a relaxed and detailed-enough tour (that you may need to take later):</p>
<ul>
<li>databricks
<ul>
<li><a href="https://docs.databricks.com/index.html">https://docs.databricks.com/index.html</a></li>
</ul>
</li>
<li>zeppelin (if you want an open-source notebook server deployed on your own computing infrastructure)
<ul>
<li><a href="https://zeppelin.apache.org/docs/0.10.1/">https://zeppelin.apache.org/docs/0.10.1/</a></li>
</ul>
</li>
</ul>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="load-datasets-into-databricks"><a class="header" href="#load-datasets-into-databricks">Load Datasets into Databricks</a></h1>
<p>Simply, hit run <code>Runa all cells in this Notebook</code> to load all the core datasets we will be using in the first couple of modules in this course.</p>
<p>It can take about 2-3 minutes once your cluster starts.</p>
<p><strong>Note</strong> This notebook should be skipped in docker-compose zeppelin instance of the course.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">wget https://github.com/lamastex/sds-datasets/raw/master/datasets-sds.zip
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">ls -al *
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">unzip datasets-sds.zip
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">pwd
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-fs">rm -r dbfs:///datasets/sds/
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res0: Boolean = true
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-fs">mkdirs dbfs:///datasets/sds/
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res1: Boolean = true
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-fs">cp -r file:///databricks/driver/datasets-sds/ dbfs:///datasets/sds/
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res2: Boolean = true
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-fs">ls /datasets/sds/
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>path</th>
<th>name</th>
<th>size</th>
<th>modificationTime</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dbfs:/datasets/sds/Rdatasets/</td>
<td>Rdatasets/</td>
<td>0.0</td>
<td>1.664296004294e12</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sds/cs100/</td>
<td>cs100/</td>
<td>0.0</td>
<td>1.664296004295e12</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sds/flights/</td>
<td>flights/</td>
<td>0.0</td>
<td>1.664296004295e12</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sds/mnist-digits/</td>
<td>mnist-digits/</td>
<td>0.0</td>
<td>1.664296004295e12</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sds/people/</td>
<td>people/</td>
<td>0.0</td>
<td>1.664296004295e12</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sds/people.json/</td>
<td>people.json/</td>
<td>0.0</td>
<td>1.664296004295e12</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sds/power-plant/</td>
<td>power-plant/</td>
<td>0.0</td>
<td>1.664296004295e12</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sds/social_media_usage.csv/</td>
<td>social_media_usage.csv/</td>
<td>0.0</td>
<td>1.664296004295e12</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sds/songs/</td>
<td>songs/</td>
<td>0.0</td>
<td>1.664296004295e12</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sds/souall.txt.gz</td>
<td>souall.txt.gz</td>
<td>3576868.0</td>
<td>1.664296002e12</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sds/spark-examples/</td>
<td>spark-examples/</td>
<td>0.0</td>
<td>1.664296004295e12</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sds/weather/</td>
<td>weather/</td>
<td>0.0</td>
<td>1.664296004295e12</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sds/wikipedia-datasets/</td>
<td>wikipedia-datasets/</td>
<td>0.0</td>
<td>1.664296004295e12</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sc.textFile(&quot;/datasets/sds/souall.txt.gz&quot;).count // should be 22258
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res4: Long = 22258
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Now you have loaded the core datasets for the course.</p>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="scala-crash-course"><a class="header" href="#scala-crash-course">Scala Crash Course</a></h1>
<p>Here we take a minimalist approach to learning just enough Scala, the language that Apache Spark is written in, to be able to use Spark effectively.</p>
<p>In the sequel we can learn more Scala concepts as they arise. This learning can be done by chasing the pointers in this crash course for a detailed deeper dive on your own time.</p>
<p>There are two basic ways in which we can learn Scala:</p>
<p><strong>1. Learn Scala in a notebook environment</strong></p>
<p>For convenience we use databricks Scala notebooks like this one here.</p>
<p>You can learn Scala locally on your own computer using Scala REPL (and Spark using Spark-Shell).</p>
<p><strong>2. Learn Scala in your own computer</strong></p>
<p>The most easy way to get Scala locally is through sbt, the Scala Build Tool. You can also use an IDE that integrates sbt.</p>
<p>See: <a href="https://docs.scala-lang.org/getting-started/index.html">https://docs.scala-lang.org/getting-started/index.html</a> to set up Scala in your own computer.</p>
<p><strong>Software Engineering NOTE:</strong> You can use <code>docker pull lamastex/dockerdev:spark3x</code> and use the docker container for local development based on minimal pointers here: <a href="https://github.com/lamastex/dockerDev">https://github.com/lamastex/dockerDev</a>. Being able to modularise and reuse codes modularly requires the building of libraries which inturn requires building them locally with tools such as <code>sbt</code> or <code>mvn</code> for instance. You can also set-up modern IDEs like <a href="https://code.visualstudio.com/">https://code.visualstudio.com/</a> or <a href="https://www.jetbrains.com/idea/">https://www.jetbrains.com/idea/</a>, etc. for this.</p>
<h2 id="scala-resources"><a class="header" href="#scala-resources">Scala Resources</a></h2>
<p>You will not be learning scala systematically and thoroughly in this course. You will learn <em>to use</em> Scala by doing various Spark jobs.</p>
<p>If you are interested in learning scala properly, then there are various resources, including:</p>
<ul>
<li><a href="http://www.scala-lang.org/">scala-lang.org</a> is the <strong>core Scala resource</strong>. Bookmark the following three links:
<ul>
<li><a href="https://docs.scala-lang.org/tour/tour-of-scala.html">tour-of-scala</a> - Bite-sized introductions to core language features.
<ul>
<li>we will go through the tour in a hurry now as some Scala familiarity is needed immediately.</li>
</ul>
</li>
<li><a href="https://docs.scala-lang.org/overviews/scala-book/introduction.html">scala-book</a> - An online book introducing the main language features
<ul>
<li>you are expected to use this resource to figure out Scala as needed.</li>
</ul>
</li>
<li><a href="https://docs.scala-lang.org/cheatsheets/index.html">scala-cheatsheet</a> - A handy cheatsheet covering the basics of Scala syntax.</li>
<li><a href="https://superruzafa.github.io/visual-scala-reference/">visual-scala-reference</a> - This guide collects some of the most common functions of the Scala Programming Language and explain them conceptual and graphically in a simple way.</li>
</ul>
</li>
<li><a href="https://docs.scala-lang.org/learn.html">Online Resources</a>, including:
<ul>
<li><a href="https://www.coursera.org/course/progfun">courseera: Functional Programming Principles in Scala</a></li>
</ul>
</li>
<li><a href="http://www.scala-lang.org/documentation/books.html">Books</a>
<ul>
<li><a href="http://www.artima.com/pins1ed/">Programming in Scala, 1st Edition, Free Online Reading</a></li>
</ul>
</li>
</ul>
<p>The main sources for the following content are (you are encouraged to read them for more background):</p>
<ul>
<li><a href="https://www.scala-lang.org/old/sites/default/files/linuxsoft_archives/docu/files/ScalaByExample.pdf">Martin Oderski's Scala by example</a></li>
<li><a href="http://lintool.github.io/SparkTutorial/slides/day1_Scala_crash_course.pdf">Scala crash course by Holden Karau</a></li>
<li><a href="https://darrenjw.wordpress.com/2013/12/30/brief-introduction-to-scala-and-breeze-for-statistical-computing/">Darren's brief introduction to scala and breeze for statistical computing</a></li>
</ul>
<h2 id="what-is-scala"><a class="header" href="#what-is-scala">What is Scala?</a></h2>
<p>&quot;Scala smoothly integrates object-oriented and functional programming. It is designed to express common programming patterns in a concise, elegant, and type-safe way.&quot; by Matrin Odersky.</p>
<ul>
<li>High-level language for the Java Virtual Machine (JVM)</li>
<li>Object oriented + functional programming</li>
<li>Statically typed</li>
<li>Comparable in speed to Java</li>
<li>Type inference saves us from having to write explicit types most of the time Interoperates with Java</li>
<li>Can use any Java class (inherit from, etc.)</li>
<li>Can be called from Java code</li>
</ul>
<p>See a quick tour here:</p>
<ul>
<li><a href="https://docs.scala-lang.org/tour/tour-of-scala.html">https://docs.scala-lang.org/tour/tour-of-scala.html</a></li>
</ul>
<h2 id="why-scala"><a class="header" href="#why-scala">Why Scala?</a></h2>
<ul>
<li>Spark was originally written in Scala, which allows concise function syntax and interactive use</li>
<li>Spark APIs for other languages include:
<ul>
<li>Java API for standalone use</li>
<li>Python API added to reach a wider user community of programmes</li>
<li>R API added more recently to reach a wider community of data analyststs</li>
<li>Unfortunately, Python and R APIs are generally behind Spark's native Scala (for eg. GraphX is only available in Scala currently and datasets are only available in Scala as of 20200918).</li>
</ul>
</li>
<li>See Darren Wilkinson's 11 reasons for <a href="https://darrenjw.wordpress.com/2013/12/23/scala-as-a-platform-for-statistical-computing-and-data-science/">scala as a platform for statistical computing and data science</a>. It is embedded in-place below for your convenience.</li>
</ul>
</div>
<div class="cell markdown">
<h2 id="scala-versus-python-for-apache-spark"><a class="header" href="#scala-versus-python-for-apache-spark">Scala Versus Python for Apache Spark</a></h2>
<p>Read: <a href="https://www.projectpro.io/article/scala-vs-python-for-apache-spark/213">https://www.projectpro.io/article/scala-vs-python-for-apache-spark/213</a> for an interesting overview.</p>
</div>
<div class="cell markdown">
<h2 id="learn-scala-in-notebook-environment"><a class="header" href="#learn-scala-in-notebook-environment">Learn Scala in Notebook Environment</a></h2>
<hr />
<h3 id="run-a-scala-cell-1"><a class="header" href="#run-a-scala-cell-1">Run a <strong>Scala Cell</strong></a></h3>
<ul>
<li>Run the following scala cell.</li>
<li>Note: There is no need for any special indicator (such as <code>%md</code>) necessary to create a Scala cell in a Scala notebook.</li>
<li>You know it is a scala notebook because of the <code>(Scala)</code> appended to the name of this notebook.</li>
<li>Make sure the cell contents updates before moving on.</li>
<li>Press <strong>Shift+Enter</strong> when in the cell to run it and proceed to the next cell.
<ul>
<li>The cells contents should update.</li>
<li>Alternately, press <strong>Ctrl+Enter</strong> when in a cell to <strong>run</strong> it, but not proceed to the next cell.</li>
</ul>
</li>
<li>characters following <code>//</code> are comments in scala. ***</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">1+1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res0: Int = 2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(System.currentTimeMillis) // press Ctrl+Enter to evaluate println that prints its argument as a line
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1664271977227
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>See <a href="https://darrenjw.wordpress.com/2013/12/23/scala-as-a-platform-for-statistical-computing-and-data-science/">Scala as a platform for statistical computing and data science</a>.</p>
</div>
<div class="cell markdown">
<h2 id="lets-get-our-hands-dirty-in-scala"><a class="header" href="#lets-get-our-hands-dirty-in-scala">Let's get our hands dirty in Scala</a></h2>
<p>We will go through the <strong>following</strong> programming concepts and tasks by building on <a href="https://docs.scala-lang.org/tour/basics.html">https://docs.scala-lang.org/tour/basics.html</a>.</p>
<ul>
<li><strong>Scala Types</strong></li>
<li><strong>Expressions and Printing</strong></li>
<li><strong>Naming and Assignments</strong></li>
<li><strong>Functions and Methods in Scala</strong></li>
<li><strong>Classes and Case Classes</strong></li>
<li><strong>Methods and Tab-completion</strong></li>
<li><strong>Objects and Traits</strong></li>
<li>Collections in Scala and Type Hierarchy</li>
<li>Functional Programming and MapReduce</li>
<li>Lazy Evaluations and Recursions</li>
</ul>
<p><strong>Remark</strong>: You need to take a computer science course (from CourseEra, for example) to properly learn Scala. Here, we will learn to use Scala by example to accomplish our data science tasks at hand. You can learn more Scala as needed from various sources pointed out above in <strong>Scala Resources</strong>.</p>
</div>
<div class="cell markdown">
<h3 id="scala-types"><a class="header" href="#scala-types">Scala Types</a></h3>
<p>In Scala, all values have a type, including numerical values and functions. The diagram below illustrates a subset of the type hierarchy.</p>
<p><img src="https://docs.scala-lang.org/resources/images/tour/unified-types-diagram.svg" alt="" /></p>
<p>For now, notice some common types we will be usinf including <code>Int</code>, <code>String</code>, <code>Double</code>, <code>Unit</code>, <code>Boolean</code>, <code>List</code>, etc. For more details see <a href="https://docs.scala-lang.org/tour/unified-types.html">https://docs.scala-lang.org/tour/unified-types.html</a>. We will return to this at the end of the notebook after seeing a brief tour of Scala now.</p>
</div>
<div class="cell markdown">
<h3 id="expressions"><a class="header" href="#expressions">Expressions</a></h3>
<p>Expressions are computable statements such as the <code>1+1</code> we have seen before.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">1+1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res2: Int = 2
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>We can print the output of a computed or evaluated expressions as a line using <code>println</code>:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(1+1) // printing 2
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(&quot;hej hej!&quot;) // printing a string
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>hej hej!
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="naming-and-assignments"><a class="header" href="#naming-and-assignments">Naming and Assignments</a></h3>
<p><strong>value and variable as <code>val</code> and <code>var</code></strong></p>
<p>You can name the results of expressions using keywords <code>val</code> and <code>var</code>.</p>
<p>Let us assign the integer value <code>5</code> to <code>x</code> as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x : Int = 5 // &lt;Ctrl+Enter&gt; to declare a value x to be integer 5. 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: Int = 5
</code></pre>
</div>
</div>
<div class="cell markdown">
<p><code>x</code> is a named result and it is a value since we used the keyword <code>val</code> when naming it.</p>
</div>
<div class="cell markdown">
<p>Scala is statically typed, but it uses built-in type inference machinery to automatically figure out that <code>x</code> is an integer or <code>Int</code> type as follows. Let's declare a value <code>x</code> to be <code>Int</code> 5 next without explictly using <code>Int</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = 5    // &lt;Ctrl+Enter&gt; to declare a value x as Int 5 (type automatically inferred)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: Int = 5
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let's declare <code>x</code> as a <code>Double</code> or double-precision floating-point type using decimal such as <code>5.0</code> (a digit has to follow the decimal point!)</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = 5.0   // &lt;Ctrl+Enter&gt; to declare a value x as Double 5
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: Double = 5.0
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Alternatively, we can assign <code>x</code> as a <code>Double</code> explicitly. Note that the decimal point is not needed in this case due to explicit typing as <code>Double</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x :  Double = 5    // &lt;Ctrl+Enter&gt; to declare a value x as Double 5 (type automatically inferred)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: Double = 5.0
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Next note that labels need to be declared on first use. We have declared <code>x</code> to be a <code>val</code> which is short for <em>value</em>. This makes <code>x</code> immutable (cannot be changed).</p>
<p>Thus, <code>x</code> cannot be just re-assigned, as the following code illustrates in the resulting error: <code>... error: reassignment to val</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//x = 10    //  uncomment and &lt;Ctrl+Enter&gt; to try to reassign val x to 10
</code></pre>
</div>
<div class="cell markdown">
<p>Scala allows declaration of mutable variables as well using <code>var</code>, as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">var y = 2    // &lt;Shift+Enter&gt; to declare a variable y to be integer 2 and go to next cell
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>y: Int = 2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">y = 3    // &lt;Shift+Enter&gt; to change the value of y to 3
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>y: Int = 3
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">y = y+1 // adds 1 to y
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>y: Int = 4
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">y += 2 // adds 2 to y
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(y) // the var y is 6 now
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>6
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="blocks"><a class="header" href="#blocks">Blocks</a></h3>
<p>Just combine expressions by surrounding them with <code>{</code> and <code>}</code> called a block.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println({
  val x = 1+1
  x+2 // expression in last line is returned for the block
})// prints 4
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>4
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println({ val x=22; x+2})
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>24
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="functions"><a class="header" href="#functions">Functions</a></h3>
<p>Functions are expressions that have parameters. A function takes arguments as input and returns expressions as output.</p>
<p>A function can be nameless or <em>anonymous</em> and simply return an output from a given input. For example, the following annymous function returns the square of the input integer.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">(x: Int) =&gt; x*x
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res10: Int =&gt; Int = $Lambda$5453/989605864@15857286
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>On the left of <code>=&gt;</code> is a list of parameters with name and type. On the right is an expression involving the parameters.</p>
</div>
<div class="cell markdown">
<p>You can also name functions:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val multiplyByItself = (x: Int) =&gt; x*x
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>multiplyByItself: Int =&gt; Int = $Lambda$5454/1749476144@41de911a
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(multiplyByItself(10))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>100
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>A function can have no parameters:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val howManyAmI = () =&gt; 1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>howManyAmI: () =&gt; Int = $Lambda$5457/1325610144@72b8fd99
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(howManyAmI()) // 1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>A function can have more than one parameter:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val multiplyTheseTwoIntegers = (a: Int, b: Int) =&gt; a*b
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>multiplyTheseTwoIntegers: (Int, Int) =&gt; Int = $Lambda$5458/1327542156@7adcf443
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(multiplyTheseTwoIntegers(2,4)) // 8
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>8
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="methods"><a class="header" href="#methods">Methods</a></h3>
<p>Methods are very similar to functions, but a few key differences exist.</p>
<p>Methods use the <code>def</code> keyword followed by a name, parameter list(s), a return type, and a body.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def square(x: Int): Int = x*x    // &lt;Shitf+Enter&gt; to define a function named square
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>square: (x: Int)Int
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Note that the return type <code>Int</code> is specified after the parameter list and a <code>:</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">square(5)    // &lt;Shitf+Enter&gt; to call this function on argument 5
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res14: Int = 25
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val y = 3    // &lt;Shitf+Enter&gt; make val y as Int 3
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>y: Int = 3
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">square(y) // &lt;Shitf+Enter&gt; to call the function on val y of the right argument type Int
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res15: Int = 9
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = 5.0     // let x be Double 5.0
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: Double = 5.0
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//square(x) // &lt;Shift+Enter&gt; to call the function on val x of type Double will give type mismatch error
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def square(x: Int): Int = { // &lt;Shitf+Enter&gt; to declare function in a block
  val answer = x*x
  answer // the last line of the function block is returned
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>square: (x: Int)Int
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">square(5000)    // &lt;Shift+Enter&gt; to call the function
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res17: Int = 25000000
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Shift+Enter&gt; to define function with input and output type as String
def announceAndEmit(text: String): String = 
{
  println(text)
  text // the last line of the function block is returned
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>announceAndEmit: (text: String)String
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Scala has a <code>return</code> keyword but it is rarely used as the expression in the last line of the multi-line block is the method's return value.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Ctrl+Enter&gt; to call function which prints as line and returns as String
announceAndEmit(&quot;roger  roger&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>roger  roger
res18: String = roger  roger
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>A method can have output expressions involving multiple parameter lists:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def multiplyAndTranslate(x: Int, y: Int)(translateBy: Int): Int = (x * y) + translateBy
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>multiplyAndTranslate: (x: Int, y: Int)(translateBy: Int)Int
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(multiplyAndTranslate(2, 3)(4))  // (2*3)+4 = 10
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>10
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>A method can have no parameter lists at all:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def time: Long = System.currentTimeMillis
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>time: Long
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(&quot;Current time in milliseconds is &quot; + time)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Current time in milliseconds is 1664271993372
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(&quot;Current time in milliseconds is &quot; + time)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Current time in milliseconds is 1664271993681
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="classes"><a class="header" href="#classes">Classes</a></h3>
<p>The <code>class</code> keyword followed by the name and constructor parameters is used to define a class.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">class Box(h: Int, w: Int, d: Int) {
  def printVolume(): Unit = println(h*w*d)
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined class Box
</code></pre>
</div>
</div>
<div class="cell markdown">
<ul>
<li>The return type of the method <code>printVolume</code> is <code>Unit</code>.</li>
<li>When the return type is <code>Unit</code> it indicates that there is nothing meaningful to return, similar to <code>void</code> in Java and C, but with a difference.</li>
<li>Because every Scala expression must have some value, there is actually a singleton value of type <code>Unit</code>, written <code>()</code> and carrying no information.</li>
</ul>
</div>
<div class="cell markdown">
<p>We can make an instance of the class with the <code>new</code> keyword.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val my1Cube = new Box(1,1,1)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>my1Cube: Box = Box@2f0a334c
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>And call the method on the instance.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">my1Cube.printVolume() // 1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Our named instance <code>my1Cube</code> of the <code>Box</code> class is immutable due to <code>val</code>.</p>
<p>You can have mutable instances of the class using <code>var</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">var myVaryingCuboid = new Box(1,3,2)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>myVaryingCuboid: Box = Box@41dfc388
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">myVaryingCuboid.printVolume()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>6
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">myVaryingCuboid = new Box(1,1,1)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>myVaryingCuboid: Box = Box@3b1898db
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">myVaryingCuboid.printVolume()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>See <a href="https://docs.scala-lang.org/tour/classes.html">https://docs.scala-lang.org/tour/classes.html</a> for more details as needed.</p>
</div>
<div class="cell markdown">
<h3 id="case-classes"><a class="header" href="#case-classes">Case Classes</a></h3>
<p>Scala has a special type of class called a <em>case class</em> that can be defined with the <code>case class</code> keyword.</p>
<p>Unlike classes, whose instances are compared by reference, instances of case classes are immutable by default and compared by value. This makes them useful for defining rows of typed values in Spark.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">case class Point(x: Int, y: Int, z: Int)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined class Point
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Case classes can be instantiated without the <code>new</code> keyword.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val point = Point(1, 2, 3)
val anotherPoint = Point(1, 2, 3)
val yetAnotherPoint = Point(2, 2, 2)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>point: Point = Point(1,2,3)
anotherPoint: Point = Point(1,2,3)
yetAnotherPoint: Point = Point(2,2,2)
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Instances of case classes are compared by value and not by reference.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">if (point == anotherPoint) {
  println(point + &quot; and &quot; + anotherPoint + &quot; are the same.&quot;)
} else {
  println(point + &quot; and &quot; + anotherPoint + &quot; are different.&quot;)
} // Point(1,2,3) and Point(1,2,3) are the same.

if (point == yetAnotherPoint) {
  println(point + &quot; and &quot; + yetAnotherPoint + &quot; are the same.&quot;)
} else {
  println(point + &quot; and &quot; + yetAnotherPoint + &quot; are different.&quot;)
} // Point(1,2,3) and Point(2,2,2) are different.
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Point(1,2,3) and Point(1,2,3) are the same.
Point(1,2,3) and Point(2,2,2) are different.
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>By contrast, instances of classes are compared by reference.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">myVaryingCuboid.printVolume() // should be 1 x 1 x 1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">my1Cube.printVolume()  // should be 1 x 1 x 1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">if (myVaryingCuboid == my1Cube) {
  println(&quot;myVaryingCuboid and my1Cube are the same.&quot;)
} else {
  println(&quot;myVaryingCuboid and my1Cube are different.&quot;)
} // they are compared by reference and are not the same.
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>myVaryingCuboid and my1Cube are different.
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>More about case classes here: <a href="https://docs.scala-lang.org/tour/case-classes.html">https://docs.scala-lang.org/tour/case-classes.html</a>.</p>
</div>
<div class="cell markdown">
<h3 id="methods-and-tab-completion"><a class="header" href="#methods-and-tab-completion">Methods and Tab-completion</a></h3>
<p>Many methods of a class can be accessed by <code>.</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val s  = &quot;hi&quot;    // &lt;Ctrl+Enter&gt; to declare val s to String &quot;hi&quot;
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>s: String = hi
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>You can place the cursor after <code>.</code> following a declared object and find out the methods available for it as shown in the image below.</p>
<p><img src="https://github.com/raazesh-sainudiin/scalable-data-science/raw/master/images/week1/tabCompletionAfterSDot.png" alt="tabCompletionAfterSDot PNG image" /></p>
<p><strong>You Try</strong> doing this next.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//s.  // place cursor after the '.' and press Tab to see all available methods for s 
</code></pre>
</div>
<div class="cell markdown">
<p>For example,</p>
<ul>
<li>scroll down to <code>contains</code> and double-click on it.</li>
<li>This should lead to <code>s.contains</code> in your cell.</li>
<li>Now add an argument String to see if <code>s</code> contains the argument, for example, try:
<ul>
<li><code>s.contains(&quot;f&quot;)</code></li>
<li><code>s.contains(&quot;&quot;)</code> and</li>
<li><code>s.contains(&quot;i&quot;)</code></li>
</ul>
</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//s    // &lt;Shift-Enter&gt; recall the value of String s
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">s.contains(&quot;f&quot;)     // &lt;Shift-Enter&gt; returns Boolean false since s does not contain the string &quot;f&quot;
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res31: Boolean = false
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">s.contains(&quot;&quot;)    // &lt;Shift-Enter&gt; returns Boolean true since s contains the empty string &quot;&quot;
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res32: Boolean = true
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">s.contains(&quot;i&quot;)    // &lt;Ctrl+Enter&gt; returns Boolean true since s contains the string &quot;i&quot;
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res33: Boolean = true
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="objects"><a class="header" href="#objects">Objects</a></h3>
<p>Objects are single instances of their own definitions using the <code>object</code> keyword. You can think of them as singletons of their own classes.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">object IdGenerator {
  private var currentId = 0
  def make(): Int = {
    currentId += 1
    currentId
  }
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined object IdGenerator
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>You can access an object through its name:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val newId: Int = IdGenerator.make()
val newerId: Int = IdGenerator.make()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>newId: Int = 1
newerId: Int = 2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(newId) // 1
println(newerId) // 2
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1
2
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>For details see <a href="https://docs.scala-lang.org/tour/singleton-objects.html">https://docs.scala-lang.org/tour/singleton-objects.html</a></p>
</div>
<div class="cell markdown">
<h3 id="traits"><a class="header" href="#traits">Traits</a></h3>
<p>Traits are abstract data types containing certain fields and methods. They can be defined using the <code>trait</code> keyword.</p>
<p>In Scala inheritance, a class can only extend one other class, but it can extend multiple traits.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">trait Greeter {
  def greet(name: String): Unit
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined trait Greeter
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Traits can have default implementations also.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">trait Greeter {
  def greet(name: String): Unit =
    println(&quot;Hello, &quot; + name + &quot;!&quot;)
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined trait Greeter
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>You can extend traits with the <code>extends</code> keyword and override an implementation with the <code>override</code> keyword:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">class DefaultGreeter extends Greeter

class SwedishGreeter extends Greeter {
  override def greet(name: String): Unit = {
    println(&quot;Hej hej, &quot; + name + &quot;!&quot;)
  }
}

class CustomizableGreeter(prefix: String, postfix: String) extends Greeter {
  override def greet(name: String): Unit = {
    println(prefix + name + postfix)
  }
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined class DefaultGreeter
defined class SwedishGreeter
defined class CustomizableGreeter
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Instantiate the classes.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val greeter = new DefaultGreeter()
val swedishGreeter = new SwedishGreeter()
val customGreeter = new CustomizableGreeter(&quot;How are you, &quot;, &quot;?&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>greeter: DefaultGreeter = DefaultGreeter@737bf119
swedishGreeter: SwedishGreeter = SwedishGreeter@6bdaad03
customGreeter: CustomizableGreeter = CustomizableGreeter@4564c74a
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Call the <code>greet</code> method in each case.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">greeter.greet(&quot;Scala developer&quot;) // Hello, Scala developer!
swedishGreeter.greet(&quot;Scala developer&quot;) // Hej hej, Scala developer!
customGreeter.greet(&quot;Scala developer&quot;) // How are you, Scala developer?
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Hello, Scala developer!
Hej hej, Scala developer!
How are you, Scala developer?
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>A class can also be made to extend multiple traits.</p>
<p>For more details see: <a href="https://docs.scala-lang.org/tour/traits.html">https://docs.scala-lang.org/tour/traits.html</a>.</p>
</div>
<div class="cell markdown">
<h3 id="main-method"><a class="header" href="#main-method">Main Method</a></h3>
<p>The main method is the entry point of a Scala program.</p>
<p>The Java Virtual Machine requires a main method, named <code>main</code>, that takes an array of strings as its only argument.</p>
<p>Using an object, you can define the main method as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">object Main {
  def main(args: Array[String]): Unit =
    println(&quot;Hello, Scala developer!&quot;)
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined object Main
</code></pre>
</div>
</div>
<div class="cell markdown">
<p><strong>What I try not do while learning a new language?</strong></p>
<ol>
<li>I don't immediately try to ask questions like: <em>how can I do this particular variation of some small thing I just learned so I can use patterns I am used to from another language I am hooked-on right now?</em></li>
<li>first go through the detailed Scala Tour on your own and then through the 50 odd lessons in the Scala Book</li>
<li>then return to 1. and ask detailed cross-language comparison questions by diving deep as needed with the source and scala docs as needed (google or duck-duck-go search!).</li>
</ol>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="scala-crash-course-continued"><a class="header" href="#scala-crash-course-continued">Scala Crash Course Continued</a></h1>
<p>Recall!</p>
<h2 id="scala-resources-1"><a class="header" href="#scala-resources-1">Scala Resources</a></h2>
<p>You will not be learning scala systematically and thoroughly in this course. You will learn <em>to use</em> Scala by doing various Spark jobs.</p>
<p>If you are interested in learning scala properly, then there are various resources, including:</p>
<ul>
<li><a href="http://www.scala-lang.org/">scala-lang.org</a> is the <strong>core Scala resource</strong>. Bookmark the following three links:
<ul>
<li><a href="https://docs.scala-lang.org/tour/tour-of-scala.html">tour-of-scala</a> - Bite-sized introductions to core language features.
<ul>
<li>we will go through the tour in a hurry now as some Scala familiarity is needed immediately.</li>
</ul>
</li>
<li><a href="https://docs.scala-lang.org/overviews/scala-book/introduction.html">scala-book</a> - An online book introducing the main language features
<ul>
<li>you are expected to use this resource to figure out Scala as needed.</li>
</ul>
</li>
<li><a href="https://docs.scala-lang.org/cheatsheets/index.html">scala-cheatsheet</a> - A handy cheatsheet covering the basics of Scala syntax.</li>
<li><a href="https://superruzafa.github.io/visual-scala-reference/">visual-scala-reference</a> - This guide collects some of the most common functions of the Scala Programming Language and explain them conceptual and graphically in a simple way.</li>
</ul>
</li>
<li><a href="https://docs.scala-lang.org/learn.html">Online Resources</a>, including:
<ul>
<li><a href="https://www.coursera.org/course/progfun">courseera: Functional Programming Principles in Scala</a></li>
</ul>
</li>
<li><a href="http://www.scala-lang.org/documentation/books.html">Books</a>
<ul>
<li><a href="http://www.artima.com/pins1ed/">Programming in Scala, 1st Edition, Free Online Reading</a></li>
</ul>
</li>
</ul>
<p>The main sources for the following content are (you are encouraged to read them for more background):</p>
<ul>
<li><a href="https://www.scala-lang.org/old/sites/default/files/linuxsoft_archives/docu/files/ScalaByExample.pdf">Martin Oderski's Scala by example</a></li>
<li><a href="http://lintool.github.io/SparkTutorial/slides/day1_Scala_crash_course.pdf">Scala crash course by Holden Karau</a></li>
<li><a href="https://darrenjw.wordpress.com/2013/12/30/brief-introduction-to-scala-and-breeze-for-statistical-computing/">Darren's brief introduction to scala and breeze for statistical computing</a></li>
</ul>
</div>
<div class="cell markdown">
<h2 id="lets-continue-getting-our-hands-dirty-in-scala"><a class="header" href="#lets-continue-getting-our-hands-dirty-in-scala">Let's continue getting our hands dirty in Scala</a></h2>
<p>We will go through the <strong>remaining</strong> programming concepts and tasks by building on <a href="https://docs.scala-lang.org/tour/basics.html">https://docs.scala-lang.org/tour/basics.html</a>.</p>
<ul>
<li>Scala Types</li>
<li>Expressions and Printing</li>
<li>Naming and Assignments</li>
<li>Functions and Methods in Scala</li>
<li>Classes and Case Classes</li>
<li>Methods and Tab-completion</li>
<li>Objects and Traits</li>
<li><strong>Collections in Scala and Type Hierarchy</strong></li>
<li><strong>Functional Programming and MapReduce</strong></li>
<li><strong>Lazy Evaluations and Recursions</strong></li>
</ul>
<p><strong>Remark</strong>: You need to take a computer science course (from CourseEra, for example) to properly learn Scala. Here, we will learn to use Scala by example to accomplish our data science tasks at hand. You can learn more Scala as needed from various sources pointed out above in <strong>Scala Resources</strong>.</p>
</div>
<div class="cell markdown">
<h3 id="scala-type-hierarchy"><a class="header" href="#scala-type-hierarchy">Scala Type Hierarchy</a></h3>
<p>In Scala, all values have a type, including numerical values and functions. The diagram below illustrates a subset of the type hierarchy.</p>
<p><img src="https://docs.scala-lang.org/resources/images/tour/unified-types-diagram.svg" alt="" /></p>
<p>For now, notice some common types we will be usinf including <code>Int</code>, <code>String</code>, <code>Double</code>, <code>Unit</code>, <code>Boolean</code>, <code>List</code>, etc.</p>
<p>Let us take a closer look at Scala Type Hierarchy now here:</p>
<ul>
<li><a href="https://docs.scala-lang.org/tour/unified-types.html">https://docs.scala-lang.org/tour/unified-types.html</a>.</li>
</ul>
</div>
<div class="cell markdown">
<h3 id="scala-collections"><a class="header" href="#scala-collections">Scala Collections</a></h3>
<p>Familiarize yourself with the main Scala collections classes here:</p>
<ul>
<li><a href="https://docs.scala-lang.org/overviews/scala-book/collections-101.html">https://docs.scala-lang.org/overviews/scala-book/collections-101.html</a></li>
</ul>
</div>
<div class="cell markdown">
<h4 id="list"><a class="header" href="#list">List</a></h4>
<p>Lists are one of the most basic data structures.</p>
<p>There are several other Scala collections and we will introduce them as needed. The other most common ones are <code>Vector</code>, <code>Array</code> and <code>Seq</code> and the <code>ArrayBuffer</code>.</p>
<p>For details on list see: - <a href="https://docs.scala-lang.org/overviews/scala-book/list-class.html">https://docs.scala-lang.org/overviews/scala-book/list-class.html</a></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Ctrl+Enter&gt; to declare (an immutable) val lst as List of Int's 1,2,3
val lst = List(1, 2, 3)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>lst: List[Int] = List(1, 2, 3)
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="vectors"><a class="header" href="#vectors">Vectors</a></h4>
<blockquote>
<p>The Vector class is an indexed, immutable sequence. The “indexed” part of the description means that you can access Vector elements very rapidly by their index value, such as accessing listOfPeople(999999).</p>
</blockquote>
<p>In general, except for the difference that Vector is indexed and List is not, the two classes work the same, so we’ll run through these examples quickly.</p>
<p>For details see: - <a href="https://docs.scala-lang.org/overviews/scala-book/vector-class.html">https://docs.scala-lang.org/overviews/scala-book/vector-class.html</a></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val vec = Vector(1,2,3)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>vec: scala.collection.immutable.Vector[Int] = Vector(1, 2, 3)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">vec(0) // access first element with index 0
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res0: Int = 1
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="arrays-sequences-and-tuples"><a class="header" href="#arrays-sequences-and-tuples">Arrays, Sequences and Tuples</a></h4>
<p>See <a href="https://www.scala-lang.org/api/current/scala/collection/index.html">https://www.scala-lang.org/api/current/scala/collection/index.html</a> for docs.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val arr = Array(1,2,3) // &lt;Shift-Enter&gt; to declare an Array
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>arr: Array[Int] = Array(1, 2, 3)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val seq = Seq(1,2,3)    // &lt;Shift-Enter&gt; to declare a Seq
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>seq: Seq[Int] = List(1, 2, 3)
</code></pre>
</div>
</div>
<div class="cell markdown">
<blockquote>
<p>A tuple is a neat class that gives you a simple way to store heterogeneous (different) items in the same container. We will use tuples for key-value pairs in Spark.</p>
</blockquote>
<p>See <a href="https://docs.scala-lang.org/overviews/scala-book/tuples.html">https://docs.scala-lang.org/overviews/scala-book/tuples.html</a></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val myTuple = ('a',1) // a 2-tuple
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>myTuple: (Char, Int) = (a,1)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">myTuple._1 // accessing the first element of the tuple. NOTE index starts at 1 not 0 for tuples
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res1: Char = a
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">myTuple._2 // accessing the second element of the tuple
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res2: Int = 1
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="functional-programming-and-mapreduce"><a class="header" href="#functional-programming-and-mapreduce">Functional Programming and MapReduce</a></h3>
<p><em>&quot;Functional programming is a style of programming that emphasizes writing applications using only pure functions and immutable values. As Alvin Alexander wrote in Functional Programming, Simplified, rather than using that description, it can be helpful to say that functional programmers have an extremely strong desire to see their code as math — to see the combination of their functions as a series of algebraic equations. In that regard, you could say that functional programmers like to think of themselves as mathematicians. That’s the driving desire that leads them to use only pure functions and immutable values, because that’s what you use in algebra and other forms of math.&quot;</em></p>
<p>See <a href="https://docs.scala-lang.org/overviews/scala-book/functional-programming.html">https://docs.scala-lang.org/overviews/scala-book/functional-programming.html</a> for short lessons in functional programming.</p>
<p>We will apply functions for processing elements of a scala collection to quickly demonstrate functional programming.</p>
<h4 id="five-ways-of-adding-1"><a class="header" href="#five-ways-of-adding-1">Five ways of adding 1</a></h4>
<p>The first four use anonymous functions and the last one uses a named method.</p>
<ol>
<li>explicit version:</li>
</ol>
<pre><code class="language-%scala">(x: Int) =&gt; x + 1  
</code></pre>
<ol>
<li>type-inferred more intuitive version:</li>
</ol>
<pre><code class="language-%scala">x =&gt; x + 1   
</code></pre>
<ol>
<li>placeholder syntax (each argument must be used exactly once):</li>
</ol>
<pre><code class="language-%scala">_ + 1 
</code></pre>
<ol>
<li>type-inferred more intuitive version with code-block for larger function body:</li>
</ol>
<pre><code class="language-%scala">x =&gt; { 
      // body is a block of code
      val integerToAdd = 1
      x + integerToAdd
}
</code></pre>
<ol>
<li>as methods using <code>def</code>:</li>
</ol>
<pre><code class="language-%scala">def addOne(x: Int): Int = x + 1
</code></pre>
</div>
<div class="cell markdown">
<p>Now, let's do some functional programming over scala collection (<code>List</code>) using some of their methods: <code>map</code>, <code>filter</code> and <code>reduce</code>. In the end we will write our first mapReduce program!</p>
<p>For more details see:</p>
<ul>
<li><a href="https://docs.scala-lang.org/overviews/scala-book/collections-methods.html">https://docs.scala-lang.org/overviews/scala-book/collections-methods.html</a></li>
</ul>
</div>
<div class="cell markdown">
<h4 id="map"><a class="header" href="#map">map</a></h4>
<p>See: <a href="https://superruzafa.github.io/visual-scala-reference/map/">https://superruzafa.github.io/visual-scala-reference/map/</a></p>
<pre><code>trait Collection[A] {
  def map[B](f: (A) =&gt; B): Collection[B]
}
</code></pre>
<p>map creates a collection using as elements the results obtained from applying the function f to each element of this collection.</p>
<p><img src="https://superruzafa.github.io/visual-scala-reference/images/functions/map.svg" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Shift+Enter&gt; to map each value x of lst with x+10 to return a new List(11, 12, 13)
lst.map(x =&gt; x + 10)  
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res3: List[Int] = List(11, 12, 13)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Shift+Enter&gt; for the same as above using place-holder syntax
lst.map( _ + 10)  
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res4: List[Int] = List(11, 12, 13)
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="filter"><a class="header" href="#filter">filter</a></h4>
<p>See: <a href="https://superruzafa.github.io/visual-scala-reference/filter/">https://superruzafa.github.io/visual-scala-reference/filter/</a></p>
<pre><code>trait Collection[A] {
  def filter(p: (A) =&gt; Boolean): Collection[A]
}
</code></pre>
<p>filter creates a collection with those elements that satisfy the predicate p and discarding the rest.</p>
<p><img src="https://superruzafa.github.io/visual-scala-reference/images/functions/filter.svg" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Shift+Enter&gt; to return a new List(1, 3) after filtering x's from lst if (x % 2 == 1) is true
lst.filter(x =&gt; (x % 2 == 1) )
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res5: List[Int] = List(1, 3)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Shift+Enter&gt; for the same as above using place-holder syntax
lst.filter( _ % 2 == 1 )
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res6: List[Int] = List(1, 3)
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="reduce"><a class="header" href="#reduce">reduce</a></h4>
<p>See: <a href="https://superruzafa.github.io/visual-scala-reference/reduce/">https://superruzafa.github.io/visual-scala-reference/reduce/</a></p>
<pre><code>trait Collection[A] {
  def reduce(op: (A, A) =&gt; A): A
}
</code></pre>
<p><code>reduce</code> applies the binary operator <code>op</code> to pairs of elements in this collection until the final result is calculated.</p>
<p><img src="https://superruzafa.github.io/visual-scala-reference/images/functions/reduce.svg" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Shift+Enter&gt; to use reduce to add elements of lst two at a time to return Int 6
lst.reduce( (x, y) =&gt; x + y )
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res7: Int = 6
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Ctrl+Enter&gt; for the same as above but using place-holder syntax
lst.reduce( _ + _ )
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res8: Int = 6
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let's combine <code>map</code> and <code>reduce</code> programs above to find the sum of after 10 has been added to every element of the original List <code>lst</code> as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">lst.map(x =&gt; x+10)
   .reduce((x,y) =&gt; x+y) // &lt;Ctrl-Enter&gt; to get Int 36 = sum(1+10,2+10,3+10)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res9: Int = 36
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="exercise-in-functional-programming"><a class="header" href="#exercise-in-functional-programming">Exercise in Functional Programming</a></h4>
<p>You should spend an hour or so going through the Functional Programming Section of the Scala Book:</p>
<ul>
<li><a href="https://docs.scala-lang.org/overviews/scala-book/functional-programming.html">https://docs.scala-lang.org/overviews/scala-book/functional-programming.html</a></li>
</ul>
<blockquote>
<p>Scala lets you write code in an object-oriented programming (OOP) style, a functional programming (FP) style, and even in a hybrid style, using both approaches in combination. This book assumes that you’re coming to Scala from an OOP language like Java, C++, or C#, so outside of covering Scala classes, there aren’t any special sections about OOP in this book. But because the FP style is still relatively new to many developers, we’ll provide a brief introduction to Scala’s support for FP in the next several lessons.</p>
</blockquote>
<blockquote>
<p>Functional programming is a style of programming that emphasizes writing applications using only pure functions and immutable values. As Alvin Alexander wrote in <a href="https://alvinalexander.com/scala/functional-programming-simplified-book">Functional Programming, Simplified</a>, rather than using that description, it can be helpful to say that functional programmers have an extremely strong desire to see their code as math — to see the combination of their functions as a series of algebraic equations. In that regard, you could say that functional programmers like to think of themselves as mathematicians. That’s the driving desire that leads them to use only pure functions and immutable values, because that’s what you use in algebra and other forms of math.</p>
</blockquote>
<blockquote>
<p>Functional programming is a large topic, and there’s no simple way to condense the entire topic into this little book, but in the following lessons we’ll give you a taste of FP, and show some of the tools Scala provides for developers to write functional code.</p>
</blockquote>
</div>
<div class="cell markdown">
<p>There are lots of methods in Scala Collections. And much more in this <em>scalable language</em>. See for example <a href="http://docs.scala-lang.org/cheatsheets/index.html">http://docs.scala-lang.org/cheatsheets/index.html</a>.</p>
</div>
<div class="cell markdown">
<h3 id="lazy-evaluation"><a class="header" href="#lazy-evaluation">Lazy Evaluation</a></h3>
<p>Another powerful programming concept we will need is <em>lazy evaluation</em> -- a form of delayed evaluation. So the value of an expression that is lazily evaluated is only available when it is actually needed.</p>
<p>This is to be contrasted with <em>eager evaluation</em> that we have seen so far -- an expression is immediately evaluated.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val eagerImmutableInt = 1 // eagerly evaluated as 1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>eagerImmutableInt: Int = 1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">var eagerMutableInt = 2 // eagerly evaluated as 2
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>eagerMutableInt: Int = 2
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let's demonstrate lazy evaluation using a <code>getTime</code> method and the keyword <code>lazy</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import java.util.Calendar
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import java.util.Calendar
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">lazy val lazyImmutableTime = Calendar.getInstance.getTime // lazily defined and not evaluated immediately
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>lazyImmutableTime: java.util.Date = &lt;lazy&gt;
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val eagerImmutableTime = Calendar.getInstance.getTime // egaerly evaluated immediately
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>eagerImmutableTime: java.util.Date = Tue Sep 27 09:49:49 UTC 2022
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(lazyImmutableTime) // evaluated when actully needed by println
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Tue Sep 27 09:49:49 UTC 2022
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(eagerImmutableTime) // prints what was already evaluated eagerly
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Tue Sep 27 09:49:49 UTC 2022
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def lazyDefinedInt = 5 // you can also use method to lazily define 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>lazyDefinedInt: Int
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">lazyDefinedInt // only evaluated now
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res12: Int = 5
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>See https://www.scala-exercises.org/scala<em>tutorial/lazy</em>evaluation for more details including the following example with <code>StringBuilder</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val builder = new StringBuilder //built-in
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>builder: StringBuilder =
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">builder.result()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res13: String = &quot;&quot;
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = { builder += 'x'; 1 } // eagerly evaluates x as 1 after appending 'x' to builder. NOTE: ';' is used to separate multiple expressions on the same line
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: Int = 1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">builder.result()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res14: String = x
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">x
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res15: Int = 1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">builder.result() // calling x again should not append x again to builder
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res16: String = x
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">lazy val y = { builder += 'y'; 2 } // lazily evaluate y later when it is called
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>y: Int = &lt;lazy&gt;
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">builder.result() // builder should remain unchanged
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res17: String = x
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def z = { builder += 'z'; 3 } // lazily evaluate z later when the method is called
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>z: Int
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">builder.result() // builder should remain unchanged
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res18: String = x
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>What should <code>builder.result()</code> be after the following arithmetic expression involving <code>x</code>,<code>y</code> and <code>z</code> is evaluated?</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">z + y + x + z + y + x
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res19: Int = 12
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="lazy-evaluation-exercise---you-try-now"><a class="header" href="#lazy-evaluation-exercise---you-try-now">Lazy Evaluation Exercise - You try Now!</a></h4>
<p>Understand why the output above is what it is!</p>
<ul>
<li>Why is <code>z</code> different in its appearance in the final <code>builder</code> string when compared to <code>x</code> and <code>y</code> as we evaluated?</li>
</ul>
<!-- -->
<pre><code>z + y + x + z + y + x
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// putting it all together
val builder = new StringBuilder

val x = { builder += 'x'; 1 }
lazy val y = { builder += 'y'; 2 }
def z = { builder += 'z'; 3 }

// comment next line after different summands to understand difference between val, lazy val and def
z + y + x  + z + y + x 

builder.result()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>builder: StringBuilder = xzyz
x: Int = 1
y: Int = &lt;lazy&gt;
z: Int
res20: String = xzyz
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="why-lazy"><a class="header" href="#why-lazy">Why Lazy?</a></h3>
<p>Imagine a more complex expression involving the evaluation of millions of values. Lazy evaluation will allow us to actually compute with big data when it may become impossible to hold all the values in memory. This is exactly what Apache Spark does as we will see.</p>
</div>
<div class="cell markdown">
<h3 id="recursions"><a class="header" href="#recursions">Recursions</a></h3>
<p>Recursion is a powerful framework when a function calls another function, including itself, until some terminal condition is reached.</p>
<p>Here we want to distinguish between two ways of implementing a recursion using a simple example of factorial.</p>
<p>Recall that for any natural number \(n\), its factorial is denoted and defined as follows:</p>
<p>\[
n!  :=  n \times (n-1) \times (n-2) \times \cdots \times 2 \times 1 
\]</p>
<p>which has the following recursive expression:</p>
<p>\[
n! = n*(n-1)! , , \qquad  0! = 1
\]</p>
<p>Let us implement it using two approaches: a naive approach that can run out of memory and another tail-recursive approach that uses constant memory. Read <a href="https://www.scala-exercises.org/scala_tutorial/tail_recursion">https://www.scala-exercises.org/scala<em>tutorial/tail</em>recursion</a> for details.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def factorialNaive(n: Int): Int =
  if (n == 0) 1 else n * factorialNaive(n - 1)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>factorialNaive: (n: Int)Int
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">factorialNaive(4)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res21: Int = 24
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>When <code>factorialNaive(4)</code> was evaluated above the following steps were actually done:</p>
<pre><code>factorial(4)
if (4 == 0) 1 else 4 * factorial(4 - 1)
4 * factorial(3)
4 * (3 * factorial(2))
4 * (3 * (2 * factorial(1)))
4 * (3 * (2 * (1 * factorial(0)))
4 * (3 * (2 * (1 * 1)))
24
</code></pre>
</div>
<div class="cell markdown">
<p>Notice how we add one more element to our expression at each recursive call. Our expressions becomes bigger and bigger until we end by reducing it to the final value. So the final expression given by a directed acyclic graph (DAG) of the pairwise multiplications given by the right-branching binary tree, whose leaves are input integers and internal nodes are the bianry <code>*</code> operator, can get very large when the input <code>n</code> is large.</p>
<p><em>Tail recursion</em> is a sophisticated way of implementing certain recursions so that memory requirements can be kept constant, as opposed to naive recursions.</p>
<blockquote>
<p><a href="https://www.scala-exercises.org/scala_tutorial/tail_recursion">Tail Recursion</a></p>
</blockquote>
<blockquote>
<p>That difference in the rewriting rules actually translates directly to a difference in the actual execution on a computer. In fact, it turns out that <strong>if you have a recursive function that calls itself as its last action, then you can reuse the stack frame of that function. This is called tail recursion.</strong></p>
</blockquote>
<blockquote>
<p>And by applying that trick, a tail recursive function can execute in constant stack space, so it's really just another formulation of an iterative process. We could say a tail recursive function is the functional form of a loop, and it executes just as efficiently as a loop.</p>
</blockquote>
<p>Implementation of tail recursion in the Exercise below uses Scala <a href="https://docs.scala-lang.org/tour/annotations.html">annotation</a>, which is a way to associate meta-information with definitions. In our case, the annotation <code>@tailrec</code> ensures that a method is indeed <a href="https://en.wikipedia.org/wiki/Tail_call">tail-recursive</a>. See the last link to understand how memory requirements can be kept constant in tail recursions.</p>
<p>We mainly want you to know that tail recursions are an important functional programming concept.</p>
</div>
<div class="cell markdown">
<h4 id="tail-recursion-exercise---you-try-now"><a class="header" href="#tail-recursion-exercise---you-try-now">Tail Recursion Exercise - You Try Now</a></h4>
<ul>
<li>Uncomment the next three cells</li>
<li>Replace <code>???</code> in the next cell with the correct values to make this a tail recursion for factorial.</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">/*
import scala.annotation.tailrec

// replace ??? with the right values to make this a tail recursion for factorial
def factorialTail(n: Int): Int = {
  @tailrec
  def iter(x: Int, result: Int): Int =
    if ( x == ???) result
    else iter(x - 1, result * x)

  iter( n, ??? )
}
*/
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//factorialTail(3) //shouldBe 6
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//factorialTail(4) //shouldBe 24
</code></pre>
</div>
<div class="cell markdown">
<p>Functional Programming is a vast subject and we are merely covering the fewest core ideas to get started with Apache Spark asap.</p>
<p>We will return to more concepts as we need them in the sequel.</p>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="introduction-to-spark"><a class="header" href="#introduction-to-spark">Introduction to Spark</a></h1>
<h2 id="spark-essentials-rdds-transformations-and-actions"><a class="header" href="#spark-essentials-rdds-transformations-and-actions">Spark Essentials: RDDs, Transformations and Actions</a></h2>
<ul>
<li>This introductory notebook describes how to get started running Spark (Scala) code in Notebooks.</li>
<li>Working with Spark's Resilient Distributed Datasets (RDDs)
<ul>
<li>creating RDDs</li>
<li>performing basic transformations on RDDs</li>
<li>performing basic actions on RDDs</li>
</ul>
</li>
</ul>
<p><strong>RECOLLECT</strong> from <code>001_WhySpark</code> notebook that <em>Spark does fault-tolerant, distributed, in-memory computing</em></p>
<p><strong>THEORY CAVEAT</strong> This module is focused on getting you to quickly write Spark programs with a high-level appreciation of the underlying concepts.</p>
<p>In the last module, we will spend more time on analyzing the core algorithms in parallel and distributed setting of a typical Spark cluster today -- where several multi-core parallel computers (Spark workers) are networked together to provide a fault-tolerant distributed computing platform.</p>
</div>
<div class="cell markdown">
<h2 id="spark-cluster-overview"><a class="header" href="#spark-cluster-overview">Spark Cluster Overview:</a></h2>
<p><strong>Driver Program, Cluster Manager and Worker Nodes</strong></p>
<p>The <em>driver</em> does the following:</p>
<ol>
<li>connects to a <em>cluster manager</em> to allocate resources across applications</li>
</ol>
<ul>
<li>acquire <em>executors</em> on cluster nodes
<ul>
<li>executor processs run compute tasks and cache data in memory or disk on a <em>worker node</em></li>
</ul>
</li>
<li>sends <em>application</em> (user program built on Spark) to the executors</li>
<li>sends <em>tasks</em> for the executors to run
<ul>
<li>task is a unit of work that will be sent to one executor</li>
</ul>
</li>
</ul>
<p><img src="http://spark.apache.org/docs/latest/img/cluster-overview.png" alt="" /></p>
<p>See <a href="http://spark.apache.org/docs/latest/cluster-overview.html">http://spark.apache.org/docs/latest/cluster-overview.html</a> for an overview of the spark cluster.</p>
</div>
<div class="cell markdown">
<h2 id="the-abstraction-of-resilient-distributed-dataset-rdd"><a class="header" href="#the-abstraction-of-resilient-distributed-dataset-rdd">The Abstraction of Resilient Distributed Dataset (RDD)</a></h2>
<p><strong>RDD is a fault-tolerant collection of elements that can be operated on in parallel.</strong></p>
<h3 id="key-points-to-note"><a class="header" href="#key-points-to-note">Key Points to Note</a></h3>
<ul>
<li>Resilient distributed datasets (RDDs) are the primary abstraction in Spark.</li>
<li>RDDs are immutable once created:
<ul>
<li>can transform it.</li>
<li>can perform actions on it.</li>
<li>but cannot change an RDD once you construct it.</li>
</ul>
</li>
<li>Spark tracks each RDD's lineage information or recipe to enable its efficient recomputation if a machine fails.</li>
<li>RDDs enable operations on collections of elements in parallel.</li>
<li>We can construct RDDs by:
<ul>
<li>parallelizing Scala collections such as lists or arrays</li>
<li>by transforming an existing RDD,</li>
<li>from files in distributed file systems such as (HDFS, S3, etc.).</li>
</ul>
</li>
<li>We can specify the number of partitions for an RDD</li>
<li>The more partitions in an RDD, the more opportunities for parallelism</li>
<li>There are <strong>two types of operations</strong> you can perform on an RDD:
<ul>
<li><strong>transformations</strong> (are lazily evaluated)
<ul>
<li>map</li>
<li>flatMap</li>
<li>filter</li>
<li>distinct</li>
<li>...</li>
</ul>
</li>
<li><strong>actions</strong> (actual evaluation happens)
<ul>
<li>count</li>
<li>reduce</li>
<li>take</li>
<li>collect</li>
<li>takeOrdered</li>
<li>...</li>
</ul>
</li>
</ul>
</li>
<li>Spark transformations enable us to create new RDDs from an existing RDD.</li>
<li>RDD transformations are lazy evaluations (results are not computed right away)</li>
<li>Spark remembers the set of transformations that are applied to a base data set (this is the lineage graph of RDD)</li>
<li>The allows Spark to automatically recover RDDs from failures and slow workers.</li>
<li>The lineage graph is a recipe for creating a result and it can be optimized before execution.</li>
<li>A transformed RDD is executed only when an action runs on it.</li>
<li>You can also persist, or cache, RDDs in memory or on disk (this speeds up iterative ML algorithms that transforms the initial RDD iteratively).</li>
<li>Here is a great reference URL for programming guides for Spark that one should try to cover first
<ul>
<li><a href="http://spark.apache.org/docs/latest/programming-guide.html">http://spark.apache.org/docs/latest/programming-guide.html</a>.</li>
<li>and specifically for RDDs: <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html">https://spark.apache.org/docs/latest/rdd-programming-guide.html</a></li>
</ul>
</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://spark.apache.org/docs/latest/rdd-programming-guide.html"
 width="95%" height="700"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<h2 id="lets-get-our-hands-dirty-in-spark"><a class="header" href="#lets-get-our-hands-dirty-in-spark">Let's get our hands dirty in Spark!</a></h2>
<p><strong>DO NOW!</strong></p>
<p>There is a peer-reviewed Assignment where you will dig deeper into Spark transformations and actions.</p>
</div>
<div class="cell markdown">
<p><strong>Let us look at the legend and overview of the visual RDD Api by doing the following first:</strong></p>
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-1.png" alt="" /></p>
</div>
<div class="cell markdown">
<h3 id="running-spark"><a class="header" href="#running-spark">Running <strong>Spark</strong></a></h3>
<p>The variable <strong>sc</strong> allows you to access a Spark Context to run your Spark programs. Recall <code>SparkContext</code> is in the Driver Program.</p>
<p><img src="http://spark.apache.org/docs/latest/img/cluster-overview.png" alt="" /></p>
<p>**NOTE: Do not create the <em>sc</em> variable - it is already initialized for you in spark-shell REPL, that includes notebook environments like databricks, Jupyter, zeppelin, etc. **</p>
</div>
<div class="cell markdown">
<h3 id="we-will-do-the-following-next"><a class="header" href="#we-will-do-the-following-next">We will do the following next:</a></h3>
<ol>
<li>Create an RDD using <code>sc.parallelize</code></li>
</ol>
<ul>
<li>Perform the <code>collect</code> action on the RDD and find the number of partitions it is made of using <code>getNumPartitions</code> action</li>
<li>Perform the <code>take</code> action on the RDD</li>
<li>Transform the RDD by <code>map</code> to make another RDD</li>
<li>Transform the RDD by <code>filter</code> to make another RDD</li>
<li>Perform the <code>reduce</code> action on the RDD</li>
<li>Transform the RDD by <code>flatMap</code> to make another RDD</li>
<li>Create a Pair RDD</li>
<li>Perform some transformations on a Pair RDD</li>
<li>Where in the cluster is your computation running?</li>
<li>Shipping Closures, Broadcast Variables and Accumulator Variables</li>
<li>Spark Essentials: Summary</li>
<li>HOMEWORK</li>
<li>Importing Standard Scala and Java libraries</li>
</ul>
</div>
<div class="cell markdown">
<h4 id="entry-point"><a class="header" href="#entry-point">Entry Point</a></h4>
<p>Now we are ready to start programming in Spark!</p>
<p>Our entry point for Spark applications is the class <code>SparkSession</code>. An instance of this object is already instantiated for us which can be easily demonstrated by running the next cell</p>
<p>We will need these docs!</p>
<ul>
<li><a href="https://spark.apache.org/docs/3.0.1/api/scala/org/apache/spark/rdd/RDD.html">RDD Scala Docs</a></li>
<li><a href="https://spark.apache.org/docs/3.0.1/api/scala/org/apache/spark/sql/Dataset.html">Dataset Scala Docs</a></li>
<li><a href="https://spark.apache.org/docs/3.0.1/api/scala/index.html">https://spark.apache.org/docs/3.0.1/api/scala/index.html</a> you can simply search for other Spark classes, methods, etc here</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(spark) // spark is already created for us in databricks or in spark-shell
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>org.apache.spark.sql.SparkSession@1b6fb477
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>NOTE that since Spark 2.0 <code>SparkSession</code> is a replacement for the other entry points: * <code>SparkContext</code>, available in our notebook as <strong>sc</strong>. * <code>SQLContext</code>, or more specifically its subclass <code>HiveContext</code>, available in our notebook as <strong>sqlContext</strong>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(sc)
println(sqlContext)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>org.apache.spark.SparkContext@61cc1a6b
org.apache.spark.sql.hive.HiveContext@1da4f2b4
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>We will be using the pre-made SparkContext <code>sc</code> when learning about RDDs.</p>
</div>
<div class="cell markdown">
<h4 id="1-create-an-rdd-using-scparallelize"><a class="header" href="#1-create-an-rdd-using-scparallelize">1. Create an RDD using <code>sc.parallelize</code></a></h4>
<p>First, let us create an RDD of three elements (of integer type <code>Int</code>) from a Scala <code>Seq</code> (or <code>List</code> or <code>Array</code>) with two partitions by using the <code>parallelize</code> method of the available Spark Context <code>sc</code> as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = sc.parallelize(Array(1, 2, 3), 2)    // &lt;Ctrl+Enter&gt; to evaluate this cell (using 2 partitions)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at command-4088905069026221:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//x.  // place the cursor after 'x.' and hit Tab to see the methods available for the RDD x we created
</code></pre>
</div>
<div class="cell markdown">
<h4 id="2-perform-the-collect-action-on-the-rdd-and-find-the-number-of-partitions-in-it-using-getnumpartitions-action"><a class="header" href="#2-perform-the-collect-action-on-the-rdd-and-find-the-number-of-partitions-in-it-using-getnumpartitions-action">2. Perform the <code>collect</code> action on the RDD and find the number of partitions in it using <code>getNumPartitions</code> action</a></h4>
<p>No action has been taken by <code>sc.parallelize</code> above. To see what is &quot;cooked&quot; by the recipe for RDD <code>x</code> we need to take an action.</p>
<p>The simplest is the <code>collect</code> action which returns all of the elements of the RDD as an <code>Array</code> to the driver program and displays it.</p>
<p><em>So you have to make sure that all of that data will fit in the driver program if you call <code>collect</code> action!</em></p>
</div>
<div class="cell markdown">
<h5 id="let-us-look-at-the-collect-action-in-detail-and-return-here-to-try-out-the-example-codes"><a class="header" href="#let-us-look-at-the-collect-action-in-detail-and-return-here-to-try-out-the-example-codes">Let us look at the <a href="contents/000_1-sds-3-x-spark//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/actions/collect">collect action in detail</a> and return here to try out the example codes.</a></h5>
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-90.png" alt="" /></p>
</div>
<div class="cell markdown">
<p>Let us perform a <code>collect</code> action on RDD <code>x</code> as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">x.collect()    // &lt;Ctrl+Enter&gt; to collect (action) elements of rdd; should be (1, 2, 3)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res4: Array[Int] = Array(1, 2, 3)
</code></pre>
</div>
</div>
<div class="cell markdown">
<p><em>CAUTION:</em> <code>collect</code> can crash the driver when called upon an RDD with massively many elements.
So, it is better to use other diplaying actions like <code>take</code> or <code>takeOrdered</code> as follows:</p>
</div>
<div class="cell markdown">
<h5 id="let-us-look-at-the-getnumpartitions-action-in-detail-and-return-here-to-try-out-the-example-codes"><a class="header" href="#let-us-look-at-the-getnumpartitions-action-in-detail-and-return-here-to-try-out-the-example-codes">Let us look at the <a href="contents/000_1-sds-3-x-spark//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/actions/getNumPartitions">getNumPartitions action in detail</a> and return here to try out the example codes.</a></h5>
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-88.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Ctrl+Enter&gt; to evaluate this cell and find the number of partitions in RDD x
x.getNumPartitions 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res5: Int = 2
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>We can see which elements of the RDD are in which parition by calling <code>glom()</code> before <code>collect()</code>.</p>
<p><code>glom()</code> flattens elements of the same partition into an <code>Array</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">x.glom().collect() // glom() flattens elements on the same partition
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res6: Array[Array[Int]] = Array(Array(1), Array(2, 3))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val a = x.glom().collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>a: Array[Array[Int]] = Array(Array(1), Array(2, 3))
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Thus from the output above, <code>Array[Array[Int]] = Array(Array(1), Array(2, 3))</code>, we know that <code>1</code> is in one partition while <code>2</code> and <code>3</code> are in another partition.</p>
</div>
<div class="cell markdown">
<h5 id="you-try"><a class="header" href="#you-try">You Try!</a></h5>
<p>Crate an RDD <code>x</code> with three elements, 1,2,3, and this time do not specifiy the number of partitions. Then the default number of partitions will be used. Find out what this is for the cluster you are attached to.</p>
<p>The default number of partitions for an RDD depends on the cluster this notebook is attached to among others - see <a href="http://spark.apache.org/docs/latest/programming-guide.html">programming-guide</a>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = sc.parallelize(Seq(1, 2, 3))    // &lt;Shift+Enter&gt; to evaluate this cell (using default number of partitions)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[3] at parallelize at command-4088905069026235:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">x.getNumPartitions // &lt;Shift+Enter&gt; to evaluate this cell
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res7: Int = 2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">x.glom().collect() // &lt;Ctrl+Enter&gt; to evaluate this cell
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res8: Array[Array[Int]] = Array(Array(1), Array(2, 3))
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="3-perform-the-take-action-on-the-rdd"><a class="header" href="#3-perform-the-take-action-on-the-rdd">3. Perform the <code>take</code> action on the RDD</a></h4>
<p>The <code>.take(n)</code> action returns an array with the first <code>n</code> elements of the RDD.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">x.take(2) // Ctrl+Enter to take two elements from the RDD x
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res9: Array[Int] = Array(1, 2)
</code></pre>
</div>
</div>
<div class="cell markdown">
<h5 id="you-try-1"><a class="header" href="#you-try-1">You Try!</a></h5>
<p>Fill in the parenthes <code>( )</code> below in order to <code>take</code> just one element from RDD <code>x</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//x.take(1) // uncomment by removing '//' before x in the cell and fill in the parenthesis to take just one element from RDD x and Cntrl+Enter
</code></pre>
</div>
<div class="cell markdown">
<hr />
<h4 id="4-transform-the-rdd-by-map-to-make-another-rdd"><a class="header" href="#4-transform-the-rdd-by-map-to-make-another-rdd">4. Transform the RDD by <code>map</code> to make another RDD</a></h4>
<p>The <code>map</code> transformation returns a new RDD that's formed by passing each element of the source RDD through a function (closure). The closure is automatically passed on to the workers for evaluation (when an action is called later).</p>
</div>
<div class="cell markdown">
<h5 id="let-us-look-at-the-map-transformation-in-detail-and-return-here-to-try-out-the-example-codes"><a class="header" href="#let-us-look-at-the-map-transformation-in-detail-and-return-here-to-try-out-the-example-codes">Let us look at the <a href="contents/000_1-sds-3-x-spark//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/map">map transformation in detail</a> and return here to try out the example codes.</a></h5>
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-18.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Shift+Enter to make RDD x and RDD y that is mapped from x
val x = sc.parallelize(Array(&quot;b&quot;, &quot;a&quot;, &quot;c&quot;)) // make RDD x: [b, a, c]
val y = x.map(z =&gt; (z,1))                    // map x into RDD y: [(b, 1), (a, 1), (c, 1)]
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[5] at parallelize at command-4088905069026244:2
y: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[6] at map at command-4088905069026244:3
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Cntrl+Enter to collect and print the two RDDs
println(x.collect().mkString(&quot;, &quot;))
println(y.collect().mkString(&quot;, &quot;))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>b, a, c
(b,1), (a,1), (c,1)
</code></pre>
</div>
</div>
<div class="cell markdown">
<hr />
<h4 id="5-transform-the-rdd-by-filter-to-make-another-rdd"><a class="header" href="#5-transform-the-rdd-by-filter-to-make-another-rdd">5. Transform the RDD by <code>filter</code> to make another RDD</a></h4>
<p>The <code>filter</code> transformation returns a new RDD that's formed by selecting those elements of the source RDD on which the function returns <code>true</code>.</p>
</div>
<div class="cell markdown">
<h5 id="let-us-look-at-the-filter-transformation-in-detail-and-return-here-to-try-out-the-example-codes"><a class="header" href="#let-us-look-at-the-filter-transformation-in-detail-and-return-here-to-try-out-the-example-codes">Let us look at the <a href="contents/000_1-sds-3-x-spark//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/filter">filter transformation in detail</a> and return here to try out the example codes.</a></h5>
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-24.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//Shift+Enter to make RDD x and filter it by (n =&gt; n%2 == 1) to make RDD y
val x = sc.parallelize(Array(1,2,3))
// the closure (n =&gt; n%2 == 1) in the filter will 
// return True if element n in RDD x has remainder 1 when divided by 2 (i.e., if n is odd)
val y = x.filter(n =&gt; n%2 == 1) 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[7] at parallelize at command-4088905069026248:2
y: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[8] at filter at command-4088905069026248:5
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Cntrl+Enter to collect and print the two RDDs
println(x.collect().mkString(&quot;, &quot;))
println(y.collect().mkString(&quot;, &quot;))
//y.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1, 2, 3
1, 3
</code></pre>
</div>
</div>
<div class="cell markdown">
<hr />
<h4 id="6-perform-the-reduce-action-on-the-rdd"><a class="header" href="#6-perform-the-reduce-action-on-the-rdd">6. Perform the <code>reduce</code> action on the RDD</a></h4>
<p>Reduce aggregates a data set element using a function (closure). This function takes two arguments and returns one and can often be seen as a binary operator. This operator has to be commutative and associative so that it can be computed correctly in parallel (where we have little control over the order of the operations!).</p>
</div>
<div class="cell markdown">
<h5 id="let-us-look-at-the-reduce-action-in-detail-and-return-here-to-try-out-the-example-codes"><a class="header" href="#let-us-look-at-the-reduce-action-in-detail-and-return-here-to-try-out-the-example-codes">Let us look at the <a href="contents/000_1-sds-3-x-spark//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/actions/reduce">reduce action in detail</a> and return here to try out the example codes.</a></h5>
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-94.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//Shift+Enter to make RDD x of inteegrs 1,2,3,4 and reduce it to sum
val x = sc.parallelize(Array(1,2,3,4))
val y = x.reduce((a,b) =&gt; a+b)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[9] at parallelize at command-4088905069026252:2
y: Int = 10
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//Cntrl+Enter to collect and print RDD x and the Int y, sum of x
println(x.collect.mkString(&quot;, &quot;))
println(y)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1, 2, 3, 4
10
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="7-transform-an-rdd-by-flatmap-to-make-another-rdd"><a class="header" href="#7-transform-an-rdd-by-flatmap-to-make-another-rdd">7. Transform an RDD by <code>flatMap</code> to make another RDD</a></h4>
<p><code>flatMap</code> is similar to <code>map</code> but each element from input RDD can be mapped to zero or more output elements. Therefore your function should return a sequential collection such as an <code>Array</code> rather than a single element as shown below.</p>
</div>
<div class="cell markdown">
<h5 id="let-us-look-at-the-flatmap-transformation-in-detail-and-return-here-to-try-out-the-example-codes"><a class="header" href="#let-us-look-at-the-flatmap-transformation-in-detail-and-return-here-to-try-out-the-example-codes">Let us look at the <a href="contents/000_1-sds-3-x-spark//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/flatMap">flatMap transformation in detail</a> and return here to try out the example codes.</a></h5>
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-31.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//Shift+Enter to make RDD x and flatMap it into RDD by closure (n =&gt; Array(n, n*100, 42))
val x = sc.parallelize(Array(1,2,3))
val y = x.flatMap(n =&gt; Array(n, n*100, 42))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[10] at parallelize at command-4088905069026256:2
y: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[11] at flatMap at command-4088905069026256:3
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//Cntrl+Enter to collect and print RDDs x and y
println(x.collect().mkString(&quot;, &quot;))
println(y.collect().mkString(&quot;, &quot;))
sc.parallelize(Array(1,2,3)).map(n =&gt; Array(n,n*100,42)).collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1, 2, 3
1, 100, 42, 2, 200, 42, 3, 300, 42
res14: Array[Array[Int]] = Array(Array(1, 100, 42), Array(2, 200, 42), Array(3, 300, 42))
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="8-create-a-pair-rdd"><a class="header" href="#8-create-a-pair-rdd">8. Create a Pair RDD</a></h4>
<p>Let's next work with RDD of <code>(key,value)</code> pairs called a <em>Pair RDD</em> or <em>Key-Value RDD</em>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Cntrl+Enter to make RDD words and display it by collect
val words = sc.parallelize(Array(&quot;a&quot;, &quot;b&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;))
words.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>words: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[14] at parallelize at command-4088905069026259:2
res15: Array[String] = Array(a, b, a, a, b, b, a, a, a, b, b)
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let's make a Pair RDD called <code>wordCountPairRDD</code> that is made of (key,value) pairs with key=word and value=1 in order to encode each occurrence of each word in the RDD <code>words</code>, as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Cntrl+Enter to make and collect Pair RDD wordCountPairRDD
val wordCountPairRDD = words.map(s =&gt; (s, 1))
wordCountPairRDD.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>wordCountPairRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[15] at map at command-4088905069026261:2
res16: Array[(String, Int)] = Array((a,1), (b,1), (a,1), (a,1), (b,1), (b,1), (a,1), (a,1), (a,1), (b,1), (b,1))
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="wide-transformations-and-shuffles"><a class="header" href="#wide-transformations-and-shuffles">Wide Transformations and Shuffles</a></h4>
<p>So far we have seen transformations that are <strong>narrow</strong> -- with no data transfer between partitions. Think of <code>map</code>.</p>
<p><code>ReduceByKey</code> and <code>GroupByKey</code> are <strong>wide</strong> transformations as data has to be shuffled across the partitions in different executors -- this is generally very expensive operation.</p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-40.png" alt="" /></p>
</div>
<div class="cell markdown">
<p>READ the <strong>Background</strong> about Shuffles in the programming guide below.</p>
<blockquote>
<p>In Spark, data is generally not distributed across partitions to be in the necessary place for a specific operation. During computations, a single task will operate on a single partition - thus, to organize all the data for a single reduceByKey reduce task to execute, Spark needs to perform an all-to-all operation. It must read from all partitions to find all the values for all keys, and then bring together values across partitions to compute the final result for each key - this is called the shuffle</p>
</blockquote>
<p>READ the <strong>Performance Impact</strong> about Shuffles in the programming guide below.</p>
<blockquote>
<p>The Shuffle is an expensive operation since it involves disk I/O, data serialization, and network I/O. To organize data for the shuffle, Spark generates sets of tasks - map tasks to organize the data, and a set of reduce tasks to aggregate it. This nomenclature comes from MapReduce and does not directly relate to Spark’s map and reduce operations.</p>
</blockquote>
<blockquote>
<p>Internally, results from individual map tasks are kept in memory until they can’t fit. Then, these are sorted based on the target partition and written to a single file. On the reduce side, tasks read the relevant sorted blocks.</p>
</blockquote>
<p><a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations">https://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations</a></p>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations&quot;,500))
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations"
 width="95%" height="500"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<h4 id="9-perform-some-transformations-on-a-pair-rdd"><a class="header" href="#9-perform-some-transformations-on-a-pair-rdd">9. Perform some transformations on a Pair RDD</a></h4>
<p>Let's next work with RDD of <code>(key,value)</code> pairs called a <em>Pair RDD</em> or <em>Key-Value RDD</em>.</p>
<p>Now some of the Key-Value transformations that we could perform include the following.</p>
<ul>
<li><strong><code>reduceByKey</code> transformation</strong>
<ul>
<li>which takes an RDD and returns a new RDD of key-value pairs, such that:
<ul>
<li>the values for each key are aggregated using the given reduced function</li>
<li>and the reduce function has to be of the type that takes two values and returns one value.</li>
</ul>
</li>
</ul>
</li>
<li><strong><code>sortByKey</code> transformation</strong>
<ul>
<li>this returns a new RDD of key-value pairs that's sorted by keys in ascending order</li>
</ul>
</li>
<li><strong><code>groupByKey</code> transformation</strong>
<ul>
<li>this returns a new RDD consisting of key and iterable-valued pairs.</li>
</ul>
</li>
</ul>
<p>Let's see some concrete examples next.</p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-44.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Cntrl+Enter to reduceByKey and collect wordcounts RDD
//val wordcounts = wordCountPairRDD.reduceByKey( _ + _ )
val wordcounts = wordCountPairRDD.reduceByKey( (value1, value2) =&gt; value1 + value2 )
wordcounts.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>wordcounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[16] at reduceByKey at command-4088905069026268:3
res18: Array[(String, Int)] = Array((b,5), (a,6))
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Now, let us do just the crucial steps and avoid collecting intermediate RDDs (something we should avoid for large datasets anyways, as they may not fit in the driver program).</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//Cntrl+Enter to make words RDD and do the word count in two lines
val words = sc.parallelize(Array(&quot;a&quot;, &quot;b&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;))
val wordcounts = words
                    .map(s =&gt; (s, 1))
                    .reduceByKey(_ + _)
                    .collect() 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>words: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[17] at parallelize at command-4088905069026270:2
wordcounts: Array[(String, Int)] = Array((b,5), (a,6))
</code></pre>
</div>
</div>
<div class="cell markdown">
<h5 id="you-try-2"><a class="header" href="#you-try-2">You Try!</a></h5>
<p>You try evaluating <code>sortByKey()</code> which will make a new RDD that consists of the elements of the original pair RDD that are sorted by Keys.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Shift+Enter and comprehend code
val words = sc.parallelize(Array(&quot;a&quot;, &quot;b&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;))
val wordCountPairRDD = words.map(s =&gt; (s, 1))
val wordCountPairRDDSortedByKey = wordCountPairRDD.sortByKey()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>words: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[20] at parallelize at command-4088905069026272:2
wordCountPairRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[21] at map at command-4088905069026272:3
wordCountPairRDDSortedByKey: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[24] at sortByKey at command-4088905069026272:4
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">wordCountPairRDD.collect() // Shift+Enter and comprehend code
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res19: Array[(String, Int)] = Array((a,1), (b,1), (a,1), (a,1), (b,1), (b,1), (a,1), (a,1), (a,1), (b,1), (b,1))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">wordCountPairRDDSortedByKey.collect() // Cntrl+Enter and comprehend code
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res20: Array[(String, Int)] = Array((a,1), (a,1), (a,1), (a,1), (a,1), (a,1), (b,1), (b,1), (b,1), (b,1), (b,1))
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>The next key value transformation we will see is <code>groupByKey</code></p>
<p>When we apply the <code>groupByKey</code> transformation to <code>wordCountPairRDD</code> we end up with a new RDD that contains two elements. The first element is the tuple <code>b</code> and an iterable <code>CompactBuffer(1,1,1,1,1)</code> obtained by grouping the value <code>1</code> for each of the five key value pairs <code>(b,1)</code>. Similarly the second element is the key <code>a</code> and an iterable <code>CompactBuffer(1,1,1,1,1,1)</code> obtained by grouping the value <code>1</code> for each of the six key value pairs <code>(a,1)</code>.</p>
<p><em>CAUTION</em>: <code>groupByKey</code> can cause a large amount of data movement across the network. It also can create very large iterables at a worker. Imagine you have an RDD where you have 1 billion pairs that have the key <code>a</code>. All of the values will have to fit in a single worker if you use group by key. So instead of a group by key, consider using reduced by key.</p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-45.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val wordCountPairRDDGroupByKey = wordCountPairRDD.groupByKey() // &lt;Shift+Enter&gt; CAUTION: this transformation can be very wide!
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>wordCountPairRDDGroupByKey: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[25] at groupByKey at command-4088905069026277:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">wordCountPairRDDGroupByKey.collect()  // Cntrl+Enter
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res21: Array[(String, Iterable[Int])] = Array((b,CompactBuffer(1, 1, 1, 1, 1)), (a,CompactBuffer(1, 1, 1, 1, 1, 1)))
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="10-understanding-closures---where-in-the-cluster-is-your-computation-running"><a class="header" href="#10-understanding-closures---where-in-the-cluster-is-your-computation-running">10. Understanding Closures - Where in the cluster is your computation running?</a></h4>
<blockquote>
<p>One of the harder things about Spark is understanding the scope and life cycle of variables and methods when executing code across a cluster. RDD operations that modify variables outside of their scope can be a frequent source of confusion. In the example below we’ll look at code that uses <code>foreach()</code> to increment a counter, but similar issues can occur for other operations as well.</p>
</blockquote>
<p><a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#understanding-closures-">https://spark.apache.org/docs/latest/rdd-programming-guide.html#understanding-closures-</a></p>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://spark.apache.org/docs/latest/rdd-programming-guide.html#understanding-closures-&quot;,500))
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://spark.apache.org/docs/latest/rdd-programming-guide.html#understanding-closures-"
 width="95%" height="500"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val data = Array(1, 2, 3, 4, 5)
var counter = 0
var rdd = sc.parallelize(data)

// Wrong: Don't do this!!
rdd.foreach(x =&gt; counter += x)

println(&quot;Counter value: &quot; + counter)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Counter value: 0
data: Array[Int] = Array(1, 2, 3, 4, 5)
counter: Int = 0
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[26] at parallelize at command-4088905069026281:3
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>From RDD programming guide:</p>
<blockquote>
<p>The behavior of the above code is undefined, and may not work as intended. To execute jobs, Spark breaks up the processing of RDD operations into tasks, each of which is executed by an executor. Prior to execution, Spark computes the task’s closure. The closure is those variables and methods which must be visible for the executor to perform its computations on the RDD (in this case foreach()). This closure is serialized and sent to each executor.</p>
</blockquote>
<blockquote>
<p>The variables within the closure sent to each executor are now copies and thus, when counter is referenced within the foreach function, it’s no longer the counter on the driver node. There is still a counter in the memory of the driver node but this is no longer visible to the executors! The executors only see the copy from the serialized closure. Thus, the final value of counter will still be zero since all operations on counter were referencing the value within the serialized closure.</p>
</blockquote>
</div>
<div class="cell markdown">
<h4 id="11-shipping-closures-broadcast-variables-and-accumulator-variables"><a class="header" href="#11-shipping-closures-broadcast-variables-and-accumulator-variables">11. Shipping Closures, Broadcast Variables and Accumulator Variables</a></h4>
<h5 id="closures-broadcast-and-accumulator-variables"><a class="header" href="#closures-broadcast-and-accumulator-variables">Closures, Broadcast and Accumulator Variables</a></h5>
<p>Spark automatically creates closures</p>
<ul>
<li>for functions that run on RDDs at workers,</li>
<li>and for any global variables that are used by those workers</li>
<li>one closure per worker is sent with every task</li>
<li>and there's no communication between workers</li>
<li>closures are one way from the driver to the worker</li>
<li>any changes that you make to the global variables at the workers
<ul>
<li>are not sent to the driver or</li>
<li>are not sent to other workers.</li>
</ul>
</li>
</ul>
<p>The problem we have is that these closures</p>
<ul>
<li>are automatically created are sent or re-sent with every job</li>
<li>with a large global variable it gets inefficient to send/resend lots of data to each worker</li>
<li>we cannot communicate that back to the driver</li>
</ul>
<p>To do this, Spark provides shared variables in two different types.</p>
<ul>
<li><strong>broadcast variables</strong>
<ul>
<li>lets us to efficiently send large read-only values to all of the workers</li>
<li>these are saved at the workers for use in one or more Spark operations.</li>
</ul>
</li>
<li><strong>accumulator variables</strong>
<ul>
<li>These allow us to aggregate values from workers back to the driver.</li>
<li>only the driver can access the value of the accumulator</li>
<li>for the tasks, the accumulators are basically write-only</li>
</ul>
</li>
</ul>
<hr />
</div>
<div class="cell markdown">
<h5 id="accumulators"><a class="header" href="#accumulators">Accumulators</a></h5>
<blockquote>
<p>Accumulators are variables that are only “added” to through an associative and commutative operation and can therefore be efficiently supported in parallel. They can be used to implement counters (as in MapReduce) or sums. Spark natively supports accumulators of numeric types, and programmers can add support for new types.</p>
</blockquote>
<p>Read: <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#accumulators">https://spark.apache.org/docs/latest/rdd-programming-guide.html#accumulators</a>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://spark.apache.org/docs/latest/rdd-programming-guide.html#accumulators&quot;,500))
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://spark.apache.org/docs/latest/rdd-programming-guide.html#accumulators"
 width="95%" height="500"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<blockquote>
<p>A numeric accumulator can be created by calling SparkContext.longAccumulator() or SparkContext.doubleAccumulator() to accumulate values of type Long or Double, respectively. Tasks running on a cluster can then add to it using the add method. However, they cannot read its value. Only the driver program can read the accumulator’s value, using its value method.</p>
</blockquote>
<blockquote>
<p>The code below shows an accumulator being used to add up the elements of an array:</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val accum = sc.longAccumulator(&quot;My Accumulator&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>accum: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 1236, name: Some(My Accumulator), value: 0)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sc.parallelize(Array(1, 2, 3, 4)).foreach(x =&gt; accum.add(x))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">accum.value
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res26: Long = 10
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">spark.range(1, 100000).foreach(x =&gt; accum.add(x)) // bigger example
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">accum.value
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res28: Long = 4999950010
</code></pre>
</div>
</div>
<div class="cell markdown">
<h5 id="broadcast-variables"><a class="header" href="#broadcast-variables">Broadcast Variables</a></h5>
<p>From <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#broadcast-variables">https://spark.apache.org/docs/latest/rdd-programming-guide.html#broadcast-variables</a>:</p>
<blockquote>
<p>Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. They can be used, for example, to give every node a copy of a large input dataset in an efficient manner. Spark also attempts to distribute broadcast variables using efficient broadcast algorithms to reduce communication cost.</p>
</blockquote>
<blockquote>
<p>Spark actions are executed through a set of stages, separated by distributed “shuffle” operations. Spark automatically broadcasts the common data needed by tasks within each stage. The data broadcasted this way is cached in serialized form and deserialized before running each task. This means that explicitly creating broadcast variables is only useful when tasks across multiple stages need the same data or when caching the data in deserialized form is important.</p>
</blockquote>
<blockquote>
<p>Broadcast variables are created from a variable v by calling SparkContext.broadcast(v). The broadcast variable is a wrapper around v, and its value can be accessed by calling the value method. The code below shows this in action.</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://spark.apache.org/docs/latest/rdd-programming-guide.html#broadcast-variables&quot;,500))
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://spark.apache.org/docs/latest/rdd-programming-guide.html#broadcast-variables"
 width="95%" height="500"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val broadcastVar = sc.broadcast(Array(1, 2, 3))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>broadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(30)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">broadcastVar.value
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res30: Array[Int] = Array(1, 2, 3)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">broadcastVar.value(0)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res31: Int = 1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rdd = sc.parallelize(1 to 10)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[34] at parallelize at command-4088905069026297:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd.collect
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res32: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd.map(x =&gt; x%3).collect
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res33: Array[Int] = Array(1, 2, 0, 1, 2, 0, 1, 2, 0, 1)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd.map(x =&gt; x+broadcastVar.value(x%3)).collect
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res34: Array[Int] = Array(3, 5, 4, 6, 8, 7, 9, 11, 10, 12)
</code></pre>
</div>
</div>
<div class="cell markdown">
<blockquote>
<p>After the broadcast variable is created, it should be used instead of the value v in any functions run on the cluster so that v is not shipped to the nodes more than once. In addition, the object v should not be modified after it is broadcast in order to ensure that all nodes get the same value of the broadcast variable (e.g. if the variable is shipped to a new node later).</p>
</blockquote>
<blockquote>
<p>To release the resources that the broadcast variable copied onto executors, call .unpersist(). If the broadcast is used again afterwards, it will be re-broadcast. To permanently release all resources used by the broadcast variable, call .destroy(). The broadcast variable can’t be used after that. Note that these methods do not block by default. To block until resources are freed, specify blocking=true when calling them.</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">broadcastVar.unpersist()
</code></pre>
</div>
<div class="cell markdown">
<h5 id="a-more-interesting-example-of-broadcast-variable"><a class="header" href="#a-more-interesting-example-of-broadcast-variable">A more interesting example of broadcast variable</a></h5>
<p>Let us broadcast maps and use them to lookup the values at each executor. This example is taken from: - <a href="https://sparkbyexamples.com/spark/spark-broadcast-variables/">https://sparkbyexamples.com/spark/spark-broadcast-variables/</a></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val states = Map((&quot;NY&quot;,&quot;New York&quot;),(&quot;CA&quot;,&quot;California&quot;),(&quot;FL&quot;,&quot;Florida&quot;))
val countries = Map((&quot;USA&quot;,&quot;United States of America&quot;),(&quot;IN&quot;,&quot;India&quot;))

val broadcastStates = spark.sparkContext.broadcast(states) // same as sc.broadcast
val broadcastCountries = spark.sparkContext.broadcast(countries)

val data = Seq((&quot;James&quot;,&quot;Smith&quot;,&quot;USA&quot;,&quot;CA&quot;),
    (&quot;Michael&quot;,&quot;Rose&quot;,&quot;USA&quot;,&quot;NY&quot;),
    (&quot;Robert&quot;,&quot;Williams&quot;,&quot;USA&quot;,&quot;CA&quot;),
    (&quot;Maria&quot;,&quot;Jones&quot;,&quot;USA&quot;,&quot;FL&quot;))

val rdd = spark.sparkContext.parallelize(data) // spark.sparkContext is the same as sc.parallelize in spark-shell/notebook

  val rdd2 = rdd.map(f =&gt; {
    val country = f._3
    val state = f._4
    val fullCountry = broadcastCountries.value.get(country).get
    val fullState = broadcastStates.value.get(state).get
    (f._1,f._2,fullCountry,fullState)
  })
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>states: scala.collection.immutable.Map[String,String] = Map(NY -&gt; New York, CA -&gt; California, FL -&gt; Florida)
countries: scala.collection.immutable.Map[String,String] = Map(USA -&gt; United States of America, IN -&gt; India)
broadcastStates: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Map[String,String]] = Broadcast(34)
broadcastCountries: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Map[String,String]] = Broadcast(35)
data: Seq[(String, String, String, String)] = List((James,Smith,USA,CA), (Michael,Rose,USA,NY), (Robert,Williams,USA,CA), (Maria,Jones,USA,FL))
rdd: org.apache.spark.rdd.RDD[(String, String, String, String)] = ParallelCollectionRDD[37] at parallelize at command-4088905069026304:12
rdd2: org.apache.spark.rdd.RDD[(String, String, String, String)] = MapPartitionsRDD[38] at map at command-4088905069026304:14
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(rdd2.collect().mkString(&quot;\n&quot;))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>(James,Smith,United States of America,California)
(Michael,Rose,United States of America,New York)
(Robert,Williams,United States of America,California)
(Maria,Jones,United States of America,Florida)
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="13-homework"><a class="header" href="#13-homework">13. HOMEWORK</a></h4>
<p>See the notebook in this folder named <code>005_RDDsTransformationsActionsHOMEWORK</code>. This notebook will give you more examples of the operations above as well as others we will be using later, including:</p>
<ul>
<li>Perform the <code>takeOrdered</code> action on the RDD</li>
<li>Transform the RDD by <code>distinct</code> to make another RDD and</li>
<li>Doing a bunch of transformations to our RDD and performing an action in a single cell.</li>
</ul>
</div>
<div class="cell markdown">
<hr />
<hr />
<h4 id="14-importing-standard-scala-and-java-libraries"><a class="header" href="#14-importing-standard-scala-and-java-libraries">14. Importing Standard Scala and Java libraries</a></h4>
<ul>
<li>For other libraries that are not available by default, you can upload other libraries to the Workspace.</li>
<li>Refer to the <strong><a href="https://docs.databricks.com/user-guide/libraries.html">Libraries</a></strong> guide for more details.</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import scala.math._
val x = min(1, 10)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import scala.math._
x: Int = 1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import java.util.HashMap
val map = new HashMap[String, Int]()
map.put(&quot;a&quot;, 1)
map.put(&quot;b&quot;, 2)
map.put(&quot;c&quot;, 3)
map.put(&quot;d&quot;, 4)
map.put(&quot;e&quot;, 5)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import java.util.HashMap
map: java.util.HashMap[String,Int] = {a=1, b=2, c=3, d=4, e=5}
res37: Int = 0
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>To read at ease more methodically go through <strong>Chapter 12. Resilient Distributed Datasets (RDDs)</strong> of <em>Spark: The Definitive Guide</em>:</p>
<ul>
<li>https://learning.oreilly.com/library/view/spark-the-definitive/9781491912201/ch12.html</li>
</ul>
<p>Note, you may need access via your library.</p>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="homework-on-rdd-transformations-and-actions"><a class="header" href="#homework-on-rdd-transformations-and-actions">HOMEWORK on RDD Transformations and Actions</a></h1>
<p>Just go through the notebook and familiarize yourself with these transformations and actions. This will help you do the autograded Assignment.</p>
</div>
<div class="cell markdown">
<ol>
<li>Perform the <code>takeOrdered</code> action on the RDD</li>
</ol>
<hr />
<p>To illustrate <code>take</code> and <code>takeOrdered</code> actions, let's create a bigger RDD named <code>rdd0_1000000</code> that is made up of a million integers from 0 to 1000000.
We will <code>sc.parallelize</code> the <code>Seq</code> Scala collection by using its <code>.range(startInteger,stopInteger)</code> method.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rdd0_1000000 = sc.parallelize(Seq.range(0, 1000000)) // &lt;Shift+Enter&gt; to create an RDD of million integers: 0,1,2,...,10^6
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rdd0_1000000: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[39] at parallelize at command-4088905069025406:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd0_1000000.take(5) // &lt;Ctrl+Enter&gt; gives the first 5 elements of the RDD, (0, 1, 2, 3, 4)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res0: Array[Int] = Array(0, 1, 2, 3, 4)
</code></pre>
</div>
</div>
<div class="cell markdown">
<p><code>takeordered(n)</code> returns <code>n</code> elements ordered in ascending order (by default) or as specified by the optional key function, as shown below.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd0_1000000.takeOrdered(5) // &lt;Shift+Enter&gt; is same as rdd0_1000000.take(5) 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res1: Array[Int] = Array(0, 1, 2, 3, 4)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd0_1000000.takeOrdered(5)(Ordering[Int].reverse) // &lt;Ctrl+Enter&gt; to get the last 5 elements of the RDD 999999, 999998, ..., 999995
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res2: Array[Int] = Array(999999, 999998, 999997, 999996, 999995)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// HOMEWORK: edit the numbers below to get the last 20 elements of an RDD made of a sequence of integers from 669966 to 969696
sc.parallelize(Seq.range(0, 10)).takeOrdered(5)(Ordering[Int].reverse) // &lt;Ctrl+Enter&gt; evaluate this cell after editing it for the right answer
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res3: Array[Int] = Array(9, 8, 7, 6, 5)
</code></pre>
</div>
</div>
<div class="cell markdown">
<ol start="2">
<li>More examples of <code>map</code></li>
</ol>
<hr />
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rdd = sc.parallelize(Seq(1, 2, 3, 4))    // &lt;Shift+Enter&gt; to evaluate this cell (using default number of partitions)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[41] at parallelize at command-4088905069025413:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd.map( x =&gt; x*2) // &lt;Ctrl+Enter&gt; to transform rdd by map that doubles each element
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res4: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[42] at map at command-4088905069025414:1
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>To see what's in the transformed RDD, let's perform the actions of <code>count</code> and <code>collect</code> on the <code>rdd.map( x =&gt; x*2)</code>, the transformation of <code>rdd</code> by the <code>map</code> given by the closure <code>x =&gt; x*2</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd.map( x =&gt; x*2).count()    // &lt;Shift+Enter&gt; to perform count (action) the element of the RDD = 4
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res5: Long = 4
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd.map( x =&gt; x*2).collect()    // &lt;Shift+Enter&gt; to perform collect (action) to show 2, 4, 6, 8
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res6: Array[Int] = Array(2, 4, 6, 8)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// HOMEWORK: uncomment the last line in this cell and modify the '&lt;Fill-In-Here&gt;' in the code below to collect and display the square (x*x) of each element of the RDD
// the answer should be Array[Int] = Array(1, 4, 9, 16) Press &lt;Cntrl+Enter&gt; to evaluate the cell after modifying '???'

//sc.parallelize(Seq(1, 2, 3, 4)).map( x =&gt; &lt;Fill-In-Here&gt; ).collect()
</code></pre>
</div>
<div class="cell markdown">
<ol start="3">
<li>More examples of <code>filter</code></li>
</ol>
<hr />
<p>Let's declare another <code>val</code> RDD named <code>rddFiltered</code> by transforming our first RDD named <code>rdd</code> via the <code>filter</code> transformation <code>x%2==0</code> (of being even).</p>
<p>This filter transformation based on the closure <code>x =&gt; x%2==0</code> will return <code>true</code> if the element, modulo two, equals zero. The closure is automatically passed on to the workers for evaluation (when an action is called later). So this will take our RDD of (1,2,3,4) and return RDD of (2, 4).</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rddFiltered = rdd.filter( x =&gt; x%2==0 )    // &lt;Ctrl+Enter&gt; to declare rddFiltered from transforming rdd
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rddFiltered: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[45] at filter at command-4088905069025420:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rddFiltered.collect()    // &lt;Ctrl+Enter&gt; to collect (action) elements of rddFiltered; should be (2, 4)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res8: Array[Int] = Array(2, 4)
</code></pre>
</div>
</div>
<div class="cell markdown">
<ol start="4">
<li>More examples of <code>reduce</code></li>
</ol>
<hr />
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rdd = sc.parallelize(Array(1,2,3,4,5))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[46] at parallelize at command-4088905069025423:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd.reduce( (x,y)=&gt;x+y ) // &lt;Shift+Enter&gt; to do reduce (action) to sum and return Int = 15
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res9: Int = 15
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd.reduce( _ + _ )    // &lt;Shift+Enter&gt; to do same sum as above and return Int = 15 (undescore syntax)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res10: Int = 15
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd.reduce( (x,y)=&gt;x*y ) // &lt;Shift+Enter&gt; to do reduce (action) to multiply and return Int = 120
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res11: Int = 120
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rdd0_1000000 = sc.parallelize(Seq.range(0, 1000000)) // &lt;Shift+Enter&gt; to create an RDD of million integers: 0,1,2,...,10^6
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rdd0_1000000: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[47] at parallelize at command-4088905069025427:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd0_1000000.reduce( (x,y)=&gt;x+y ) // &lt;Ctrl+Enter&gt; to do reduce (action) to sum and return Int 1783293664
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res12: Int = 1783293664
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// the following correctly returns Int = 0 although for wrong reason 
// we have flowed out of Int's numeric limits!!! (but got lucky with 0*x=0 for any Int x)
// &lt;Shift+Enter&gt; to do reduce (action) to multiply and return Int = 0
rdd0_1000000.reduce( (x,y)=&gt;x*y ) 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res13: Int = 0
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Ctrl+Enter&gt; to do reduce (action) to multiply 1*2*...*9*10 and return correct answer Int = 3628800
sc.parallelize(Seq.range(1, 11)).reduce( (x,y)=&gt;x*y ) 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res14: Int = 3628800
</code></pre>
</div>
</div>
<div class="cell markdown">
<p><strong>CAUTION: Know the limits of your numeric types!</strong></p>
</div>
<div class="cell markdown">
<p>The minimum and maximum value of <code>Int</code> and <code>Long</code> types are as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">(Int.MinValue , Int.MaxValue)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res15: (Int, Int) = (-2147483648,2147483647)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">(Long.MinValue, Long.MaxValue)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res16: (Long, Long) = (-9223372036854775808,9223372036854775807)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Ctrl+Enter&gt; to do reduce (action) to multiply 1*2*...*20 and return wrong answer as Int = -2102132736
//  we have overflowed out of Int's in a circle back to negative Ints!!! (rigorous distributed numerics, anyone?)
sc.parallelize(Seq.range(1, 21)).reduce( (x,y)=&gt;x*y ) 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res17: Int = -2102132736
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//&lt;Ctrl+Enter&gt; we can accomplish the multiplication using Long Integer types 
// by adding 'L' ro integer values, Scala infers that it is type Long
sc.parallelize(Seq.range(1L, 21L)).reduce( (x,y)=&gt;x*y ) 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res18: Long = 2432902008176640000
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>As the following products over Long Integers indicate, they are limited too!</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala"> // &lt;Shift+Enter&gt; for wrong answer Long = -8718968878589280256 (due to Long's numeric limits)
sc.parallelize(Seq.range(1L, 61L)).reduce( (x,y)=&gt;x*y )
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res19: Long = -8718968878589280256
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Cntrl+Enter&gt; for wrong answer Long = 0 (due to Long's numeric limits)
sc.parallelize(Seq.range(1L, 100L)).reduce( (x,y)=&gt;x*y ) 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res20: Long = 0
</code></pre>
</div>
</div>
<div class="cell markdown">
<hr />
<ol start="5">
<li>Let us do a bunch of transformations to our RDD and perform an action</li>
</ol>
<hr />
<ul>
<li>start from a Scala <code>Seq</code>,</li>
<li><code>sc.parallelize</code> the list to create an RDD,</li>
<li><code>filter</code> that RDD, creating a new filtered RDD,</li>
<li>do a <code>map</code> transformation that maps that RDD to a new mapped RDD,</li>
<li>and finally, perform a <code>reduce</code> action to sum the elements in the RDD.</li>
</ul>
<p>This last <code>reduce</code> action causes the <code>parallelize</code>, the <code>filter</code>, and the <code>map</code> transformations to actually be executed, and return a result back to the driver machine.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sc.parallelize(Seq(1, 2, 3, 4))    // &lt;Ctrl+Enter&gt; will return Array(4, 8)
  .filter(x =&gt; x%2==0)             // (2, 4) is the filtered RDD
  .map(x =&gt; x*2)                   // (4, 8) is the mapped RDD
  .reduce(_+_)                     // 4+8=12 is the final result from reduce
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res21: Int = 12
</code></pre>
</div>
</div>
<div class="cell markdown">
<ol start="6">
<li>Transform the RDD by <code>distinct</code> to make another RDD</li>
</ol>
<hr />
<p>Let's declare another RDD named <code>rdd2</code> that has some repeated elements to apply the <code>distinct</code> transformation to it. That would give us a new RDD that only contains the distinct elements of the input RDD.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rdd2 = sc.parallelize(Seq(4, 1, 3, 2, 2, 2, 3, 4))    // &lt;Ctrl+Enter&gt; to declare rdd2
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[56] at parallelize at command-4088905069025443:1
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let's apply the <code>distinct</code> transformation to <code>rdd2</code> and have it return a new RDD named <code>rdd2Distinct</code> that contains the distinct elements of the source RDD <code>rdd2</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rdd2Distinct = rdd2.distinct() // &lt;Ctrl+Enter&gt; transformation: distinct gives distinct elements of rdd2
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rdd2Distinct: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[59] at distinct at command-4088905069025445:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd2Distinct.collect()    // &lt;Ctrl+Enter&gt; to collect (action) as Array(4, 2, 1, 3)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res22: Array[Int] = Array(4, 2, 1, 3)
</code></pre>
</div>
</div>
<div class="cell markdown">
<ol start="7">
<li>more flatMap</li>
</ol>
<hr />
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rdd = sc. parallelize(Array(1,2,3)) // &lt;Shift+Enter&gt; to create an RDD of three Int elements 1,2,3
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[60] at parallelize at command-4088905069025448:1
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let us pass the <code>rdd</code> above to a map with a closure that will take in each element <code>x</code> and return <code>Array(x, x+5)</code>. So each element of the mapped RDD named <code>rddOfArrays</code> is an <code>Array[Int]</code>, an array of integers.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Shift+Enter&gt; to make RDD of Arrays, i.e., RDD[Array[int]]
val rddOfArrays = rdd.map( x =&gt; Array(x, x+5) ) 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rddOfArrays: org.apache.spark.rdd.RDD[Array[Int]] = MapPartitionsRDD[61] at map at command-4088905069025450:2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rddOfArrays.collect() // &lt;Ctrl+Enter&gt; to see it is RDD[Array[int]] = (Array(1, 6), Array(2, 7), Array(3, 8))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res23: Array[Array[Int]] = Array(Array(1, 6), Array(2, 7), Array(3, 8))
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Now let's observer what happens when we use <code>flatMap</code> to transform the same <code>rdd</code> and create another RDD called <code>rddfM</code>.</p>
<p>Interestingly, <code>flatMap</code> <em>flattens</em> our <code>rdd</code> by taking each <code>Array</code> (or sequence in general) and truning it into individual elements.</p>
<p>Thus, we end up with the RDD <code>rddfM</code> consisting of the elements (1, 6, 2, 7, 3, 8) as shown from the output of <code>rddfM.collect</code> below.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rddfM = rdd.flatMap(x =&gt; Array(x, x+5))    // &lt;Shift+Enter&gt; to flatMap the rdd using closure (x =&gt; Array(x, x+5))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rddfM: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[62] at flatMap at command-4088905069025453:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rddfM.collect    // &lt;Ctrl+Enter&gt; to collect rddfM = (1, 6, 2, 7, 3, 8)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res24: Array[Int] = Array(1, 6, 2, 7, 3, 8)
</code></pre>
</div>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="word-count-on-us-state-of-the-union-sou-addresses"><a class="header" href="#word-count-on-us-state-of-the-union-sou-addresses">Word Count on US State of the Union (SoU) Addresses</a></h1>
<ul>
<li>Word Count in big data is the equivalent of <code>Hello World</code> in programming</li>
<li>We count the number of occurences of each word in the first and last (2016) SoU addresses.</li>
</ul>
<p>You should have loaded data by now, i.e., run all cells in the Notebook <code>002_02_dbcCEdataLoader</code>.</p>
</div>
<div class="cell markdown">
<p>An interesting analysis of the textual content of the <em>State of the Union (SoU)</em> addresses by all US presidents was done in:</p>
<ul>
<li><a href="http://www.pnas.org/content/112/35/10837.full">Alix Rule, Jean-Philippe Cointet, and Peter S. Bearman, Lexical shifts, substantive changes, and continuity in State of the Union discourse, 1790–2014, PNAS 2015 112 (35) 10837-10844; doi:10.1073/pnas.1512221112</a>.</li>
</ul>
<p><img src="https://www.pnas.org/cms/10.1073/pnas.1512221112/asset/52ec0970-5ebe-4119-a1d1-f6b4e83be604/assets/graphic/pnas.1512221112fig05.jpeg" alt="" /></p>
<p><a href="http://www.pnas.org/content/112/35/10837.full">Fig. 5</a>. A river network captures the flow across history of US political discourse, as perceived by contemporaries. Time moves along the x axis. Clusters on semantic networks of 300 most frequent terms for each of 10 historical periods are displayed as vertical bars. Relations between clusters of adjacent periods are indexed by gray flows, whose density reflects their degree of connection. Streams that connect at any point in history may be considered to be part of the same system, indicated with a single color.</p>
<h2 id="let-us-investigate-this-dataset-ourselves"><a class="header" href="#let-us-investigate-this-dataset-ourselves">Let us investigate this dataset ourselves!</a></h2>
<ol>
<li>We first get the source text data by scraping and parsing from <a href="http://stateoftheunion.onetwothree.net/texts/index.html">http://stateoftheunion.onetwothree.net/texts/index.html</a> as explained in <a href="contents/000_1-sds-3-x-spark//#workspace/scalable-data-science/xtraResources/sdsDatasets/scraperUSStateofUnionAddresses">scraping and parsing SoU addresses</a>.</li>
</ol>
<ul>
<li>All of the SOU addresses from the founding of the United States until 2017 has been scraped into a single file.</li>
<li>This data is already made available in our distributed file system.</li>
<li>We only do the simplest word count with this data in this notebook and will do more sophisticated analyses in the sequel (including topic modeling, etc).</li>
</ul>
</div>
<div class="cell markdown">
<h2 id="key-data-management-concepts"><a class="header" href="#key-data-management-concepts">Key Data Management Concepts</a></h2>
<h3 id="the-structure-spectrum"><a class="header" href="#the-structure-spectrum">The Structure Spectrum</a></h3>
<p>Let's peruse through the following blog from IBM to get an idea of structured, unstructured and semi-structured data:</p>
<ul>
<li><a href="https://www.ibm.com/cloud/blog/structured-vs-unstructured-data">https://www.ibm.com/cloud/blog/structured-vs-unstructured-data</a></li>
</ul>
<p>In this notebook we will be working with <strong>unstructured</strong> or <strong>schema-never</strong> data (plain text files).</p>
<p>Later on we will be working with structured or tabular data as well as semi-structured data.</p>
<hr />
</div>
<div class="cell markdown">
<h3 id="dbfs-and-dbutils---where-is-this-dataset-in-our-distributed-file-system-inside-databricks"><a class="header" href="#dbfs-and-dbutils---where-is-this-dataset-in-our-distributed-file-system-inside-databricks">DBFS and dbutils - where is this dataset in our distributed file system inside databricks?</a></h3>
<ul>
<li>Since we are on the databricks cloud, it has a file system called DBFS</li>
<li>DBFS is similar to HDFS, the Hadoop distributed file system</li>
<li>dbutils allows us to interact with dbfs.</li>
<li>The 'display' command displays the list of files in a given directory in the file system.</li>
</ul>
<p><strong>Note:</strong> If you are on zeppelin or a non-databricks environment then use hdfs or drop into shell via <code>%sh</code> to <code>ls</code> the files in the local file system.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-fs">ls /datasets/sds
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>path</th>
<th>name</th>
<th>size</th>
<th>modificationTime</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dbfs:/datasets/sds/Rdatasets/</td>
<td>Rdatasets/</td>
<td>0.0</td>
<td>1.66429635041e12</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sds/cs100/</td>
<td>cs100/</td>
<td>0.0</td>
<td>1.66429635041e12</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sds/flights/</td>
<td>flights/</td>
<td>0.0</td>
<td>1.66429635041e12</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sds/mnist-digits/</td>
<td>mnist-digits/</td>
<td>0.0</td>
<td>1.66429635041e12</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sds/people/</td>
<td>people/</td>
<td>0.0</td>
<td>1.66429635041e12</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sds/people.json/</td>
<td>people.json/</td>
<td>0.0</td>
<td>1.66429635041e12</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sds/power-plant/</td>
<td>power-plant/</td>
<td>0.0</td>
<td>1.66429635041e12</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sds/social_media_usage.csv/</td>
<td>social_media_usage.csv/</td>
<td>0.0</td>
<td>1.66429635041e12</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sds/songs/</td>
<td>songs/</td>
<td>0.0</td>
<td>1.66429635041e12</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sds/souall.txt.gz</td>
<td>souall.txt.gz</td>
<td>3576868.0</td>
<td>1.664296002e12</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sds/spark-examples/</td>
<td>spark-examples/</td>
<td>0.0</td>
<td>1.66429635041e12</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sds/weather/</td>
<td>weather/</td>
<td>0.0</td>
<td>1.66429635041e12</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sds/wikipedia-datasets/</td>
<td>wikipedia-datasets/</td>
<td>0.0</td>
<td>1.66429635041e12</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell markdown">
<h3 id="read-the-file-into-spark-context-as-an-rdd-of-strings"><a class="header" href="#read-the-file-into-spark-context-as-an-rdd-of-strings">Read the file into Spark Context as an RDD of Strings</a></h3>
<ul>
<li>The <code>textFile</code> method on the available <code>SparkContext</code> <code>sc</code> can read the gnu-zip compressed text file <code>/datasets/sds/souall.txt.gz</code> into Spark and create an RDD of Strings
<ul>
<li>but this is done lazily until an action is taken on the RDD <code>sou</code>!</li>
</ul>
</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val sou = sc.textFile(&quot;/datasets/sds/souall.txt.gz&quot;) // Cntrl+Enter to read the gzipped textfile as RDD[String]
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>sou: org.apache.spark.rdd.RDD[String] = /datasets/sds/souall.txt.gz MapPartitionsRDD[3] at textFile at command-2942007275186754:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou.take(5) // Cntrl+Enter to read the first 5 lines of the gzipped textfile as RDD[String]
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res2: Array[String] = Array(&quot;George Washington &quot;, &quot;&quot;, &quot;January 8, 1790 &quot;, &quot;Fellow-Citizens of the Senate and House of Representatives: &quot;, &quot;I embrace with great satisfaction the opportunity which now presents itself of congratulating you on the present favorable prospects of our public affairs. The recent accession of the important state of North Carolina to the Constitution of the United States (of which official information has been received), the rising credit and respectability of our country, the general and increasing good will toward the government of the Union, and the concord, peace, and plenty with which we are blessed are circumstances auspicious in an eminent degree to our national prosperity. &quot;)
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="perform-some-actions-on-the-rdd"><a class="header" href="#perform-some-actions-on-the-rdd">Perform some actions on the RDD</a></h3>
<ul>
<li>Each String in the RDD <code>sou</code> represents one line of data from the file and can be made to perform one of the following actions:
<ul>
<li>count the number of elements in the RDD <code>sou</code> (i.e., the number of lines in the text file <code>/datasets/sds/souall.txt.gz</code>) using <code>sou.count</code></li>
<li>display the contents of the RDD using <code>take</code></li>
<li>do not use <code>collect</code> as it will bring all the lines to the Driver, which could crash the Driver if the RDD is very large.</li>
</ul>
</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou.count // &lt;Shift+Enter&gt; to get the number of elements or number of lines ending in line breaks '\n' in sou
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res3: Long = 22258
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou.take(30) // &lt;Shift+Enter&gt; to display the first 30 elements of RDD, including the beginning of the second SOU by George Washington
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res4: Array[String] = Array(&quot;George Washington &quot;, &quot;&quot;, &quot;January 8, 1790 &quot;, &quot;Fellow-Citizens of the Senate and House of Representatives: &quot;, &quot;I embrace with great satisfaction the opportunity which now presents itself of congratulating you on the present favorable prospects of our public affairs. The recent accession of the important state of North Carolina to the Constitution of the United States (of which official information has been received), the rising credit and respectability of our country, the general and increasing good will toward the government of the Union, and the concord, peace, and plenty with which we are blessed are circumstances auspicious in an eminent degree to our national prosperity. &quot;, &quot;In resuming your consultations for the general good you can not but derive encouragement from the reflection that the measures of the last session have been as satisfactory to your constituents as the novelty and difficulty of the work allowed you to hope. Still further to realize their expectations and to secure the blessings which a gracious Providence has placed within our reach will in the course of the present important session call for the cool and deliberate exertion of your patriotism, firmness, and wisdom. &quot;, &quot;Among the many interesting objects which will engage your attention that of providing for the common defense will merit particular regard. To be prepared for war is one of the most effectual means of preserving peace. &quot;, &quot;A free people ought not only to be armed, but disciplined; to which end a uniform and well-digested plan is requisite; and their safety and interest require that they should promote such manufactories as tend to render them independent of others for essential, particularly military, supplies. &quot;, &quot;The proper establishment of the troops which may be deemed indispensable will be entitled to mature consideration. In the arrangements which may be made respecting it it will be of importance to conciliate the comfortable support of the officers and soldiers with a due regard to economy. &quot;, &quot;There was reason to hope that the pacific measures adopted with regard to certain hostile tribes of Indians would have relieved the inhabitants of our southern and western frontiers from their depredations, but you will perceive from the information contained in the papers which I shall direct to be laid before you (comprehending a communication from the Commonwealth of Virginia) that we ought to be prepared to afford protection to those parts of the Union, and, if necessary, to punish aggressors. &quot;, &quot;The interests of the United States require that our intercourse with other nations should be facilitated by such provisions as will enable me to fulfill my duty in that respect in the manner which circumstances may render most conducive to the public good, and to this end that the compensation to be made to the persons who may be employed should, according to the nature of their appointments, be defined by law, and a competent fund designated for defraying the expenses incident to the conduct of foreign affairs. &quot;, &quot;Various considerations also render it expedient that the terms on which foreigners may be admitted to the rights of citizens should be speedily ascertained by a uniform rule of naturalization. &quot;, &quot;Uniformity in the currency, weights, and measures of the United States is an object of great importance, and will, I am persuaded, be duly attended to. &quot;, &quot;The advancement of agriculture, commerce, and manufactures by all proper means will not, I trust, need recommendation; but I can not forbear intimating to you the expediency of giving effectual encouragement as well to the introduction of new and useful inventions from abroad as to the exertions of skill and genius in producing them at home, and of facilitating the intercourse between the distant parts of our country by a due attention to the post-office and post-roads. &quot;, &quot;Nor am I less persuaded that you will agree with me in opinion that there is nothing which can better deserve your patronage than the promotion of science and literature. Knowledge is in every country the surest basis of public happiness. In one in which the measures of government receive their impressions so immediately from the sense of the community as in ours it is proportionably essential. &quot;, &quot;To the security of a free constitution it contributes in various ways--by convincing those who are intrusted with the public administration that every valuable end of government is best answered by the enlightened confidence of the people, and by teaching the people themselves to know and to value their own rights; to discern and provide against invasions of them; to distinguish between oppression and the necessary exercise of lawful authority; between burthens proceeding from a disregard to their convenience and those resulting from the inevitable exigencies of society; to discriminate the spirit of liberty from that of licentiousness-- cherishing the first, avoiding the last--and uniting a speedy but temperate vigilance against encroachments, with an inviolable respect to the laws. &quot;, &quot;Whether this desirable object will be best promoted by affording aids to seminaries of learning already established, by the institution of a national university, or by any other expedients will be well worthy of a place in the deliberations of the legislature. &quot;, &quot;Gentlemen of the House of Representatives: &quot;, &quot;I saw with peculiar pleasure at the close of the last session the resolution entered into by you expressive of your opinion that an adequate provision for the support of the public credit is a matter of high importance to the national honor and prosperity. In this sentiment I entirely concur; and to a perfect confidence in your best endeavors to devise such a provision as will be truly with the end I add an equal reliance on the cheerful cooperation of the other branch of the legislature. &quot;, &quot;It would be superfluous to specify inducements to a measure in which the character and interests of the United States are so obviously so deeply concerned, and which has received so explicit a sanction from your declaration. &quot;, &quot;Gentlemen of the Senate and House of Representatives: &quot;, &quot;I have directed the proper officers to lay before you, respectively, such papers and estimates as regard the affairs particularly recommended to your consideration, and necessary to convey to you that information of the state of the Union which it is my duty to afford. &quot;, The welfare of our country is the great object to which our cares and efforts ought to be directed, and I shall derive great satisfaction from a cooperation with you in the pleasing though arduous task of insuring to our fellow citizens the blessings which they have a right to expect from a free, efficient, and equal government., &quot;&quot;, &quot;George Washington &quot;, &quot;&quot;, &quot;December 8, 1790 &quot;, &quot;Fellow-Citizens of the Senate and House of Representatives: &quot;, &quot;In meeting you again I feel much satisfaction in being able to repeat my congratulations on the favorable prospects which continue to distinguish our public affairs. The abundant fruits of another year have blessed our country with plenty and with the means of a flourishing commerce. &quot;, &quot;The progress of public credit is witnessed by a considerable rise of American stock abroad as well as at home, and the revenues allotted for this and other national purposes have been productive beyond the calculations by which they were regulated. This latter circumstance is the more pleasing, as it is not only a proof of the fertility of our resources, but as it assures us of a further increase of the national respectability and credit, and, let me add, as it bears an honorable testimony to the patriotism and integrity of the mercantile and marine part of our citizens. The punctuality of the former in discharging their engagements has been exemplary. &quot;)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou.take(5).foreach(println) // &lt;Shift+Enter&gt; to display the first 5 elements of RDD line by line
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>George Washington 

January 8, 1790 
Fellow-Citizens of the Senate and House of Representatives: 
I embrace with great satisfaction the opportunity which now presents itself of congratulating you on the present favorable prospects of our public affairs. The recent accession of the important state of North Carolina to the Constitution of the United States (of which official information has been received), the rising credit and respectability of our country, the general and increasing good will toward the government of the Union, and the concord, peace, and plenty with which we are blessed are circumstances auspicious in an eminent degree to our national prosperity. 
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou.zipWithIndex.count
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res6: Long = 22258
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>SInce we have several SOU addresses we can use <code>zipWithIndex</code> and filter to extract the specific lines in a range of interest.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val souWithIndices = sou.zipWithIndex()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>souWithIndices: org.apache.spark.rdd.RDD[(String, Long)] = ZippedWithIndexRDD[5] at zipWithIndex at command-69839727109967:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">souWithIndices.take(5) // &lt;Cntrl+Enter&gt; to display all the elements of RDD zipped with index starting from 0
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res7: Array[(String, Long)] = Array((&quot;George Washington &quot;,0), (&quot;&quot;,1), (&quot;January 8, 1790 &quot;,2), (&quot;Fellow-Citizens of the Senate and House of Representatives: &quot;,3), (&quot;I embrace with great satisfaction the opportunity which now presents itself of congratulating you on the present favorable prospects of our public affairs. The recent accession of the important state of North Carolina to the Constitution of the United States (of which official information has been received), the rising credit and respectability of our country, the general and increasing good will toward the government of the Union, and the concord, peace, and plenty with which we are blessed are circumstances auspicious in an eminent degree to our national prosperity. &quot;,4))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">souWithIndices.map(lineWithIndex =&gt; lineWithIndex._1).take(5) // the first element via ._1 as String of each tuple... taking first 5
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res8: Array[String] = Array(&quot;George Washington &quot;, &quot;&quot;, &quot;January 8, 1790 &quot;, &quot;Fellow-Citizens of the Senate and House of Representatives: &quot;, &quot;I embrace with great satisfaction the opportunity which now presents itself of congratulating you on the present favorable prospects of our public affairs. The recent accession of the important state of North Carolina to the Constitution of the United States (of which official information has been received), the rising credit and respectability of our country, the general and increasing good will toward the government of the Union, and the concord, peace, and plenty with which we are blessed are circumstances auspicious in an eminent degree to our national prosperity. &quot;)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">souWithIndices.map(lineWithIndex =&gt; lineWithIndex._2).take(5) // the second element via ._2 as Long of each tuple... taking first 5
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res9: Array[Long] = Array(0, 1, 2, 3, 4)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">souWithIndices.filter(lineWithIndex =&gt; 1 &lt;= lineWithIndex._2 &amp;&amp; lineWithIndex._2 &lt;= 3).collect // collecting lines 1-3
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res10: Array[(String, Long)] = Array((&quot;&quot;,1), (&quot;January 8, 1790 &quot;,2), (&quot;Fellow-Citizens of the Senate and House of Representatives: &quot;,3))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// the last 148 lines are from the 2017 SOU by Donald J. Trump
souWithIndices.filter(lineWithIndex =&gt; 22258-148 &lt; lineWithIndex._2 &amp;&amp; lineWithIndex._2 &lt;= 22258).collect
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res11: Array[(String, Long)] = Array((&quot;Donald J. Trump &quot;,22111), (&quot;&quot;,22112), (&quot;February 28, 2017 &quot;,22113), (&quot;Thank you very much. Mr. Speaker, Mr. Vice President, members of Congress, the first lady of the United States ... &quot;,22114), (&quot;... and citizens of America, tonight, as we mark the conclusion of our celebration of Black History Month, we are reminded of our nation's path toward civil rights and the work that still remains to be done. &quot;,22115), (&quot;Recent threats ... &quot;,22116), (&quot;Recent threats targeting Jewish community centers and vandalism of Jewish cemeteries, as well as last week's shooting in Kansas City, remind us that while we may be a nation divided on policies, we are a country that stands united in condemning hate and evil in all of its very ugly forms. &quot;,22117), (&quot;Each American generation passes the torch of truth, liberty and justice, in an unbroken chain all the way down to the present. That torch is now in our hands. And we will use it to light up the world. &quot;,22118), (&quot;I am here tonight to deliver a message of unity and strength, and it is a message deeply delivered from my heart. A new chapter ... &quot;,22119), (&quot;... of American greatness is now beginning. A new national pride is sweeping across our nation. And a new surge of optimism is placing impossible dreams firmly within our grasp. What we are witnessing today is the renewal of the American spirit. Our allies will find that America is once again ready to lead. &quot;,22120), (&quot;All the nations of the world friend or foe will find that America is strong, America is proud, and America is free. In nine years, the United States will celebrate the 250th anniversary of our founding, 250 years since the day we declared our independence. It will be one of the great milestones in the history of the world. &quot;,22121), (&quot;But what will America look like as we reach our 250th year? What kind of country will we leave for our children? I will not allow the mistakes of recent decades past to define the course of our future. &quot;,22122), (&quot;For too long, we've watched our middle class shrink as we've exported our jobs and wealth to foreign countries. We've financed and built one global project after another, but ignored the fates of our children in the inner cities of Chicago, Baltimore, Detroit and so many other places throughout our land. &quot;,22123), (&quot;We've defended the borders of other nations while leaving our own borders wide open for anyone to cross, and for drugs to pour in at a now unprecedented rate. And we've spent trillions and trillions of dollars overseas, while our infrastructure at home has so badly crumbled. &quot;,22124), (&quot;Then, in 2016, the earth shifted beneath our feet. The rebellion started as a quiet protest, spoken by families of all colors and creeds, families who just wanted a fair shot for their children, and a fair hearing for their concerns. &quot;,22125), (&quot;But then the quiet voices became a loud chorus, as thousands of citizens now spoke out together, from cities small and large, all across our country. &quot;,22126), (&quot;Finally, the chorus became an earthquake, and the people turned out by the tens of millions, and they were all united by one very simple, but crucial demand, that America must put its own citizens first, because only then can we truly make America great again. &quot;,22127), (&quot;Dying industries will come roaring back to life. Heroic veterans will get the care they so desperately need. Our military will be given the resources its brave warriors so richly deserve. &quot;,22128), (&quot;Crumbling infrastructure will be replaced with new roads, bridges, tunnels, airports and railways, gleaming across our very, very beautiful land. Our terrible drug epidemic will slow down and ultimately stop. And our neglected inner cities will see a rebirth of hope, safety and opportunity. &quot;,22129), (&quot;Above all else, we will keep our promises to the American people. &quot;,22130), (&quot;Thank you. It's been a little over a month since my inauguration, and I want to take this moment to update the nation on the progress I've made in keeping those promises. Since my election, Ford, Fiat-Chrysler, General Motors, Sprint, Softbank, Lockheed, Intel, Walmart and many others have announced that they will invest billions and billions of dollars in the United States and will create tens of thousands of new American jobs. &quot;,22131), (&quot;The stock market has gained almost $3 trillion in value since the election on Nov. 8, a record. We've saved taxpayers hundreds of millions of dollars by bringing down the price of fantastic and it is a fantastic new F-35 jet fighter, and we'll be saving billions more on contracts all across our government. &quot;,22132), (&quot;We have placed a hiring freeze on nonmilitary and nonessential federal workers. &quot;,22133), (&quot;We have begun to drain the swamp of government corruption by imposing a five-year ban on lobbying by executive branch officials and a lifetime ban ... &quot;,22134), (&quot;Thank you. Thank you. And a lifetime ban on becoming lobbyists for a foreign government. We have undertaken a historic effort to massively reduce job-crushing regulations, creating a deregulation task force inside of every government agency ... &quot;,22135), (&quot;... and we're imposing a new rule which mandates that for every one new regulation, two old regulations must be eliminated. &quot;,22136), (&quot;We're going to stop the regulations that threaten the future and livelihood of our great coal miners. &quot;,22137), (&quot;We have cleared the way for the construction of the Keystone and Dakota Access Pipelines ... &quot;,22138), (&quot;... thereby creating tens of thousands of jobs. And I've issued a new directive that new American pipelines be made with American steel. &quot;,22139), (&quot;We have withdrawn the United States from the job-killing Trans-Pacific Partnership. &quot;,22140), (&quot;And with the help of Prime Minister Justin Trudeau, we have formed a council with our neighbors in Canada to help ensure that women entrepreneurs have access to the networks, markets and capital they need to start a business and live out their financial dreams. &quot;,22141), (&quot;To protect our citizens, I have directed the Department of Justice to form a task force on reducing violent crime. I have further ordered the Departments of Homeland Security and Justice, along with the Department of State and the director of national intelligence, to coordinate an aggressive strategy to dismantle the criminal cartels that have spread all across our nation. &quot;,22142), (&quot;We will stop the drugs from pouring into our country and poisoning our youth, and we will expand treatment for those who have become so badly addicted. &quot;,22143), (&quot;At the same time, my administration has answered the pleas of the American people for immigration enforcement and border security. &quot;,22144), (&quot;By finally enforcing our immigration laws, we will raise wages, help the unemployed, save billions and billions of dollars, and make our communities safer for everyone. &quot;,22145), (&quot;We want all Americans to succeed, but that can't happen in an environment of lawless chaos. &quot;,22146), (&quot;We must restore integrity and the rule of law at our borders. &quot;,22147), (&quot;For that reason, we will soon begin the construction of a great, great wall along our southern border. &quot;,22148), (&quot;As we speak tonight, we are removing gang members, drug dealers and criminals that threaten our communities and prey on our very innocent citizens. Bad ones are going out as I speak, and as I promised throughout the campaign. To any in Congress who do not believe we should enforce our laws, I would ask you this one question: What would you say to the American family that loses their jobs, their income or their loved one because America refused to uphold its laws and defend its borders? &quot;,22149), (&quot;Our obligation is to serve, protect and defend the citizens of the United States. We are also taking strong measures to protect our nation from radical Islamic terrorism. &quot;,22150), (&quot;According to data provided by the Department of Justice, the vast majority of individuals convicted of terrorism and terrorism-related offenses since 9/11 came here from outside of our country. We have seen the attacks at home, from Boston to San Bernardino to the Pentagon and, yes, even the World Trade Center. We have seen the attacks in France, in Belgium, in Germany and all over the world. &quot;,22151), (&quot;It is not compassionate, but reckless to allow uncontrolled entry from places where proper vetting cannot occur. &quot;,22152), (&quot;Those given the high honor of admission to the United States should support this country and love its people and its values. We cannot allow a beachhead of terrorism to form inside America, and we cannot allow our nation to become a sanctuary for extremists. &quot;,22153), (&quot;That is why my administration has been working on improved vetting procedures, and we will shortly take new steps to keep our nation safe, and to keep those out who will do us harm. &quot;,22154), (&quot;As promised, I directed the Department of Defense to develop a plan to demolish and destroy ISIS, a network of lawless savages that have slaughtered Muslims and Christians, and men, women and children of all faiths and all beliefs. We will work with our allies, including our friends and allies in the Muslim world, to extinguish this vile enemy from our planet. &quot;,22155), (&quot;I have also imposed new sanctions on entities and individuals who support Iran's ballistic missile program, and reaffirmed our unbreakable alliance with the state of Israel. &quot;,22156), (&quot;Finally, I have kept my promise to appoint a justice to the United States Supreme Court, from my list of 20 judges, who will defend our Constitution. &quot;,22157), (&quot;I am greatly honored to have Maureen Scalia with us in the gallery tonight. &quot;,22158), (&quot;Thank you, Maureen. Her late, great husband, Antonin Scalia, will forever be a symbol of American justice. &quot;,22159), (&quot;To fill his seat, we have chosen Judge Neil Gorsuch, a man of incredible skill and deep devotion to the law. He was confirmed unanimously by the Court of Appeals, and I am asking the Senate to swiftly approve his nomination. &quot;,22160), (&quot;Tonight, as I outline the next steps we must take as a country, we must honestly acknowledge the circumstances we inherited. Ninety-four million Americans are out of the labor force. Over 43 million people are now living in poverty. And over 43 million Americans are on food stamps. &quot;,22161), (&quot;More than one in five people in their prime working years are not working. We have the worst financial recovery in 65 years. In the last eight years, the past administration has put on more new debt than nearly all of the other presidents combined. &quot;,22162), (&quot;We've lost more than one-fourth of our manufacturing jobs since Nafta was approved, and we've lost 60,000 factories since China joined the World Trade Organization in 2001. Our trade deficit in goods with the world last year was nearly $800 billion. And overseas, we have inherited a series of tragic foreign policy disasters. &quot;,22163), (&quot;Solving these and so many other pressing problems will require us to work past the differences of party. It will require us to tap into the American spirit that has overcome every challenge throughout our long and storied history. But to accomplish our goals at home and abroad, we must restart the engine of the American economy, making it easier for companies to do business in the United States and much, much harder for companies to leave our country. &quot;,22164), (&quot;Right now, American companies are taxed at one of the highest rates anywhere in the world. My economic team is developing historic tax reform that will reduce the tax rate on our companies so they can compete and thrive anywhere and with anyone. &quot;,22165), (&quot;It will be a big, big cut. &quot;,22166), (&quot;At the same time, we will provide massive tax relief for the middle class. We must create a level playing field for American companies and our workers have to do it. &quot;,22167), (&quot;Currently, when we ship products out of America, many other countries make us pay very high tariffs and taxes, but when foreign companies ship their products into America, we charge them nothing or almost nothing. &quot;,22168), (&quot;I just met with officials and workers from a great American company, Harley-Davidson. In fact, they proudly displayed five of their magnificent motorcycles, made in the U.S.A., on the front lawn of the White House. &quot;,22169), (&quot;And they wanted me to ride one, and I said, &quot;No, thank you.&quot; &quot;,22170), (&quot;At our meeting, I asked them, &quot;How are you doing? How is business?&quot; They said that it's good. I asked them further, &quot;How are you doing with other countries, mainly international sales?&quot; &quot;,22171), (&quot;They told me without even complaining, because they have been so mistreated for so long that they've become used to it that it's very hard to do business with other countries, because they tax our goods at such a high rate. They said that in the case of another country, they taxed their motorcycles at 100 percent. &quot;,22172), (&quot;They weren't even asking for a change. But I am. I believe ... &quot;,22173), (&quot;I believe strongly in free trade, but it also has to be fair trade. It's been a long time since we had fair trade. &quot;,22174), (&quot;The first Republican president, Abraham Lincoln, warned that &quot;the abandonment of the protective policy by the American government will produce want and ruin among our people.&quot; &quot;,22175), (&quot;Lincoln was right, and it's time we heeded his advice and his words. &quot;,22176), (&quot;I am not going to let America and its great companies and workers be taken advantage of us any longer. They have taken advantage of our country no longer. &quot;,22177), (&quot;I am going to bring back millions of jobs. Protecting our workers also means reforming our system of legal immigration. &quot;,22178), (&quot;The current, outdated system depresses wages for our poorest workers and puts great pressure on taxpayers. Nations around the world, like Canada, Australia and many others, have a merit-based immigration system. &quot;,22179), (&quot;It's a basic principle that those seeking to enter a country ought to be able to support themselves financially. Yet in America we do not enforce this rule, straining the very public resources that our poorest citizens rely upon. &quot;,22180), (&quot;According to the National Academy of Sciences, our current immigration system costs American taxpayers many billions of dollars a year. Switching away from this current system of lower-skilled immigration, and instead adopting a merit-based system, we will have so many more benefits. It will save countless dollars, raise workers' wages and help struggling families, including immigrant families, enter the middle class. And they will do it quickly, and they will be very, very happy, indeed. &quot;,22181), (&quot;I believe that real and positive immigration reform is possible, as long as we focus on the following goals: to improve jobs and wages for Americans, to strengthen our nation's security and to restore respect for our laws. &quot;,22182), (&quot;If we are guided by the well-being of American citizens, then I believe Republicans and Democrats can work together to achieve an outcome that has eluded our country for decades. &quot;,22183), (&quot;Another Republican president, Dwight D. Eisenhower, initiated the last truly great national infrastructure program: the building of the interstate highway system. The time has come for a new program of national rebuilding. &quot;,22184), (&quot;America has spent approximately $6 trillion in the Middle East, all the while our infrastructure at home is crumbling. &quot;,22185), (&quot;With the $6 trillion, we could have rebuilt our country twice, and maybe even three times, if we had people who had the ability to negotiate. &quot;,22186), (&quot;To launch our national rebuilding, I will be asking Congress to approve legislation that produces a $1 trillion investment in infrastructure of the United States, financed through both public and private capital, creating millions of new jobs. &quot;,22187), (&quot;This effort will be guided by two core principles: Buy American and hire American. &quot;,22188), (&quot;Tonight, I am also calling on this Congress to repeal and replace Obamacare ... &quot;,22189), (&quot;... with reforms that expand choice, increase access, lower costs and at the same time provide better health care. &quot;,22190), (&quot;Mandating every American to buy government-approved health insurance was never the right solution for our country. &quot;,22191), (&quot;The way to make health insurance available to everyone is to lower the cost of health insurance, and that is what we are going to do. &quot;,22192), (&quot;Obamacare premiums nationwide have increased by double and triple digits. As an example, Arizona went up 116 percent last year alone. Gov. Matt Bevin of Kentucky just said Obamacare is failing in his state, the state of Kentucky, and it's unsustainable and collapsing. &quot;,22193), (&quot;One third of the counties have only one insurer, and they're losing them fast, they are losing them so fast. They're leaving. And many Americans have no choice at all. There's no choice left. &quot;,22194), (&quot;Remember when you were told that you could keep your doctor and keep your plan? We now know that all of those promises have been totally broken. Obamacare is collapsing, and we must act decisively to protect all Americans. &quot;,22195), (&quot;Action is not a choice; it is a necessity. So I am calling on all Democrats and Republicans in Congress to work with us to save Americans from this imploding Obamacare disaster. &quot;,22196), (&quot;Here are the principles that should guide Congress as we move to create a better health care system for all Americans. &quot;,22197), (&quot;First, we should ensure that Americans with pre-existing conditions have access to coverage and that we have a stable transition for Americans currently enrolled in the health care exchanges. &quot;,22198), (&quot;Secondly, we should help Americans purchase their own coverage, through the use of tax credits and expanded health savings accounts, but it must be the plan they want, not the plan forced on them by our government. &quot;,22199), (&quot;Thirdly, we should give our state governors the resources and flexibility they need with Medicaid to make sure no one is left out. &quot;,22200), (&quot;Fourth, we should implement legal reforms that protect patients and doctors from unnecessary costs that drive up the price of insurance and work to bring down the artificially high price of drugs and bring them down immediately. &quot;,22201), (&quot;And finally, the time has come to give Americans the freedom to purchase health insurance across state lines ... &quot;,22202), (&quot;... which will create a truly competitive national marketplace that will bring costs way down and provide far better care. So important. &quot;,22203), (&quot;Everything that is broken in our country can be fixed. Every problem can be solved. And every hurting family can find healing and hope. Our citizens deserve this, and so much more, so why not join forces and finally get the job done and get it done right? &quot;,22204), (&quot;On this and so many other things, Democrats and Republicans should get together and unite for the good of our country and for the good of the American people. &quot;,22205), (&quot;My administration wants to work with members of both parties to make child care accessible and affordable, to help ensure new parents that they have paid family leave ... &quot;,22206), (&quot;... to invest in women's health, and to promote clean air and clean water, and to rebuild our military and our infrastructure. &quot;,22207), (&quot;True love for our people requires us to find common ground, to advance the common good, and to cooperate on behalf of every American child who deserves a much brighter future. &quot;,22208), (&quot;An incredible young woman is with us this evening who should serve as an inspiration to us all. Today is Rare Disease Day, and joining us in the gallery is a rare disease survivor, Megan Crowley. Megan ... &quot;,22209), (&quot;Megan was diagnosed with Pompe disease, a rare and serious illness, when she was 15 months old. She was not expected to live past 5. On receiving this news, Megan's dad, John, fought with everything he had to save the life of his precious child. He founded a company to look for a cure and helped develop the drug that saved Megan's life. Today she is 20 years old and a sophomore at Notre Dame. &quot;,22210), (&quot;Megan's story is about the unbounded power of a father's love for a daughter. But our slow and burdensome approval process at the Food and Drug Administration keeps too many advances, like the one that saved Megan's life, from reaching those in need. &quot;,22211), (&quot;If we slash the restraints, not just at the F.D.A. but across our government, then we will be blessed with far more miracles just like Megan. &quot;,22212), (&quot;In fact, our children will grow up in a nation of miracles. But to achieve this future, we must enrich the mind and the souls of every American child. Education is the civil rights issue of our time. &quot;,22213), (&quot;I am calling upon members of both parties to pass an education bill that funds school choice for disadvantaged youth, including millions of African-American and Latino children. &quot;,22214), (&quot;These families should be free to choose the public, private, charter, magnet, religious or home school that is right for them. &quot;,22215), (&quot;Joining us tonight in the gallery is a remarkable woman, Denisha Merriweather. As a young girl, Denisha struggled in school and failed third grade twice. But then she was able to enroll in a private center for learning great learning center with the help of a tax credit and a scholarship program. Today, she is the first in her family to graduate, not just from high school, but from college. Later this year, she will get her master's degree in social work. We want all children to be able to break the cycle of poverty just like Denisha. &quot;,22216), (&quot;But to break the cycle of poverty, we must also break the cycle of violence. The murder rate in 2015 experienced its largest single-year increase in nearly half a century. In Chicago, more than 4,000 people were shot last year alone, and the murder rate so far this year has been even higher. This is not acceptable in our society. &quot;,22217), (&quot;Every American child should be able to grow up in a safe community, to attend a great school and to have access to a high-paying job. &quot;,22218), (&quot;But to create this future, we must work with not against not against the men and women of law enforcement. &quot;,22219), (&quot;We must build bridges of cooperation and trust, not drive the wedge of disunity and it's really, it's what it is, division. It's pure, unadulterated division. We have to unify. Police and sheriffs are members of our community. They're friends and neighbors, they're mothers and fathers, sons and daughters, and they leave behind loved ones every day who worry about whether or not they'll come home safe and sound. We must support the incredible men and women of law enforcement. &quot;,22220), (&quot;And we must support the victims of crime. I have ordered the Department of Homeland Security to create an office to serve American victims. The office is called Voice, Victims of Immigration Crime Engagement. &quot;,22221), (&quot;We are providing a voice to those who have been ignored by our media and silenced by special interests. Joining us ... &quot;,22222), (&quot;Joining us in the audience tonight are four very brave Americans whose government failed them. Their names are Jamiel Shaw, Susan Oliver, Jenna Oliver and Jessica Davis. Jamiel's 17-year-old son was viciously murdered by an illegal immigrant gang member who had just been released from prison. Jamiel Shaw Jr. was an incredible young man with unlimited potential who was getting ready to go to college, where he would have excelled as a great college quarterback. &quot;,22223), (&quot;But he never got the chance. His father, who is in the audience tonight, has become a very good friend of mine. Jamiel, thank you. Thank you. &quot;,22224), (&quot;Also with us are Susan Oliver and Jessica Davis. Their husbands Deputy Sheriff Danny Oliver and Detective Michael Davis were slain in the line of duty in California. They were pillars of their community. These brave men were viciously gunned down by an illegal immigrant with a criminal record and two prior deportations. Should have never been in our country. &quot;,22225), (&quot;Sitting with Susan is her daughter, Jenna. Jenna, I want you to know that your father was a hero and that tonight you have the love of an entire country supporting you and praying for you. &quot;,22226), (&quot;To Jamiel, Jenna, Susan and Jessica, I want you to know that we will never stop fighting for justice. Your loved ones will never, ever be forgotten. We will always honor their memory. &quot;,22227), (&quot;Finally, to keep America safe, we must provide the men and women of the United States military with the tools they need to prevent war if they must they have to fight and they only have to win. &quot;,22228), (&quot;I am sending Congress a budget that rebuilds the military, eliminates the defense sequester... &quot;,22229), (&quot;... and calls for one of the largest increases in national defense spending in American history. &quot;,22230), (&quot;My budget will also increase funding for our veterans. Our veterans have delivered for this nation, and now we must deliver for them. &quot;,22231), (&quot;The challenges we face as a nation are great. But our people are even greater. And none are greater or braver than those who fight for America in uniform. &quot;,22232), (&quot;We are blessed to be joined tonight by Carryn Owens, the widow of U.S. Navy special operator, Senior Chief William &quot;Ryan&quot; Owens. Ryan died as he lived, a warrior and a hero, battling against terrorism and securing our nation. &quot;,22233), (&quot;I just spoke to our great General [Jim] Mattis just now who reconfirmed that, and I quote, &quot;Ryan was a part of a highly successful raid that generated large amounts of vital intelligence that will lead to many more victories in the future against our enemy.&quot; &quot;,22234), (&quot;Ryan's legacy is etched into eternity. Thank you. &quot;,22235), (&quot;And Ryan is looking down right now. You know that. And he's very happy, because I think he just broke a record. &quot;,22236), (&quot;For as the Bible teaches us, there is no greater act of love than to lay down one's life for one's friends. Ryan laid down his life for his friends, for his country and for our freedom. And we will never forget Ryan. &quot;,22237), (&quot;To those allies who wonder what kind of a friend America will be, look no further than the heroes who wear our uniform. Our foreign policy calls for a direct, robust and meaningful engagement with the world. It is American leadership based on vital security interests that we share with our allies all across the globe. &quot;,22238), (&quot;We strongly support NATO, an alliance forged through the bonds of two world wars, that dethroned fascism ... &quot;,22239), (&quot;... and a Cold War and defeated communism. &quot;,22240), (&quot;But our partners must meet their financial obligations. And now, based on our very strong and frank discussions, they are beginning to do just that. In fact, I can tell you the money is pouring in. Very nice. &quot;,22241), (&quot;We expect our partners, whether in NATO, in the Middle East or in the Pacific, to take a direct and meaningful role in both strategic and military operations, and pay their fair share of the cost have to do that. &quot;,22242), (&quot;We will respect historic institutions, but we will respect the foreign rights of all nations. And they have to respect our rights as a nation, also. &quot;,22243), (&quot;Free nations are the best vehicle for expressing the will of the people, and America respects the right of all nations to chart their own path. My job is not to represent the world. My job is to represent the United States of America. &quot;,22244), (&quot;But we know that America is better off when there is less conflict, not more. We must learn from the mistakes of the past. We have seen the war and the destruction that have ravaged and raged throughout the world. All across the world. &quot;,22245), (&quot;The only long-term solution for these humanitarian disasters, in many cases, is to create the conditions where displaced persons can safely return home and begin the long, long process of rebuilding. &quot;,22246), (&quot;America is willing to find new friends, and to forge new partnerships, where shared interests align. We want harmony and stability, not war and conflict. We want peace, wherever peace can be found. America is friends today with former enemies. Some of our closest allies, decades ago, fought on the opposite side of these terrible, terrible wars. This history should give us all faith in the possibilities for a better world. &quot;,22247), (&quot;Hopefully, the 250th year for America will see a world that is more peaceful, more just, and more free. &quot;,22248), (&quot;On our 100th anniversary in 1876, citizens from across our nation came to Philadelphia to celebrate America's centennial. At that celebration, the country's builders and artists and inventors showed off their wonderful creations. Alexander Graham Bell displayed his telephone for the first time. Remington unveiled the first typewriter. An early attempt was made at electric light. Thomas Edison showed an automatic telegraph and an electric pen. Imagine the wonders our country could know in America's 250th year. &quot;,22249), (&quot;Think of the marvels we could achieve if we simply set free the dreams of our people. Cures to the illnesses that have always plagued us are not too much to hope. American footprints on distant worlds are not too big a dream. Millions lifted from welfare to work is not too much to expect. And streets where mothers are safe from fear schools where children learn in peace, and jobs where Americans prosper and grow are not too much to ask. &quot;,22250), (&quot;When we have all of this, we will have made America greater than ever before, for all Americans. This is our vision. This is our mission. But we can only get there together. We are one people, with one destiny. &quot;,22251), (&quot;We all bleed the same blood. We all salute the same great American flag. And we are all made by the same God. &quot;,22252), (&quot;When we fulfill this vision, when we celebrate our 250 years of glorious freedom, we will look back on tonight as when this new chapter of American greatness began. The time for small thinking is over. The time for trivial fights is behind us. We just need the courage to share the dreams that fill our hearts, the bravery to express the hopes that stir our souls, and the confidence to turn those hopes and those dreams into action. &quot;,22253), (&quot;From now on, America will be empowered by our aspirations, not burdened by our fears, inspired by the future, not bound by failures of the past, and guided by a vision, not blinded by our doubts. &quot;,22254), (&quot;I am asking all citizens to embrace this renewal of the American spirit. I am asking all members of Congress to join me in dreaming big and bold and daring things for our country. I am asking everyone watching tonight to seize this moment. Believe in yourselves. Believe in your future. And believe, once more, in America. &quot;,22255), (Thank you, God bless you, and God bless the United States.,22256), (&quot;&quot;,22257))
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="cache-the-rdd-in-distributed-memory-to-avoid-recreating-it-for-each-action"><a class="header" href="#cache-the-rdd-in-distributed-memory-to-avoid-recreating-it-for-each-action">Cache the RDD in (distributed) memory to avoid recreating it for each action</a></h3>
<ul>
<li>Above, every time we took an action on the same RDD, the RDD was reconstructed from the textfile.
<ul>
<li>Spark's advantage compared to Hadoop MapReduce is the ability to cache or store the RDD in distributed memory across the nodes.</li>
</ul>
</li>
<li>Let's use <code>.cache()</code> after creating an RDD so that it is in memory after the first action (and thus avoid reconstruction for subsequent actions).
<ul>
<li>count the number of elements in the RDD <code>sou</code> (i.e., the number of lines in the text file <code>/datasets/sds/souall.txt.gz</code>) using <code>sou.count()</code></li>
<li>display the contents of the RDD using <code>take</code> or <code>collect</code>.</li>
</ul>
</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Shift+Enter to read in the textfile as RDD[String] and cache it in distributed memory
val sou = sc.textFile(&quot;/datasets/sds/souall.txt.gz&quot;) // Cntrl+Enter to read the gzipped textfile as RDD[String]
sou.cache() // cache the RDD in memory
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>sou: org.apache.spark.rdd.RDD[String] = /datasets/sds/souall.txt.gz MapPartitionsRDD[11] at textFile at command-4088905069026019:2
res12: sou.type = /datasets/sds/souall.txt.gz MapPartitionsRDD[11] at textFile at command-4088905069026019:2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou.count() // Shift+Enter during this count action the RDD is constructed from texfile and cached
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res13: Long = 22258
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou.count() // Shift+Enter during this count action the cached RDD is used (notice less time taken by the same command)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res14: Long = 22258
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Now, let's go to SparkUI and verify that this RDD is actually cached. The details would look something akin to the following.</p>
<table><thead><tr><th>ID | RD</th><th>RDD Name</th><th>Storage Level</th><th>Cached Partitions | Fract</th><th>Fraction Cached | Si</th><th>Size in Memory</th><th>Size on Disk |</th></tr></thead><tbody>
<tr><td>xxx |/d</td><td>/datasets/sds/souall.txt.gz</td><td>Memory Deserialized 1x Replicated |</td><td>1| 10</td><td>100% | 20.</td><td>20.1 MiB |</td><td>0.0 B |</td></tr>
</tbody></table>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou.take(5) // &lt;Cntrl+Enter&gt; to display the first 5 elements of the cached RDD
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res15: Array[String] = Array(&quot;George Washington &quot;, &quot;&quot;, &quot;January 8, 1790 &quot;, &quot;Fellow-Citizens of the Senate and House of Representatives: &quot;, &quot;I embrace with great satisfaction the opportunity which now presents itself of congratulating you on the present favorable prospects of our public affairs. The recent accession of the important state of North Carolina to the Constitution of the United States (of which official information has been received), the rising credit and respectability of our country, the general and increasing good will toward the government of the Union, and the concord, peace, and plenty with which we are blessed are circumstances auspicious in an eminent degree to our national prosperity. &quot;)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou.getStorageLevel
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res16: org.apache.spark.storage.StorageLevel = StorageLevel(memory, deserialized, 1 replicas)
</code></pre>
</div>
</div>
<div class="cell markdown">
<p><strong>Question</strong></p>
<ul>
<li>what happens when we call the following now?</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val souWithIndices = sou.zipWithIndex()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>souWithIndices: org.apache.spark.rdd.RDD[(String, Long)] = ZippedWithIndexRDD[12] at zipWithIndex at command-69839727109975:1
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let's quickly go back to SparkUI's Storage and see what is cached now.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">souWithIndices.getStorageLevel
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res17: org.apache.spark.storage.StorageLevel = StorageLevel(1 replicas)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">souWithIndices.count // let's call an action and look at the DAG Visualization of this job
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res18: Long = 22258
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>What if we tried to cache <code>souWithIndices</code>?</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">souWithIndices.cache() // check SparkUI -&gt; Storage
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res19: souWithIndices.type = ZippedWithIndexRDD[12] at zipWithIndex at command-69839727109975:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">souWithIndices.count() // need to act once to actually cache it; check SparkUI -&gt; Storage again
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res20: Long = 22258
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Now, you will notice that both RDDs are cached in memory now.</p>
<p>This is merely illustrative and makes little difference in this example. However, if you do a whole lot of transformations to an RDD and plan to use it in the immediate future then it may be worthwhile caching the intermediate RDD to save time. This is often done in iterative ML algorithms.</p>
</div>
<div class="cell markdown">
<h4 id="lifecycle-of-a-spark-program---summary"><a class="header" href="#lifecycle-of-a-spark-program---summary">Lifecycle of a Spark Program - Summary</a></h4>
<ul>
<li>create RDDs from:
<ul>
<li>some external data source (such as a distributed file system)</li>
<li>parallelized collection in your driver program</li>
</ul>
</li>
<li>lazily transform these RDDs into new RDDs</li>
<li>cache some of those RDDs for future reuse</li>
<li>you perform actions to execute parallel computation to produce results</li>
</ul>
</div>
<div class="cell markdown">
<h3 id="transform-lines-to-words"><a class="header" href="#transform-lines-to-words">Transform lines to words</a></h3>
<ul>
<li>We need to loop through each line and split the line into words</li>
<li>For now, let us split using whitespace</li>
<li>More sophisticated regular expressions can be used to split the line (as we will see soon)</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou
.flatMap(line =&gt; line.split(&quot; &quot;))
.take(100)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res21: Array[String] = Array(George, Washington, &quot;&quot;, January, 8,, 1790, Fellow-Citizens, of, the, Senate, and, House, of, Representatives:, I, embrace, with, great, satisfaction, the, opportunity, which, now, presents, itself, of, congratulating, you, on, the, present, favorable, prospects, of, our, public, affairs., The, recent, accession, of, the, important, state, of, North, Carolina, to, the, Constitution, of, the, United, States, (of, which, official, information, has, been, received),, the, rising, credit, and, respectability, of, our, country,, the, general, and, increasing, good, will, toward, the, government, of, the, Union,, and, the, concord,, peace,, and, plenty, with, which, we, are, blessed, are, circumstances, auspicious, in, an, eminent, degree, to)
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="naive-word-count"><a class="header" href="#naive-word-count">Naive word count</a></h3>
<p>At a first glace, to do a word count of George Washingtons SoU address, we are templed to do the following:</p>
<ul>
<li>just break each line by the whitespace character &quot; &quot; and find the words using a <code>flatMap</code></li>
<li>then do the <code>map</code> with the closure <code>word =&gt; (word, 1)</code> to initialize each <code>word</code> with a integer count of <code>1</code>
<ul>
<li>ie., transform each word to a <em>(key, value)</em> pair or <code>Tuple</code> such as <code>(word, 1)</code></li>
</ul>
</li>
<li>then count all <em>value</em>s with the same <em>key</em> (<code>word</code> is the Key in our case) by doing a
<ul>
<li><code>reduceByKey(_+_)</code></li>
</ul>
</li>
<li>and finally <code>collect()</code> to display the results.</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou
.flatMap( line =&gt; line.split(&quot; &quot;) )
.map( word =&gt; (word, 1) )
.reduceByKey(_+_)
.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res22: Array[(String, Int)] = Array((rebuttal.,1), (supplied.,5), (delinquents.,1), (undecaying,1), (successions,1), (Victims',1), (meets,,2), (42,427,1), (tough,44), (briefly,,2), (hemisphere.&quot;,1), (chart.,1), (fairness,,12), ($47,673,756,1), (TAX,5), (&quot;quarantined&quot;.,1), (execute,52), (relieves,4), (afterward,1), (fight;,2), (entitlements,1), (Enjoying,,1), (unfelt,2), (requested--and,1), (8,,31), (insurrection,72), (homeland;,1), (Employees,1), (Biddle,,1), (bankruptcies,4), (178,1), (cynics,,1), (demands--problems,1), (underscored,2), (savages,,6), (Exactly,5), (47,650.00,1), (dangers,122), (pacified,1), (prohibited.,3), (disposed.,1), (Chihuahua,,1), (constant,,4), (Commerce,107), ($15,739,871,1), (edible,1), (instantaneous,1), (Level,1), ($6,879,300.93.,1), (cottons,1), (consist,,1), (unacceptable,5), (me.&quot;,2), (goods.,17), (used,,18), (Juarez,,3), (Stockton,1), (tyrant,,1), (Convention--this,1), (regulator,4), ($30-billion,1), (Pago-Pago,1), (problems.,50), (matron,2), (defiance,36), (ceased.,8), (viewed,24), (trade&quot;,1), (deservedly,2), (unforeseen,19), (ravages.,1), (1961--from,1), (1932,8), (Hills,5), (tradition--American,1), (Abusing,1), (Andrade,1), ($700,000,000,2), (Reward,1), (Clay,,2), (week--28,1), (set-tier,,1), (Congressmen.,1), (325,000.,1), (dollar's,4), (Congo's,1), ($458,544,233.03,,1), (policing.,3), (1835-36,1), (animating,3), (cities,162), (alternative,60), (strut,1), (demoralization.,3), (commodities--changes,1), (consent,,19), (workplace.,4), (comprehensiveness.,1), (renders,,2), (reiterating,1), (Accomplishing,1), (sequence,2), (panels;,1), (visible,,1), (logically,8), (Conflicts,2), ($33,784,417.95.,1), (slackness,1), (right,992), (belated,1), (insurer,1), (Saxe-Coburg,,1), (well-regulated,4), (polluters,4), (12,000,000,5), (breakwaters,1), (uninjured,1), (sham,1), (Bosphorus,2), (motor-car,1), (perpetuation,,1), (bite,,1), (degrades,1), (constructing,23), (reductions,,3), (photograph,2), (SOUTH,2), (Improved,1), (hydrogen-powered,1), (spelled,3), (impute,3), (Knowledge,3), (advance.,13), (incomparable,1), (seeing,28), ($88,922.10.,1), (Undistributed,1), (1998,4), (Palmer's,1), (organization.,27), (strangely,2), (east,39), (Mustering-out,1), (accede,,1), (anti-terrorism,1), (lines,238), (Regency.,1), (to-day;,3), (fails,19), (attributing,1), (morality.,1), (POLLUTION,1), (unimpaired,15), (Services,,2), (reestablished;,1), (veto.,9), (vessels.&quot;,1), (volcanic,1), (Truman's,1), ($10,000,000;,2), (Laws,8), (Hezbollah,,2), (tired,,1), (align,1), (archipelago,7), (beloved,34), (How,62), ((say,1), (deadweight,1), (ask--is,1), (praised,,1), (Y.--are,1), (marred,5), (11907,,1), (calamitously.,1), (affairs),,1), ($529,692,460.50.,1), (inviting,24), (small-arms,1), (hour,49), (twentieth,8), (peace--including,2), (mortal,6), (1,492,907.41,1), (conquest;,1), (sires,1), (instituted.,6), ($404,878.53,,1), (disturbed.&quot;,1), (obedience,,1), (devices,11), (Castro.,1), (anachronistic,1), (firm.,1), (Tanner,2), (modus,12), (entrance,,1), (interested?,1), (civilian,,1), (sober,,1), (interfere,60), (million.,27), (surprised,6), (nowadays,1), (cultivated.,1), (hit.,2), (murdering,4), (learnt,2), (exhausting,8), (1,154,471,762,1), (discernible,1), (excitement,,11), (devalued,,1), (strives,2), (probation.,2), (request,118), (systematically,15), (living.,32), (de-Baathification,1), (literally,,1), (roaming,1), (abstract,16), (benefactions,3), (&quot;commitments&quot;,1), (easy.&quot;,1), ($11,000,000;,1), (154,745,782,1), (overland,,1), (Fascist,3), (gratify,5), (22,083,1), (defer,,1), (Asia;,2), (wrapped,1), (enemy.&quot;,2), (Jamestown,,2), (geographically,,1), (laboratories,7), (26,487,795.67,1), (basin.,1), (support:,1), (warms,1), (adjournment,,2), (to-day's,1), (Atlantic;,1), (Jack,,1), ($1,095,440.33,1), (wide-sweeping,1), (namely,,19), (1846.,6), (Regents,1), (Fifth,3), (waters;,2), (heightened,7), (10,240,2), (offending,10), (DISCOUNT,1), (perpetuates,1), (brotherly,3), (rations,6), (cards,4), (insurgents.,9), (arrived,55), (impromptu,1), ($2,684,220.89,,1), (soldiery,,4), (demoralization,,1), (AGRICULTURE,14), (Italian,25), (1896--more,1), (trusts.,6), (self-defeating.,1), (legally.,1), (protection.,53), (Sacramento,3), (floor,14), (order--for,1), (fourth-class,11), ($165,000,000,1), (talent,,2), (column.,1), (Coast,36), (funded,37), ($2,481,000,1), (1,464;,1), (liberated,19), (Martens,,1), (canal.,15), (renown,2), (USES,1), (throng,1), (cherish,,3), (capacities,,4), (dislike,4), (birds,6), (placing,50), (extinguishment,,2), (WOMEN,2), (people--calling,1), (Former,2), (education--at,1), (enactments,27), (&quot;hereafter,1), (Go,4), (grew,,2), (Mount,8), (customers,10), (factor,65), (cuts,87), (turnabout,,1), (Secretly,1), (peace--making,1), ($40,000,000,6), ($27,036,389.41.,1), (equally.,2), (sue,1), (pathos,1), (deplores,1), (stabilize,15), (commerce--it,1), (asthma,,1), (mark.,2), (1823,,3), (cell.,3), (Ecuador,16), (exult,1), (robber,1), (Adversity,1), (grown,,3), (Except,6), ($100,000,000,,2), (liberalize,3), (outstripped,1), (stationary,,2), (doorways--on,1), (concern;,1), (Carolina),1), (affirmative,,2), (Lisa,1), (offer--$1.25,1), (Servia,4), (rabble-rousing,1), (commented,5), ($92,546,999;,1), (1930's--at,1), (160,000,1), ($74,065,896.99,,1), (outworn,,1), (Holland.,4), (prescriptions.,1), ($104,000,000.,1), (10?,1), (park--it's,1), (consulates,8), (inequities--then,1), (accommodations,,1), (California,,40), (untold,8), (might,,19), (prayer,13), (crises.,6), ($192,000,000,,1), (sell;,1), (wing,2), (Kyi,1), (marching,,1), (melted,1), (.outside,1), (2nd,,3), (happy,161), (converting,12), (procurement.,1), (sooner.,1), (reductions,76), (what.,1), (likened,1), (types--the,1), (agencies;,4), (inordinate,5), (712.,1), (unofficial,7), (plane.,1), (appurtenances,2), (surplus--until,1), (neglected;,1), (defined,40), (horrors,15), (country.&quot;,8), (journeyed,1), (unchallenged,2), (assuredly,12), (loyal.,1), (North-West,1), (broadcast,,1), (valor,11), ($134,178,756.96,,1), (ocean-going,2), (2001.,8), (war--in,1), (Augustine.,1), (saps,2), (consultation,,3), (forgotten;,1), (statesmen.,1), (sexual,3), (Nation--if,1), (Kosovo,,1), (weeks.,5), (wide;,1), (aggressor,,2), (Rensselaer,,1), (block,10), (unconsciously,,1), (potholes.&quot;,1), (scales,5), (Creation,1), (enormous.,1), (urgency.,4), (owners;,2), (resource&quot;--of,1), ($2,000,000,000,1), (Allen,5), (discreditable.,1), (networks,,4), (Versailles,1), (products;,1), (Endive.,1), (tends,40), ($160,000,000.,2), (masons,,1), (entrepreneurs.,2), (below,,3), (manager,2), (to,59686), (sewer,1), (1811.,2), (sustains,6), (surely,83), (subsidies.,5), (towering,4), (reactions,1), (caution.,7), (Syria,4), (because:,1), (apples--and,1), (missionaries,,3), (Bosnia's,1), (Objections,1), (Contracts,6), (propagation,2), (quadrupled,2), ($720,039,039.79,1), (quasi-judicial,1), (wrought,,1), (BILATERAL,1), (1987,,1), (about--challenging,1), (run--free,1), (speeding,7), (fixity,3), (he,1068), (skins.,1), (democracies.&quot;,1), (embedded,1), (Emory,2), (mercantile,15), (responses,4), ($907,500,000.,1), (shortcomings,12), (subserviency,2), (families,,58), (woman.,2), (uniqueness,1), (rapidly,213), (protectorate,7), (selfishness,10), (release,59), (despair,,2), (formidable,,3), ($250,5), (1975,8), (men's,5), (qualified.,1), (dispositions,28), (mat,1), (M,2), (agriculture;,1), (projects--navigation,,1), (notes--a,2), (Blessed,3), (Monterey,2), (Shan-tung,,1), (occupants,5), (crests,1), ($6,285,294.55,1), (practices;,2), (situations--the,1), (widowers,1), ((5),6), (toils,6), ($42,818,000.,1), (facie,2), (experimenting,1), (murdered,9), (News,,1), (aviation.,2), (construction,,51), (succumbing,1), (Brandeis,1), ($3,600,1), (ARMY,5), (comprehensive,188), (seek,,5), (indefinitely,14), (town,25), (person,136), (Rudy,1), (bankrupting,1), (Ukraine's,1), ($4.128,1), (Carefully,1), (Judeo-Christian,1), (effacement,1), (charities.,1), (tight-knit,5), ($36,394,976.92,1), (Prompt,6), (aliens,34), (manual.,1), (Thirty-eighth,2), (asserted,45), (children,394), (We,3437), (Democratic,,1), (crusiers,1), (guaranteed,34), (audit,,1), (Tejeda,,2), (ill,,6), ($52,964,887.36,1), (Governors,13), (Peiping,1), (Certainly,,1), (Proliferation,1), (associates.,5), (limiting,37), (shop-worn,1), (here--not,1), (customhouses,5), (intense,14), (florins,2), (Consumer,4), ($720,000,1), (vindicated,11), (threatened,122), (deputed,6), (contention,19), (confiscation,8), (rarely,,1), (thing--the,1), (seizures,,2), (centralization.,2), (accountable.&quot;,1), (Children.,1), (defaulting,2), (gallon.,2), (thanking,1), (compete.,6), (&quot;give,3), (29th,,7), (questions:,2), (IMPACT,1), (worry.,2), (Exemptions,,1), (ultraconservatives,1), (disturbance.,8), (House--as,1), (brotherhood,,2), (engagement.,1), (chair.,1), (cherished,32), (progrowth,1), (TRIDENT,2), (homelands,1), (satisfactory.,35), (affecting,,1), (cited,3), (safely,,3), (undisputed,,1), (prejudices,,7), (hydroelectric,5), (Conger,2), (prop,3), (Representatives.,30), (resident,,1), (10,000,000,,1), (1849,12), (motivate,1), (proprietorship,1), (Polynesian,1), (doing,192), (buses,1), (wounded,,10), ($742,000,000,,1), (decision-making.,4), (vindicate,16), (Peninsula,5), ($28,1), (DC,,1), (unforeseeable,1), (foretell,5), (unadjudicated.,1), (1947,40), (valuesthat's,1), (Experiment,2), (sectarian,7), (outcry,2), (preposition,1), (Pvt.,1), (freight,,7), (attracted,18), (Elihu,2), (disclosed,23), (fine,,5), (abroad--to,1), (territory.&quot;,1), (vague,11), (Comonfort,3), (there,2024), (4.5,5), ($9,500,000,000.,1), (computations,1), (1967,2), (easy-going,,1), (disfranchisement,2), ($247,354,1), (municipal,73), (ethic,,1), (portion,,5), (seconds.,1), (retooled.,1), (O'Neill,3), (discusses,1), ($409,967,470,,1), (expenditure.,24), (objectionable.,2), (verifying,,1), (dedicated.,2), (kindest,6), (defrayed,10), (Welland,2), (1985.,7), (exchanged,47), (prejudices,15), (1813,2), (independence.,54), (navigation.,19), (mulberry,3), (propelled,1), (append,1), ($7,000,000,000,2), (detected,7), (Protestant,,2), (verified.,3), (lengthy,6), (laws--strengthening,1), (bestowal,5), (Convention,48), (income--that,1), (organs&quot;,1), (contributed,60), (opinion;,3), ($55,402,465.46.,2), (Hubert,2), ($3,473,922,055,,1), (leadership;,1), (speedier,2), (coins,,3), (purposes--the,1), (multitude,17), (unsoundness,1), (historian,,1), (hemisphere;,1), (crafty,1), (ores,,1), (rounds.,1), (probable,99), (revealed,16), (Connecting,2), (bribe-giver,,1), (reclamation,,2), (helpfulness,2), (Clean,5), (discriminating,,1), (tribute.,2), (credence,1), ($79,333,966.,1), (efficacious,,1), (stems,1), (definite.,2), (computation,1), (Producers,1), (Third.,11), (styling,1), ($23,747,864.66,,1), (contribution.,3), ((except,5), (parliamentary,6), (anyplace,1), (extraordinaries,1), (warmly,6), (rectification,2), (purification,2), (rule-making,1), (militia;,1), (inspire,34), (articles,226), (136,2), (monarchy,,1), (&amp;,6), (transformations,1), (intimidation,,3), (Coinciding,1), (pay--so,1), (Justices,3), (whereby,,2), (Empire.,22), (resisted;,1), (betterments,2), (heavy.,2), (four-point,2), (compel,41), (allotting,2), (Low,,1), (Peace,55), (natural.,2), (charitable,14), (unscrupulously,1), (tree,3), (permits,31), (prohibited.&quot;,1), (archives,14), (annuities,,2), (fancied,,1), (animosities.,1), (subjection.,1), (respected,29), (Kickapoo,,1), (offensive,,8), (LAWS,4), (research,,32), (writer,1), (Forty-Five,1), (workload,2), (chances,19), (21,000,1), (preventable,9), (towards,47), (carefully-phased,1), (Contreras,1), (composite,1), (convey,12), (safety--these,,1), (exclusion.,2), (commander,,7), (24,557,409,1), (gelatine,1), (modicum,1), (plants,45), (defensible.,1), (Board;,1), ($8,967,609,1), (fetters.&quot;,1), (gunnery,3), (inward.,1), (five-sixths,1), (world?,2), (nation--an,1), (AMPHITHEATER,1), (entering.,1), (reaffirmations,1), (J.M.,1), (caps,2), (4o67,1), (restoration,,3), (launch,23), (brow.&quot;,1), (reminiscent,1), (originated.&quot;,1), (rents.,1), (navigate,3), ($110,000,000,1), (podium,,4), (1977:,1), (permissible,,1), (14,410,1), (penalizes,2), (couch,1), (ends:,2), (Optimistic,1), (slightest,51), (sped,1), (Republics.&quot;,1), (advance;,2), (rebuilds,1), (compelling,16), (militia,,28), (reconsideration.,2), (truly,92), (bestowed,36), (hastened,7), (locally,,1), (residence.,3), (hedge,1), (shortness,2), (promiscuous,1), (sum,,21), (Policy&quot;,1), (wool,,2), (harshest,1), (symbolically,1), (&quot;accepted&quot;,1), (testament,2), (prepare,91), (undecided,3), (Pa.,,1), (Charlotte,,2), (admission,106), (confirms,9), (unseeing,1), (valuation,,6), (truthful,3), (self-examination,1), ($24,829,163.54,1), ($215,725,995,,1), (governments--it,1), (Danish,11), (772,700,000,1), (1956.,1), (splendidly,2), (firearms,2), (mobile,5), (loving,5), (Treasure,1), (ministrations,1), (outposts,3), (obligations.&quot;,1), (inspecting,,1), (Apprehending,,1), (expatriation,,8), (idealism,8), (accounts.,11), (confessions,3), (temples,1), (unmodified.,1), (expediency,,3), (Hill,2), (longevity,1), ($800,000,000,1), (philosophy,11), (instrumental.,1), (genuine.,1), (barred,,1), (today,189), (Hay-Pauncefote,1), (braking,1), (7,000,000.,1), (75,000.,1), (Liverpool,4), (Strategically,,1), (Tokugawa,,1), (autos,,1), (254,550.00,1), (money,611), ($1,431,565.78.,1), (chieftain,1), (But--I,1), (denatured,2), (mailing,2), (1,297.,1), (patriotic,115), (universal,85), (1954,,4), (rededicate,5), (utterance,1), (work--two,1), (myself,,14), (monarchy.,2), (uneasy,3), (online,2), (upper,16), (patrolling,3), (30,000,000,2), (disappeared.,3), (depredations,36), (oppressors,,1), (vigorously,43), (Sandinistas,5), ($27,4), (begging,1), (dreary,,1), (deeds,22), (examination,,18), (CONDITION,2), (rocket,4), ($71,226,846,,1), (subjects--there,1), (asked:,1), (appropriating,21), (&quot;states,1), (0111,,1), (perspective,7), (wits,1), (deposit.,3), (them--we,1), (shooting,,1), (rub.,1), (badly.&quot;,1), (Leasing,2), (1767,1), (Leonard,3), (FBI,9), (154,747,1), (Hoods,1), (lofty,,1), (anti-recessionary,1), (problem;,1), (Massachusetts,11), (nicest,1), (1787.,3), (believe.,3), (OUR,19), (unify.,1), (graduation,21), (choices,,12), (&quot;unconstitutional,&quot;,1), (opinions,,14), (materialistic,,1), (midshipman,2), (pictures,2), (Sackville,1), (best-prepared,2), (twenty.,1), (fields,,9), (expedients,16), (1923,,1), (complied,33), (astronomical,6), (seize.,1), (careful.,1), (secession;,1), (Majority,5), (matters.,10), (premises,16), (happy,,7), (handle,20), (strange,,2), (believed,,72), (sites,26), (&quot;because,1), (incursion,,1), (&quot;Her,1), (rabbits,,1), (fishermen.,2), (independent-treasury,1), (recognized,174), (sloops,6), (Pupils,1), (terrorism.,10), (flag,,30), (auxiliary.,1), (alliances,29), (80,500,000,1), (1,074,974.35,,1), (larger,239), ($18,000,000,,1), (Fewer,1), (incompleteness,1), (style,,2), (Yakutsk,1), (Arenas,1), (instructs,1), (test,,2), (blasting,1), (Durango,1), (stress,,1), (success.,91), (calm,,4), ($11,507,670.,1), (year--to,1), (long,918), (modified.,6), (happier?,1), (loading,1), (reduced--and,1), (predominate,1), (armistice.,1), (availing,7), (sailing,17), (at,6519))
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Unfortunately, as you can see from the <code>collect</code> above:</p>
<ul>
<li>the words have punctuations at the end which means that the same words are being counted as different words. Eg: importance</li>
<li>empty words are being counted</li>
</ul>
<p>So we need a bit of <code>regex</code>'ing or regular-expression matching (all readily available from Scala via Java String types).</p>
<p>We will cover the three things we want to do with a simple example from Middle Earth!</p>
<ul>
<li>replace all multiple whitespace characters with one white space character &quot; &quot;</li>
<li>replace all punction characters we specify within <code>[</code> and <code>]</code> such as <code>[,?.!:;]</code> by the empty string <code>&quot;&quot;</code> (i.e., remove these punctuation characters)</li>
<li>convert everything to lower-case.</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val example = &quot;Master, Master!   It's me, Sméagol... mhrhm*%* But they took away our precious, they wronged us. Gollum will protect us..., Master, it's me Sméagol.&quot;
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>example: String = Master, Master!   It's me, Sméagol... mhrhm*%* But they took away our precious, they wronged us. Gollum will protect us..., Master, it's me Sméagol.
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">example
  .replaceAll(&quot;\\s+&quot;, &quot; &quot;) //replace multiple whitespace characters (including space, tab, new line, etc.) with one whitespace &quot; &quot;
  .replaceAll(&quot;&quot;&quot;([,?.!:;])&quot;&quot;&quot;, &quot;&quot;) // replace the following punctions characters: , ? . ! : ; . with the empty string &quot;&quot;
  .toLowerCase() // converting to lower-case
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res23: String = master master it's me sméagol mhrhm*%* but they took away our precious they wronged us gollum will protect us master it's me sméagol
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="more-sophisticated-word-count"><a class="header" href="#more-sophisticated-word-count">More sophisticated word count</a></h3>
<p>We are now ready to do a word count of George Washington's SoU on January 8th 1790 as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val wordCount_sou = 
 sou.flatMap(line =&gt; 
              line.replaceAll(&quot;\\s+&quot;, &quot; &quot;) //replace multiple whitespace characters (including space, tab, new line, etc.) with one whitespace &quot; &quot;
                  .replaceAll(&quot;&quot;&quot;([,?.!:;])&quot;&quot;&quot;, &quot;&quot;) // replace the following punctions characters: , ? . ! : ; . with the empty string &quot;&quot;
                  .toLowerCase() // converting to lower-case
                  .split(&quot; &quot;)
            ).map(x =&gt; (x, 1))
             .reduceByKey(_+_)
    
wordCount_sou.count // there are 34812 distinct words
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>wordCount_sou: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[19] at reduceByKey at command-4088905069026032:8
res24: Long = 34812
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">wordCount_sou.take(30) // some of the counts are
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res25: Array[(String, Int)] = Array((reunion,9), ($654137907-89,1), (divisional,1), (devices--except,1), (successions,1), (undecaying,1), (instant--in,1), ($668000000,1), (subdivided,1), (b-1b,1), (american-british,1), (fuller,24), (leeway,1), (instrumentality,20), (tough,52), (countervail,3), (crying,12), (governments--or,1), (snatch,2), (breath,5), (boldest,1), (delusion,14), (execute,53), (relieves,4), (nonunion,2), (minorities,20), (afterward,2), (ignore,32), (entitlements,5), (eventuality,1))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val top10 = wordCount_sou.sortBy(_._2, false).take(10) // sorted by most frequent words limited to top 10
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>top10: Array[(String, Int)] = Array((the,150091), (of,97090), (and,60810), (to,60725), (in,38660), (a,27935), (that,21657), (for,19014), (be,18727), (our,17440))
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="doing-it-all-together-for-the-sou-addresses-of-george-washington-and-donald-trump"><a class="header" href="#doing-it-all-together-for-the-sou-addresses-of-george-washington-and-donald-trump">Doing it all together for the SOU Addresses of George Washington and Donald Trump</a></h3>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">souWithIndices
    .filter(lineWithIndex =&gt; (22258-148 &lt; lineWithIndex._2 &amp;&amp; lineWithIndex._2 &lt;= 22258) // line numbers for Donlad Trump's 2017 SOU
                               || lineWithIndex._2 &lt; 30  // line numbers for George Washington's first SoU
           )
    .map( lineWithIndex =&gt; lineWithIndex._1 )
    .flatMap(line =&gt; 
         line.replaceAll(&quot;\\s+&quot;, &quot; &quot;) //replace multiple whitespace characters (including space, tab, new line, etc.) with one whitespace &quot; &quot;
             .replaceAll(&quot;&quot;&quot;([,?.!:;])&quot;&quot;&quot;, &quot;&quot;) // replace the following punctions characters: , ? . ! : ; . with the empty string &quot;&quot;
             .toLowerCase() // converting to lower-case
             .split(&quot; &quot;))
    .map(x =&gt; (x,1))
    .reduceByKey(_+_)
    .sortBy(_._2, false)
    .collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res26: Array[(String, Int)] = Array((the,344), (and,260), (of,230), (to,211), (our,129), (a,121), (we,104), (in,103), (that,81), (is,71), (will,71), (for,66), (have,57), (i,52), (be,48), (with,47), (as,39), (by,39), (are,38), (from,35), (all,35), (you,35), (on,34), (american,33), (this,32), (they,32), (not,32), (but,30), (it,28), (america,27), (their,27), (country,26), (an,24), (us,22), (at,22), (which,22), (so,21), (new,20), (who,20), (great,20), (must,20), (one,18), (united,18), (world,17), (people,17), (has,17), (should,17), (was,16), (states,16), (very,16), (my,15), (am,15), (those,15), (americans,15), (more,15), (citizens,14), (just,14), (your,14), (&quot;&quot;,14), (been,13), (them,13), (tonight,13), (now,13), (nation,13), (many,13), (national,13), (can,13), (other,12), (government,12), (every,12), (time,11), (his,11), (work,11), (across,10), (also,10), (thank,10), (year,10), (do,10), (public,10), (down,10), (it's,10), (its,9), (free,9), (much,9), (want,9), (health,9), (when,9), (home,9), (jobs,9), (than,9), (right,8), (immigration,8), (made,8), (good,8), (congress,8), (last,8), (out,8), (believe,8), (children,8), (or,8), (system,8), (first,8), (since,8), (state,8), (future,8), (support,8), (we've,8), (foreign,7), (he,7), (long,7), (need,7), (nations,7), (create,7), (companies,7), (know,7), (where,7), (were,7), (justice,7), (help,7), (years,7), (only,7), (what,7), (billions,6), (high,6), (if,6), (allies,6), (millions,6), (even,6), (may,6), (no,6), (trade,6), (workers,6), (care,6), (rights,6), (asking,6), (then,6), (get,6), (dollars,6), (too,6), (into,6), (never,6), (keep,6), (tax,6), (finally,6), (respect,6), (same,6), (security,6), (members,6), (life,6), (she,6), (friends,6), (infrastructure,6), (make,6), (me,6), (past,6), (administration,6), (better,6), (against,6), (military,6), (find,5), (community,5), (department,5), (further,5), (access,5), (had,5), (interests,5), (able,5), (school,5), (safe,5), (credit,5), (law,5), (peace,5), (there,5), (obamacare,5), (protect,5), (choice,5), (child,5), (history,5), (laws,5), (would,5), (men,5), (house,5), (up,5), (today,5), (like,5), (going,5), (middle,5), (families,5), (such,5), (fair,5), (plan,5), (because,5), (war,5), (measures,5), (over,5), (these,5), (rate,5), (women,5), (love,5), (insurance,5), (hope,5), (own,5), (provide,5), (dreams,5), (greater,4), (end,4), (megan,4), (proper,4), (megan's,4), (bring,4), (stop,4), (throughout,4), (big,4), (trillion,4), (well,4), (spirit,4), (affairs,4), (regard,4), (joining,4), (representatives,4), (countries,4), (ought,4), (wages,4), (increase,4), (while,4), (250th,4), (terrorism,4), (oliver,4), (means,4), (recent,4), (allow,4), (leave,4), (become,4), (family,4), (jenna,4), (truly,4), (said,4), (drug,4), (jamiel,4), (best,4), (way,4), (senate,4), (resources,4), (blessed,4), (look,4), (program,4), (incredible,4), (job,4), (general,4), (defense,4), (require,4), (both,4), (save,4), (two,4), (another,4), (uniform,4), (could,4), (come,4), (ryan,4), (directed,4), (costs,4), (together,4), (borders,4), (her,4), (rule,4), (susan,4), (take,4), (they're,4), (old,3), (republicans,3), (price,3), (veterans,3), (cities,3), (jessica,3), (defend,3), (everyone,3), (creating,3), (regulations,3), (celebrate,3), (taxpayers,3), (business,3), (president,3), (seen,3), (thousands,3), (victims,3), (immigrant,3), (tens,3), (serve,3), (ensure,3), (calling,3), (done,3), (importance,3), (confidence,3), (others,3), (nothing,3), (any,3), (task,3), (million,3), (circumstances,3), (duty,3), (day,3), (record,3), (render,3), (working,3), (back,3), (far,3), (grow,3), (disease,3), (drugs,3), (information,3), (direct,3), (poverty,3), (session,3), (common,3), (force,3), (financial,3), (democrats,3), (deserve,3), (learning,3), (private,3), (davis,3), (expect,3), (current,3), (constitution,3), (ones,3), (cycle,3), (including,3), (promises,3), (necessary,3), (object,3), (class,3), (give,3), (friend,3), (nearly,3), (here,3), (rebuilding,3), (present,3), (policy,3), (brave,3), (loved,3), (before,3), (vision,3), (gallery,3), (8,3), (break,3), (whether,3), (satisfaction,3), (freedom,3), (share,3), (cannot,3), (important,3), (saved,3), (through,3), (historic,3), (guided,3), (honor,3), (cooperation,3), (between,3), (strong,3), (enforcement,3), (center,3), (union,3), (again,3), (college,3), (according,3), (fact,3), (decades,3), (achieve,3), (denisha,3), (abroad,3), (terrible,3), (god,3), (ban,3), (crime,3), (rare,3), (young,3), (shot,2), (behind,2), (less,2), (part,2), (we're,2), (toward,2), (disasters,2), (small,2), (trillions,2), (torch,2), (vetting,2), (washington,2), (third,2), (inherited,2), (east,2), (meeting,2), (poorest,2), (enter,2), (integrity,2), (things,2), (lay,2), (came,2), (george,2), (twice,2), (largest,2), (left,2), (illegal,2), (civil,2), (souls,2), (buy,2), (spoke,2), (office,2), (equal,2), (greatness,2), (inside,2), (persuaded,2), (showed,2), (fight,2), (lawless,2), (let,2), (mr,2), (goods,2), (due,2), (jewish,2), (nato,2), (welfare,2), (themselves,2), (message,2), (&quot;how,2), (essential,2), (happy,2), (communities,2), (off,2), (reason,2), (anniversary,2), (capital,2), (develop,2), (moment,2), (one's,2), (course,2), (within,2), (displayed,2), (ready,2), (bridges,2), (everything,2), (43,2), (embrace,2), (plenty,2), (add,2), (beginning,2), (threaten,2), (shaw,2), (consideration,2), (miracles,2), (parts,2), (given,2), (budget,2), (taxed,2), (branch,2), (form,2), (gentlemen,2), (leaving,2), (neighbors,2), (lincoln,2), (border,2), (spent,2), (mothers,2), (promised,2), (wanted,2), (reach,2), (strongly,2), (ignored,2), (joined,2), (upon,2), (think,2), (opportunity,2), (approve,2), (threats,2), (opinion,2), (doing,2), (250,2), (america's,2), (drive,2), (chicago,2), (former,2), (value,2), (canada,2), (delivered,2), (prepared,2), (almost,2), (celebration,2), (legal,2), (losing,2), (derive,2), (along,2), (immediately,2), (wars,2), (longer,2), (chapter,2), (slow,2), (lost,2), (learn,2), (crumbling,2), (badly,2), (kentucky,2), (man,2), (fantastic,2), (food,2), (invest,2), (intelligence,2), (attention,2), (based,2), (principles,2), (legislature,2), (large,2), (republican,2), (father,2), (lifetime,2), (goals,2), (told,2), (youth,2), (conflict,2), (month,2), (economy,2), (alone,2), (process,2), (most,2), (raise,2), (skill,2), (degree,2), (persons,2), (places,2), (motorcycles,2), (intercourse,2), (broken,2), (answered,2), (encouragement,2), (election,2), (percent,2), (represent,2), (asked,2), (effectual,2), (begin,2), (meaningful,2), (society,2), (i've,2), (fought,2), (provision,2), (electric,2), (laid,2), (currently,2), (commerce,2), (quiet,2), (scalia,2), (afford,2), (audience,2), (hopes,2), (chorus,2), ($6,2), (lead,2), (collapsing,2), (solution,2), (prime,2), (land,2), (engagement,2), (voice,2), (products,2), (clean,2), (distinguish,2), (about,2), (favorable,2), (distant,2), (ask,2), (live,2), (shall,2), (special,2), (southern,2), (bless,2), (1790,2), (coverage,2), (pacific,2), (still,2), (once,2), (pleasing,2), (fill,2), (particularly,2), (advantage,2), (patriotism,2), (officers,2), (financed,2), (restore,2), (pouring,2), (expand,2), (promote,2), (overseas,2), (owens,2), (among,2), (speak,2), (various,2), (fast,2), (partners,2), (construction,2), (failed,2), (merit-based,2), (papers,2), (steps,2), (parties,2), (blessings,2), (pay,2), (pipelines,2), (fellow-citizens,2), (effort,2), (anyone,2), (fulfill,2), (deliver,2), (see,2), (ordered,2), (why,2), (renewal,2), (inner,2), (prosperity,2), (stock,2), (vital,2), (hero,2), (murder,2), (cost,2), (officials,2), (act,2), (maureen,2), (20,2), (attacks,2), (reform,2), (action,2), (trust,2), (reforms,2), (nation's,2), (ship,2), (gang,2), (taken,2), (join,2), (education,2), (viciously,2), (reduce,2), (deeply,2), (calls,2), (company,2), (ever,2), (division,2), (kind,2), (purchase,2), (light,2), (respectability,2), (anywhere,2), (criminal,2), (imposing,2), (individuals,2), (use,2), (safety,2), (conditions,2), (daughter,2), (five,2), (homeland,2), (alliance,2), (placed,2), (court,2), (put,2), (progress,2), (path,2), (woman,2), (mistakes,2), (enforce,2), (prospects,2), (liberty,2), (providing,2), (always,2), (lower,2), (became,2), (justin,1), (accession,1), (surge,1), (desirable,1), (secure,1), (dismantle,1), (germany,1), (federal,1), (unveiled,1), (weights,1), (looking,1), (inevitable,1), (supporting,1), (discriminate,1), (entry,1), (partnership,1), (bears,1), (well-being,1), (safely,1), (bad,1), (eight,1), (punish,1), (son,1), (swamp,1), (strength,1), (tragic,1), (chosen,1), (heroic,1), (official,1), (series,1), (received),1), (fourth,1), (lived,1), (increases,1), (satisfactory,1), (footprints,1), (sciences,1), (unnecessary,1), (found,1), (commonwealth,1), (totally,1), (builders,1), (latter,1), (savages,1), (shrink,1), (china,1), (discussions,1), (graduate,1), (uniting,1), (divided,1), (produces,1), (foe,1), (trans-pacific,1), (antonin,1), (giving,1), (project,1), (networks,1), (win,1), (names,1), (exigencies,1), (months,1), (keystone,1), (firmly,1), (weren't,1), (approval,1), (undertaken,1), (blood,1), (fighter,1), (went,1), (insurer,1), (post-roads,1), (pentagon,1), (field,1), (entirely,1), (truth,1), (husband,1), (refused,1), (lobbyists,1), (bravery,1), (genius,1), (legacy,1), (go,1), (harm,1), (agency,1), (repeal,1), (daring,1), (precious,1), (lines,1), (eliminates,1), (congratulating,1), (country's,1), (accounts,1), (vehicle,1), (endeavors,1), (richly,1), (exertions,1), (detroit,1), (outcome,1), (align,1), (released,1), (muslims,1), (efforts,1), (supplies,1), (unemployed,1), (confirmed,1), (senior,1), (city,1), (developing,1), (compensation,1), (adopted,1), (carolina,1), (declaration,1), (obviously,1), (worlds,1), (recommendation,1), (reminded,1), (enemies,1), (open,1), (ford,1), (defeated,1), (facilitated,1), (forged,1), (taking,1), (forgotten,1), (raid,1), (coordinate,1), (perceive,1), (marine,1), (hands,1), (choose,1), (struggled,1), (doubts,1), (father's,1), (attended,1), (rebirth,1), (judge,1), (formed,1), (bible,1), (globe,1), (belgium,1), (muslim,1), (watching,1), (flexibility,1), (john,1), (pressure,1), (sequester,1), (last--and,1), (ravaged,1), (little,1), (speedily,1), (inventors,1), (d,1), (pour,1), (deficit,1), (enemy,1), (jr,1), (african-american,1), (considerations,1), (entire,1), (2016,1), (creations,1), (tariffs,1), (institutions,1), (dakota,1), (salute,1), (markets,1), (jamiel's,1), (engagements,1), (forget,1), (earth,1), (placing,1), (unanimously,1), (extremists,1), (appoint,1), (level,1), (protective,1), (dream,1), (perfect,1), (social,1), (impressions,1), (j,1), (question,1), (produce,1), (forces,1), (quote,1), (he's,1), (circumstance,1), (sales&quot;,1), (conducive,1), (obligation,1), (say,1), (basis,1), (defended,1), (investment,1), (tools,1), (cures,1), (fellow,1), (job-crushing,1), (saw,1), (crucial,1), (consultations,1), (countless,1), (cherishing,1), (protecting,1), (water,1), (experienced,1), (navy,1), (religious,1), (manufacturing,1), (side,1), (survivor,1), (ground,1), (faith,1), (teaching,1), (worthy,1), (cleared,1), (they've,1), (france,1), (fda,1), (pleasure,1), (resuming,1), (stamps,1), (extinguish,1), (glorious,1), (nice,1), (operations,1), (optimism,1), (strategy,1), (proud,1), (abandonment,1), (defined,1), (inviolable,1), (philadelphia,1), (inspired,1), (fighting,1), (illnesses,1), (helped,1), (return,1), (burdened,1), (novelty,1), (advances,1), (replace,1), (reconfirmed,1), (girl,1), (later,1), (global,1), (data,1), (slain,1), (died,1), (sheriffs,1), (destruction,1), (1876,1), (rely,1), (expected,1), (100th,1), (easier,1), (council,1), (dethroned,1), (boston,1), (close,1), (manner,1), (rebellion,1), (role,1), (ago,1), (governors,1), (they'll,1), (affording,1), (adequate,1), (deep,1), (declared,1), (restart,1), (sanctions,1), (turned,1), (loud,1), (core,1), (debt,1), (list,1), (provisions,1), (kansas,1), (specify,1), (unadulterated,1), (credits,1), (uphold,1), (wonder,1), (call,1), (beliefs,1), (2017,1), (fund,1), (protection,1), (improve,1), (campaign,1), (unlimited,1), (restraints,1), (ways--by,1), (recommended,1), (mercantile,1), (thirdly,1), (double,1), (speedy,1), (wherever,1), (funding,1), (silenced,1), (2015,1), (hard,1), (vandalism,1), (introduction,1), (dealers,1), (thomas,1), (colors,1), (contained,1), (flag,1), (intel,1), (story,1), (terrorism-related,1), (murdered,1), (vice,1), (slash,1), (tap,1), (trudeau,1), (rebuilt,1), (january,1), (bold,1), (violence,1), (israel,1), (ability,1), (founded,1), (ballistic,1))
</code></pre>
</div>
</div>
<div class="cell markdown">
<h2 id="youtry-homework"><a class="header" href="#youtry-homework">YouTry: HOMEWORK</a></h2>
<ul>
<li>HOWEWORK WordCount 1: <code>sortBy</code></li>
<li>HOMEWORK WordCount 2: <code>dbutils.fs</code>
<ul>
<li>for databricks environment only; won't work in zeppelin which may use hdfs or local file system or another</li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<h5 id="homework-wordcount-1-sortby"><a class="header" href="#homework-wordcount-1-sortby">HOMEWORK WordCount 1. <code>sortBy</code></a></h5>
<p>Let's understand <code>sortBy</code> a bit more carefully.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val example = &quot;Master, Master!   It's me, Sméagol... mhrhm*%* But they took away our precious, they wronged us. Gollum will protect us..., Master, it's me Sméagol.&quot;
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>example: String = Master, Master!   It's me, Sméagol... mhrhm*%* But they took away our precious, they wronged us. Gollum will protect us..., Master, it's me Sméagol.
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val sou = sc.textFile(&quot;/datasets/sds/souall.txt.gz&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>sou: org.apache.spark.rdd.RDD[String] = /datasets/sds/souall.txt.gz MapPartitionsRDD[32] at textFile at command-4088905069026050:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rddWords = sou
    .flatMap(line =&gt; 
         line.replaceAll(&quot;\\s+&quot;, &quot; &quot;) //replace multiple whitespace characters (including space, tab, new line, etc.) with one whitespace &quot; &quot;
             .replaceAll(&quot;&quot;&quot;([,?.!:;])&quot;&quot;&quot;, &quot;&quot;) // replace the following punctions characters: , ? . ! : ; . with the empty string &quot;&quot;
             .toLowerCase() // converting to lower-case
             .split(&quot; &quot;)) // split by single whitespace
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rddWords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[33] at flatMap at command-4088905069026051:2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rddWords.take(10)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res27: Array[String] = Array(george, washington, &quot;&quot;, january, 8, 1790, fellow-citizens, of, the, senate)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val wordCounts = rddWords
                  .map(x =&gt; (x,1))
                  .reduceByKey(_+_)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>wordCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[35] at reduceByKey at command-4088905069026053:3
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val top10 = wordCounts.sortBy(_._2, false).take(10)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>top10: Array[(String, Int)] = Array((the,150091), (of,97090), (and,60810), (to,60725), (in,38660), (a,27935), (that,21657), (for,19014), (be,18727), (our,17440))
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Make your code easy to read for other developers ;)
Use 'case classes' with well defined variable names that everyone can understand</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val top10 = wordCounts.sortBy(
  {
  case (word, count) =&gt; count
  }, false) // here false says the order is not ascending
.take(10)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>top10: Array[(String, Int)] = Array((the,150091), (of,97090), (and,60810), (to,60725), (in,38660), (a,27935), (that,21657), (for,19014), (be,18727), (our,17440))
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>If you just want a total count of all words in the file</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rddWords.count
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res28: Long = 1778012
</code></pre>
</div>
</div>
<div class="cell markdown">
<h5 id="youttry-homework-wordcount-2-dbutilsfs"><a class="header" href="#youttry-homework-wordcount-2-dbutilsfs">YoutTry: HOMEWORK WordCount 2: <code>dbutils.fs</code></a></h5>
<p>Have a brief look at what other commands dbutils.fs supports. We will introduce them as needed in databricks</p>
<ul>
<li>This is for databricks environment only; <code>dbutils.fs</code> won't work in zeppelin which may use hdfs or local file system or another</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">dbutils.fs.help // some of these were used to ETL this data into /datasets/sds/sou 
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<div class = "ansiout"><b>dbutils.fs</b> provides utilities for working with FileSystems. Most methods in
this package can take either a DBFS path (e.g., "/foo" or "dbfs:/foo"), or
another FileSystem URI.
<p>For more info about a method, use <b>dbutils.fs.help(&quot;methodName&quot;)</b>.</p>
<p>In notebooks, you can also use the %fs shorthand to access DBFS. The %fs shorthand maps
straightforwardly onto dbutils calls. For example, &quot;%fs head --maxBytes=10000 /file/path&quot;
translates into &quot;dbutils.fs.head(&quot;/file/path&quot;, maxBytes = 10000)&quot;.
<h3 id="mount"><a class="header" href="#mount">mount</a></h3><b>mount(source: String, mountPoint: String, encryptionType: String = &quot;&quot;, owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -&gt; Mounts the given source directory into DBFS at the given mount point<br /><b>mounts: Seq</b> -&gt; Displays information about what is mounted within DBFS<br /><b>refreshMounts: boolean</b> -&gt; Forces all machines in this cluster to refresh their mount cache, ensuring they receive the most recent information<br /><b>unmount(mountPoint: String): boolean</b> -&gt; Deletes a DBFS mount point<br /><b>updateMount(source: String, mountPoint: String, encryptionType: String = &quot;&quot;, owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -&gt; Similar to mount(), but updates an existing mount point (if present) instead of creating a new one<br /><br /><h3 id="fsutils"><a class="header" href="#fsutils">fsutils</a></h3><b>cp(from: String, to: String, recurse: boolean = false): boolean</b> -&gt; Copies a file or directory, possibly across FileSystems<br /><b>head(file: String, maxBytes: int = 65536): String</b> -&gt; Returns up to the first 'maxBytes' bytes of the given file as a String encoded in UTF-8<br /><b>ls(dir: String): Seq</b> -&gt; Lists the contents of a directory<br /><b>mkdirs(dir: String): boolean</b> -&gt; Creates the given directory if it does not exist, also creating any necessary parent directories<br /><b>mv(from: String, to: String, recurse: boolean = false): boolean</b> -&gt; Moves a file or directory, possibly across FileSystems<br /><b>put(file: String, contents: String, overwrite: boolean = false): boolean</b> -&gt; Writes the given String out to a file, encoded in UTF-8<br /><b>rm(dir: String, recurse: boolean = false): boolean</b> -&gt; Removes a file or directory<br /><br /></div></p>
</div>
</div>
<div class="cell markdown">
<p>Feel free to explore syntax by writing your own codes in your notebooks.</p>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="piped-rdds-and-more"><a class="header" href="#piped-rdds-and-more">Piped RDDs and More</a></h1>
<p>This is a very useful but <em>Advanced Topic</em>.</p>
<p>This notebooks, demonstrates how you can go through Chapters of the recommended book <em>Spark: The Definitive Guide</em> and get your hands dirty with the codes in it to learn by coding, in addition to reading.</p>
</div>
<div class="cell markdown">
<p>Here we will first take excerpts with minor modifications from the end of <strong>Chapter 12. Resilient Distributed Datasets (RDDs)</strong> of <em>Spark: The Definitive Guide</em>:</p>
<ul>
<li>https://learning.oreilly.com/library/view/spark-the-definitive/9781491912201/ch12.html</li>
</ul>
<p>Next, we will do Bayesian AB Testing using PipedRDDs.</p>
</div>
<div class="cell markdown">
<p>First, we create the toy RDDs as in <em>The Definitive Guide</em>:</p>
<blockquote>
<p><strong>From a Local Collection</strong></p>
</blockquote>
<blockquote>
<p>To create an RDD from a collection, you will need to use the parallelize method on a SparkContext (within a SparkSession). This turns a single node collection into a parallel collection. When creating this parallel collection, you can also explicitly state the number of partitions into which you would like to distribute this array. In this case, we are creating two partitions:</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val myCollection = &quot;Spark The Definitive Guide : Big Data Processing Made Simple&quot;  .split(&quot; &quot;)
val words = spark.sparkContext.parallelize(myCollection, 2)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>myCollection: Array[String] = Array(Spark, The, Definitive, Guide, :, Big, Data, Processing, Made, Simple)
words: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at command-4088905069025460:2
</code></pre>
</div>
</div>
<div class="cell markdown">
<blockquote>
<p><strong>glom</strong> from <em>The Definitive Guide</em></p>
</blockquote>
<blockquote>
<p><code>glom</code> is an interesting function that takes every partition in your dataset and converts them to arrays. This can be useful if you’re going to collect the data to the driver and want to have an array for each partition. However, this can cause serious stability issues because if you have large partitions or a large number of partitions, it’s simple to crash the driver.</p>
</blockquote>
</div>
<div class="cell markdown">
<p>Let's use <code>glom</code> to see how our <code>words</code> are distributed among the two partitions we used explicitly.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">words.glom.collect 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res1: Array[Array[String]] = Array(Array(Spark, The, Definitive, Guide, :), Array(Big, Data, Processing, Made, Simple))
</code></pre>
</div>
</div>
<div class="cell markdown">
<blockquote>
<p><strong>Checkpointing</strong> from <em>The Definitive Guide</em></p>
</blockquote>
<blockquote>
<p>One feature not available in the DataFrame API is the concept of checkpointing. Checkpointing is the act of saving an RDD to disk so that future references to this RDD point to those intermediate partitions on disk rather than recomputing the RDD from its original source. This is similar to caching except that it’s not stored in memory, only disk. This can be helpful when performing iterative computation, similar to the use cases for caching:</p>
</blockquote>
<p>Let's create a directory for checkpointing of RDDs in the sequel.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-fs">mkdirs /datasets/sds/ScaDaMaLe/checkpointing/
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res2: Boolean = true
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">spark.sparkContext.setCheckpointDir(&quot;/datasets/sds/ScaDaMaLe/checkpointing&quot;)
words.checkpoint()
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<p>Now, when we reference this RDD, it will derive from the checkpoint instead of the source data. This can be a helpful optimization.</p>
</blockquote>
</div>
<div class="cell markdown">
<h2 id="youtry"><a class="header" href="#youtry">YouTry</a></h2>
<p>Just some more words in <code>haha_words</code> with <code>\n</code>, the End-Of-Line (EOL) characters, in-place.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val haha_words = sc.parallelize(Seq(&quot;ha\nha&quot;, &quot;he\nhe\nhe&quot;, &quot;ho\nho\nho\nho&quot;),3)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>haha_words: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[2] at parallelize at command-4088905069025471:1
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let's use <code>glom</code> to see how our <code>haha_words</code> are distributed among the partitions</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">haha_words.glom.collect
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res4: Array[Array[String]] =
Array(Array(ha
ha), Array(he
he
he), Array(ho
ho
ho
ho))
</code></pre>
</div>
</div>
<div class="cell markdown">
<blockquote>
<h1 id="pipe-rdds-to-system-commands"><a class="header" href="#pipe-rdds-to-system-commands">Pipe RDDs to System Commands</a></h1>
</blockquote>
<blockquote>
<p>The pipe method is probably one of Spark’s more interesting methods. With pipe, you can return an RDD created by piping elements to a forked external process. The resulting RDD is computed by executing the given process once per partition. All elements of each input partition are written to a process’s stdin as lines of input separated by a newline. The resulting partition consists of the process’s stdout output, with each line of stdout resulting in one element of the output partition. A process is invoked even for empty partitions.</p>
</blockquote>
<blockquote>
<p>The print behavior can be customized by providing two functions.</p>
</blockquote>
<p>We can use a simple example and pipe each partition to the command wc. Each row will be passed in as a new line, so if we perform a line count, we will get the number of lines, one per partition:</p>
</div>
<div class="cell markdown">
<p>The following produces a <code>PipedRDD</code>:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val wc_l_PipedRDD = words.pipe(&quot;wc -l&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>wc_l_PipedRDD: org.apache.spark.rdd.RDD[String] = PipedRDD[4] at pipe at command-4088905069025476:1
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Now, we take an action via <code>collect</code> to bring the results to the Driver.</p>
<p>NOTE: Be careful what you collect! You can always write the output to parquet of binary files in <code>dbfs:///</code> if the returned output is large.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">wc_l_PipedRDD.collect
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res5: Array[String] = Array(5, 5)
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>In this case, we got the number of lines returned by <code>wc -l</code> per partition.</p>
</div>
<div class="cell markdown">
<h2 id="youtry-1"><a class="header" href="#youtry-1">YouTry</a></h2>
<p>Try to make sense of the next few cells where we do NOT specifiy the number of partitions explicitly and let Spark decide on the number of partitions automatically.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val haha_words = sc.parallelize(Seq(&quot;ha\nha&quot;, &quot;he\nhe\nhe&quot;, &quot;ho\nho\nho\nho&quot;),3)
haha_words.glom.collect
val wc_l_PipedRDD_haha_words = haha_words.pipe(&quot;wc -l&quot;)
wc_l_PipedRDD_haha_words.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>haha_words: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[5] at parallelize at command-4088905069025483:1
wc_l_PipedRDD_haha_words: org.apache.spark.rdd.RDD[String] = PipedRDD[7] at pipe at command-4088905069025483:3
res6: Array[String] = Array(2, 3, 4)
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Do you understand why the above <code>collect</code> statement returns what it does?</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val haha_words_again = sc.parallelize(Seq(&quot;ha\nha&quot;, &quot;he\nhe\nhe&quot;, &quot;ho\nho\nho\nho&quot;))
haha_words_again.glom.collect
val wc_l_PipedRDD_haha_words_again = haha_words_again.pipe(&quot;wc -l&quot;)
wc_l_PipedRDD_haha_words_again.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>haha_words_again: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[8] at parallelize at command-4088905069025485:1
wc_l_PipedRDD_haha_words_again: org.apache.spark.rdd.RDD[String] = PipedRDD[10] at pipe at command-4088905069025485:3
res7: Array[String] = Array(2, 7)
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Did you understand why some of the results are <code>0</code> in the last <code>collect</code> statement?</p>
</div>
<div class="cell markdown">
<blockquote>
<h1 id="mappartitions"><a class="header" href="#mappartitions">mapPartitions</a></h1>
</blockquote>
<blockquote>
<p>The previous command revealed that Spark operates on a per-partition basis when it comes to actually executing code. You also might have noticed earlier that the return signature of a map function on an RDD is actually <code>MapPartitionsRDD</code>.</p>
</blockquote>
<p>Or <code>ParallelCollectionRDD</code> in our case.</p>
<blockquote>
<p>This is because map is just a row-wise alias for <code>mapPartitions</code>, which makes it possible for you to map an individual partition (represented as an iterator). That’s because physically on the cluster we operate on each partition individually (and not a specific row). A simple example creates the value “1” for every partition in our data, and the sum of the following expression will count the number of partitions we have:</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">words.mapPartitions(part =&gt; Iterator[Int](1)).sum() // 2.0
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res8: Double = 2.0
</code></pre>
</div>
</div>
<div class="cell markdown">
<blockquote>
<p>Naturally, this means that we operate on a per-partition basis and therefore it allows us to perform an operation on that <em>entire</em> partition. This is valuable for performing something on an entire subdataset of your RDD. You can gather all values of a partition class or group into one partition and then operate on that entire group using arbitrary functions and controls. An example use case of this would be that you could pipe this through some custom machine learning algorithm and train an individual model for that company’s portion of the dataset. A Facebook engineer has an interesting demonstration of their particular implementation of the pipe operator with a similar use case demonstrated at <a href="https://spark-summit.org/east-2017/events/experiences-with-sparks-rdd-apis-for-complex-custom-applications/">Spark Summit East 2017</a>.</p>
</blockquote>
<blockquote>
<p>Other functions similar to <code>mapPartitions</code> include <code>mapPartitionsWithIndex</code>. With this you specify a function that accepts an index (within the partition) and an iterator that goes through all items within the partition. The partition index is the partition number in your RDD, which identifies where each record in our dataset sits (and potentially allows you to debug). You might use this to test whether your map functions are behaving correctly:</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def indexedFunc(partitionIndex:Int, withinPartIterator: Iterator[String]) = {  withinPartIterator.toList.map(    
  value =&gt; s&quot;Partition: $partitionIndex =&gt; $value&quot;).iterator
                                                                            }
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>indexedFunc: (partitionIndex: Int, withinPartIterator: Iterator[String])Iterator[String]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">words.mapPartitionsWithIndex(indexedFunc).collect() // let's call our indexed function
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res9: Array[String] = Array(Partition: 0 =&gt; Spark, Partition: 0 =&gt; The, Partition: 0 =&gt; Definitive, Partition: 0 =&gt; Guide, Partition: 0 =&gt; :, Partition: 1 =&gt; Big, Partition: 1 =&gt; Data, Partition: 1 =&gt; Processing, Partition: 1 =&gt; Made, Partition: 1 =&gt; Simple)
</code></pre>
</div>
</div>
<div class="cell markdown">
<blockquote>
<h1 id="foreachpartition"><a class="header" href="#foreachpartition">foreachPartition</a></h1>
</blockquote>
<blockquote>
<p>Although <code>mapPartitions</code> needs a return value to work properly, this next function does not. <code>foreachPartition</code> simply iterates over all the partitions of the data. The difference is that the function has no return value. This makes it great for doing something with each partition like writing it out to a database. In fact, this is how many data source connectors are written. You can create</p>
</blockquote>
<p>your</p>
<blockquote>
<p>own text file source if you want by specifying outputs to the temp directory with a random ID:</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">words.foreachPartition { iter =&gt;  
  import java.io._  
  import scala.util.Random  
  val randomFileName = new Random().nextInt()  
  val pw = new PrintWriter(new File(s&quot;/tmp/random-file-${randomFileName}.txt&quot;))  
  while (iter.hasNext) {
    pw.write(iter.next())  
  }  
  pw.close()
}
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<p>You’ll find these two files if you scan your /tmp directory.</p>
</blockquote>
<p>You need to scan for the file across all the nodes. As the file may not be in the Driver node's <code>/tmp/</code> directory but in those of the executors that hosted the partition.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">pwd
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div style="break-before: page; page-break-before: always;"></div><h1 id="editors"><a class="header" href="#editors">Editors</a></h1>
<p>Here is a list of the editors who have helped improve this book</p>
<ul>
<li><a href="https://www.linkedin.com/in/raazesh-sainudiin-45955845/">Raazesh Sainudiin</a></li>
<li><a href="https://github.com/kTorp">Kristoffer Torp</a></li>
<li><a href="https://www.linkedin.com/in/oskar-%C3%A5sbrink-847a76231/">Oskar Åsbrink</a></li>
<li><a href="https://www.linkedin.com/in/tilo-wiklund-682aa496/">Tilo Wiklund</a></li>
<li><a href="https://www.linkedin.com/in/dan-lilja-a2ab8096/">Dan Lilja</a></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->

        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </body>
</html>
