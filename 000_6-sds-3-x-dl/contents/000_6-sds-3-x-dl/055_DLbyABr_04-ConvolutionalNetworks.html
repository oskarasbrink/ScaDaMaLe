<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>055_DLbyABr_04-ConvolutionalNetworks - sds-3.x/ScaDaMaLe</title>


        <!-- Custom HTML head -->

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../../favicon.svg">
        <link rel="shortcut icon" href="../../favicon.png">
        <link rel="stylesheet" href="../../css/variables.css">
        <link rel="stylesheet" href="../../css/general.css">
        <link rel="stylesheet" href="../../css/chrome.css">
        <link rel="stylesheet" href="../../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../../highlight.css">
        <link rel="stylesheet" href="../../tomorrow-night.css">
        <link rel="stylesheet" href="../../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../../scroll-mdbook-outputs.css">

        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../../contents/000_6-sds-3-x-dl/000_6-sds-3-x-dl.html">000_6-sds-3-x-dl</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_6-sds-3-x-dl/049_DeepLearningIntro.html">049_DeepLearningIntro</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_6-sds-3-x-dl/050_DLbyABr_01-Intro.html">050_DLbyABr_01-Intro</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_6-sds-3-x-dl/051_DLbyABr_02-Neural-Networks.html">051_DLbyABr_02-Neural-Networks</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_6-sds-3-x-dl/052_DLbyABr_02a-Keras-DFFN.html">052_DLbyABr_02a-Keras-DFFN</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_6-sds-3-x-dl/053_DLbyABr_03-HelloTensorFlow.html">053_DLbyABr_03-HelloTensorFlow</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_6-sds-3-x-dl/054_DLbyABr_03a-BatchTensorFlowWithMatrices.html">054_DLbyABr_03a-BatchTensorFlowWithMatrices</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_6-sds-3-x-dl/055_DLbyABr_04-ConvolutionalNetworks.html" class="active">055_DLbyABr_04-ConvolutionalNetworks</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_6-sds-3-x-dl/056_DLbyABr_04a-Hands-On-MNIST-MLP.html">056_DLbyABr_04a-Hands-On-MNIST-MLP</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_6-sds-3-x-dl/057_DLbyABr_04b-Hands-On-MNIST-CNN.html">057_DLbyABr_04b-Hands-On-MNIST-CNN</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_6-sds-3-x-dl/058_DLbyABr_04c-CIFAR-10.html">058_DLbyABr_04c-CIFAR-10</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_6-sds-3-x-dl/059_DLbyABr_05-RecurrentNetworks.html">059_DLbyABr_05-RecurrentNetworks</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_6-sds-3-x-dl/060_DLByABr_05a-LSTM-Solution.html">060_DLByABr_05a-LSTM-Solution</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_6-sds-3-x-dl/061_DLByABr_05b-LSTM-Language.html">061_DLByABr_05b-LSTM-Language</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_6-sds-3-x-dl/062_DLbyABr_06-GenerativeNetworks.html">062_DLbyABr_06-GenerativeNetworks</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_6-sds-3-x-dl/063_DLbyABr_07-ReinforcementLearning.html">063_DLbyABr_07-ReinforcementLearning</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_6-sds-3-x-dl/064_DLbyABr_08-Operations.html">064_DLbyABr_08-Operations</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_6-sds-3-x-dl/064x_MLOps_with_Pytorch_and_MLflow_for_Image_Classification.html">064x_MLOps_with_Pytorch_and_MLflow_for_Image_Classification</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">sds-3.x/ScaDaMaLe</h1>

                    <div class="right-buttons">
                        <a href="../../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
<p>This is a 2019-2021 augmentation and update of <a href="https://www.linkedin.com/in/adbreind">Adam Breindel</a>'s initial notebooks.</p>
<p><em>Thanks to <a href="https://www.linkedin.com/in/christianvonkoch/">Christian von Koch</a> and <a href="https://www.linkedin.com/in/william-anz%C3%A9n-b52003199/">William Anzén</a> for their contributions towards making these materials Spark 3.0.1 and Python 3+ compliant.</em></p>
</div>
<div class="cell markdown">
<h1 id="convolutional-neural-networks"><a class="header" href="#convolutional-neural-networks">Convolutional Neural Networks</a></h1>
<h2 id="aka-cnn-convnet"><a class="header" href="#aka-cnn-convnet">aka CNN, ConvNet</a></h2>
</div>
<div class="cell markdown">
<p>As a baseline, let's start a lab running with what we already know.</p>
<p>We'll take our deep feed-forward multilayer perceptron network, with ReLU activations and reasonable initializations, and apply it to learning the MNIST digits.</p>
<p>The main part of the code looks like the following (full code you can run is in the next cell):</p>
<pre><code># imports, setup, load data sets

model = Sequential()
model.add(Dense(20, input_dim=784, kernel_initializer='normal', activation='relu'))
model.add(Dense(15, kernel_initializer='normal', activation='relu'))
model.add(Dense(10, kernel_initializer='normal', activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])

categorical_labels = to_categorical(y_train, num_classes=10)

history = model.fit(X_train, categorical_labels, epochs=100, batch_size=100)

# print metrics, plot errors
</code></pre>
<p>Note the changes, which are largely about building a classifier instead of a regression model: * Output layer has one neuron per category, with softmax activation * <strong>Loss function is cross-entropy loss</strong> * Accuracy metric is categorical accuracy</p>
</div>
<div class="cell markdown">
<p>Let's hold pointers into wikipedia for these new concepts.</p>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression"
 width="95%" height="500"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://en.wikipedia.org/wiki/Softmax_function"
 width="95%" height="380"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<p>The following is from: <a href="https://www.quora.com/How-does-Keras-calculate-accuracy">https://www.quora.com/How-does-Keras-calculate-accuracy</a>.</p>
<p><strong>Categorical accuracy:</strong></p>
<pre><code class="language-%python">def categorical_accuracy(y_true, y_pred):
 return K.cast(K.equal(K.argmax(y_true, axis=-1),
 K.argmax(y_pred, axis=-1)),
 K.floatx())
</code></pre>
<blockquote>
<p><code>K.argmax(y_true)</code> takes the highest value to be the prediction and matches against the comparative set.</p>
</blockquote>
</div>
<div class="cell markdown">
<p>Watch (1:39) * <a href="https://www.youtube.com/watch?v=tRsSi_sqXjI"><img src="http://img.youtube.com/vi/tRsSi_sqXjI/0.jpg" alt="Udacity: Deep Learning by Vincent Vanhoucke - Cross-entropy" /></a></p>
<p>Watch (1:54) * <a href="https://www.youtube.com/watch?v=x449QQDhMDE"><img src="http://img.youtube.com/vi/x449QQDhMDE/0.jpg" alt="Udacity: Deep Learning by Vincent Vanhoucke - Minimizing Cross-entropy" /></a></p>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-python">from keras.models import Sequential
from keras.layers import Dense
from keras.utils import to_categorical
import sklearn.datasets
import datetime
import matplotlib.pyplot as plt
import numpy as np

train_libsvm = &quot;/dbfs/databricks-datasets/mnist-digits/data-001/mnist-digits-train.txt&quot;
test_libsvm = &quot;/dbfs/databricks-datasets/mnist-digits/data-001/mnist-digits-test.txt&quot;

X_train, y_train = sklearn.datasets.load_svmlight_file(train_libsvm, n_features=784)
X_train = X_train.toarray()

X_test, y_test = sklearn.datasets.load_svmlight_file(test_libsvm, n_features=784)
X_test = X_test.toarray()

model = Sequential()
model.add(Dense(20, input_dim=784, kernel_initializer='normal', activation='relu'))
model.add(Dense(15, kernel_initializer='normal', activation='relu'))
model.add(Dense(10, kernel_initializer='normal', activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])

categorical_labels = to_categorical(y_train, num_classes=10)
start = datetime.datetime.today()

history = model.fit(X_train, categorical_labels, epochs=40, batch_size=100, validation_split=0.1, verbose=2)

scores = model.evaluate(X_test, to_categorical(y_test, num_classes=10))

print
for i in range(len(model.metrics_names)):
	print(&quot;%s: %f&quot; % (model.metrics_names[i], scores[i]))

print (&quot;Start: &quot; + str(start))
end = datetime.datetime.today()
print (&quot;End: &quot; + str(end))
print (&quot;Elapse: &quot; + str(end-start))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Using TensorFlow backend.
WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
Train on 54000 samples, validate on 6000 samples
Epoch 1/40
 - 2s - loss: 0.5150 - categorical_accuracy: 0.8445 - val_loss: 0.2426 - val_categorical_accuracy: 0.9282
Epoch 2/40
 - 2s - loss: 0.2352 - categorical_accuracy: 0.9317 - val_loss: 0.1763 - val_categorical_accuracy: 0.9487
Epoch 3/40
 - 2s - loss: 0.1860 - categorical_accuracy: 0.9454 - val_loss: 0.1526 - val_categorical_accuracy: 0.9605
Epoch 4/40
 - 1s - loss: 0.1602 - categorical_accuracy: 0.9527 - val_loss: 0.1600 - val_categorical_accuracy: 0.9573
Epoch 5/40
 - 2s - loss: 0.1421 - categorical_accuracy: 0.9575 - val_loss: 0.1464 - val_categorical_accuracy: 0.9590
Epoch 6/40
 - 1s - loss: 0.1277 - categorical_accuracy: 0.9611 - val_loss: 0.1626 - val_categorical_accuracy: 0.9568
Epoch 7/40
 - 1s - loss: 0.1216 - categorical_accuracy: 0.9610 - val_loss: 0.1263 - val_categorical_accuracy: 0.9665
Epoch 8/40
 - 2s - loss: 0.1136 - categorical_accuracy: 0.9656 - val_loss: 0.1392 - val_categorical_accuracy: 0.9627
Epoch 9/40
 - 2s - loss: 0.1110 - categorical_accuracy: 0.9659 - val_loss: 0.1306 - val_categorical_accuracy: 0.9632
Epoch 10/40
 - 1s - loss: 0.1056 - categorical_accuracy: 0.9678 - val_loss: 0.1298 - val_categorical_accuracy: 0.9643
Epoch 11/40
 - 1s - loss: 0.1010 - categorical_accuracy: 0.9685 - val_loss: 0.1523 - val_categorical_accuracy: 0.9583
Epoch 12/40
 - 2s - loss: 0.1003 - categorical_accuracy: 0.9692 - val_loss: 0.1540 - val_categorical_accuracy: 0.9583
Epoch 13/40
 - 1s - loss: 0.0929 - categorical_accuracy: 0.9713 - val_loss: 0.1348 - val_categorical_accuracy: 0.9647
Epoch 14/40
 - 1s - loss: 0.0898 - categorical_accuracy: 0.9716 - val_loss: 0.1476 - val_categorical_accuracy: 0.9628
Epoch 15/40
 - 1s - loss: 0.0885 - categorical_accuracy: 0.9722 - val_loss: 0.1465 - val_categorical_accuracy: 0.9633
Epoch 16/40
 - 2s - loss: 0.0840 - categorical_accuracy: 0.9736 - val_loss: 0.1584 - val_categorical_accuracy: 0.9623
Epoch 17/40
 - 2s - loss: 0.0844 - categorical_accuracy: 0.9730 - val_loss: 0.1530 - val_categorical_accuracy: 0.9598
Epoch 18/40
 - 2s - loss: 0.0828 - categorical_accuracy: 0.9739 - val_loss: 0.1395 - val_categorical_accuracy: 0.9662
Epoch 19/40
 - 2s - loss: 0.0782 - categorical_accuracy: 0.9755 - val_loss: 0.1640 - val_categorical_accuracy: 0.9623
Epoch 20/40
 - 1s - loss: 0.0770 - categorical_accuracy: 0.9760 - val_loss: 0.1638 - val_categorical_accuracy: 0.9568
Epoch 21/40
 - 1s - loss: 0.0754 - categorical_accuracy: 0.9763 - val_loss: 0.1773 - val_categorical_accuracy: 0.9608
Epoch 22/40
 - 2s - loss: 0.0742 - categorical_accuracy: 0.9769 - val_loss: 0.1767 - val_categorical_accuracy: 0.9603
Epoch 23/40
 - 2s - loss: 0.0762 - categorical_accuracy: 0.9762 - val_loss: 0.1623 - val_categorical_accuracy: 0.9597
Epoch 24/40
 - 1s - loss: 0.0724 - categorical_accuracy: 0.9772 - val_loss: 0.1647 - val_categorical_accuracy: 0.9635
Epoch 25/40
 - 1s - loss: 0.0701 - categorical_accuracy: 0.9781 - val_loss: 0.1705 - val_categorical_accuracy: 0.9623
Epoch 26/40
 - 2s - loss: 0.0702 - categorical_accuracy: 0.9777 - val_loss: 0.1673 - val_categorical_accuracy: 0.9658
Epoch 27/40
 - 2s - loss: 0.0682 - categorical_accuracy: 0.9788 - val_loss: 0.1841 - val_categorical_accuracy: 0.9607
Epoch 28/40
 - 2s - loss: 0.0684 - categorical_accuracy: 0.9790 - val_loss: 0.1738 - val_categorical_accuracy: 0.9623
Epoch 29/40
 - 2s - loss: 0.0670 - categorical_accuracy: 0.9786 - val_loss: 0.1880 - val_categorical_accuracy: 0.9610
Epoch 30/40
 - 2s - loss: 0.0650 - categorical_accuracy: 0.9790 - val_loss: 0.1765 - val_categorical_accuracy: 0.9650
Epoch 31/40
 - 2s - loss: 0.0639 - categorical_accuracy: 0.9793 - val_loss: 0.1774 - val_categorical_accuracy: 0.9602
Epoch 32/40
 - 2s - loss: 0.0660 - categorical_accuracy: 0.9791 - val_loss: 0.1885 - val_categorical_accuracy: 0.9622
Epoch 33/40
 - 1s - loss: 0.0636 - categorical_accuracy: 0.9795 - val_loss: 0.1928 - val_categorical_accuracy: 0.9595
Epoch 34/40
 - 1s - loss: 0.0597 - categorical_accuracy: 0.9805 - val_loss: 0.1948 - val_categorical_accuracy: 0.9593
Epoch 35/40
 - 2s - loss: 0.0631 - categorical_accuracy: 0.9797 - val_loss: 0.2019 - val_categorical_accuracy: 0.9563
Epoch 36/40
 - 2s - loss: 0.0600 - categorical_accuracy: 0.9812 - val_loss: 0.1852 - val_categorical_accuracy: 0.9595
Epoch 37/40
 - 2s - loss: 0.0597 - categorical_accuracy: 0.9812 - val_loss: 0.1794 - val_categorical_accuracy: 0.9637
Epoch 38/40
 - 2s - loss: 0.0588 - categorical_accuracy: 0.9812 - val_loss: 0.1933 - val_categorical_accuracy: 0.9625
Epoch 39/40
 - 2s - loss: 0.0629 - categorical_accuracy: 0.9805 - val_loss: 0.2177 - val_categorical_accuracy: 0.9582
Epoch 40/40
 - 2s - loss: 0.0559 - categorical_accuracy: 0.9828 - val_loss: 0.1875 - val_categorical_accuracy: 0.9642

   32/10000 [..............................] - ETA: 0s
 1568/10000 [===&gt;..........................] - ETA: 0s
 3072/10000 [========&gt;.....................] - ETA: 0s
 4672/10000 [=============&gt;................] - ETA: 0s
 6496/10000 [==================&gt;...........] - ETA: 0s
 8192/10000 [=======================&gt;......] - ETA: 0s
10000/10000 [==============================] - 0s 30us/step
loss: 0.227984
categorical_accuracy: 0.954900
Start: 2021-02-10 10:20:06.310772
End: 2021-02-10 10:21:10.141213
Elapse: 0:01:03.830441
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>after about a minute we have:</p>
<pre><code>...

Epoch 40/40
1s - loss: 0.0610 - categorical_accuracy: 0.9809 - val_loss: 0.1918 - val_categorical_accuracy: 0.9583

...
 
loss: 0.216120

categorical_accuracy: 0.955000

Start: 2017-12-06 07:35:33.948102

End: 2017-12-06 07:36:27.046130

Elapse: 0:00:53.098028
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">import matplotlib.pyplot as plt

fig, ax = plt.subplots()
fig.set_size_inches((5,5))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
display(fig)
</code></pre>
</div>
<div class="cell markdown">
<p>What are the big takeaways from this experiment?</p>
<ol>
<li>We get pretty impressive &quot;apparent error&quot; accuracy right from the start! A small network gets us to training accuracy 97% by epoch 20</li>
<li>The model <em>appears</em> to continue to learn if we let it run, although it does slow down and oscillate a bit.</li>
<li>Our test accuracy is about 95% after 5 epochs and never gets better ... it gets worse!</li>
<li>Therefore, we are overfitting very quickly... most of the &quot;training&quot; turns out to be a waste.</li>
<li>For what it's worth, we get 95% accuracy without much work.</li>
</ol>
<p>This is not terrible compared to other, non-neural-network approaches to the problem. After all, we could probably tweak this a bit and do even better.</p>
<p>But we talked about using deep learning to solve &quot;95%&quot; problems or &quot;98%&quot; problems ... where one error in 20, or 50 simply won't work. If we can get to &quot;multiple nines&quot; of accuracy, then we can do things like automate mail sorting and translation, create cars that react properly (all the time) to street signs, and control systems for robots or drones that function autonomously.</p>
<p>Try two more experiments (try them separately): 1. Add a third, hidden layer. 2. Increase the size of the hidden layers.</p>
<p>Adding another layer slows things down a little (why?) but doesn't seem to make a difference in accuracy.</p>
<p>Adding a lot more neurons into the first topology slows things down significantly -- 10x as many neurons, and only a marginal increase in accuracy. Notice also (in the plot) that the learning clearly degrades after epoch 50 or so.</p>
<p>... We need a new approach!</p>
<hr />
<p>... let's think about this:</p>
<h3 id="what-is-layer-2-learning-from-layer-1-combinations-of-pixels"><a class="header" href="#what-is-layer-2-learning-from-layer-1-combinations-of-pixels">What is layer 2 learning from layer 1? Combinations of pixels</a></h3>
<h4 id="combinations-of-pixels-contain-information-but"><a class="header" href="#combinations-of-pixels-contain-information-but">Combinations of pixels contain information but...</a></h4>
<p>There are a lot of them (combinations) and they are &quot;fragile&quot;</p>
<p>In fact, in our last experiment, we basically built a model that memorizes a bunch of &quot;magic&quot; pixel combinations.</p>
<p>What might be a better way to build features?</p>
<ul>
<li>When humans perform this task, we look not at arbitrary pixel combinations, but certain geometric patterns -- lines, curves, loops.</li>
<li>These features are made up of combinations of pixels, but they are far from arbitrary</li>
<li>We identify these features regardless of translation, rotation, etc.</li>
</ul>
<p>Is there a way to get the network to do the same thing?</p>
<p>I.e., in layer one, identify pixels. Then in layer 2+, identify abstractions over pixels that are translation-invariant 2-D shapes?</p>
<p>We could look at where a &quot;filter&quot; that represents one of these features (e.g., and edge) matches the image.</p>
<p>How would this work?</p>
<h3 id="convolution"><a class="header" href="#convolution">Convolution</a></h3>
<p>Convolution in the general mathematical sense is define as follows:</p>
<img src="https://i.imgur.com/lurC2Cx.png" width=300>
<p>The convolution we deal with in deep learning is a simplified case. We want to compare two signals. Here are two visualizations, courtesy of Wikipedia, that help communicate how convolution emphasizes features:</p>
<img src="http://i.imgur.com/EDCaMl2.png" width=500>
<hr />
<h4 id="heres-an-animation-where-we-change-tau"><a class="header" href="#heres-an-animation-where-we-change-tau">Here's an animation (where we change \({\tau}\))</a></h4>
<img src="http://i.imgur.com/0BFcnaw.gif">
<p><strong>In one sense, the convolution captures and quantifies the pattern matching over space</strong></p>
<p>If we perform this in two dimensions, we can achieve effects like highlighting edges:</p>
<img src="http://i.imgur.com/DKEXIII.png">
<p>The matrix here, also called a convolution kernel, is one of the functions we are convolving. Other convolution kernels can blur, &quot;sharpen,&quot; etc.</p>
<h3 id="so-well-drop-in-a-number-of-convolution-kernels-and-the-network-will-learn-where-to-use-them-nope-better-than-that"><a class="header" href="#so-well-drop-in-a-number-of-convolution-kernels-and-the-network-will-learn-where-to-use-them-nope-better-than-that">So we'll drop in a number of convolution kernels, and the network will learn where to use them? Nope. Better than that.</a></h3>
<h2 id="well-program-in-the-idea-of-discrete-convolution-and-the-network-will-learn-what-kernels-extract-meaningful-features"><a class="header" href="#well-program-in-the-idea-of-discrete-convolution-and-the-network-will-learn-what-kernels-extract-meaningful-features">We'll program in the <em>idea</em> of discrete convolution, and the network will learn what kernels extract meaningful features!</a></h2>
<p>The values in a (fixed-size) convolution kernel matrix will be variables in our deep learning model. Although inuitively it seems like it would be hard to learn useful params, in fact, since those variables are used repeatedly across the image data, it &quot;focuses&quot; the error on a smallish number of parameters with a lot of influence -- so it should be vastly <em>less</em> expensive to train than just a huge fully connected layer like we discussed above.</p>
<p>This idea was developed in the late 1980s, and by 1989, Yann LeCun (at AT&amp;T/Bell Labs) had built a practical high-accuracy system (used in the 1990s for processing handwritten checks and mail).</p>
<p><strong>How do we hook this into our neural networks?</strong></p>
<ul>
<li>
<p>First, we can preserve the geometric properties of our data by &quot;shaping&quot; the vectors as 2D instead of 1D.</p>
</li>
<li>
<p>Then we'll create a layer whose value is not just activation applied to weighted sum of inputs, but instead it's the result of a dot-product (element-wise multiply and sum) between the kernel and a patch of the input vector (image).</p>
<ul>
<li>This value will be our &quot;pre-activation&quot; and optionally feed into an activation function (or &quot;detector&quot;)</li>
</ul>
</li>
</ul>
<img src="http://i.imgur.com/ECyi9lL.png">
<p>If we perform this operation at lots of positions over the image, we'll get lots of outputs, as many as one for every input pixel.</p>
<img src="http://i.imgur.com/WhOrJ0Y.jpg">
<ul>
<li>So we'll add another layer that &quot;picks&quot; the highest convolution pattern match from nearby pixels, which
<ul>
<li>makes our pattern match a little bit translation invariant (a fuzzy location match)</li>
<li>reduces the number of outputs significantly</li>
</ul>
</li>
<li>This layer is commonly called a pooling layer, and if we pick the &quot;maximum match&quot; then it's a &quot;max pooling&quot; layer.</li>
</ul>
<img src="http://i.imgur.com/9iPpfpb.png">
<p><strong>The end result is that the kernel or filter together with max pooling creates a value in a subsequent layer which represents the appearance of a pattern in a local area in a prior layer.</strong></p>
<p><strong>Again, the network will be given a number of &quot;slots&quot; for these filters and will learn (by minimizing error) what filter values produce meaningful features. This is the key insight into how modern image-recognition networks are able to generalize -- i.e., learn to tell 6s from 7s or cats from dogs.</strong></p>
<img src="http://i.imgur.com/F8eH3vj.png">
<h2 id="ok-lets-build-our-first-convnet"><a class="header" href="#ok-lets-build-our-first-convnet">Ok, let's build our first ConvNet:</a></h2>
<p>First, we want to explicity shape our data into a 2-D configuration. We'll end up with a 4-D tensor where the first dimension is the training examples, then each example is 28x28 pixels, and we'll explicitly say it's 1-layer deep. (Why? with color images, we typically process over 3 or 4 channels in this last dimension)</p>
<p>A step by step animation follows: * http://cs231n.github.io/assets/conv-demo/index.html</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">train_libsvm = &quot;/dbfs/databricks-datasets/mnist-digits/data-001/mnist-digits-train.txt&quot;
test_libsvm = &quot;/dbfs/databricks-datasets/mnist-digits/data-001/mnist-digits-test.txt&quot;

X_train, y_train = sklearn.datasets.load_svmlight_file(train_libsvm, n_features=784)
X_train = X_train.toarray()

X_test, y_test = sklearn.datasets.load_svmlight_file(test_libsvm, n_features=784)
X_test = X_test.toarray()

X_train = X_train.reshape( (X_train.shape[0], 28, 28, 1) )
X_train = X_train.astype('float32')
X_train /= 255
y_train = to_categorical(y_train, num_classes=10)

X_test = X_test.reshape( (X_test.shape[0], 28, 28, 1) )
X_test = X_test.astype('float32')
X_test /= 255
y_test = to_categorical(y_test, num_classes=10)
</code></pre>
</div>
<div class="cell markdown">
<p>Now the model:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D

model = Sequential()

model.add(Conv2D(8, # number of kernels 
				(4, 4), # kernel size
                padding='valid', # no padding; output will be smaller than input
                input_shape=(28, 28, 1)))

model.add(Activation('relu'))

model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Flatten())
model.add(Dense(128))
model.add(Activation('relu')) # alternative syntax for applying activation

model.add(Dense(10))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
</code></pre>
</div>
<div class="cell markdown">
<p>... and the training loop and output:</p>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-python">start = datetime.datetime.today()

history = model.fit(X_train, y_train, batch_size=128, epochs=8, verbose=2, validation_split=0.1)

scores = model.evaluate(X_test, y_test, verbose=1)

print
for i in range(len(model.metrics_names)):
	print(&quot;%s: %f&quot; % (model.metrics_names[i], scores[i]))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Train on 54000 samples, validate on 6000 samples
Epoch 1/8
 - 21s - loss: 0.3099 - acc: 0.9143 - val_loss: 0.1025 - val_acc: 0.9728
Epoch 2/8
 - 32s - loss: 0.0972 - acc: 0.9712 - val_loss: 0.0678 - val_acc: 0.9810
Epoch 3/8
 - 35s - loss: 0.0653 - acc: 0.9803 - val_loss: 0.0545 - val_acc: 0.9858
Epoch 4/8
 - 43s - loss: 0.0495 - acc: 0.9852 - val_loss: 0.0503 - val_acc: 0.9865
Epoch 5/8
 - 42s - loss: 0.0392 - acc: 0.9884 - val_loss: 0.0503 - val_acc: 0.9845
Epoch 6/8
 - 41s - loss: 0.0319 - acc: 0.9904 - val_loss: 0.0554 - val_acc: 0.9850
Epoch 7/8
 - 40s - loss: 0.0250 - acc: 0.9927 - val_loss: 0.0437 - val_acc: 0.9885
Epoch 8/8
 - 47s - loss: 0.0226 - acc: 0.9929 - val_loss: 0.0465 - val_acc: 0.9872

   32/10000 [..............................] - ETA: 5s
  160/10000 [..............................] - ETA: 5s
  288/10000 [..............................] - ETA: 4s
  416/10000 [&gt;.............................] - ETA: 4s
  544/10000 [&gt;.............................] - ETA: 4s
  672/10000 [=&gt;............................] - ETA: 4s
  800/10000 [=&gt;............................] - ETA: 4s
  928/10000 [=&gt;............................] - ETA: 4s
 1056/10000 [==&gt;...........................] - ETA: 4s
 1184/10000 [==&gt;...........................] - ETA: 4s
 1280/10000 [==&gt;...........................] - ETA: 4s
 1408/10000 [===&gt;..........................] - ETA: 4s
 1536/10000 [===&gt;..........................] - ETA: 3s
 1664/10000 [===&gt;..........................] - ETA: 3s
 1760/10000 [====&gt;.........................] - ETA: 3s
 1856/10000 [====&gt;.........................] - ETA: 3s
 1984/10000 [====&gt;.........................] - ETA: 3s
 2080/10000 [=====&gt;........................] - ETA: 3s
 2176/10000 [=====&gt;........................] - ETA: 3s
 2272/10000 [=====&gt;........................] - ETA: 3s
 2336/10000 [======&gt;.......................] - ETA: 3s
 2464/10000 [======&gt;.......................] - ETA: 3s
 2528/10000 [======&gt;.......................] - ETA: 3s
 2656/10000 [======&gt;.......................] - ETA: 3s
 2720/10000 [=======&gt;......................] - ETA: 3s
 2848/10000 [=======&gt;......................] - ETA: 3s
 2944/10000 [=======&gt;......................] - ETA: 3s
 3072/10000 [========&gt;.....................] - ETA: 3s
 3200/10000 [========&gt;.....................] - ETA: 3s
 3328/10000 [========&gt;.....................] - ETA: 3s
 3456/10000 [=========&gt;....................] - ETA: 3s
 3584/10000 [=========&gt;....................] - ETA: 3s
 3712/10000 [==========&gt;...................] - ETA: 3s
 3840/10000 [==========&gt;...................] - ETA: 3s
 3968/10000 [==========&gt;...................] - ETA: 3s
 4064/10000 [===========&gt;..................] - ETA: 3s
 4160/10000 [===========&gt;..................] - ETA: 3s
 4256/10000 [===========&gt;..................] - ETA: 3s
 4352/10000 [============&gt;.................] - ETA: 2s
 4416/10000 [============&gt;.................] - ETA: 2s
 4512/10000 [============&gt;.................] - ETA: 2s
 4576/10000 [============&gt;.................] - ETA: 2s
 4704/10000 [=============&gt;................] - ETA: 2s
 4800/10000 [=============&gt;................] - ETA: 2s
 4896/10000 [=============&gt;................] - ETA: 2s
 4928/10000 [=============&gt;................] - ETA: 2s
 5056/10000 [==============&gt;...............] - ETA: 2s
 5152/10000 [==============&gt;...............] - ETA: 2s
 5280/10000 [==============&gt;...............] - ETA: 2s
 5344/10000 [===============&gt;..............] - ETA: 2s
 5472/10000 [===============&gt;..............] - ETA: 2s
 5536/10000 [===============&gt;..............] - ETA: 2s
 5600/10000 [===============&gt;..............] - ETA: 2s
 5728/10000 [================&gt;.............] - ETA: 2s
 5792/10000 [================&gt;.............] - ETA: 2s
 5920/10000 [================&gt;.............] - ETA: 2s
 6016/10000 [=================&gt;............] - ETA: 2s
 6048/10000 [=================&gt;............] - ETA: 2s
 6144/10000 [=================&gt;............] - ETA: 2s
 6240/10000 [=================&gt;............] - ETA: 2s
 6336/10000 [==================&gt;...........] - ETA: 2s
 6464/10000 [==================&gt;...........] - ETA: 2s
 6592/10000 [==================&gt;...........] - ETA: 2s
 6720/10000 [===================&gt;..........] - ETA: 1s
 6816/10000 [===================&gt;..........] - ETA: 1s
 6944/10000 [===================&gt;..........] - ETA: 1s
 7040/10000 [====================&gt;.........] - ETA: 1s
 7168/10000 [====================&gt;.........] - ETA: 1s
 7296/10000 [====================&gt;.........] - ETA: 1s
 7424/10000 [=====================&gt;........] - ETA: 1s
 7552/10000 [=====================&gt;........] - ETA: 1s
 7648/10000 [=====================&gt;........] - ETA: 1s
 7744/10000 [======================&gt;.......] - ETA: 1s
 7840/10000 [======================&gt;.......] - ETA: 1s
 7936/10000 [======================&gt;.......] - ETA: 1s
 8064/10000 [=======================&gt;......] - ETA: 1s
 8128/10000 [=======================&gt;......] - ETA: 1s
 8256/10000 [=======================&gt;......] - ETA: 1s
 8352/10000 [========================&gt;.....] - ETA: 0s
 8480/10000 [========================&gt;.....] - ETA: 0s
 8576/10000 [========================&gt;.....] - ETA: 0s
 8672/10000 [=========================&gt;....] - ETA: 0s
 8768/10000 [=========================&gt;....] - ETA: 0s
 8864/10000 [=========================&gt;....] - ETA: 0s
 8960/10000 [=========================&gt;....] - ETA: 0s
 9056/10000 [==========================&gt;...] - ETA: 0s
 9152/10000 [==========================&gt;...] - ETA: 0s
 9216/10000 [==========================&gt;...] - ETA: 0s
 9344/10000 [===========================&gt;..] - ETA: 0s
 9472/10000 [===========================&gt;..] - ETA: 0s
 9568/10000 [===========================&gt;..] - ETA: 0s
 9632/10000 [===========================&gt;..] - ETA: 0s
 9760/10000 [============================&gt;.] - ETA: 0s
 9856/10000 [============================&gt;.] - ETA: 0s
 9952/10000 [============================&gt;.] - ETA: 0s
10000/10000 [==============================] - 6s 583us/step
loss: 0.040131
acc: 0.986400
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">fig, ax = plt.subplots()
fig.set_size_inches((5,5))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
display(fig)
</code></pre>
</div>
<div class="cell markdown">
<h3 id="our-mnist-convnet"><a class="header" href="#our-mnist-convnet">Our MNIST ConvNet</a></h3>
<p>In our first convolutional MNIST experiment, we get to almost 99% validation accuracy in just a few epochs (a minutes or so on CPU)!</p>
<p>The training accuracy is effectively 100%, though, so we've almost completely overfit (i.e., memorized the training data) by this point and need to do a little work if we want to keep learning.</p>
<p>Let's add another convolutional layer:</p>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-python">model = Sequential()

model.add(Conv2D(8, # number of kernels 
						(4, 4), # kernel size
                        padding='valid',
                        input_shape=(28, 28, 1)))

model.add(Activation('relu'))

model.add(Conv2D(8, (4, 4)))
model.add(Activation('relu'))

model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Flatten())
model.add(Dense(128))
model.add(Activation('relu'))

model.add(Dense(10))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

history = model.fit(X_train, y_train, batch_size=128, epochs=15, verbose=2, validation_split=0.1)

scores = model.evaluate(X_test, y_test, verbose=1)

print
for i in range(len(model.metrics_names)):
	print(&quot;%s: %f&quot; % (model.metrics_names[i], scores[i]))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Train on 54000 samples, validate on 6000 samples
Epoch 1/15
 - 104s - loss: 0.2681 - acc: 0.9224 - val_loss: 0.0784 - val_acc: 0.9768
Epoch 2/15
 - 116s - loss: 0.0733 - acc: 0.9773 - val_loss: 0.0581 - val_acc: 0.9843
Epoch 3/15
 - 114s - loss: 0.0511 - acc: 0.9847 - val_loss: 0.0435 - val_acc: 0.9873
Epoch 4/15
 - 115s - loss: 0.0391 - acc: 0.9885 - val_loss: 0.0445 - val_acc: 0.9880
Epoch 5/15
 - 105s - loss: 0.0307 - acc: 0.9904 - val_loss: 0.0446 - val_acc: 0.9890
Epoch 6/15
 - 105s - loss: 0.0251 - acc: 0.9923 - val_loss: 0.0465 - val_acc: 0.9875
Epoch 7/15
 - 102s - loss: 0.0193 - acc: 0.9936 - val_loss: 0.0409 - val_acc: 0.9892
Epoch 8/15
 - 100s - loss: 0.0162 - acc: 0.9948 - val_loss: 0.0468 - val_acc: 0.9878
Epoch 9/15
 - 103s - loss: 0.0138 - acc: 0.9956 - val_loss: 0.0447 - val_acc: 0.9893
Epoch 10/15
 - 104s - loss: 0.0122 - acc: 0.9957 - val_loss: 0.0482 - val_acc: 0.9900
Epoch 11/15
 - 102s - loss: 0.0097 - acc: 0.9969 - val_loss: 0.0480 - val_acc: 0.9895
Epoch 12/15
 - 82s - loss: 0.0089 - acc: 0.9970 - val_loss: 0.0532 - val_acc: 0.9882
Epoch 13/15
 - 93s - loss: 0.0080 - acc: 0.9973 - val_loss: 0.0423 - val_acc: 0.9913
Epoch 14/15
 - 92s - loss: 0.0074 - acc: 0.9976 - val_loss: 0.0557 - val_acc: 0.9883
Epoch 15/15
 - 92s - loss: 0.0043 - acc: 0.9987 - val_loss: 0.0529 - val_acc: 0.9902

   32/10000 [..............................] - ETA: 4s
  128/10000 [..............................] - ETA: 6s
  256/10000 [..............................] - ETA: 5s
  352/10000 [&gt;.............................] - ETA: 6s
  448/10000 [&gt;.............................] - ETA: 6s
  512/10000 [&gt;.............................] - ETA: 6s
  608/10000 [&gt;.............................] - ETA: 6s
  736/10000 [=&gt;............................] - ETA: 5s
  800/10000 [=&gt;............................] - ETA: 6s
  896/10000 [=&gt;............................] - ETA: 5s
 1024/10000 [==&gt;...........................] - ETA: 5s
 1120/10000 [==&gt;...........................] - ETA: 5s
 1216/10000 [==&gt;...........................] - ETA: 5s
 1312/10000 [==&gt;...........................] - ETA: 5s
 1408/10000 [===&gt;..........................] - ETA: 5s
 1504/10000 [===&gt;..........................] - ETA: 5s
 1600/10000 [===&gt;..........................] - ETA: 5s
 1664/10000 [===&gt;..........................] - ETA: 5s
 1760/10000 [====&gt;.........................] - ETA: 5s
 1856/10000 [====&gt;.........................] - ETA: 5s
 1952/10000 [====&gt;.........................] - ETA: 5s
 2048/10000 [=====&gt;........................] - ETA: 5s
 2144/10000 [=====&gt;........................] - ETA: 4s
 2240/10000 [=====&gt;........................] - ETA: 4s
 2336/10000 [======&gt;.......................] - ETA: 4s
 2432/10000 [======&gt;.......................] - ETA: 4s
 2496/10000 [======&gt;.......................] - ETA: 4s
 2592/10000 [======&gt;.......................] - ETA: 4s
 2688/10000 [=======&gt;......................] - ETA: 4s
 2784/10000 [=======&gt;......................] - ETA: 4s
 2880/10000 [=======&gt;......................] - ETA: 4s
 2976/10000 [=======&gt;......................] - ETA: 4s
 3072/10000 [========&gt;.....................] - ETA: 4s
 3168/10000 [========&gt;.....................] - ETA: 4s
 3232/10000 [========&gt;.....................] - ETA: 4s
 3360/10000 [=========&gt;....................] - ETA: 4s
 3456/10000 [=========&gt;....................] - ETA: 4s
 3552/10000 [=========&gt;....................] - ETA: 4s
 3648/10000 [=========&gt;....................] - ETA: 3s
 3776/10000 [==========&gt;...................] - ETA: 3s
 3872/10000 [==========&gt;...................] - ETA: 3s
 3968/10000 [==========&gt;...................] - ETA: 3s
 4096/10000 [===========&gt;..................] - ETA: 3s
 4192/10000 [===========&gt;..................] - ETA: 3s
 4288/10000 [===========&gt;..................] - ETA: 3s
 4384/10000 [============&gt;.................] - ETA: 3s
 4480/10000 [============&gt;.................] - ETA: 3s
 4576/10000 [============&gt;.................] - ETA: 3s
 4672/10000 [=============&gt;................] - ETA: 3s
 4768/10000 [=============&gt;................] - ETA: 3s
 4864/10000 [=============&gt;................] - ETA: 3s
 4960/10000 [=============&gt;................] - ETA: 3s
 5024/10000 [==============&gt;...............] - ETA: 3s
 5120/10000 [==============&gt;...............] - ETA: 3s
 5216/10000 [==============&gt;...............] - ETA: 3s
 5312/10000 [==============&gt;...............] - ETA: 2s
 5408/10000 [===============&gt;..............] - ETA: 2s
 5504/10000 [===============&gt;..............] - ETA: 2s
 5600/10000 [===============&gt;..............] - ETA: 2s
 5696/10000 [================&gt;.............] - ETA: 2s
 5792/10000 [================&gt;.............] - ETA: 2s
 5920/10000 [================&gt;.............] - ETA: 2s
 6016/10000 [=================&gt;............] - ETA: 2s
 6080/10000 [=================&gt;............] - ETA: 2s
 6176/10000 [=================&gt;............] - ETA: 2s
 6272/10000 [=================&gt;............] - ETA: 2s
 6368/10000 [==================&gt;...........] - ETA: 2s
 6464/10000 [==================&gt;...........] - ETA: 2s
 6560/10000 [==================&gt;...........] - ETA: 2s
 6656/10000 [==================&gt;...........] - ETA: 2s
 6752/10000 [===================&gt;..........] - ETA: 2s
 6848/10000 [===================&gt;..........] - ETA: 1s
 6912/10000 [===================&gt;..........] - ETA: 1s
 7008/10000 [====================&gt;.........] - ETA: 1s
 7104/10000 [====================&gt;.........] - ETA: 1s
 7200/10000 [====================&gt;.........] - ETA: 1s
 7328/10000 [====================&gt;.........] - ETA: 1s
 7424/10000 [=====================&gt;........] - ETA: 1s
 7552/10000 [=====================&gt;........] - ETA: 1s
 7648/10000 [=====================&gt;........] - ETA: 1s
 7712/10000 [======================&gt;.......] - ETA: 1s
 7776/10000 [======================&gt;.......] - ETA: 1s
 7872/10000 [======================&gt;.......] - ETA: 1s
 7968/10000 [======================&gt;.......] - ETA: 1s
 8032/10000 [=======================&gt;......] - ETA: 1s
 8128/10000 [=======================&gt;......] - ETA: 1s
 8224/10000 [=======================&gt;......] - ETA: 1s
 8288/10000 [=======================&gt;......] - ETA: 1s
 8352/10000 [========================&gt;.....] - ETA: 1s
 8448/10000 [========================&gt;.....] - ETA: 1s
 8544/10000 [========================&gt;.....] - ETA: 0s
 8640/10000 [========================&gt;.....] - ETA: 0s
 8704/10000 [=========================&gt;....] - ETA: 0s
 8800/10000 [=========================&gt;....] - ETA: 0s
 8864/10000 [=========================&gt;....] - ETA: 0s
 8992/10000 [=========================&gt;....] - ETA: 0s
 9056/10000 [==========================&gt;...] - ETA: 0s
 9152/10000 [==========================&gt;...] - ETA: 0s
 9248/10000 [==========================&gt;...] - ETA: 0s
 9344/10000 [===========================&gt;..] - ETA: 0s
 9408/10000 [===========================&gt;..] - ETA: 0s
 9504/10000 [===========================&gt;..] - ETA: 0s
 9600/10000 [===========================&gt;..] - ETA: 0s
 9696/10000 [============================&gt;.] - ETA: 0s
 9792/10000 [============================&gt;.] - ETA: 0s
 9888/10000 [============================&gt;.] - ETA: 0s
 9984/10000 [============================&gt;.] - ETA: 0s
10000/10000 [==============================] - 7s 663us/step
loss: 0.040078
acc: 0.990700
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>While that's running, let's look at a number of &quot;famous&quot; convolutional networks!</p>
<h3 id="lenet-yann-lecun-1998"><a class="header" href="#lenet-yann-lecun-1998">LeNet (Yann LeCun, 1998)</a></h3>
<img src="http://i.imgur.com/k5hMtMK.png">
<img src="http://i.imgur.com/ERV9pHW.gif">
</div>
<div class="cell markdown">
<img src="http://i.imgur.com/TCN9C4P.png">
</div>
<div class="cell markdown">
<h3 id="alexnet-2012"><a class="header" href="#alexnet-2012">AlexNet (2012)</a></h3>
<img src="http://i.imgur.com/CpokDKV.jpg">
<img src="http://i.imgur.com/Ld2QhXr.jpg">
</div>
<div class="cell markdown">
<h3 id="back-to-our-labs-still-overfitting"><a class="header" href="#back-to-our-labs-still-overfitting">Back to our labs: Still Overfitting</a></h3>
<p>We're making progress on our test error -- about 99% -- but just a bit for all the additional time, due to the network overfitting the data.</p>
<p>There are a variety of techniques we can take to counter this -- forms of regularization.</p>
<p>Let's try a relatively simple solution solution that works surprisingly well: add a pair of <code>Dropout</code> filters, a layer that randomly omits a fraction of neurons from each training batch (thus exposing each neuron to only part of the training data).</p>
<p>We'll add more convolution kernels but shrink them to 3x3 as well.</p>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-python">model = Sequential()

model.add(Conv2D(32, # number of kernels 
						(3, 3), # kernel size
                        padding='valid',
                        input_shape=(28, 28, 1)))

model.add(Activation('relu'))

model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))

model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Dropout(rate=1-0.25)) # &lt;- regularize, new parameter rate added (rate=1-keep_prob)
model.add(Flatten())
model.add(Dense(128))
model.add(Activation('relu'))

model.add(Dropout(rate=1-0.5)) # &lt;-regularize, new parameter rate added (rate=1-keep_prob)
model.add(Dense(10))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(X_train, y_train, batch_size=128, epochs=15, verbose=2)

scores = model.evaluate(X_test, y_test, verbose=2)

print
for i in range(len(model.metrics_names)):
	print(&quot;%s: %f&quot; % (model.metrics_names[i], scores[i]))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Epoch 1/15
 - 370s - loss: 0.3865 - acc: 0.8783
Epoch 2/15
 - 353s - loss: 0.1604 - acc: 0.9522
Epoch 3/15
 - 334s - loss: 0.1259 - acc: 0.9617
Epoch 4/15
 - 337s - loss: 0.1071 - acc: 0.9666
Epoch 5/15
 - 265s - loss: 0.0982 - acc: 0.9699
Epoch 6/15
 - 250s - loss: 0.0923 - acc: 0.9716
Epoch 7/15
 - 244s - loss: 0.0845 - acc: 0.9740
Epoch 8/15
 - 247s - loss: 0.0811 - acc: 0.9747
Epoch 9/15
 - 246s - loss: 0.0767 - acc: 0.9766
Epoch 10/15
 - 246s - loss: 0.0749 - acc: 0.9764
Epoch 11/15
 - 247s - loss: 0.0708 - acc: 0.9776
Epoch 12/15
 - 244s - loss: 0.0698 - acc: 0.9779
Epoch 13/15
 - 248s - loss: 0.0667 - acc: 0.9794
Epoch 14/15
 - 244s - loss: 0.0653 - acc: 0.9799
Epoch 15/15
 - 249s - loss: 0.0645 - acc: 0.9801
loss: 0.023579
acc: 0.991300
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>While that's running, let's look at some more recent ConvNet architectures:</p>
<h3 id="vgg16-2014"><a class="header" href="#vgg16-2014">VGG16 (2014)</a></h3>
<img src="http://i.imgur.com/gl4kZDf.png">
</div>
<div class="cell markdown">
<h3 id="googlenet-2014"><a class="header" href="#googlenet-2014">GoogLeNet (2014)</a></h3>
<img src="http://i.imgur.com/hvmtDqN.png">
<p><em>&quot;Inception&quot; layer: parallel convolutions at different resolutions</em></p>
<h3 id="residual-networks-2015-"><a class="header" href="#residual-networks-2015-">Residual Networks (2015-)</a></h3>
<p>Skip layers to improve training (error propagation). Residual layers learn from details at multiple previous layers.</p>
<img src="http://i.imgur.com/32g8Ykl.png">
</div>
<div class="cell markdown">
<hr />
<blockquote>
<p><strong>ASIDE: Atrous / Dilated Convolutions</strong></p>
</blockquote>
<blockquote>
<p>An atrous or dilated convolution is a convolution filter with &quot;holes&quot; in it. Effectively, it is a way to enlarge the filter spatially while not adding as many parameters or attending to every element in the input.</p>
</blockquote>
<blockquote>
<p>Why? Covering a larger input volume allows recognizing coarser-grained patterns; restricting the number of parameters is a way of regularizing or constraining the capacity of the model, making training easier.</p>
</blockquote>
<hr />
</div>
<div class="cell markdown">
<h2 id="lab-wrapup"><a class="header" href="#lab-wrapup"><em>Lab Wrapup</em></a></h2>
<p>From the last lab, you should have a test accuracy of over 99.1%</p>
<p>For one more activity, try changing the optimizer to old-school &quot;sgd&quot; -- just to see how far we've come with these modern gradient descent techniques in the last few years.</p>
<p>Accuracy will end up noticeably worse ... about 96-97% test accuracy. Two key takeaways:</p>
<ul>
<li>Without a good optimizer, even a very powerful network design may not achieve results</li>
<li>In fact, we could replace the word &quot;optimizer&quot; there with
<ul>
<li>initialization</li>
<li>activation</li>
<li>regularization</li>
<li>(etc.)</li>
</ul>
</li>
<li>All of these elements we've been working with operate together in a complex way to determine final performance</li>
</ul>
</div>
<div class="cell markdown">
<p>Of course this world evolves fast - see the new kid in the CNN block -- <strong>capsule networks</strong></p>
<blockquote>
<p>Hinton: “The pooling operation used in convolutional neural networks is a big mistake and the fact that it works so well is a disaster.”</p>
</blockquote>
<p>Well worth the 8 minute read: * <a href="https://medium.com/ai%C2%B3-theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b">https://medium.com/ai%C2%B3-theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b</a></p>
<p>To understand deeper: * original paper: <a href="https://arxiv.org/abs/1710.09829">https://arxiv.org/abs/1710.09829</a></p>
<p><a href="https://keras.io/examples/cifar10_cnn_capsule/">Keras capsule network example</a></p>
</div>
<div class="cell markdown">
<h1 id="more-resources"><a class="header" href="#more-resources">More resources</a></h1>
<ul>
<li>http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/</li>
<li>https://openai.com/</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../../contents/000_6-sds-3-x-dl/054_DLbyABr_03a-BatchTensorFlowWithMatrices.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next" href="../../contents/000_6-sds-3-x-dl/056_DLbyABr_04a-Hands-On-MNIST-MLP.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../../contents/000_6-sds-3-x-dl/054_DLbyABr_03a-BatchTensorFlowWithMatrices.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next" href="../../contents/000_6-sds-3-x-dl/056_DLbyABr_04a-Hands-On-MNIST-MLP.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="../../elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="../../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->


    </body>
</html>
