<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>034_04_MapMatching_on_a_G1_LT - sds-3.x/ScaDaMaLe</title>


        <!-- Custom HTML head -->

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../../favicon.svg">
        <link rel="shortcut icon" href="../../favicon.png">
        <link rel="stylesheet" href="../../css/variables.css">
        <link rel="stylesheet" href="../../css/general.css">
        <link rel="stylesheet" href="../../css/chrome.css">
        <link rel="stylesheet" href="../../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../../highlight.css">
        <link rel="stylesheet" href="../../tomorrow-night.css">
        <link rel="stylesheet" href="../../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../../scroll-mdbook-outputs.css">

        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../../contents/000_5-sds-2-x-geo/000_5-sds-2-x-geo-changes.html">000_5-sds-2-x-geo-changes</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_5-sds-2-x-geo/001_CrashCourseGIS.html">001_CrashCourseGIS</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_5-sds-2-x-geo/031_GeospatialAnalyticsInMagellan.html">031_GeospatialAnalyticsInMagellan</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_5-sds-2-x-geo/031a_MagellanOSMIngestion.html">031a_MagellanOSMIngestion</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_5-sds-2-x-geo/032_NYtaxisInMagellan.html">032_NYtaxisInMagellan</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_5-sds-2-x-geo/032a_MSR_BeijingTaxiTrajectories_MagellanQueries.html">032a_MSR_BeijingTaxiTrajectories_MagellanQueries</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_5-sds-2-x-geo/032b_UberMapMatchingAndVisualization.html">032b_UberMapMatchingAndVisualization</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_5-sds-2-x-geo/032d_MobileSampleSQL.html">032d_MobileSampleSQL</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_5-sds-2-x-geo/032e_CallDetailRecords.html">032e_CallDetailRecords</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_5-sds-2-x-geo/032x_ComingAttractions.html">032x_ComingAttractions</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_5-sds-2-x-geo/033_00_Intro2PointsOnGraphs.html">033_00_Intro2PointsOnGraphs</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_5-sds-2-x-geo/033_01_OSMtoGraphXUppsalaTiny.html">033_01_OSMtoGraphXUppsalaTiny</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_5-sds-2-x-geo/033_02_OSMtoGraphX_LT.html">033_02_OSMtoGraphX_LT</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_5-sds-2-x-geo/033_03_LT_PageRank.html">033_03_LT_PageRank</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_5-sds-2-x-geo/034_01_MapMatching_with_GeoMatch_UppsalaTiny.html">034_01_MapMatching_with_GeoMatch_UppsalaTiny</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_5-sds-2-x-geo/034_02_MapMatching_on_a_Graph_UppsalaTiny.html">034_02_MapMatching_on_a_Graph_UppsalaTiny</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_5-sds-2-x-geo/034_03_MapMatching_on_a_Graph_LT.html">034_03_MapMatching_on_a_Graph_LT</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_5-sds-2-x-geo/034_04_MapMatching_on_a_G1_LT.html" class="active">034_04_MapMatching_on_a_G1_LT</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_5-sds-2-x-geo/034_05DistributionOfStates.html">034_05DistributionOfStates</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_5-sds-2-x-geo/034_06SimulatingArrivalTimesNHPP_Inversion.html">034_06SimulatingArrivalTimesNHPP_Inversion</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_5-sds-2-x-geo/035_01_Arcgis_coordinates_transformation.html">035_01_Arcgis_coordinates_transformation</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_5-sds-2-x-geo/035_02_Segmentation_municipalities_Magellan.html">035_02_Segmentation_municipalities_Magellan</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_5-sds-2-x-geo/035_03_Visualization_municipalities.html">035_03_Visualization_municipalities</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_5-sds-2-x-geo/035_04_MapMatching_intersections.html">035_04_MapMatching_intersections</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_5-sds-2-x-geo/035_05_UndirectedG0.html">035_05_UndirectedG0</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_5-sds-2-x-geo/035_06_ConnectedComponent_PageRank.html">035_06_ConnectedComponent_PageRank</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_5-sds-2-x-geo/035_07_PoissonRegression.html">035_07_PoissonRegression</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">sds-3.x/ScaDaMaLe</h1>

                    <div class="right-buttons">
                        <a href="../../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div class="cell markdown">
<h2 id="map-matching-events-on-a-state-space--coarsened-road-graph-with-geomatch"><a class="header" href="#map-matching-events-on-a-state-space--coarsened-road-graph-with-geomatch">Map-Matching Events on a State Space / Coarsened Road Graph with GeoMatch</a></h2>
<p>Stavroula Rafailia Vlachou (<a href="https://www.linkedin.com/in/stavroula-rafailia-vlachou/">LinkedIn</a>) and Raazesh Sainudiin (<a href="https://www.linkedin.com/in/raazesh-sainudiin-45955845/">LinkedIn</a>).</p>
<pre><code>This project was supported by SENSMETRY through a Data Science Project Internship 
between 2022-01-17 and 2022-06-05 to Stavroula R. Vlachou and
Databricks University Alliance with infrastructure credits from AWS to 
Raazesh Sainudiin, Department of Mathematics, Uppsala University, Sweden.
</code></pre>
<p>2022, Uppsala, Sweden</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.graphx._
import sqlContext.implicits._
import scala.collection.JavaConversions._
import org.cusp.bdi.gm.GeoMatch
import org.cusp.bdi.gm.geom.GMPoint
import org.cusp.bdi.gm.geom.GMLineString
import com.esri.arcgisruntime.geometry.{Point, SpatialReference, GeometryEngine}
import com.esri.arcgisruntime.geometry.GeometryEngine.project
import com.esri.arcgisruntime._
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import org.apache.spark.graphx._
import sqlContext.implicits._
import scala.collection.JavaConversions._
import org.cusp.bdi.gm.GeoMatch
import org.cusp.bdi.gm.geom.GMPoint
import org.cusp.bdi.gm.geom.GMLineString
import com.esri.arcgisruntime.geometry.{Point, SpatialReference, GeometryEngine}
import com.esri.arcgisruntime.geometry.GeometryEngine.project
import com.esri.arcgisruntime._
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="road-network"><a class="header" href="#road-network">Road Network</a></h3>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">spark.conf.set(&quot;spark.sql.parquet.binaryAsString&quot;, true)

val nodes_df = spark.read.parquet(&quot;dbfs:/datasets/osm/lithuania/lithuania.osm.pbf.node.parquet&quot;)
val ways_df = spark.read.parquet(&quot;dbfs:/datasets/osm/lithuania/lithuania.osm.pbf.way.parquet&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>nodes_df: org.apache.spark.sql.DataFrame = [id: bigint, version: int ... 7 more fields]
ways_df: org.apache.spark.sql.DataFrame = [id: bigint, version: int ... 6 more fields]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//convert the nodes to Dataset containing the fields of interest

case class NodeEntry(nodeId: Long, latitude: Double, longitude: Double, tags: Seq[String])

val nodeDS = nodes_df.map(node =&gt; 
  NodeEntry(node.getAs[Long](&quot;id&quot;),
       node.getAs[Double](&quot;latitude&quot;),
       node.getAs[Double](&quot;longitude&quot;),
       node.getAs[Seq[Row]](&quot;tags&quot;).map{case Row(key:String, value:String) =&gt; value}
)).cache()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined class NodeEntry
nodeDS: org.apache.spark.sql.Dataset[NodeEntry] = [nodeId: bigint, latitude: double ... 2 more fields]
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>The first step is to obtain the state space. The State Space consists of road segments and intersection points. The road segments correspond to the edges of the graph while the intersection points can be retrieved from the ways and the nodes dataset as those nodes that lie in at least one way. All coordinates should be in the spatial reference system 3035. To implement the map matching it is better to keep all intermediate points from each edge.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(&quot;dbfs:/LT&quot;))
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>path</th>
<th>name</th>
<th>size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dbfs:/LT/intersections/</td>
<td>intersections/</td>
<td>0.0</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell markdown">
<h3 id="obtaining-the-state-space"><a class="header" href="#obtaining-the-state-space">Obtaining the State Space</a></h3>
</div>
<div class="cell markdown">
<h4 id="intersection-points"><a class="header" href="#intersection-points">Intersection points</a></h4>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val intersections = spark.read.parquet(&quot;dbfs:/LT/intersections&quot;)
intersections.show(1)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+----------------+
|intersectionNode|
+----------------+
|       270958413|
+----------------+
only showing top 1 row

intersections: org.apache.spark.sql.DataFrame = [intersectionNode: bigint]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">intersections.count
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res5: Long = 162325
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>The next step is to obtain the coordinates of the intersection points and convert them into decimal degrees.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val intersection_points = nodeDS.join(intersections, intersections(&quot;intersectionNode&quot;) === nodeDS(&quot;nodeId&quot;)).drop(&quot;tags&quot;, &quot;nodeId&quot;).select(&quot;intersectionNode&quot;, &quot;latitude&quot;, &quot;longitude&quot;)
intersection_points.show(1)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+----------------+----------+------------------+
|intersectionNode|  latitude|         longitude|
+----------------+----------+------------------+
|        15389886|54.7309125|25.239701200000003|
+----------------+----------+------------------+
only showing top 1 row

intersection_points: org.apache.spark.sql.DataFrame = [intersectionNode: bigint, latitude: double ... 1 more field]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">intersection_points.count()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res8: Long = 162325
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.sql.functions.{concat, lit}
val concat_coordinates = intersection_points.select($&quot;intersectionNode&quot;,concat($&quot;latitude&quot;,lit(&quot; &quot;),$&quot;longitude&quot;).alias(&quot;coordinates&quot;))
concat_coordinates.show(1, false)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+----------------+-----------------------------+
|intersectionNode|coordinates                  |
+----------------+-----------------------------+
|15389886        |54.7309125 25.239701200000003|
+----------------+-----------------------------+
only showing top 1 row

import org.apache.spark.sql.functions.{concat, lit}
concat_coordinates: org.apache.spark.sql.DataFrame = [intersectionNode: bigint, coordinates: string]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val firstIntersectionStates = concat_coordinates.select(concat(lit(&quot;LineString:&quot;),$&quot;intersectionNode&quot;).alias(&quot;LineString&quot;),$&quot;coordinates&quot;)
firstIntersectionStates.show(1, false)
val firstIntersectionStates_rdd = firstIntersectionStates.rdd
firstIntersectionStates_rdd.take(1)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+-------------------+-----------------------------+
|LineString         |coordinates                  |
+-------------------+-----------------------------+
|LineString:15389886|54.7309125 25.239701200000003|
+-------------------+-----------------------------+
only showing top 1 row

firstIntersectionStates: org.apache.spark.sql.DataFrame = [LineString: string, coordinates: string]
firstIntersectionStates_rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[545] at rdd at command-197980058855229:3
res11: Array[org.apache.spark.sql.Row] = Array([LineString:15389886,54.7309125 25.239701200000003])
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">if(!ArcGISRuntimeEnvironment.isInitialized())
    {
      ArcGISRuntimeEnvironment.setInstallDirectory(&quot;/dbfs/arcGISRuntime/arcgis-runtime-sdk-java-100.4.0/&quot;)
      ArcGISRuntimeEnvironment.initialize() 
    }
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def project_to_meters(lon: String, lat: String): String = { 
    
    if(!ArcGISRuntimeEnvironment.isInitialized())
    {
      ArcGISRuntimeEnvironment.setInstallDirectory(&quot;/dbfs/arcGISRuntime/arcgis-runtime-sdk-java-100.4.0/&quot;)
      ArcGISRuntimeEnvironment.initialize() 
    }
  
    val initial_point = new Point(lon.toDouble, lat.toDouble, SpatialReference.create(4326))
    val reprojection = GeometryEngine.project(initial_point, SpatialReference.create(3035))
    reprojection.toString
}
spark.udf.register(&quot;project_to_meters&quot;, project_to_meters(_:String, _:String):String)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>project_to_meters: (lon: String, lat: String)String
res14: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function2&gt;,StringType,Some(List(StringType, StringType)))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val intersections_reprojected = firstIntersectionStates_rdd.map(line =&gt; line.toString.replaceAll(&quot;\\[&quot;,&quot;&quot;).replaceAll(&quot;\\]&quot;,&quot;&quot;)).map(line =&gt; {val parts = line.replaceAll(&quot;\&quot;&quot;,&quot;&quot;).split(&quot;,&quot;);val arrCoords = parts.slice(1,parts.length).map(xyStr =&gt; {val xy = xyStr.split(&quot; &quot;);val reprojection = project_to_meters(xy(1).toString, xy(0).toString);val coords = reprojection.replaceAll(&quot;,&quot;,&quot;&quot;).replaceAll(&quot;\\[&quot;,&quot;&quot;).split(&quot; &quot;).slice(1,reprojection.length);val xy_new = coords(0).toString +&quot; &quot;+ coords(1).toString;xy_new});(parts(0).toString, arrCoords)})
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>intersections_reprojected: org.apache.spark.rdd.RDD[(String, Array[String])] = MapPartitionsRDD[547] at map at command-197980058855232:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">intersections_reprojected.take(1)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res15: Array[(String, Array[String])] = Array((LineString:15389886,Array(5294624.872733 3617234.130316)))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val intersections_unpacked = intersections_reprojected.map(item =&gt; item._1.toString + &quot;,&quot; + item._2(0).toString)
intersections_unpacked.take(1)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>intersections_unpacked: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[548] at map at command-197980058855234:1
res16: Array[String] = Array(LineString:15389886,5294624.872733 3617234.130316)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rdd_first_set_intersections = intersections_unpacked.mapPartitions(_.map(line =&gt;{val parts = line.replaceAll(&quot;\&quot;&quot;,&quot;&quot;).split(',');val arrCoords = parts.slice(1, parts.length).map(xyStr =&gt; {val xy = xyStr.split(' ');(xy(0).toDouble.toInt, xy(1).toDouble.toInt)});new GMPoint(parts(0), arrCoords(0))}))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rdd_first_set_intersections: org.apache.spark.rdd.RDD[org.cusp.bdi.gm.geom.GMPoint] = MapPartitionsRDD[549] at mapPartitions at command-197980058855235:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd_first_set_intersections.take(1)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res17: Array[org.cusp.bdi.gm.geom.GMPoint] = Array(GMPoint(LineString:15389886,(5294624,3617234)))
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Next, we need to obtain the set of points that are to be map matched. In this case the set of points corresponds to the accident events occuring in LT.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val events = spark.read.format(&quot;csv&quot;).load(&quot;/FileStore/tables/LTnodes.csv&quot;).rdd.map(line =&gt; line.toString)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>events: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[563] at map at command-197980058855239:1
</code></pre>
</div>
</div>
<div class="cell markdown">
<p><code>events.take(1)</code></p>
<p>Output:</p>
<p><code>Array([Point LT2019XXX,52aaa.18bbb,36ccc.21ddd])</code></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val all_accidents = spark.read.format(&quot;csv&quot;).load(&quot;FileStore/tables/LTnodes.csv&quot;).toDF(&quot;PointId&quot;, &quot;longitude&quot;, &quot;latitude&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>all_accidents: org.apache.spark.sql.DataFrame = [PointId: string, longitude: string ... 1 more field]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rddSecondSet = events.mapPartitions(_.map(line =&gt; {val parts = line.replaceAll(&quot;\&quot;&quot;,&quot;&quot;).replaceAll(&quot;\\[&quot;,&quot;&quot;).replaceAll(&quot;\\]&quot;,&quot;&quot;).split(',');new GMPoint(parts(0), (parts(1).toDouble.toInt, parts(2).toDouble.toInt))}))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rddSecondSet: org.apache.spark.rdd.RDD[org.cusp.bdi.gm.geom.GMPoint] = MapPartitionsRDD[572] at mapPartitions at command-197980058855241:1
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Implement Map Matching</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val geoMatch = new GeoMatch(false, 256, 20, (-1, -1, -1, -1)) //n(=dimension of the Hilber curve) should be a power of 2. 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>geoMatch: org.cusp.bdi.gm.GeoMatch = GeoMatch(false,256,20.0,(-1,-1,-1,-1))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val resultRDD = geoMatch.spatialJoinKNN(rdd_first_set_intersections, rddSecondSet, 1, false)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>resultRDD: org.apache.spark.rdd.RDD[(org.cusp.bdi.gm.geom.GMPoint, scala.collection.mutable.ListBuffer[org.cusp.bdi.gm.geom.GMPoint])] = MapPartitionsRDD[585] at mapPartitions at GeoMatch.scala:94
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>The output of the above command with IDs and locations anonymised is as follows:</p>
<pre><code>+----------------------------------------+---------------------------------------------+
|k                                       |line                                         |
+----------------------------------------+---------------------------------------------+
|[Point LT20xyABCDEF, [521xxxx, 362yyyy]]|[[LineString:1254578sss, [521zzzz, 362zzzz]]]|
+----------------------------------------+---------------------------------------------+
only showing top 1 rows
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">resultRDD.map(element =&gt; (element._1.payload, element._2.map(_.payload))).filter(element =&gt; (element._2.isEmpty)).count()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res19: Long = 8246
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">resultRDD.map(element =&gt; (element._1.payload, element._2.map(_.payload))).filter(element =&gt; !(element._2.isEmpty)).count()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res20: Long = 3743
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val unmatched_events = resultRDD.filter(element =&gt; (element._2.isEmpty)).map(element =&gt; element._1.payload).toDF(&quot;id&quot;)

val second_set_second_round = unmatched_events.join(all_accidents, unmatched_events(&quot;id&quot;) === all_accidents(&quot;PointId&quot;)).drop(&quot;id&quot;).rdd.map(line =&gt; line.toString)

val rddSecondSetSecondRound = second_set_second_round.mapPartitions(_.map(line =&gt; {val parts = line.replaceAll(&quot;\&quot;&quot;,&quot;&quot;).replaceAll(&quot;\\[&quot;,&quot;&quot;).replaceAll(&quot;\\]&quot;,&quot;&quot;).split(',');new GMPoint(parts(0), (parts(1).toDouble.toInt, parts(2).toDouble.toInt))}))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>unmatched_events: org.apache.spark.sql.DataFrame = [id: string]
second_set_second_round: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[599] at map at command-197980058855248:3
rddSecondSetSecondRound: org.apache.spark.rdd.RDD[org.cusp.bdi.gm.geom.GMPoint] = MapPartitionsRDD[600] at mapPartitions at command-197980058855248:5
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val edges = spark.read.parquet(&quot;dbfs:/_checkpoint/edges_LT_100&quot;)
val vertices = spark.read.parquet(&quot;dbfs:/_checkpoint/vertices_LT_100&quot;).toDF(&quot;vertexId&quot;, &quot;latitude&quot;, &quot;longitude&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>edges: org.apache.spark.sql.DataFrame = [src: bigint, dst: bigint]
vertices: org.apache.spark.sql.DataFrame = [vertexId: bigint, latitude: double ... 1 more field]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">edges.show(1)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+--------+----------+
|     src|       dst|
+--------+----------+
|31451266|4397542060|
+--------+----------+
only showing top 1 row
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val src_coordinates = edges.join(vertices,vertices(&quot;vertexId&quot;) === edges(&quot;src&quot;), &quot;left_outer&quot;).drop(&quot;vertexId&quot;).withColumnRenamed(&quot;latitude&quot;, &quot;src_latitude&quot;).withColumnRenamed(&quot;longitude&quot;,&quot;src_longitude&quot;)
val edge_coordinates = src_coordinates.join(vertices,vertices(&quot;vertexId&quot;) === src_coordinates(&quot;dst&quot;)).drop(&quot;vertexId&quot;).withColumnRenamed(&quot;latitude&quot;, &quot;dst_latitude&quot;).withColumnRenamed(&quot;longitude&quot;, &quot;dst_longitude&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>src_coordinates: org.apache.spark.sql.DataFrame = [src: bigint, dst: bigint ... 2 more fields]
edge_coordinates: org.apache.spark.sql.DataFrame = [src: bigint, dst: bigint ... 4 more fields]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.sql.functions.{concat, lit}
val concat_coordinates = edge_coordinates.select($&quot;src&quot;,concat($&quot;src_latitude&quot;,lit(&quot; &quot;),$&quot;src_longitude&quot;).alias(&quot;src_coordinates&quot;), $&quot;dst&quot;,concat($&quot;dst_latitude&quot;,lit(&quot; &quot;),$&quot;dst_longitude&quot;).alias(&quot;dst_coordinates&quot;))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import org.apache.spark.sql.functions.{concat, lit}
concat_coordinates: org.apache.spark.sql.DataFrame = [src: bigint, src_coordinates: string ... 2 more fields]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">concat_coordinates.show(1, false)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+----------+---------------------+--------+-------------------------------------+
|src       |src_coordinates      |dst     |dst_coordinates                      |
+----------+---------------------+--------+-------------------------------------+
|4095919448|54.6666894 25.1168508|31447217|54.666942600000006 25.115928200000003|
+----------+---------------------+--------+-------------------------------------+
only showing top 1 row
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val linestring_coordinates = concat_coordinates.select($&quot;src&quot;, $&quot;dst&quot;,concat($&quot;src_coordinates&quot;, lit(&quot;,&quot;), $&quot;dst_coordinates&quot;).alias(&quot;list_of_coordinates&quot;))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>linestring_coordinates: org.apache.spark.sql.DataFrame = [src: bigint, dst: bigint ... 1 more field]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">linestring_coordinates.show(1, false)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+----------+--------+-----------------------------------------------------------+
|src       |dst     |list_of_coordinates                                        |
+----------+--------+-----------------------------------------------------------+
|4095919448|31447217|54.6666894 25.1168508,54.666942600000006 25.115928200000003|
+----------+--------+-----------------------------------------------------------+
only showing top 1 row
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val first = linestring_coordinates.select(concat(lit(&quot;LineString:&quot;),$&quot;src&quot;,lit(&quot;+&quot;), $&quot;dst&quot;).alias(&quot;LineString&quot;),$&quot;list_of_coordinates&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>first: org.apache.spark.sql.DataFrame = [LineString: string, list_of_coordinates: string]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val first_rdd = first.rdd
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>first_rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[675] at rdd at command-197980058855258:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import com.esri.arcgisruntime.geometry.{Point, SpatialReference, GeometryEngine}
import com.esri.arcgisruntime.geometry.GeometryEngine.project
import com.esri.arcgisruntime._
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import com.esri.arcgisruntime.geometry.{Point, SpatialReference, GeometryEngine}
import com.esri.arcgisruntime.geometry.GeometryEngine.project
import com.esri.arcgisruntime._
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">if(!ArcGISRuntimeEnvironment.isInitialized())
    {
      ArcGISRuntimeEnvironment.setInstallDirectory(&quot;/dbfs/arcGISRuntime/arcgis-runtime-sdk-java-100.4.0/&quot;)
      ArcGISRuntimeEnvironment.initialize() 
    }
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def project_to_meters(lon: String, lat: String): String = { 
    
    if(!ArcGISRuntimeEnvironment.isInitialized())
    {
      ArcGISRuntimeEnvironment.setInstallDirectory(&quot;/dbfs/arcGISRuntime/arcgis-runtime-sdk-java-100.4.0/&quot;)
      ArcGISRuntimeEnvironment.initialize() 
    }
  
    val initial_point = new Point(lon.toDouble, lat.toDouble, SpatialReference.create(4326))
    val reprojection = GeometryEngine.project(initial_point, SpatialReference.create(3035))
    reprojection.toString
}
spark.udf.register(&quot;project_to_meters&quot;, project_to_meters(_:String, _:String):String)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>project_to_meters: (lon: String, lat: String)String
res31: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function2&gt;,StringType,Some(List(StringType, StringType)))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">first_rdd.take(1)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res32: Array[org.apache.spark.sql.Row] = Array([LineString:4095919448+31447217,54.6666894 25.1168508,54.666942600000006 25.115928200000003])
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val ways_reprojected = first_rdd.map(line =&gt; line.toString.replaceAll(&quot;\\[&quot;,&quot;&quot;).replaceAll(&quot;\\]&quot;,&quot;&quot;)).map(line =&gt; {val parts = line.replaceAll(&quot;\&quot;&quot;,&quot;&quot;).split(&quot;,&quot;);val arrCoords = parts.slice(1,parts.length).map(xyStr =&gt; {val xy = xyStr.split(&quot; &quot;);val reprojection = project_to_meters(xy(1).toString, xy(0).toString);val coords = reprojection.replaceAll(&quot;,&quot;,&quot;&quot;).replaceAll(&quot;\\[&quot;,&quot;&quot;).split(&quot; &quot;).slice(1,reprojection.length);val xy_new = coords(0).toString +&quot; &quot;+ coords(1).toString;xy_new});(parts(0).toString,arrCoords)})
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>ways_reprojected: org.apache.spark.rdd.RDD[(String, Array[String])] = MapPartitionsRDD[677] at map at command-197980058855264:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">ways_reprojected.take(1)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res33: Array[(String, Array[String])] = Array((LineString:4095919448+31447217,Array(5288428.785893 3608569.901562, 5288364.771866 3608585.141629)))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">ways_reprojected.map(item =&gt; item._2(1)).take(1)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res34: Array[String] = Array(5288364.771866 3608585.141629)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val ways_unpacked = ways_reprojected.map(item =&gt; item._1.toString + &quot;,&quot; + item._2(0).toString + &quot;,&quot; + item._2(1).toString)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>ways_unpacked: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[679] at map at command-197980058855265:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rdd_first_set = ways_unpacked.mapPartitions(_.map(line =&gt;{val parts = line.replaceAll(&quot;\&quot;&quot;,&quot;&quot;).split(',');val arrCoords = parts.slice(1, parts.length).map(xyStr =&gt; {val xy = xyStr.split(' ');(xy(0).toDouble.toInt, xy(1).toDouble.toInt)});new GMLineString(parts(0), arrCoords)}))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rdd_first_set: org.apache.spark.rdd.RDD[org.cusp.bdi.gm.geom.GMLineString] = MapPartitionsRDD[680] at mapPartitions at command-197980058855267:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd_first_set.count()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res35: Long = 730237
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def unpack_lat(str: String): String = {
        val lat = str.replaceAll(&quot;,&quot;,&quot;&quot;).replaceAll(&quot;\\[&quot;,&quot;&quot;).split(&quot; &quot;)(2)
        return lat
}
spark.udf.register(&quot;unpack_lat&quot;, unpack_lat(_:String): String)

def unpack_lon(str: String): String = {
        val lon = str.replaceAll(&quot;,&quot;,&quot;&quot;).replaceAll(&quot;\\[&quot;,&quot;&quot;).split(&quot; &quot;)(1)
        return lon
}
spark.udf.register(&quot;unpack_lon&quot;, unpack_lon(_:String): String)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>unpack_lat: (str: String)String
unpack_lon: (str: String)String
res36: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function1&gt;,StringType,Some(List(StringType)))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val geoMatchSecond = new GeoMatch(false, 256, 200, (-1, -1, -1, -1)) //n(=dimension of the Hilber curve) should be a power of 2. 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>geoMatchSecond: org.cusp.bdi.gm.GeoMatch = GeoMatch(false,256,200.0,(-1,-1,-1,-1))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val resultRDDsecond = geoMatchSecond.spatialJoinKNN(rdd_first_set, rddSecondSetSecondRound, 1, false)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>resultRDDsecond: org.apache.spark.rdd.RDD[(org.cusp.bdi.gm.geom.GMPoint, scala.collection.mutable.ListBuffer[org.cusp.bdi.gm.geom.GMLineString])] = MapPartitionsRDD[693] at mapPartitions at GeoMatch.scala:94
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">resultRDDsecond.map(element =&gt; (element._1.payload, element._2.map(_.payload))).filter(element =&gt; (element._2.isEmpty)).count()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res37: Long = 275
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>The next step is for each state to obtain the count</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val res = resultRDDsecond.map(element =&gt; (element._1.payload, element._2.map(_.payload))).filter(element =&gt; !(element._2.isEmpty))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res: org.apache.spark.rdd.RDD[(String, scala.collection.mutable.ListBuffer[String])] = MapPartitionsRDD[697] at filter at command-197980058855277:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val res_df = res.map(element =&gt; (element._1, element._2(0))).toDF(&quot;PointId&quot;, &quot;State&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res_df: org.apache.spark.sql.DataFrame = [PointId: string, State: string]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val edge_counts = res_df.groupBy(&quot;State&quot;).count
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>edge_counts: org.apache.spark.sql.DataFrame = [State: string, count: bigint]
</code></pre>
</div>
</div>
<div class="cell markdown">
<p><code>edge_counts.show(2, false)</code></p>
<p>Output:</p>
<pre><code>+--------------------------------+-----+
|State                           |count|
+--------------------------------+-----+
|LineString:469327286+3637433937 |a    |
|LineString:2488853231+272553182 |b    |
|LineString:5074963276+2221962222|c    |
+--------------------------------+-----+
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val res1 = resultRDD.map(element =&gt; (element._1.payload, element._2.map(_.payload))).filter(element =&gt; !(element._2.isEmpty))
val res1_df = res1.map(element =&gt; (element._1, element._2(0))).toDF(&quot;PointId&quot;, &quot;State&quot;)
val intersection_counts = res1_df.groupBy(&quot;State&quot;).count
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res1: org.apache.spark.rdd.RDD[(String, scala.collection.mutable.ListBuffer[String])] = MapPartitionsRDD[706] at filter at command-197980058855280:1
res1_df: org.apache.spark.sql.DataFrame = [PointId: string, State: string]
intersection_counts: org.apache.spark.sql.DataFrame = [State: string, count: bigint]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.sql.functions._
val state_counts = edge_counts.union(intersection_counts)
state_counts.agg(sum(&quot;count&quot;)).show()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+----------+
|sum(count)|
+----------+
|     11714|
+----------+

import org.apache.spark.sql.functions._
state_counts: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [State: string, count: bigint]
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Find the states with no matched events, assign count value equal to 0 and union them with the rest of the states_counts</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val all_intersection_states = rdd_first_set_intersections.toDF(&quot;stateId&quot;, &quot;coords&quot;).drop(&quot;coords&quot;)
val all_edge_states = rdd_first_set.toDF(&quot;stateId&quot;, &quot;coords&quot;).drop(&quot;coords&quot;)
val all_states = all_intersection_states.union(all_edge_states)
all_states.count
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>all_intersection_states: org.apache.spark.sql.DataFrame = [stateId: string]
all_edge_states: org.apache.spark.sql.DataFrame = [stateId: string]
all_states: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [stateId: string]
res49: Long = 892562
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val s1 = all_states.join(state_counts, all_states(&quot;stateId&quot;) === state_counts(&quot;State&quot;), &quot;left_outer&quot;).drop(&quot;State&quot;)
val s_final = s1.na.fill(0)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>s1: org.apache.spark.sql.DataFrame = [stateId: string, count: bigint]
s_final: org.apache.spark.sql.DataFrame = [stateId: string, count: bigint]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../../contents/000_5-sds-2-x-geo/034_03_MapMatching_on_a_Graph_LT.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next" href="../../contents/000_5-sds-2-x-geo/034_05DistributionOfStates.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../../contents/000_5-sds-2-x-geo/034_03_MapMatching_on_a_Graph_LT.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next" href="../../contents/000_5-sds-2-x-geo/034_05DistributionOfStates.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="../../elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="../../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->


    </body>
</html>
