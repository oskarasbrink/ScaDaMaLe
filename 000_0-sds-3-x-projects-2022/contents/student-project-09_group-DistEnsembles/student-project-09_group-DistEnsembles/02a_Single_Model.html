<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>02a_Single_Model - sds-3.x/ScaDaMaLe</title>


        <!-- Custom HTML head -->

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../../../favicon.svg">
        <link rel="shortcut icon" href="../../../favicon.png">
        <link rel="stylesheet" href="../../../css/variables.css">
        <link rel="stylesheet" href="../../../css/general.css">
        <link rel="stylesheet" href="../../../css/chrome.css">
        <link rel="stylesheet" href="../../../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../../../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../../../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../../../highlight.css">
        <link rel="stylesheet" href="../../../tomorrow-night.css">
        <link rel="stylesheet" href="../../../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../../../scroll-mdbook-outputs.css">

        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../../../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/00_Introduction.html"><strong aria-hidden="true">1.</strong> student-project-01_group-GraphOfWiki_00_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/01_DataLoading_redirectsTable.html"><strong aria-hidden="true">1.1.</strong> 01_DataLoading_redirectsTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/02_DataLoading_pagesTable.html"><strong aria-hidden="true">1.2.</strong> 02_DataLoading_pagesTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/03_DataLoading_pagelinksTable.html"><strong aria-hidden="true">1.3.</strong> 03_DataLoading_pagelinksTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/04_DataLoading_categorylinksTable.html"><strong aria-hidden="true">1.4.</strong> 04_DataLoading_categorylinksTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/05_DataLoading_categoryTable.html"><strong aria-hidden="true">1.5.</strong> 05_DataLoading_categoryTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/06_redirectRemoval.html"><strong aria-hidden="true">1.6.</strong> 06_redirectRemoval</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/07_createArticleGraph.html"><strong aria-hidden="true">1.7.</strong> 07_createArticleGraph</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/08_explorationArticleGraph.html"><strong aria-hidden="true">1.8.</strong> 08_explorationArticleGraph</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/09_fullGraphAnalysis.html"><strong aria-hidden="true">1.9.</strong> 09_fullGraphAnalysis</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/10_explorativeMotifs.html"><strong aria-hidden="true">1.10.</strong> 10_explorativeMotifs</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/11_conclusionDiscussionAndFutureWork.html"><strong aria-hidden="true">1.11.</strong> 11_conclusionDiscussionAndFutureWork</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/12_gameNotebookSetup.html"><strong aria-hidden="true">1.12.</strong> 12_gameNotebookSetup</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/13_gameNotebook.html"><strong aria-hidden="true">1.13.</strong> 13_gameNotebook</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/99_PlanningAndNotes.html"><strong aria-hidden="true">1.14.</strong> 99_PlanningAndNotes</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-02_group-DDLOfVision/student-project-02_group-DDLOfVision/00_vqa_introduction.html"><strong aria-hidden="true">2.</strong> student-project-02_group-DDLOfVision_00_vqa_introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-02_group-DDLOfVision/student-project-02_group-DDLOfVision/01_vqa_model_training.html"><strong aria-hidden="true">2.1.</strong> 01_vqa_model_training</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-02_group-DDLOfVision/student-project-02_group-DDLOfVision/02_vqa_model_inference.html"><strong aria-hidden="true">2.2.</strong> 02_vqa_model_inference</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-02_group-DDLOfVision/student-project-02_group-DDLOfVision/0y_test_mnist-pytorch.html"><strong aria-hidden="true">2.3.</strong> 0y_test_mnist-pytorch</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/00_ingest_data.html"><strong aria-hidden="true">3.</strong> student-project-03_group-WikiKG90mv2_00_ingest_data</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/01_fetch_descriptions.html"><strong aria-hidden="true">3.1.</strong> 01_fetch_descriptions</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/02_load_data.html"><strong aria-hidden="true">3.2.</strong> 02_load_data</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/03_data_exploration.html"><strong aria-hidden="true">3.3.</strong> 03_data_exploration</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/04_analysing_relation_types.html"><strong aria-hidden="true">3.4.</strong> 04_analysing_relation_types</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/05_Motif_search_defination_code.html"><strong aria-hidden="true">3.5.</strong> 05_Motif_search_defination_code</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/06_python_analysis.html"><strong aria-hidden="true">3.6.</strong> 06_python_analysis</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/08_pagerank.html"><strong aria-hidden="true">3.7.</strong> 08_pagerank</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/09_outro_discussion.html"><strong aria-hidden="true">3.8.</strong> 09_outro_discussion</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/0x_Motif_search_defination_code.html"><strong aria-hidden="true">3.9.</strong> 0x_Motif_search_defination_code</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/10_motif_mining_WikiKGv2.html"><strong aria-hidden="true">3.10.</strong> 10_motif_mining_WikiKGv2</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-04_group-FedMLMedicalApp/student-project-04_group-FedMLMedicalApp/00_Notebook_Presentation.html"><strong aria-hidden="true">4.</strong> student-project-04_group-FedMLMedicalApp_00_Notebook_Presentation</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-04_group-FedMLMedicalApp/student-project-04_group-FedMLMedicalApp/01_BrainTumorSegmentation_Centralized_Training.html"><strong aria-hidden="true">4.1.</strong> 01_BrainTumorSegmentation_Centralized_Training</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-04_group-FedMLMedicalApp/student-project-04_group-FedMLMedicalApp/02_Federated_Learning_BrainTumorSegmentation.html"><strong aria-hidden="true">4.2.</strong> 02_Federated_Learning_BrainTumorSegmentation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-04_group-FedMLMedicalApp/student-project-04_group-FedMLMedicalApp/data_upload_test.html"><strong aria-hidden="true">4.3.</strong> data_upload_test</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/00_introduction.html"><strong aria-hidden="true">5.</strong> student-project-05_group-DistOpt_00_introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/01_Bayesian_optimization.html"><strong aria-hidden="true">5.1.</strong> 01_Bayesian_optimization</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/02_Gaussian_processes.html"><strong aria-hidden="true">5.2.</strong> 02_Gaussian_processes</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/03_acquisition_functions.html"><strong aria-hidden="true">5.3.</strong> 03_acquisition_functions</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/04_scalable_Bayesian_optimization.html"><strong aria-hidden="true">5.4.</strong> 04_scalable_Bayesian_optimization</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/05_implementation_documentation.html"><strong aria-hidden="true">5.5.</strong> 05_implementation_documentation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/06_our_implementation.html"><strong aria-hidden="true">5.6.</strong> 06_our_implementation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/07_additional_code.html"><strong aria-hidden="true">5.7.</strong> 07_additional_code</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-07_group-ExpsZerOInit/student-project-07_group-ExpsZerOInit/00_introduction.html"><strong aria-hidden="true">6.</strong> student-project-07_group-ExpsZerOInit_00_introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-07_group-ExpsZerOInit/student-project-07_group-ExpsZerOInit/01_resnet.html"><strong aria-hidden="true">6.1.</strong> 01_resnet</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-08_group-WikiSearch/student-project-08_group-WikiSearch/00_Introduction.html"><strong aria-hidden="true">7.</strong> student-project-08_group-WikiSearch_00_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-08_group-WikiSearch/student-project-08_group-WikiSearch/01_InputParsing.html"><strong aria-hidden="true">7.1.</strong> 01_InputParsing</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-08_group-WikiSearch/student-project-08_group-WikiSearch/02_PageRank.html"><strong aria-hidden="true">7.2.</strong> 02_PageRank</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-08_group-WikiSearch/student-project-08_group-WikiSearch/03_QuerySearch.html"><strong aria-hidden="true">7.3.</strong> 03_QuerySearch</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/00_Introduction.html"><strong aria-hidden="true">8.</strong> student-project-09_group-DistEnsembles_00_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/01_Data.html"><strong aria-hidden="true">8.1.</strong> 01_Data</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/02_Main.html"><strong aria-hidden="true">8.2.</strong> 02_Main</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/02_non_distributed.html"><strong aria-hidden="true">8.3.</strong> 02_non_distributed</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/02a_Single_Model.html" class="active"><strong aria-hidden="true">8.4.</strong> 02a_Single_Model</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/03_Evaluation.html"><strong aria-hidden="true">8.5.</strong> 03_Evaluation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/03_RDDs.html"><strong aria-hidden="true">8.6.</strong> 03_RDDs</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/03a_Ensemble.html"><strong aria-hidden="true">8.7.</strong> 03a_Ensemble</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/041_Data_Preprocessing.html"><strong aria-hidden="true">8.8.</strong> 041_Data_Preprocessing</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/00_introduction.html"><strong aria-hidden="true">9.</strong> student-project-10_group-RDI_00_introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/01_prepare_data.html"><strong aria-hidden="true">9.1.</strong> 01_prepare_data</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/02_baseline.html"><strong aria-hidden="true">9.2.</strong> 02_baseline</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/03_single_machine.html"><strong aria-hidden="true">9.3.</strong> 03_single_machine</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/04_distributed_learning.html"><strong aria-hidden="true">9.4.</strong> 04_distributed_learning</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-11_group-CollaborativeFiltering/student-project-11_group-CollaborativeFiltering/01_Introduction.html"><strong aria-hidden="true">10.</strong> student-project-11_group-CollaborativeFiltering_01_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-11_group-CollaborativeFiltering/student-project-11_group-CollaborativeFiltering/01a_Implementation.html"><strong aria-hidden="true">10.1.</strong> 01a_Implementation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-11_group-CollaborativeFiltering/student-project-11_group-CollaborativeFiltering/02_FirstOrderMethods.html"><strong aria-hidden="true">10.2.</strong> 02_FirstOrderMethods</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../editors.html"><strong aria-hidden="true">11.</strong> Editors</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">sds-3.x/ScaDaMaLe</h1>

                    <div class="right-buttons">
                        <a href="../../../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div class="cell markdown">
<ul>
<li>Data loading
<ul>
<li>-&gt; RDD</li>
</ul>
</li>
<li>Data splitting
<ul>
<li>Take one subject for held-out testing</li>
<li>For the remaining subjects:
<ul>
<li>Use at least 50% of data as unlabeled data</li>
</ul>
</li>
</ul>
</li>
<li>Ensemble training
<ul>
<li>Do 1 epoch of supervised training</li>
<li>Generate pseudo-labels for unlabeled data</li>
<li>Repeat</li>
</ul>
</li>
<li>Compare to training ensemble on only labeled data</li>
</ul>
</div>
<div class="cell markdown">
<h1 id="licensing"><a class="header" href="#licensing">Licensing</a></h1>
</div>
<div class="cell markdown">
<p>Parts of this code are taken from <a href="https://github.com/facebookresearch/VideoPose3D">VideoPose3D</a> repository.</p>
<pre><code>Copyright (c) 2018-present, Facebook, Inc.
All rights reserved.

This source code is licensed under the license found in the
LICENSE file in the root directory of this source tree.
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-fs">ls VideoPose3D/humaneva
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>path</th>
<th>name</th>
<th>size</th>
<th>modificationTime</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dbfs:/VideoPose3D/humaneva/data_2d_humaneva15_detectron_pt_coco.npz</td>
<td>data_2d_humaneva15_detectron_pt_coco.npz</td>
<td>1.6876431e7</td>
<td>1.669130266e12</td>
</tr>
<tr class="even">
<td>dbfs:/VideoPose3D/humaneva/data_2d_humaneva15_gt.npz</td>
<td>data_2d_humaneva15_gt.npz</td>
<td>2956013.0</td>
<td>1.669129922e12</td>
</tr>
<tr class="odd">
<td>dbfs:/VideoPose3D/humaneva/data_3d_humaneva15.npz</td>
<td>data_3d_humaneva15.npz</td>
<td>1578843.0</td>
<td>1.669129924e12</td>
</tr>
<tr class="even">
<td>dbfs:/VideoPose3D/humaneva/humaneva_cameras_extrinsic_params.npz</td>
<td>humaneva_cameras_extrinsic_params.npz</td>
<td>1251.0</td>
<td>1.669130248e12</td>
</tr>
<tr class="odd">
<td>dbfs:/VideoPose3D/humaneva/humaneva_cameras_intrinsic_params.npz</td>
<td>humaneva_cameras_intrinsic_params.npz</td>
<td>548.0</td>
<td>1.669130248e12</td>
</tr>
<tr class="even">
<td>dbfs:/VideoPose3D/humaneva/humaneva_skeleton.npz</td>
<td>humaneva_skeleton.npz</td>
<td>539.0</td>
<td>1.669130248e12</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell markdown">
<h1 id="imports"><a class="header" href="#imports">Imports</a></h1>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
import os
import sys
import errno
from time import time
import copy
import hashlib
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation, writers
import subprocess as sp
from tqdm import tqdm
import shutil

assert(torch.cuda.is_available())

ROOTDIR = '/dbfs/VideoPose3D'
</code></pre>
</div>
<div class="cell markdown">
<h1 id="utilities"><a class="header" href="#utilities">Utilities</a></h1>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def wrap(func, *args, unsqueeze=False):
    &quot;&quot;&quot;
    Wrap a torch function so it can be called with NumPy arrays.
    Input and return types are seamlessly converted.
    &quot;&quot;&quot;
    
    # Convert input types where applicable
    args = list(args)
    for i, arg in enumerate(args):
        if type(arg) == np.ndarray:
            args[i] = torch.from_numpy(arg)
            if unsqueeze:
                args[i] = args[i].unsqueeze(0)
        
    result = func(*args)
    
    # Convert output types where applicable
    if isinstance(result, tuple):
        result = list(result)
        for i, res in enumerate(result):
            if type(res) == torch.Tensor:
                if unsqueeze:
                    res = res.squeeze(0)
                result[i] = res.numpy()
        return tuple(result)
    elif type(result) == torch.Tensor:
        if unsqueeze:
            result = result.squeeze(0)
        return result.numpy()
    else:
        return result

def qrot(q, v):
    &quot;&quot;&quot;
    Rotate vector(s) v about the rotation described by quaternion(s) q.
    Expects a tensor of shape (*, 4) for q and a tensor of shape (*, 3) for v,
    where * denotes any number of dimensions.
    Returns a tensor of shape (*, 3).
    &quot;&quot;&quot;
    assert q.shape[-1] == 4
    assert v.shape[-1] == 3
    assert q.shape[:-1] == v.shape[:-1]

    qvec = q[..., 1:]
    uv = torch.cross(qvec, v, dim=len(q.shape)-1)
    uuv = torch.cross(qvec, uv, dim=len(q.shape)-1)
    return (v + 2 * (q[..., :1] * uv + uuv))

def qinverse(q, inplace=False):
    # We assume the quaternion to be normalized
    if inplace:
        q[..., 1:] *= -1
        return q
    else:
        w = q[..., :1]
        xyz = q[..., 1:]
        return torch.cat((w, -xyz), dim=len(q.shape)-1)
</code></pre>
</div>
<div class="cell markdown">
<h3 id="camera-utilities"><a class="header" href="#camera-utilities">Camera utilities</a></h3>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def normalize_screen_coordinates(X, w, h): 
    assert X.shape[-1] == 2
    # Normalize so that [0, w] is mapped to [-1, 1], while preserving the aspect ratio
    return X/w*2 - [1, h/w]
    
def image_coordinates(X, w, h):
    assert X.shape[-1] == 2
    # Reverse camera frame normalization
    return (X + [1, h/w])*w/2

def world_to_camera(X, R, t):
    Rt = wrap(qinverse, R) # Invert rotation
    return wrap(qrot, np.tile(Rt, (*X.shape[:-1], 1)), X - t) # Rotate and translate

def camera_to_world(X, R, t):
    return wrap(qrot, np.tile(R, (*X.shape[:-1], 1)), X) + t
</code></pre>
</div>
<div class="cell markdown">
<h3 id="data-utilities"><a class="header" href="#data-utilities">Data utilities</a></h3>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">class Skeleton:
    def __init__(self, parents, joints_left, joints_right):
        assert len(joints_left) == len(joints_right)
        
        self._parents = np.array(parents)
        self._joints_left = joints_left
        self._joints_right = joints_right
        self._compute_metadata()
    
    def num_joints(self):
        return len(self._parents)
    
    def parents(self):
        return self._parents
    
    def has_children(self):
        return self._has_children
    
    def children(self):
        return self._children
    
    def remove_joints(self, joints_to_remove):
        &quot;&quot;&quot;
        Remove the joints specified in 'joints_to_remove'.
        &quot;&quot;&quot;
        valid_joints = []
        for joint in range(len(self._parents)):
            if joint not in joints_to_remove:
                valid_joints.append(joint)

        for i in range(len(self._parents)):
            while self._parents[i] in joints_to_remove:
                self._parents[i] = self._parents[self._parents[i]]
                
        index_offsets = np.zeros(len(self._parents), dtype=int)
        new_parents = []
        for i, parent in enumerate(self._parents):
            if i not in joints_to_remove:
                new_parents.append(parent - index_offsets[parent])
            else:
                index_offsets[i:] += 1
        self._parents = np.array(new_parents)
        
        
        if self._joints_left is not None:
            new_joints_left = []
            for joint in self._joints_left:
                if joint in valid_joints:
                    new_joints_left.append(joint - index_offsets[joint])
            self._joints_left = new_joints_left
        if self._joints_right is not None:
            new_joints_right = []
            for joint in self._joints_right:
                if joint in valid_joints:
                    new_joints_right.append(joint - index_offsets[joint])
            self._joints_right = new_joints_right

        self._compute_metadata()
        
        return valid_joints
    
    def joints_left(self):
        return self._joints_left
    
    def joints_right(self):
        return self._joints_right
        
    def _compute_metadata(self):
        self._has_children = np.zeros(len(self._parents)).astype(bool)
        for i, parent in enumerate(self._parents):
            if parent != -1:
                self._has_children[parent] = True

        self._children = []
        for i, parent in enumerate(self._parents):
            self._children.append([])
        for i, parent in enumerate(self._parents):
            if parent != -1:
                self._children[parent].append(i)

class MocapDataSubset(torch.utils.data.Dataset):
    def __init__(self, fps, skeleton):
        self._skeleton = skeleton
        self._fps = fps
        self._data = None # Must be filled by subclass
        self._cameras = None # Must be filled by subclass
        self._data_list = None # Must be filled by subclass
        self.receptive_field = 1 # Must be filled by subclass
    
    def remove_joints(self, joints_to_remove):
        kept_joints = self._skeleton.remove_joints(joints_to_remove)
        for subject in self._data.keys():
            for action in self._data[subject].keys():
                s = self._data[subject][action]
                if 'positions' in s:
                    s['positions'] = s['positions'][:, kept_joints]
                
        
    def __getitem__(self, idx):
        # idx = torch.randint(len(self._data_list),[1])
        data = self._data_list[idx]
        pos_3d = data['positions_3d'][0]
        pos_2d = data['kps'][0]
        i = torch.randint(pos_3d.shape[0] - self.receptive_field + 1, [1])
        sample = {
            'pos_2d': pos_2d[i:i+self.receptive_field], 
            'pos_3d': pos_3d[i+(self.receptive_field - 1)//2][None]
        }
        return sample

    def __len__(self):
        return 1280
        
    def subjects(self):
        return self._data.keys()
    
    def fps(self):
        return self._fps
    
    def skeleton(self):
        return self._skeleton
        
    def cameras(self):
        return self._cameras
    
    def supports_semi_supervised(self):
        # This method can be overridden
        return False
 
</code></pre>
</div>
<div class="cell markdown">
<h3 id="humaneva-dataset"><a class="header" href="#humaneva-dataset">HumanEva dataset</a></h3>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">humaneva_skeleton_data = np.load(f'{ROOTDIR}/humaneva/humaneva_skeleton.npz', allow_pickle=True)['data'].item()
humaneva_skeleton = Skeleton(parents=humaneva_skeleton_data['parents'],
                             joints_left=humaneva_skeleton_data['joints_left'],
                             joints_right=humaneva_skeleton_data['joints_right'])
humaneva_cameras_intrinsic_params = list(np.load(f'{ROOTDIR}/humaneva/humaneva_cameras_intrinsic_params.npz', allow_pickle=True)['data'])
humaneva_cameras_extrinsic_params = np.load(f'{ROOTDIR}/humaneva/humaneva_cameras_extrinsic_params.npz', allow_pickle=True)['data'].item()

class HumanEvaDataSubset(MocapDataSubset):
    def __init__(self, path, keypoints_path,
                 subjects=None,
                 prefixes=['Train/', 'Validate/', 'Unlabeled/Train/', 'Unlabeled/Validate/', 'Unlabeled/'],
                 local_copy=None,
                 stride=1,
                 pad=0):
        if local_copy is not None:
            if not os.path.exists(local_copy):
                os.makedirs(local_copy)
            path_new = os.path.join(local_copy, os.path.split(path)[1])
            keypoints_path_new = os.path.join(local_copy, os.path.split(keypoints_path)[1])
            shutil.copyfile(path, path_new)
            shutil.copyfile(keypoints_path, keypoints_path_new)
            path = path_new
            keypoints_path = keypoints_path_new
#             print(path)
#             print(keypoints_path)
            
        super().__init__(fps=60, skeleton=humaneva_skeleton)
        self._cameras = copy.deepcopy(humaneva_cameras_extrinsic_params)
        subjects = self._cameras.keys() if subjects is None else subjects
        for subject in subjects:
            for i, cam in enumerate(self._cameras[subject]):
                cam.update(humaneva_cameras_intrinsic_params[i])
                for k, v in cam.items():
                    if k not in ['id', 'res_w', 'res_h']:
                        cam[k] = np.array(v, dtype='float32')
                if 'translation' in cam:
                    cam['translation'] = cam['translation']/1000 # mm to meters
                
        keys = list(self._cameras.keys())
        for subject in keys:
            cameras = self._cameras[subject]
            if subject in subjects:
                for prefix in prefixes:
                    self._cameras[prefix + subject] = cameras
            del self._cameras[subject]
        
        subjects = list(self._cameras.keys())
        
        # Load serialized dataset
        data = np.load(path, allow_pickle=True)['positions_3d'].item()
        
        self._data = {}
        print(data.keys())
        for subject in subjects:
            actions = data[subject]
            self._data[subject] = {}
            for action_name in actions.keys():
                positions = actions[action_name]
                positions_3d = []
                for cam in self._cameras[subject]:
                    pos_3d = world_to_camera(positions, R=cam['orientation'], t=cam['translation'])
                    pos_3d[:, 1:] -= pos_3d[:, :1]
                    # Remove global offset, but keep trajectory in first position
                    positions_3d.append(pos_3d)
                self._data[subject][action_name] = {
                    'subject': subject,
                    'action_name': action_name,
                    'positions': positions,
                    'positions_3d': positions_3d,
                    'cameras': self._cameras[subject],
                }
        
        keypoints = np.load(keypoints_path, allow_pickle=True)
        keypoints_metadata = keypoints['metadata'].item()
        keypoints_symmetry = keypoints_metadata['keypoints_symmetry']
        kps_left, kps_right = list(keypoints_symmetry[0]), list(keypoints_symmetry[1])
        joints_left, joints_right = list(self.skeleton().joints_left()), list(self.skeleton().joints_right())
        keypoints = keypoints['positions_2d'].item()
        for subject in subjects:
            assert subject in keypoints, 'Subject {} is missing from the 2D detections dataset'.format(subject)
            for action in self._data[subject].keys():
                assert action in keypoints[subject], 'Action {} of subject {} is missing from the 2D detections dataset'.format(action, subject)
                if 'positions_3d' not in self._data[subject][action]:
                    continue
                for cam_idx in range(len(keypoints[subject][action])):
                    # We check for &gt;= instead of == because some videos in H3.6M contain extra frames
                    mocap_length = self._data[subject][action]['positions_3d'][cam_idx].shape[0]
                    assert keypoints[subject][action][cam_idx].shape[0] &gt;= mocap_length
                    if keypoints[subject][action][cam_idx].shape[0] &gt; mocap_length:
                        # Shorten sequence
                        keypoints[subject][action][cam_idx] = keypoints[subject][action][cam_idx][:mocap_length]
                assert len(keypoints[subject][action]) == len(self._data[subject][action]['positions_3d'])

        keys = list(keypoints.keys())
        for subject in keys:
            if subject in subjects:
                for action in keypoints[subject]:
                    for cam_idx, kps in enumerate(keypoints[subject][action]):
                        # Normalize camera frame
                        cam = self._cameras[subject][cam_idx]
                        kps[..., :2] = normalize_screen_coordinates(kps[..., :2], w=cam['res_w'], h=cam['res_h'])
                        keypoints[subject][action][cam_idx] = kps
                        if action in self._data[subject].keys():
                            self._data[subject][action]['kps'] = keypoints[subject][action]
            else:
                del keypoints[subject]
        self._keypoints = keypoints
        self.receptive_field = 2 * pad + 1
        
        self._data_list = []
        for subject in subjects:
            for action_name in self._data[subject].keys():
                if len(self._data[subject][action_name]['positions_3d'][0]) &gt;= self.receptive_field:
                    self._data_list.append(self._data[subject][action_name])
</code></pre>
</div>
<div class="cell markdown">
<h1 id="model-definitions"><a class="header" href="#model-definitions">Model Definitions</a></h1>
</div>
<div class="cell markdown">
<h3 id="temporalmodel"><a class="header" href="#temporalmodel">TemporalModel</a></h3>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">class TemporalModelBase(nn.Module):
    &quot;&quot;&quot;
    Do not instantiate this class.
    &quot;&quot;&quot;
    
    def __init__(self, num_joints_in, in_features, num_joints_out,
                 filter_widths, causal, dropout, channels):
        super().__init__()
        
        # Validate input
        for fw in filter_widths:
            assert fw % 2 != 0, 'Only odd filter widths are supported'
        
        self.num_joints_in = num_joints_in
        self.in_features = in_features
        self.num_joints_out = num_joints_out
        self.filter_widths = filter_widths
        
        self.drop = nn.Dropout(dropout)
        self.relu = nn.ReLU(inplace=True)
        
        self.pad = [ filter_widths[0] // 2 ]
        self.expand_bn = nn.BatchNorm1d(channels, momentum=0.1)
        self.shrink = nn.Conv1d(channels, num_joints_out*3, 1)
        

    def set_bn_momentum(self, momentum):
        self.expand_bn.momentum = momentum
        for bn in self.layers_bn:
            bn.momentum = momentum
            
    def receptive_field(self):
        &quot;&quot;&quot;
        Return the total receptive field of this model as # of frames.
        &quot;&quot;&quot;
        frames = 0
        for f in self.pad:
            frames += f
        return 1 + 2*frames
    
    def total_causal_shift(self):
        &quot;&quot;&quot;
        Return the asymmetric offset for sequence padding.
        The returned value is typically 0 if causal convolutions are disabled,
        otherwise it is half the receptive field.
        &quot;&quot;&quot;
        frames = self.causal_shift[0]
        next_dilation = self.filter_widths[0]
        for i in range(1, len(self.filter_widths)):
            frames += self.causal_shift[i] * next_dilation
            next_dilation *= self.filter_widths[i]
        return frames
        
    def forward(self, x):
        assert len(x.shape) == 4
        assert x.shape[-2] == self.num_joints_in
        assert x.shape[-1] == self.in_features
        
        sz = x.shape[:3]

        x = x.view(x.shape[0], x.shape[1], -1)

        x = x.permute(0, 2, 1)

        
        x = self._forward_blocks(x)
        
        x = x.permute(0, 2, 1)
        x = x.view(sz[0], -1, self.num_joints_out, 3)
        
        return x    

class TemporalModel(TemporalModelBase):
    &quot;&quot;&quot;
    Reference 3D pose estimation model with temporal convolutions.
    This implementation can be used for all use-cases.
    &quot;&quot;&quot;
    
    def __init__(self, num_joints_in, in_features, num_joints_out,
                 filter_widths, causal=False, dropout=0.25, channels=1024, dense=False):
        &quot;&quot;&quot;
        Initialize this model.
        
        Arguments:
        num_joints_in -- number of input joints (e.g. 17 for Human3.6M)
        in_features -- number of input features for each joint (typically 2 for 2D input)
        num_joints_out -- number of output joints (can be different than input)
        filter_widths -- list of convolution widths, which also determines the # of blocks and receptive field
        causal -- use causal convolutions instead of symmetric convolutions (for real-time applications)
        dropout -- dropout probability
        channels -- number of convolution channels
        dense -- use regular dense convolutions instead of dilated convolutions (ablation experiment)
        &quot;&quot;&quot;
        super().__init__(num_joints_in, in_features, num_joints_out, filter_widths, causal, dropout, channels)
        
        self.expand_conv = nn.Conv1d(num_joints_in*in_features, channels, filter_widths[0], bias=False)
        
        layers_conv = []
        layers_bn = []
        
        self.causal_shift = [ (filter_widths[0]) // 2 if causal else 0 ]
        next_dilation = filter_widths[0]
        for i in range(1, len(filter_widths)):
            self.pad.append((filter_widths[i] - 1)*next_dilation // 2)
            self.causal_shift.append((filter_widths[i]//2 * next_dilation) if causal else 0)
            
            layers_conv.append(nn.Conv1d(channels, channels,
                                         filter_widths[i] if not dense else (2*self.pad[-1] + 1),
                                         dilation=next_dilation if not dense else 1,
                                         bias=False))
            layers_bn.append(nn.BatchNorm1d(channels, momentum=0.1))
            layers_conv.append(nn.Conv1d(channels, channels, 1, dilation=1, bias=False))
            layers_bn.append(nn.BatchNorm1d(channels, momentum=0.1))
            
            next_dilation *= filter_widths[i]
            
        self.layers_conv = nn.ModuleList(layers_conv)
        self.layers_bn = nn.ModuleList(layers_bn)
        
    def _forward_blocks(self, x):
        
        x = self.drop(self.relu(self.expand_bn(self.expand_conv(x))))

        for i in range(len(self.pad) - 1):
            pad = self.pad[i+1]
            shift = self.causal_shift[i+1]
            res = x[:, :, pad + shift : x.shape[2] - pad + shift]
            
            x = self.drop(self.relu(self.layers_bn[2*i](self.layers_conv[2*i](x))))
            x = res + self.drop(self.relu(self.layers_bn[2*i + 1](self.layers_conv[2*i + 1](x))))
        
        x = self.shrink(x)

        return x
    #constructing an instance of this class based on a state dictionary (network parameters)
    
    @staticmethod
    def from_state_dict(params,hyperparams):
        net=TemporalModel(*hyperparams)
        net.load_state_dict(params)
        return net
</code></pre>
</div>
<div class="cell markdown">
<h3 id="losses"><a class="header" href="#losses">Losses</a></h3>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def mpjpe(predicted, target):
    &quot;&quot;&quot;
    Mean per-joint position error (i.e. mean Euclidean distance),
    often referred to as &quot;Protocol #1&quot; in many papers.
    &quot;&quot;&quot;
    assert predicted.shape == target.shape
    return torch.mean(torch.norm(predicted - target, dim=len(target.shape)-1))
    
def weighted_mpjpe(predicted, target, w):
    &quot;&quot;&quot;
    Weighted mean per-joint position error (i.e. mean Euclidean distance)
    &quot;&quot;&quot;
    assert predicted.shape == target.shape
    assert w.shape[0] == predicted.shape[0]
    return torch.mean(w * torch.norm(predicted - target, dim=len(target.shape)-1))

def p_mpjpe(predicted, target):
    &quot;&quot;&quot;
    Pose error: MPJPE after rigid alignment (scale, rotation, and translation),
    often referred to as &quot;Protocol #2&quot; in many papers.
    &quot;&quot;&quot;
    assert predicted.shape == target.shape
    
    muX = np.mean(target, axis=1, keepdims=True)
    muY = np.mean(predicted, axis=1, keepdims=True)
    
    X0 = target - muX
    Y0 = predicted - muY

    normX = np.sqrt(np.sum(X0**2, axis=(1, 2), keepdims=True))
    normY = np.sqrt(np.sum(Y0**2, axis=(1, 2), keepdims=True))
    
    X0 /= normX
    Y0 /= normY

    H = np.matmul(X0.transpose(0, 2, 1), Y0)
    U, s, Vt = np.linalg.svd(H)
    V = Vt.transpose(0, 2, 1)
    R = np.matmul(V, U.transpose(0, 2, 1))

    # Avoid improper rotations (reflections), i.e. rotations with det(R) = -1
    sign_detR = np.sign(np.expand_dims(np.linalg.det(R), axis=1))
    V[:, :, -1] *= sign_detR
    s[:, -1] *= sign_detR.flatten()
    R = np.matmul(V, U.transpose(0, 2, 1)) # Rotation

    tr = np.expand_dims(np.sum(s, axis=1, keepdims=True), axis=2)

    a = tr * normX / normY # Scale
    t = muX - a*np.matmul(muY, R) # Translation
    
    # Perform rigid transformation on the input
    predicted_aligned = a*np.matmul(predicted, R) + t
    
    # Return MPJPE
    return np.mean(np.linalg.norm(predicted_aligned - target, axis=len(target.shape)-1))
    
def n_mpjpe(predicted, target):
    &quot;&quot;&quot;
    Normalized MPJPE (scale only), adapted from:
    https://github.com/hrhodin/UnsupervisedGeometryAwareRepresentationLearning/blob/master/losses/poses.py
    &quot;&quot;&quot;
    assert predicted.shape == target.shape
    
    norm_predicted = torch.mean(torch.sum(predicted**2, dim=3, keepdim=True), dim=2, keepdim=True)
    norm_target = torch.mean(torch.sum(target*predicted, dim=3, keepdim=True), dim=2, keepdim=True)
    scale = norm_target / norm_predicted
    return mpjpe(scale * predicted, target)

def mean_velocity_error(predicted, target):
    &quot;&quot;&quot;
    Mean per-joint velocity error (i.e. mean Euclidean distance of the 1st derivative)
    &quot;&quot;&quot;
    assert predicted.shape == target.shape
    
    velocity_predicted = np.diff(predicted, axis=0)
    velocity_target = np.diff(target, axis=0)
    
    return np.mean(np.linalg.norm(velocity_predicted - velocity_target, axis=len(target.shape)-1))
</code></pre>
</div>
<div class="cell markdown">
<h1 id="setting-up-arguments"><a class="header" href="#setting-up-arguments">Setting up arguments</a></h1>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">class Args:
    # General arguments
    dataset = 'humaneva'    # target dataset # h36m or humaneva
    keypoints = 'cpn_ft_h36m_dbb'    # 2D detections to use
    subjects_train = 'Train/S1,Train/S2,Train/S3' # 'S1,S5,S6,S7,S8'    # training subjects separated by comma
    subjects_test = 'Validate/S4' # 'S9,S11'    # test subjects separated by comma
    subjects_unlabeled = ''    # unlabeled subjects separated by comma for self-supervision
    actions = '*' #'Walk,Jog,Box' #'*'    # actions to train/test on, separated by comma, or * for all
    checkpoint = f'/{ROOTDIR}/checkpoint'    # checkpoint directory
    checkpoint_frequency = 10    # create a checkpoint every N epochs
    resume = ''    # checkpoint to resume (file name)
    evaluate = ''    # checkpoint to evaluate (file name)
    render = False # visualize a particular video
    by_subject = True # False # break down error by subject (on evaluation)
    export_training_curves = False # save training curves as .png images

    # Model arguments
    stride = 1    # chunk size to use during training
    epochs = 1000 # 60    # number of training epochs
    batch_size = 128 # 1024    # batch size in terms of predicted frames
    dropout = 0.25    # dropout probability
    learning_rate = 0.001    # initial learning rate
    lr_decay = 0.996 # 0.95    # learning rate decay per epoch
    data_augmentation = True # disable train-time flipping
    test_time_augmentation = True # disable test-time flipping
    architecture = '3,3,3'    # filter widths separated by comma
    causal = False # use causal convolutions for real-time processing
    channels = 1024    # number of channels in convolution layers

    # Experimental
    subset = 1    # reduce dataset size by fraction
    downsample = 1    # downsample frame rate by factor (semi-supervised)
    warmup = 1    # warm-up epochs for semi-supervision
    no_eval = False # disable epoch evaluation while training (small speed-up)
    dense = False # use dense convolutions instead of dilated convolutions
    disable_optimizations = False # disable optimized model for single-frame predictions
    linear_projection = False # use only linear coefficients for semi-supervised projection
    bone_length = False    # disable bone length term in semi-supervised settings
    no_proj = False # disable projection for semi-supervised setting
    bone_length_term = True

    # Visualization
    viz_subject = ''    # subject to render
    viz_action = ''    # action to render
    viz_camera = 0    # camera to render
    viz_video = ''    # path to input video
    viz_skip = 0    # skip first N frames of input video
    viz_output = ''    # output file name (.gif or .mp4)
    viz_export = ''    # output file name for coordinates
    viz_bitrate = 3000    # bitrate for mp4 videos
    viz_no_ground_truth = False    # do not show ground-truth poses
    viz_limit = -1    # only render first N frames
    viz_downsample = 1    # downsample FPS by a factor N
    viz_size = 5    # image size

args = Args()

if args.resume and args.evaluate:
    print('Invalid flags: --resume and --evaluate cannot be set at the same time')
    exit()
    
if args.export_training_curves and args.no_eval:
    print('Invalid flags: --export-training-curves and --no-eval cannot be set at the same time')
    exit()
</code></pre>
</div>
<div class="cell markdown">
<h1 id="preparing-dataset"><a class="header" href="#preparing-dataset">Preparing dataset</a></h1>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">filter_widths = [int(x) for x in args.architecture.split(',')]
receptive_field = np.prod(filter_widths) # model_pos.receptive_field()
print('INFO: Receptive field: {} frames'.format(receptive_field))
pad = (receptive_field - 1) // 2 # Padding on each side

if args.dataset.startswith('humaneva'):
    dataset_path = f'{ROOTDIR}/{args.dataset}/data_3d_{args.dataset}15.npz'
    keypoints_path = f'{ROOTDIR}/{args.dataset}/data_2d_{args.dataset}15_gt.npz'
    print(keypoints_path, dataset_path)
    train_dataset = HumanEvaDataSubset(dataset_path, keypoints_path, [s.replace('Train/','') for s in args.subjects_train.split(',')], prefixes=['Train/'], pad=pad)
    val_dataset = HumanEvaDataSubset(dataset_path, keypoints_path, [s.replace('Validate/','') for s in args.subjects_test.split(',')], prefixes=['Validate/'], pad=pad)
else:
    raise KeyError('Invalid dataset')
</code></pre>
</div>
<div class="cell markdown">
<h1 id="single-node-training"><a class="header" href="#single-node-training">Single-Node Training</a></h1>
</div>
<div class="cell markdown">
<h3 id="preparing-dataloaders"><a class="header" href="#preparing-dataloaders">Preparing dataloaders</a></h3>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=True)
</code></pre>
</div>
<div class="cell markdown">
<h3 id="model-initialization"><a class="header" href="#model-initialization">Model initialization</a></h3>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># Non-optimized version for non-single-frame preditictions
# Also the only possible option for stride &gt; 1 and/or dense filters
#print(train_dataset.__getitem__(0)[&quot;pos_2d&quot;].shape)

model_pos_train = TemporalModel(train_dataset.__getitem__(0)[&quot;pos_2d&quot;].shape[-2],
                                train_dataset.__getitem__(0)[&quot;pos_2d&quot;].shape[-1],
                                train_dataset.skeleton().num_joints(),
                                filter_widths=filter_widths,
                                causal=args.causal,
                                dropout=args.dropout,
                                channels=args.channels,
                                dense=args.dense)
model_pos = TemporalModel(train_dataset.__getitem__(0)[&quot;pos_2d&quot;].shape[-2],
                          train_dataset.__getitem__(0)[&quot;pos_2d&quot;].shape[-1],
                          train_dataset.skeleton().num_joints(),
                          filter_widths=filter_widths,
                          causal=args.causal,
                          dropout=args.dropout,
                          channels=args.channels,
                          dense=args.dense)
if args.causal:
    print('INFO: Using causal convolutions')
    causal_shift = pad
else:
    causal_shift = 0

model_params = 0
for parameter in model_pos.parameters():
    model_params += parameter.numel()
print('INFO: Trainable parameter count:', model_params)

if torch.cuda.is_available():
    model_pos = model_pos.cuda()
    model_pos_train = model_pos_train.cuda()
</code></pre>
</div>
<div class="cell markdown">
<h3 id="loading-pre-trained-weights"><a class="header" href="#loading-pre-trained-weights">Loading pre-trained weights</a></h3>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">if args.resume or args.evaluate:
    chk_filename = os.path.join(args.checkpoint, args.resume if args.resume else args.evaluate)
    print('Loading checkpoint', chk_filename)
    checkpoint = torch.load(chk_filename, map_location=lambda storage, loc: storage)
    print('This model was trained for {} epochs'.format(checkpoint['epoch']))
    model_pos_train.load_state_dict(checkpoint['model_pos'])
    model_pos.load_state_dict(checkpoint['model_pos'])
    
    if args.evaluate and 'model_traj' in checkpoint:
        # Load trajectory model if it contained in the checkpoint (e.g. for inference in the wild)
        model_traj = TemporalModel(poses_valid_2d[0].shape[-2],
                                   poses_valid_2d[0].shape[-1],
                                   1,
                                   filter_widths=filter_widths,
                                   causal=args.causal,
                                   dropout=args.dropout,
                                   channels=args.channels,
                                   dense=args.dense)
        if torch.cuda.is_available():
            model_traj = model_traj.cuda()
        model_traj.load_state_dict(checkpoint['model_traj'])
    else:
        model_traj = None

print('INFO: Testing on {} frames'.format(len(val_dataset)*args.batch_size))
</code></pre>
</div>
<div class="cell markdown">
<h3 id="training-loop"><a class="header" href="#training-loop">Training Loop</a></h3>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">if not args.evaluate:
    # This is just to confirm we really want to (re-) train the model
    # Otherwise this cell will be skipped
    #cameras_train, poses_train, poses_train_2d = fetch(subjects_train, action_filter, subset=args.subset)
    lr = args.learning_rate
    optimizer = optim.Adam(model_pos_train.parameters(), lr=lr, amsgrad=True)
    lr_decay = args.lr_decay
    
    losses_3d_train = []
    losses_3d_train_eval = []
    losses_3d_valid = []

    epoch = 0
    initial_momentum = 0.1
    final_momentum = 0.001
    
    print('INFO: Training on {} frames'.format(len(train_dataset)*args.batch_size))
    
    if args.resume:
        epoch = checkpoint['epoch']
        if 'optimizer' in checkpoint and checkpoint['optimizer'] is not None:
            optimizer.load_state_dict(checkpoint['optimizer'])
            #train_generator.set_random_state(checkpoint['random_state'])
        else:
            print('WARNING: this checkpoint does not contain an optimizer state. The optimizer will be reinitialized.')
        lr = checkpoint['lr']

    print('** Note: reported losses are averaged over all frames and test-time augmentation is not used here.')
    print('** The final evaluation will be carried out after the last training epoch.')
    
    # Pos model only
    while epoch &lt; args.epochs:
        start_time = time()
        epoch_loss_3d_train = 0
        epoch_loss_traj_train = 0
        epoch_loss_2d_train_unlabeled = 0
        N = 0
        N_semi = 0
        model_pos_train.train()
        # Regular supervised scenario
        for batch in tqdm(train_dataloader):
            inputs_3d = batch['pos_3d']
            inputs_2d = batch['pos_2d']
            if torch.cuda.is_available():
                inputs_3d = inputs_3d.cuda()
                inputs_2d = inputs_2d.cuda()
            inputs_3d[:, :, 0] = 0
            optimizer.zero_grad()

            # Predict 3D poses
            predicted_3d_pos = model_pos_train(inputs_2d)
            loss_3d_pos = mpjpe(predicted_3d_pos, inputs_3d)
            epoch_loss_3d_train += inputs_3d.shape[0]*inputs_3d.shape[1] * loss_3d_pos.item()
            N += inputs_3d.shape[0]*inputs_3d.shape[1]
            loss_total = loss_3d_pos
            loss_total.backward()
            optimizer.step()
            
        losses_3d_train.append(epoch_loss_3d_train / N)
                
        # End-of-epoch evaluation
        with torch.no_grad():
            model_pos.load_state_dict(model_pos_train.state_dict())
            model_pos.eval()
            
            # if not args.no_eval:
            # Evaluate on test set
            epoch_loss_3d_valid = 0
            epoch_loss_traj_valid = 0
            epoch_loss_2d_valid = 0
            N = 0
            for batch in val_dataloader:
                inputs_3d = batch['pos_3d']
                inputs_2d = batch['pos_2d']
                if torch.cuda.is_available():
                    inputs_3d = inputs_3d.cuda()
                    inputs_2d = inputs_2d.cuda()
                inputs_traj = inputs_3d[:, :, :1].clone()
                inputs_3d[:, :, 0] = 0
                
                # Predict 3D poses
                predicted_3d_pos = model_pos(inputs_2d)
                loss_3d_pos = mpjpe(predicted_3d_pos, inputs_3d)
                epoch_loss_3d_valid += inputs_3d.shape[0]*inputs_3d.shape[1] * loss_3d_pos.item()
                N += inputs_3d.shape[0]*inputs_3d.shape[1]   
            losses_3d_valid.append(epoch_loss_3d_valid / N)
            
            # Evaluate on training set, this time in evaluation mode
            epoch_loss_3d_train_eval = 0
            epoch_loss_traj_train_eval = 0
            epoch_loss_2d_train_labeled_eval = 0
            N = 0
            for batch in train_dataloader:
                inputs_3d = batch['pos_3d']
                inputs_2d = batch['pos_2d']
                if torch.cuda.is_available():
                    inputs_3d = inputs_3d.cuda()
                    inputs_2d = inputs_2d.cuda()
                inputs_traj = inputs_3d[:, :, :1].clone()
                inputs_3d[:, :, 0] = 0
                
                # Compute 3D poses
                predicted_3d_pos = model_pos(inputs_2d)
                loss_3d_pos = mpjpe(predicted_3d_pos, inputs_3d)
                epoch_loss_3d_train_eval += inputs_3d.shape[0]*inputs_3d.shape[1] * loss_3d_pos.item()
                N += inputs_3d.shape[0]*inputs_3d.shape[1]
            losses_3d_train_eval.append(epoch_loss_3d_train_eval / N)
        
        # Calculate inference time            
        elapsed = (time() - start_time)/60
        print('[%d] time %.2f lr %f 3d_train %f 3d_eval %f 3d_valid %f' % (
            epoch + 1,
            elapsed,
            lr,
            losses_3d_train[-1] * 1000,
            losses_3d_train_eval[-1] * 1000,
            losses_3d_valid[-1] * 1000))
        
        # Decay learning rate exponentially
        lr *= lr_decay
        for param_group in optimizer.param_groups:
            param_group['lr'] *= lr_decay
        epoch += 1
        
        # Decay BatchNorm momentum
        momentum = initial_momentum * np.exp(-epoch/args.epochs * np.log(initial_momentum/final_momentum))
        model_pos_train.set_bn_momentum(momentum)
        
        # Save checkpoint if necessary
        if epoch % args.checkpoint_frequency == 0:
            chk_path = os.path.join(args.checkpoint, 'epoch_{}.bin'.format(epoch))
            print('Saving checkpoint to', chk_path)
            
            torch.save({
                'epoch': epoch,
                'lr': lr,
                #'random_state': train_generator.random_state(),
                'optimizer': optimizer.state_dict(),
                'model_pos': model_pos_train.state_dict(),
                'model_traj': None,
                'random_state_semi': None,
            }, chk_path)

        # Save training curves after every epoch, as .png images (if requested)
        if args.export_training_curves and epoch &gt; 3:
            if 'matplotlib' not in sys.modules:
                import matplotlib
                matplotlib.use('Agg')
                import matplotlib.pyplot as plt
            
            plt.figure()
            epoch_x = np.arange(3, len(losses_3d_train)) + 1
            plt.plot(epoch_x, losses_3d_train[3:], '--', color='C0')
            plt.plot(epoch_x, losses_3d_train_eval[3:], color='C0')
            plt.plot(epoch_x, losses_3d_valid[3:], color='C1')
            plt.legend(['3d train', '3d train (eval)', '3d valid (eval)'])
            plt.ylabel('MPJPE (m)')
            plt.xlabel('Epoch')
            plt.xlim((3, epoch))
            plt.savefig(os.path.join(args.checkpoint, 'loss_3d.png')) 
</code></pre>
</div>
<div class="cell markdown">
<h1 id="distributed-training"><a class="header" href="#distributed-training">Distributed Training</a></h1>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">### Distributed Package

import horovod.torch as hvd
from sparkdl import HorovodRunner
from torch.utils.data.distributed import DistributedSampler
from horovod.spark.common.store import DBFSLocalStore
</code></pre>
</div>
<div class="cell markdown">
<h3 id="preparing-dataloaders-1"><a class="header" href="#preparing-dataloaders-1">Preparing dataloaders</a></h3>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">filter_widths = [int(x) for x in args.architecture.split(',')]
receptive_field = np.prod(filter_widths) # model_pos.receptive_field()
print('INFO: Receptive field: {} frames'.format(receptive_field))
pad = (receptive_field - 1) // 2 # Padding on each side

if args.dataset.startswith('humaneva'):
    dataset_path = f'{ROOTDIR}/{args.dataset}/data_3d_{args.dataset}15.npz'
    keypoints_path = f'{ROOTDIR}/{args.dataset}/data_2d_{args.dataset}15_gt.npz'
    print(keypoints_path, dataset_path)
    train_dataset = HumanEvaDataSubset(dataset_path, keypoints_path, local_copy=f'data-{hvd.rank()}', subjects=[s.replace('Train/','') for s in args.subjects_train.split(',')], prefixes=['Train/'], pad=pad)
    val_dataset = HumanEvaDataSubset(dataset_path, keypoints_path, [s.replace('Validate/','') for s in args.subjects_test.split(',')], prefixes=['Validate/'], pad=pad)
else:
    raise KeyError('Invalid dataset')
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># Setup store for intermediate datadiate data
store = DBFSLocalStore(work_dir)
 
# Load MNIST data from databricks-datasets
# So that this notebook can run quickly, this example uses the .limit() option. Using only limited data decreases the model's accuracy; remove this option for better accuracy. 
train_df = spark.read.format(&quot;libsvm&quot;) \
  .option('numFeatures', '784') \
  .load(&quot;/databricks-datasets/mnist-digits/data-001/mnist-digits-train.txt&quot;) \
  .limit(60).repartition(num_proc)
</code></pre>
</div>
<div class="cell markdown">
<h3 id="model-initialization-1"><a class="header" href="#model-initialization-1">Model initialization</a></h3>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># Non-optimized version for non-single-frame preditictions
# Also the only possible option for stride &gt; 1 and/or dense filters
#print(train_dataset.__getitem__(0)[&quot;pos_2d&quot;].shape)

model_pos_train = TemporalModel(train_dataset.__getitem__(0)[&quot;pos_2d&quot;].shape[-2],
                                train_dataset.__getitem__(0)[&quot;pos_2d&quot;].shape[-1],
                                train_dataset.skeleton().num_joints(),
                                filter_widths=filter_widths,
                                causal=args.causal,
                                dropout=args.dropout,
                                channels=args.channels,
                                dense=args.dense)
model_pos = TemporalModel(train_dataset.__getitem__(0)[&quot;pos_2d&quot;].shape[-2],
                          train_dataset.__getitem__(0)[&quot;pos_2d&quot;].shape[-1],
                          train_dataset.skeleton().num_joints(),
                          filter_widths=filter_widths,
                          causal=args.causal,
                          dropout=args.dropout,
                          channels=args.channels,
                          dense=args.dense)
if args.causal:
    print('INFO: Using causal convolutions')
    causal_shift = pad
else:
    causal_shift = 0

model_params = 0
for parameter in model_pos.parameters():
    model_params += parameter.numel()
print('INFO: Trainable parameter count:', model_params)

if torch.cuda.is_available():
    model_pos = model_pos.cuda()
    model_pos_train = model_pos_train.cuda()
</code></pre>
</div>
<div class="cell markdown">
<h3 id="training-loop-1"><a class="header" href="#training-loop-1">Training Loop</a></h3>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def train_hvd():

    # Initialize Horovod
    hvd.init()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    if device.type == 'cuda':
        torch.cuda.set_device(hvd.local_rank())
    
    train_sampler = DistributedSampler(train_dataset, num_replicas=hvd.size(), rank=hvd.rank())
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size//args.stride, sampler=train_sampler)
    ## we did not use train_eval_loader##
    train_eval_sampler = DistributedSampler(train_eval_dataset, num_replicas=hvd.size(), rank=hvd.rank())
    train_eval_loader = torch.utils.data.DataLoader(train_eval_dataset, batch_size=1, sampler=train_eval_sampler)

    print('INFO: Training on {} frames'.format(len(train_dataset)*args.batch_size))

    # Model
    model = model_pos_train.to(device)

    lr = args.learning_rate
    optimizer = optim.Adam(model.parameters(), lr=lr*hvd.size(), amsgrad=True)

    # Wrap the local optimizer with hvd.DistributedOptimizer so that Horovod handles the distributed optimization
    optimizer = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())

    # Broadcast initial parameters so all workers start with the same parameters
    hvd.broadcast_parameters(model.state_dict(), root_rank=0)


    lr_decay = args.lr_decay

    #losses_3d_train = []
    #losses_3d_train_eval = []
    #losses_3d_valid = []

    #epoch = 0
    initial_momentum = 0.1
    final_momentum = 0.001

    #Pos model only
    for epoch in range(1,args.epochs+1):
        train_one_epoch(model, device, train_loader, optimizer, epoch)

        if hvd.rank() == 0:
            save_checkpoint(model,optimizer,epoch)
        # Decay learning rate exponentially
        lr *= lr_decay
        for param_group in optimizer.param_groups:
            param_group['lr'] *= lr_decay
        epoch += 1

        # Decay BatchNorm momentum
        momentum = initial_momentum * np.exp(-epoch/args.epochs * np.log(initial_momentum/final_momentum))
        model.set_bn_momentum(momentum)


</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">#Distributed training using Horovod runner
if not args.evaluate:
    hr = HorovodRunner(np=2)
    hr.run(train_hvd)
</code></pre>
</div>
<div class="cell markdown">
<h1 id="temporary-code"><a class="header" href="#temporary-code">Temporary code</a></h1>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()
        self.fc1 = nn.Linear(320, 50)
        self.fc2 = nn.Linear(50, 10)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, 320)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return F.log_softmax(x)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># Specify training parameters
batch_size = 100
num_epochs = 5
momentum = 0.5
log_interval = 100

def train_one_epoch(model, device, data_loader, optimizer, epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(data_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        if batch_idx % log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(data_loader) * len(data),
                100. * batch_idx / len(data_loader), loss.item()))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">from time import time
import os
PYTORCH_DIR = '/dbfs/ml/horovod_pytorch'
LOG_DIR = os.path.join(PYTORCH_DIR, str(time()), 'MNISTDemo')
os.makedirs(LOG_DIR)

def save_checkpoint(model, optimizer, epoch):
    filepath = LOG_DIR + '/checkpoint-{epoch}.pth.tar'.format(epoch=epoch)
    state = {
        'model': model.state_dict(),
        'optimizer': optimizer.state_dict(),
    }
    torch.save(state, filepath)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">import torch.optim as optim
from torchvision import datasets, transforms

def train(learning_rate):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    train_dataset = datasets.MNIST('data', 
                                   train=True,
                                   download=True,
                                   transform=transforms.Compose([transforms.ToTensor(),
                                                                 transforms.Normalize((0.1307,), (0.3081,))]))
    data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    model = Net().to(device)
    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    for epoch in range(1, num_epochs + 1):
        train_one_epoch(model, device, data_loader, optimizer, epoch)
        save_checkpoint(model, optimizer, epoch)

# Runs in  49.65 seconds on 3 node    GPU cluster
# Runs in 118.2 seconds on 3 node non-GPU cluster
train(learning_rate = 0.001)train(0.001)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">import horovod.torch as hvd
from sparkdl import HorovodRunner
from torch.utils.data.distributed import DistributedSampler

def train_hvd(learning_rate):
    # Initialize Horovod
    hvd.init()  
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    if device.type == 'cuda':
        # Pin GPU to local rank
        torch.cuda.set_device(hvd.local_rank())
    
    train_dataset = datasets.MNIST(
        # Use different root directory for each worker to avoid conflicts
        root='data-%d'% hvd.rank(),  
        train=True, 
        download=True,
        transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
      )
  
    # Configure the sampler so that each worker gets a distinct sample of the input dataset
    train_sampler = DistributedSampler(train_dataset, num_replicas=hvd.size(), rank=hvd.rank())
    # Use train_sampler to load a different sample of data on each worker
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)
    model = Net().to(device)
  
    # The effective batch size in synchronous distributed training is scaled by the number of workers
    # Increase learning_rate to compensate for the increased batch size
    optimizer = optim.SGD(model.parameters(), lr=learning_rate * hvd.size(), momentum=momentum)

    # Wrap the local optimizer with hvd.DistributedOptimizer so that Horovod handles the distributed optimization
    optimizer = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())
  
    # Broadcast initial parameters so all workers start with the same parameters
    hvd.broadcast_parameters(model.state_dict(), root_rank=0)

    for epoch in range(1, num_epochs + 1):
        train_one_epoch(model, device, train_loader, optimizer, epoch)
        # Save checkpoints only on worker 0 to prevent conflicts between workers
        if hvd.rank() == 0:
            save_checkpoint(model, optimizer, epoch)

# Runs in 51.63 seconds on 3 node     GPU cluster
# Runs in 96.6  seconds on 3 node non-GPU cluster
hr = HorovodRunner(np=2)
hr.run(train_hvd, learning_rate = 0.001)
</code></pre>
</div>
<div class="cell markdown">
<h1 id="temporary-2"><a class="header" href="#temporary-2">Temporary 2</a></h1>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">import horovod.torch as hvd
from sparkdl import HorovodRunner
from torch.utils.data.distributed import DistributedSampler

</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def train_hvd(dummy_var=1):
    hvd.init()
    print('data-%d'% hvd.rank())
    if torch.cuda.is_available():
        # Pin GPU to local rank
        torch.cuda.set_device(hvd.local_rank())

    filter_widths = [int(x) for x in args.architecture.split(',')]
    receptive_field = np.prod(filter_widths) # model_pos.receptive_field()
    print('INFO: Receptive field: {} frames'.format(receptive_field))
    pad = (receptive_field - 1) // 2 # Padding on each side

    if args.dataset.startswith('humaneva'):
        dataset_path = f'{ROOTDIR}/{args.dataset}/data_3d_{args.dataset}15.npz'
        keypoints_path = f'{ROOTDIR}/{args.dataset}/data_2d_{args.dataset}15_gt.npz'
        print(keypoints_path, dataset_path)
        # !!! The problem is in the line below !!!
        train_dataset = HumanEvaDataSubset(dataset_path, keypoints_path, local_copy=f'data-{hvd.rank()}', subjects=[s.replace('Train/','') for s in args.subjects_train.split(',')], prefixes=['Train/'], pad=pad)
    else:
        raise KeyError('Invalid dataset')
        
#     train_sampler = DistributedSampler(train_dataset, num_replicas=hvd.size(), rank=hvd.rank())
#     train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, sampler=train_sampler)

#     model_pos_train = TemporalModel(train_dataset.__getitem__(0)[&quot;pos_2d&quot;].shape[-2],
#                                     train_dataset.__getitem__(0)[&quot;pos_2d&quot;].shape[-1],
#                                     train_dataset.skeleton().num_joints(),
#                                     filter_widths=filter_widths,
#                                     causal=args.causal,
#                                     dropout=args.dropout,
#                                     channels=args.channels,
#                                     dense=args.dense)
#     if args.causal:
#         print('INFO: Using causal convolutions')
#         causal_shift = pad
#     else:
#         causal_shift = 0
#     model_params = 0
#     for parameter in model_pos_train.parameters():
#         model_params += parameter.numel()
#     print('INFO: Trainable parameter count:', model_params)

#     if torch.cuda.is_available():
#         model_pos_train = model_pos_train.cuda()

#     # The effective batch size in synchronous distributed training is scaled by the number of workers
#     # Increase learning_rate to compensate for the increased batch size
#     lr = args.learning_rate
#     optimizer = optim.Adam(model_pos_train.parameters(), lr=lr*hvd.size(), amsgrad=True)
#     optimizer = hvd.DistributedOptimizer(optimizer, named_parameters=model_pos_train.named_parameters())
#     hvd.broadcast_parameters(model_pos_train.state_dict(), root_rank=0)

#     for epoch in range(1, num_epochs + 1):
#         print(epoch)
#         train_one_epoch(model, device, train_dataloader, optimizer, epoch)
        # Save checkpoints only on worker 0 to prevent conflicts between workers
#         if hvd.rank() == 0:
#             save_checkpoint(model, optimizer, epoch)

hr = HorovodRunner(np=2)
hr.run(train_hvd, dummy_var=1)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">print('data-%d'% hvd.rank())
</code></pre>
</div>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/02_non_distributed.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next" href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/03_Evaluation.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/02_non_distributed.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next" href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/03_Evaluation.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="../../../elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="../../../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->


    </body>
</html>
