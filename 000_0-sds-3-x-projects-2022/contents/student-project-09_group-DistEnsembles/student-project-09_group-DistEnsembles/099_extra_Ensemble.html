<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>099_extra_Ensemble - sds-3.x/ScaDaMaLe</title>


        <!-- Custom HTML head -->

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../../../favicon.svg">
        <link rel="shortcut icon" href="../../../favicon.png">
        <link rel="stylesheet" href="../../../css/variables.css">
        <link rel="stylesheet" href="../../../css/general.css">
        <link rel="stylesheet" href="../../../css/chrome.css">
        <link rel="stylesheet" href="../../../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../../../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../../../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../../../highlight.css">
        <link rel="stylesheet" href="../../../tomorrow-night.css">
        <link rel="stylesheet" href="../../../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../../../scroll-mdbook-outputs.css">

        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../../../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../../../introduction.html">Introduction</a></li><li class="chapter-item expanded affix "><li class="part-title">Projects</li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/00_Introduction.html"><strong aria-hidden="true">1.</strong> student-project-01_group-GraphOfWiki_00_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/01_DataLoading_redirectsTable.html"><strong aria-hidden="true">1.1.</strong> 01_DataLoading_redirectsTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/02_DataLoading_pagesTable.html"><strong aria-hidden="true">1.2.</strong> 02_DataLoading_pagesTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/03_DataLoading_pagelinksTable.html"><strong aria-hidden="true">1.3.</strong> 03_DataLoading_pagelinksTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/04_DataLoading_categorylinksTable.html"><strong aria-hidden="true">1.4.</strong> 04_DataLoading_categorylinksTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/05_DataLoading_categoryTable.html"><strong aria-hidden="true">1.5.</strong> 05_DataLoading_categoryTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/06_redirectRemoval.html"><strong aria-hidden="true">1.6.</strong> 06_redirectRemoval</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/07_createArticleGraph.html"><strong aria-hidden="true">1.7.</strong> 07_createArticleGraph</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/08_explorationArticleGraph.html"><strong aria-hidden="true">1.8.</strong> 08_explorationArticleGraph</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/09_fullGraphAnalysis.html"><strong aria-hidden="true">1.9.</strong> 09_fullGraphAnalysis</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/10_explorativeMotifs.html"><strong aria-hidden="true">1.10.</strong> 10_explorativeMotifs</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/11_conclusionDiscussionAndFutureWork.html"><strong aria-hidden="true">1.11.</strong> 11_conclusionDiscussionAndFutureWork</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/12_gameNotebookSetup.html"><strong aria-hidden="true">1.12.</strong> 12_gameNotebookSetup</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/13_gameNotebook.html"><strong aria-hidden="true">1.13.</strong> 13_gameNotebook</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-02_group-DDLOfVision/student-project-02_group-DDLOfVision/00_vqa_introduction.html"><strong aria-hidden="true">2.</strong> student-project-02_group-DDLOfVision_00_vqa_introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-02_group-DDLOfVision/student-project-02_group-DDLOfVision/01_vqa_model_training.html"><strong aria-hidden="true">2.1.</strong> 01_vqa_model_training</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-02_group-DDLOfVision/student-project-02_group-DDLOfVision/02_vqa_model_inference.html"><strong aria-hidden="true">2.2.</strong> 02_vqa_model_inference</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG2/student-project-03_group-WikiKG2/00_ingest_data.html"><strong aria-hidden="true">3.</strong> student-project-03_group-WikiKG2_00_ingest_data</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG2/student-project-03_group-WikiKG2/01_fetch_descriptions.html"><strong aria-hidden="true">3.1.</strong> 01_fetch_descriptions</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG2/student-project-03_group-WikiKG2/02_load_data.html"><strong aria-hidden="true">3.2.</strong> 02_load_data</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG2/student-project-03_group-WikiKG2/03_data_exploration.html"><strong aria-hidden="true">3.3.</strong> 03_data_exploration</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG2/student-project-03_group-WikiKG2/04_analysing_relation_types.html"><strong aria-hidden="true">3.4.</strong> 04_analysing_relation_types</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG2/student-project-03_group-WikiKG2/05_motif_search.html"><strong aria-hidden="true">3.5.</strong> 05_motif_search</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG2/student-project-03_group-WikiKG2/06_produce_pagerank_vectors.html"><strong aria-hidden="true">3.6.</strong> 06_produce_pagerank_vectors</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG2/student-project-03_group-WikiKG2/07_pagerank_for_classification.html"><strong aria-hidden="true">3.7.</strong> 07_pagerank_for_classification</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-04_group-FedMLMedicalApp/student-project-04_group-FedMLMedicalApp/00_Notebook_Presentation.html"><strong aria-hidden="true">4.</strong> student-project-04_group-FedMLMedicalApp_00_Notebook_Presentation</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-04_group-FedMLMedicalApp/student-project-04_group-FedMLMedicalApp/01_BrainTumorSegmentation_Centralized_Training.html"><strong aria-hidden="true">4.1.</strong> 01_BrainTumorSegmentation_Centralized_Training</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-04_group-FedMLMedicalApp/student-project-04_group-FedMLMedicalApp/02_Federated_Learning_BrainTumorSegmentation.html"><strong aria-hidden="true">4.2.</strong> 02_Federated_Learning_BrainTumorSegmentation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-04_group-FedMLMedicalApp/student-project-04_group-FedMLMedicalApp/data_upload_test.html"><strong aria-hidden="true">4.3.</strong> data_upload_test</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/00_introduction.html"><strong aria-hidden="true">5.</strong> student-project-05_group-DistOpt_00_introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/01_Bayesian_optimization.html"><strong aria-hidden="true">5.1.</strong> 01_Bayesian_optimization</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/02_Gaussian_processes.html"><strong aria-hidden="true">5.2.</strong> 02_Gaussian_processes</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/03_acquisition_functions.html"><strong aria-hidden="true">5.3.</strong> 03_acquisition_functions</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/04_scalable_Bayesian_optimization.html"><strong aria-hidden="true">5.4.</strong> 04_scalable_Bayesian_optimization</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/05_implementation_documentation.html"><strong aria-hidden="true">5.5.</strong> 05_implementation_documentation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/06_our_implementation.html"><strong aria-hidden="true">5.6.</strong> 06_our_implementation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/07_additional_code.html"><strong aria-hidden="true">5.7.</strong> 07_additional_code</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-07_group-ExpsZerOInit/student-project-07_group-ExpsZerOInit/00_introduction_resnet.html"><strong aria-hidden="true">6.</strong> student-project-07_group-ExpsZerOInit_00_introduction_resnet</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-07_group-ExpsZerOInit/student-project-07_group-ExpsZerOInit/01_transformer.html"><strong aria-hidden="true">6.1.</strong> 01_transformer</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-07_group-ExpsZerOInit/student-project-07_group-ExpsZerOInit/02_ddpm.html"><strong aria-hidden="true">6.2.</strong> 02_ddpm</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-08_group-WikiSearch/student-project-08_group-WikiSearch/00_Introduction.html"><strong aria-hidden="true">7.</strong> student-project-08_group-WikiSearch_00_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-08_group-WikiSearch/student-project-08_group-WikiSearch/01_InputParsing.html"><strong aria-hidden="true">7.1.</strong> 01_InputParsing</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-08_group-WikiSearch/student-project-08_group-WikiSearch/02_PageRank.html"><strong aria-hidden="true">7.2.</strong> 02_PageRank</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-08_group-WikiSearch/student-project-08_group-WikiSearch/03_QuerySearch.html"><strong aria-hidden="true">7.3.</strong> 03_QuerySearch</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/00_Introduction.html"><strong aria-hidden="true">8.</strong> student-project-09_group-DistEnsembles_00_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/01_Human_Pose_Data.html"><strong aria-hidden="true">8.1.</strong> 01_Human_Pose_Data</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/02_Ensemble_Training.html"><strong aria-hidden="true">8.2.</strong> 02_Ensemble_Training</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/03_Ensemble_Evaluation.html"><strong aria-hidden="true">8.3.</strong> 03_Ensemble_Evaluation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/099_extra_Ensemble.html" class="active"><strong aria-hidden="true">8.4.</strong> 099_extra_Ensemble</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/00_introduction.html"><strong aria-hidden="true">9.</strong> student-project-10_group-RDI_00_introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/01_prepare_data.html"><strong aria-hidden="true">9.1.</strong> 01_prepare_data</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/02_baseline.html"><strong aria-hidden="true">9.2.</strong> 02_baseline</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/03_single_machine.html"><strong aria-hidden="true">9.3.</strong> 03_single_machine</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/04_distributed_learning.html"><strong aria-hidden="true">9.4.</strong> 04_distributed_learning</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-11_group-CollaborativeFiltering/student-project-11_group-CollaborativeFiltering/01_Introduction.html"><strong aria-hidden="true">10.</strong> student-project-11_group-CollaborativeFiltering_01_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-11_group-CollaborativeFiltering/student-project-11_group-CollaborativeFiltering/02_AlgorithmsBeyondALS.html"><strong aria-hidden="true">10.1.</strong> 02_AlgorithmsBeyondALS</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-11_group-CollaborativeFiltering/student-project-11_group-CollaborativeFiltering/03_Implementation.html"><strong aria-hidden="true">10.2.</strong> 03_Implementation</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-12_group-FedLearnOpt/student-project-12_group-FedLearnOpt/01_Federated_Learning_Introduction.html"><strong aria-hidden="true">11.</strong> student-project-12_group-FedLearnOpt_01_Federated_Learning_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-12_group-FedLearnOpt/student-project-12_group-FedLearnOpt/02_Horovod_Introduction.html"><strong aria-hidden="true">11.1.</strong> 02_Horovod_Introduction</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-12_group-FedLearnOpt/student-project-12_group-FedLearnOpt/03_Implementations.html"><strong aria-hidden="true">11.2.</strong> 03_Implementations</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-13_group-DRL/student-project-13_group-DRL/00_DistributedRL.html"><strong aria-hidden="true">12.</strong> student-project-13_group-DRL_00_DistributedRL</a></li><li class="spacer"></li><li class="chapter-item expanded "><a href="../../../contents/student-project-14_group-EarthObs/student-project-14_group-EarthObs/00_Introduction.html"><strong aria-hidden="true">13.</strong> student-project-14_group-EarthObs_00_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-14_group-EarthObs/student-project-14_group-EarthObs/01_Download_data.html"><strong aria-hidden="true">13.1.</strong> 01_Download_data</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-14_group-EarthObs/student-project-14_group-EarthObs/02_Image_preprocessing.html"><strong aria-hidden="true">13.2.</strong> 02_Image_preprocessing</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-14_group-EarthObs/student-project-14_group-EarthObs/03_Model_Architecture_and_Training.html"><strong aria-hidden="true">13.3.</strong> 03_Model_Architecture_and_Training</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-14_group-EarthObs/student-project-14_group-EarthObs/04_Prediction_And_Visualisation.html"><strong aria-hidden="true">13.4.</strong> 04_Prediction_And_Visualisation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-14_group-EarthObs/student-project-14_group-EarthObs/05_Conclusions.html"><strong aria-hidden="true">13.5.</strong> 05_Conclusions</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-projects-BrIntSuSvConclusion/student-projects-BrIntSuSvConclusion/BrIntSuSv.html"><strong aria-hidden="true">14.</strong> student-projects-BrIntSuSvConclusion_BrIntSuSv</a></li><li class="spacer"></li><li class="chapter-item expanded "><a href="../../../editors.html"><strong aria-hidden="true">15.</strong> Editors</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">sds-3.x/ScaDaMaLe</h1>

                    <div class="right-buttons">
                        <a href="../../../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div class="cell markdown">
<p>Distributed ensembles of deep neural networks Here we provide the necessary code for creating and training a distributed ensemble of deep neural networks</p>
</div>
<div class="cell markdown">
<h4 id="assumptions"><a class="header" href="#assumptions">Assumptions</a></h4>
<ul>
<li>The driver node fits a small subset of the data, but otherwise does not the data not fit on one node</li>
<li>The model parameters fit in the driver node, and at least one set of model parameters fits on a worker node.</li>
<li>Test set fits on driver node</li>
</ul>
</div>
<div class="cell markdown">
<ul>
<li>How to aggregate regressed values for each keypoint:
<ul>
<li>Average results? Smoothing effect</li>
<li>Robust mean + discard if the predictions disagree</li>
<li>Change the model to predict sigma. Use sigma for the weighted mean</li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<h4 id="todo"><a class="header" href="#todo">TODO</a></h4>
<ul>
<li>Can we do the random split of train, into labeled and unlabeled, without collecting. What if the train data is too big to collect, but yes we only collect the group names? Set a train key (labeled/unlabeled) and map by key?</li>
<li>Do we need to collect test data? Is it possible to just collect test losses? Hecne, doing the test predictions on the workers with the trained models already there.</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">import numpy as np
import torch
import pyspark.sql.functions as F
from pyspark.sql import Window
from pyspark.sql.functions import collect_list, size, udf
from pyspark.sql.types import BooleanType

from pyspark.sql.functions import udf

from itertools import groupby
#from pyspark.ml import Pipeline
from pyspark.rdd import PipelinedRDD

from pathlib import Path
import os
import matplotlib.pyplot as plt
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">ls /dbfs/VideoPose3D/humaneva/
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">from pyspark.sql.types import StructType, StringType, DoubleType, IntegerType

humaneva_train_path = &quot;/VideoPose3D/humaneva/humaneva15_train.csv&quot;
humaneva_test_path = &quot;/VideoPose3D/humaneva/humaneva15_test.csv&quot;

def load_data_from_csv(file_location):
    &quot;&quot;&quot;Load and preprocess HumanEva data
    Args:
        file_location: file location from which to load the data
        
    Returns:
        df: spark DataFrame
    &quot;&quot;&quot;
    file_type = &quot;csv&quot;
    infer_schema = &quot;true&quot;
    first_row_is_header = False
    delimiter = &quot;,&quot;
    
    schema = StructType() \
      .add(&quot;Idx&quot;,IntegerType(),True) \
      .add(&quot;Subject&quot;,StringType(),True) \
      .add(&quot;Action&quot;,StringType(),True) \
      .add(&quot;Camera&quot;,StringType(),True)
    for i in range(15):
        schema = schema.add(f&quot;u{i}&quot;,DoubleType(),True).add(f&quot;v{i}&quot;,DoubleType(),True)
    for i in range(15):
        schema = schema.add(f&quot;X{i}&quot;,DoubleType(),True).add(f&quot;Y{i}&quot;,DoubleType(),True).add(f&quot;Z{i}&quot;,DoubleType(),True)
    
    # Load the data from file
    df = spark.read.csv(file_location, header=True, schema=schema, sep=',')
    return df

df_train = load_data_from_csv(humaneva_train_path)
df_test = load_data_from_csv(humaneva_test_path)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">from pyspark.sql import functions as F

df_train = df_train.withColumn(&quot;Group&quot;, F.concat_ws(', ', &quot;Subject&quot;, &quot;Action&quot;, &quot;Camera&quot;)).drop(&quot;Subject&quot;, &quot;Action&quot;, &quot;Camera&quot;)
df_test = df_test.withColumn(&quot;Group&quot;, F.concat_ws(', ', &quot;Subject&quot;, &quot;Action&quot;, &quot;Camera&quot;)).drop(&quot;Subject&quot;, &quot;Action&quot;, &quot;Camera&quot;)
display(df_train)
</code></pre>
</div>
<div class="cell markdown">
<h4 id="assemble-feature-and-target-columns"><a class="header" href="#assemble-feature-and-target-columns">Assemble feature and target columns</a></h4>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">from pyspark.ml.feature import VectorAssembler

feature_names = []
target_names = []

n_keypoints = 15
for i in range(n_keypoints):
    feature_names.append(&quot;u{}&quot;.format(i))
    feature_names.append(&quot;v{}&quot;.format(i))
    target_names.append(&quot;X{}&quot;.format(i))
    target_names.append(&quot;Y{}&quot;.format(i))
    target_names.append(&quot;Z{}&quot;.format(i))
    
feature_assembler = VectorAssembler(inputCols=feature_names, outputCol=&quot;features&quot;)
target_assembler = VectorAssembler(inputCols=target_names, outputCol=&quot;targets&quot;)

def assemble_vectors(df):
    df = feature_assembler.transform(df)
    df = target_assembler.transform(df)
    df = df.drop(*feature_names).drop(*target_names)
    return df

df_train = assemble_vectors(df_train)
df_test = assemble_vectors(df_test)
</code></pre>
</div>
<div class="cell markdown">
<h4 id="create-receptive-fields"><a class="header" href="#create-receptive-fields">Create receptive fields</a></h4>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">receptive_field = 27

w = Window.orderBy(&quot;Idx&quot;).partitionBy([&quot;Group&quot;]).rowsBetween(Window.currentRow-receptive_field//2, Window.currentRow+receptive_field//2)

def create_receptive_fields(df):
    df = df.withColumn(&quot;feature_sequence&quot;, collect_list(&quot;features&quot;).over(w))
    df = df.withColumn(&quot;group_sequence&quot;, collect_list(&quot;Group&quot;).over(w))
    df = df.filter(size(df.group_sequence) == receptive_field)
    df = df.drop(&quot;features&quot;)
    return df

df_train_receptive = create_receptive_fields(df_train)
df_test_receptive = create_receptive_fields(df_test)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">display(df_train_receptive)
</code></pre>
</div>
<div class="cell markdown">
<h4 id="split-training-set-into-labeled-and-unlabeled-based-on-chunks"><a class="header" href="#split-training-set-into-labeled-and-unlabeled-based-on-chunks">Split training set into labeled and unlabeled based on chunks</a></h4>
<p>Since we are exploring in semi-supervised learning, we will have both labeled and unlabeled training data. Therefore, here we randomly split the data, with respect to the group, into an unlabeled and labeled set. To have a realistic semi-supervised setting, we assume that the unlabeled training data is slighly larger than the labeled training data. The targets are droped for the unlabeled training set.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">from random import sample, seed

## find right random seed to compensate for the different size of each chunk
seed(0) # seed 0 gives ok split
chunks = df_train_receptive.select(&quot;Group&quot;).distinct().collect()
chunks = [x[&quot;Group&quot;] for x in chunks]

num_chunks = len(chunks)
num_unlabeled = int(num_chunks*0.6)

unlabeled_chunks = sample(chunks, num_unlabeled)
labeled_chunks = [x for x in chunks if x not in unlabeled_chunks]

df_train_receptive_unlabeled = df_train_receptive.filter(df_train_receptive.Group.isin(unlabeled_chunks))
df_train_receptive_unlabeled = df_train_receptive_unlabeled.drop(&quot;targets&quot;)
df_train_receptive_labeled = df_train_receptive.filter(~df_train_receptive.Group.isin(unlabeled_chunks))
</code></pre>
</div>
<div class="cell markdown">
<h4 id="function-for-asserting-that-all-elements-in-list-are-equal-not-used-right-now-but-might-be-useful"><a class="header" href="#function-for-asserting-that-all-elements-in-list-are-equal-not-used-right-now-but-might-be-useful">Function for asserting that all elements in list are equal (not used right now but might be useful)</a></h4>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def all_equal(iterable):
    g = groupby(iterable)
    return next(g, True) and not next(g, False)
    
udf_all_equal = udf(all_equal, BooleanType())
</code></pre>
</div>
<div class="cell markdown">
<h4 id="define-model"><a class="header" href="#define-model">Define model</a></h4>
<p>Here we define the 3D pose estimation model with temporal convolutions and corresponding hyperparameters. Each ensemble will use this model to train on labeled data (including pseudolabels) and make predictions on unlabeled data.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">from torch import nn

class Args:
    # Data arguments
    num_joints = 15
    
    # Model arguments
    stride = 1    # chunk size to use during training
    epochs = 10 # 100    # number of training epochs
    batch_size = 128     # batch size in terms of predicted frames
    dropout = 0.25    # dropout probability
    learning_rate = 0.001    # initial learning rate
    lr_decay = 0.996     # learning rate decay per epoch
    data_augmentation = True # disable train-time flipping
    test_time_augmentation = True # disable test-time flipping
    architecture = '3,3,3'    # filter widths separated by comma
    channels = 1024    # number of channels in convolution layers

args = Args()
filter_widths = [int(x) for x in args.architecture.split(',')]
receptive_field = np.prod(filter_widths) # model_pos.receptive_field()
print('INFO: Receptive field: {} frames'.format(receptive_field))
pad = (receptive_field - 1) // 2 # Padding on each side
hyperparams = [args.num_joints, 2, args.num_joints, filter_widths, args.dropout, args.channels]

class TemporalModelBase(nn.Module):
    def __init__(self, num_joints_in, in_features, num_joints_out,
                 filter_widths, dropout, channels):
        super().__init__()
        # Validate input
        for fw in filter_widths:
            assert fw % 2 != 0, 'Only odd filter widths are supported'
        self.num_joints_in = num_joints_in
        self.in_features = in_features
        self.num_joints_out = num_joints_out
        self.filter_widths = filter_widths
        self.drop = nn.Dropout(dropout)
        self.relu = nn.ReLU(inplace=True)
        self.pad = [ filter_widths[0] // 2 ]
        self.expand_bn = nn.BatchNorm1d(channels, momentum=0.1)
        self.shrink = nn.Conv1d(channels, num_joints_out*3, 1)
        

    def set_bn_momentum(self, momentum):
        self.expand_bn.momentum = momentum
        for bn in self.layers_bn:
            bn.momentum = momentum
        
    def forward(self, pos2D):
        assert len(pos2D.shape) == 4 # pos2D: B x 27 x 15 x 2
        assert pos2D.shape[-2] == self.num_joints_in # 15
        assert pos2D.shape[-1] == self.in_features   # 2     
        sz = pos2D.shape[:3] # B x 27 x 15
        pos2D = pos2D.view(pos2D.shape[0], pos2D.shape[1], -1) # B x 27 x 15 * 2
        pos2D = pos2D.permute(0, 2, 1) # B x 15 * 2 x 27
        pos3D = self._forward_blocks(pos2D)
        pos3D = pos3D.permute(0, 2, 1)
        pos3D = pos3D.view(sz[0], -1, self.num_joints_out, 3)
        return pos3D

class TemporalModel(TemporalModelBase):
    def __init__(self, num_joints_in, in_features, num_joints_out,
                 filter_widths, dropout=0.25, channels=1024):
        &quot;&quot;&quot;
        Reference 3D pose estimation model with temporal convolutions.Initialize this model.
        
        Arg:
            num_joints_in -- number of input joints (e.g. 17 for Human3.6M)
            in_features -- number of input features for each joint (typically 2 for 2D input)
            num_joints_out -- number of output joints (can be different than input)
            filter_widths -- list of convolution widths, which also determines the # of blocks and receptive field
            dropout -- dropout probability
            channels -- number of convolution channels
        &quot;&quot;&quot;
        super().__init__(num_joints_in, in_features, num_joints_out, filter_widths, dropout, channels)
        self.expand_conv = nn.Conv1d(num_joints_in*in_features, channels, filter_widths[0], bias=False)
        layers_conv = []
        layers_bn = []
        next_dilation = filter_widths[0] # 3
        for i in range(1, len(filter_widths)):
            self.pad.append((filter_widths[i] - 1)*next_dilation // 2) # [1, 3, 9]
            layers_conv.append(nn.Conv1d(channels, channels,
                                         filter_widths[i],
                                         dilation=next_dilation,
                                         bias=False))
            layers_bn.append(nn.BatchNorm1d(channels, momentum=0.1))
            layers_conv.append(nn.Conv1d(channels, channels, 1, dilation=1, bias=False))
            layers_bn.append(nn.BatchNorm1d(channels, momentum=0.1))
            next_dilation *= filter_widths[i] # 3, 9, 27
        self.layers_conv = nn.ModuleList(layers_conv)
        self.layers_bn = nn.ModuleList(layers_bn)
        
    def _forward_blocks(self, pos2D):
        # pos2D: B x 15 * 2 x 27
        x = self.drop(self.relu(self.expand_bn(self.expand_conv(pos2D)))) # B x 1024 x 25
        for i in range(len(self.pad) - 1):
            pad = self.pad[i+1] # 3, 9
            res = x[:, :, pad : x.shape[2] - pad] # B x 1024 x 19, B x 1024 x 1
            x = self.drop(self.relu(self.layers_bn[2*i](self.layers_conv[2*i](x)))) # B x 1024 x 19, B x 1024 x 1
            x = res + self.drop(self.relu(self.layers_bn[2*i + 1](self.layers_conv[2*i + 1](x))))
        pos3D = self.shrink(x) # B x 15*3 x 1
        return pos3D
    
    @staticmethod
    def from_state_dict(params, hyperparams):
        net = TemporalModel(*hyperparams)
        net.load_state_dict(params)
        return net
</code></pre>
</div>
<div class="cell markdown">
<h4 id="loss"><a class="header" href="#loss">Loss</a></h4>
<p>Here we define the loss used for training and evaluation.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def mpjpe(predicted, target):
    &quot;&quot;&quot;
    Mean per-joint position error (i.e. mean Euclidean distance),
    often referred to as &quot;Protocol #1&quot; in many papers.
    &quot;&quot;&quot;
    assert predicted.shape == target.shape
    return torch.mean(torch.norm(predicted - target, dim=len(target.shape)-1))
</code></pre>
</div>
<div class="cell markdown">
<h4 id="define-dataset"><a class="header" href="#define-dataset">Define dataset</a></h4>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">&quot;&quot;&quot;
class DataSet(torch.utils.data.Dataset):
    def __init__(self, pos2D, pos3D, receptive_field):
        self.pos2D = pos2D # self.pos2D: [N_1 x 15 x 2, ..., N_B x 15 x 2]
        self.pos3D = pos3D # self.pos3D: [N_1 x 15 x 3, ..., N_B x 15 x 3]
        self.receptive_field = receptive_field

    def __len__(self):
        return self.x.shape[0]

    def __getitem__(self, ind):
        pos2D = self.pos2D[ind] # pos2D: N x 15 x 2
        pos3D = self.pos3D[ind] # pos3D: N x 15 x 3
        i = torch.randint(pos_3d.shape[0] - self.receptive_field + 1, [1])
        pos2D_sample = pos2D[i:i+self.receptive_field]                 # pos2D_sample: 27 x 15 x 2
        pos3D_sample = pos3D[i+(self.receptive_field - 1) // 2 ][None] # pos3D_sample:  1 x 15 x 3
        return pos2D_sample, pos3D_sample
&quot;&quot;&quot;

class DataSet(torch.utils.data.Dataset):
    def __init__(self, pos2D, pos3D):
        self.pos2D = pos2D # self.pos2D: B x 27 x 15 * 2
        self.pos3D = pos3D # self.pos3D: B x 1 x 15 * 3

    def __len__(self):
        return self.pos2D.shape[0]

    def __getitem__(self, ind):
        pos2D = self.pos2D[ind] # pos2D: B x 27 x 15 * 2 -&gt; 27 x 15 * 2
        pos3D = self.pos3D[ind] # pos2D: B x 1 x 15 * 3 -&gt; 1 x 15 * 2
        return pos2D, pos3D
</code></pre>
</div>
<div class="cell markdown">
<h4 id="train-and-predict-methods"><a class="header" href="#train-and-predict-methods">Train and predict methods</a></h4>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def train(params, hyperparams, data, args):
  
    x,y = zip(*data)
    pos2D, pos3D  = torch.stack(x), torch.stack(y)

    model = TemporalModel.from_state_dict(params, hyperparams)
    model.train()
    
    lr = args.learning_rate
    lr_decay = args.lr_decay
    train_data = DataSet(pos2D, pos3D)
    dataloader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=True)
    opt = torch.optim.Adam(model.parameters(), lr=lr, amsgrad=True)
    initial_momentum = 0.1
    final_momentum = 0.001
    
    losses_3d_train = []
    for epoch in range(args.epochs):
        epoch_loss_3d_train = 0
        N = 0
        
        for batch in dataloader:
            inputs_2d, inputs_3d = batch
            if torch.cuda.is_available():
                inputs_3d = inputs_3d.cuda()
                inputs_2d = inputs_2d.cuda()
                model = model.cuda()
            inputs_3d[:, :, 0] = 0
            # Predict 3D poses
            predicted_3d_pos = model(inputs_2d)
            # Calcuclate MPJPE loss
            loss_3d_pos = mpjpe(predicted_3d_pos, inputs_3d)
            epoch_loss_3d_train += inputs_3d.shape[0]*inputs_3d.shape[1] * loss_3d_pos.item()
            N += inputs_3d.shape[0]*inputs_3d.shape[1]
            loss_total = loss_3d_pos
            opt.zero_grad()
            loss_total.backward()
            # Make one optimization step on batch
            opt.step()
        losses_3d_train.append(epoch_loss_3d_train / N)
        print('[%d] lr %f 3d_train %f' % (
                epoch + 1,
                lr,
                losses_3d_train[-1] * 1000))
        # Decay learning rate exponentially
        lr *= lr_decay
        for param_group in opt.param_groups:
            param_group['lr'] *= lr_decay
            
    err = mpjpe(model(pos2D.cuda()), pos3D.cuda())
    lossval = float(err.detach().cpu().numpy())
    return model.state_dict(), lossval


def predict(params, hyperparams, x):    
    model = TemporalModel.from_state_dict(params, hyperparams)
    model.eval()
    if torch.cuda.is_available():
        x = x.cuda()
        model.cuda() 
    return model(x).detach().cpu()
</code></pre>
</div>
<div class="cell markdown">
<h4 id="train-ensemble"><a class="header" href="#train-ensemble">Train ensemble</a></h4>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">&quot;&quot;&quot;
# Broadcast hyperparams of models
hyperparams_rdd = sc.broadcast(hyperparams)

model_params = []
for i in range(n_models):
    model = TemporalModel(*hyperparams)
    model_params.append(model.state_dict())
    
model_params_rdd = sc.parallelize(model_params)

def train_distributed(model_params, hyperparams):
    pass
    

def train_ensemble_distributed(model_params, data, hyperparams):
    pass

    for m in len(model_params.count()):
        data.mapParitions(lambda k: train_distributed(k, model_params, hyperparams))

    
&quot;&quot;&quot;   
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def train_ensemble(n_models, model_params, data, hyperparams):
    
    model_data = []
    
    args = Args()
    
    assert model_params.count() == n_models
    assert len(data) == n_models, f&quot;Lenght mismatch, lenght of data is {len(data)}, while number of models are {n_models}&quot;
    

    
        
    models_trained = model_params.map(lambda t: train(*(t,hyperparams,data, args)))    
    
    models_params = models_trained.map(lambda t: t._1)
    train_losses = models_trained.map(lambda t: t._2)
    
    
    print(f&quot;Training losses: {[x[1] for x in models_trained]}&quot;)
    
    return models_params, train_losses.collect()
        
</code></pre>
</div>
<div class="cell markdown">
<h4 id="ensemble-predictions"><a class="header" href="#ensemble-predictions">Ensemble predictions</a></h4>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def ensemble_predictions(models, hyperparams, test_x):
    pred_iter = _pred_models_iter(models, hyperparams, test_x)
    return pred_iter.map(lambda t: predict(*t))

def ensemble_predictions_reduced(models, hyperparams, test_x, reduce_fn):
    return ensemble_predictions(models, hyperparams, test_x).reduce(reduce_fn)

def _pred_models_iter(models, hyperparams, test_x):
    if isinstance(models, PipelinedRDD):
        return models.map(lambda model: (model, test_x))
    elif isinstance(models, list): # our case
        models_and_data = [(params, hyperparams, test_x) for params in models]
        return sc.parallelize(models_and_data)
    else:
        raise TypeError(&quot;'models' must be an RDD or a list&quot;)
        

def evaluate_avg_on_set(models, hyperparams, dataset, n_models):
    predictions_sum = ensemble_predictions_reduced(models, hyperparams, dataset, lambda x, y: x + y) # Tensor output
    predictions_avg = predictions_sum/n_models
    
    return predictions_avg


        
</code></pre>
</div>
<div class="cell markdown">
<h4 id="final-conversion-of-dataframes-to-torch-tensors"><a class="header" href="#final-conversion-of-dataframes-to-torch-tensors">Final conversion of dataframes to torch tensors</a></h4>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">display(df_train_receptive_labeled)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">### We do not have targets for unlabelled dataset
def toTensorLabeled(x):
    fs = x[&quot;feature_sequence&quot;]
    target = x[&quot;targets&quot;]
    
    feature_tensor = []
    for f in fs:
        feature_tensor.append(f)
    
    xx = torch.tensor(feature_tensor,dtype=torch.float)
    yy = torch.tensor(target,dtype=torch.float)
    
    return xx.view(27, 15, 2), yy.view(1, 15, 3)

def toTensorUnlabeled(x):
    fs = x[&quot;feature_sequence&quot;]
    feature_tensor = []
    for f in fs:
        feature_tensor.append(f)
    
    xx = torch.tensor(feature_tensor, dtype=torch.float)
    
    return xx.view(27, 15, 2)   

  

labeled_tensor = df_train_receptive_labeled.withColumn(&quot;feature_sequence&quot;, )
#unlabeled_tensor_rdd = df_train_receptive_unlabeled.rdd.map(toTensorUnlabeled)
#test_tensor_rdd = df_test_receptive.rdd.map(toTensorLabeled)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">print(labeled_tensor_rdd.getNumPartitions())
print(unlabeled_tensor_rdd.getNumPartitions())
print(test_tensor_rdd.getNumPartitions())
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell markdown">
<h4 id="saving-trained-models"><a class="header" href="#saving-trained-models">Saving trained models</a></h4>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def save_models(models_state_dict,save_models_dir: Path ,iter: int) -&gt; None:
    &quot;&quot;&quot;
    Save models after training of iteration
    
    Args:
        models: list of state dicts of pytorch nn.Module models to be saved
        save_models_dir: Path to dir where models are being saved
        iter: iteration
    &quot;&quot;&quot;
    
    # Create saving path if it does not exist
    save_models_dir.mkdir(parents=True, exist_ok=True)
    
    for i_model, model in enumerate(models):
        torch.save(model, os.path.join(save_models_dir,f&quot;ensemble{i_model}_iter{iter}.ckpt&quot;))
 
</code></pre>
</div>
<div class="cell markdown">
<h4 id="training-loop-for-semi-supervised-learning"><a class="header" href="#training-loop-for-semi-supervised-learning">Training loop (for semi-supervised learning)</a></h4>
<p>The hypothesis of training ensemble models in a distributed way is that we coud obtain better label estimation.Sepcifically, the prediction of the test sample is obtained by avergaing the prediciton from each memeber. Therefore, we incoporate th</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">n_models = 2
subset_size = 1000
total_size = n_models * subset_size

def get_labeled_subset():
    # Data is loaded into driver's memory
    data = labeled_tensor_rdd.takeSample(False, total_size)
    x, y = zip(*data)
    return torch.stack(x), torch.stack(y)

def get_unlabeled_subset():
    # Data is loaded into driver's memory
    data = unlabeled_tensor_rdd.takeSample(False, total_size)
    return torch.stack(data)

def split_for_ensemble(x,y):
    '''
    Splits data so that each member acesses unique data for training
    '''
    full_size = x.shape[0]
    split_size = full_size//n_models +1
    x = torch.split(x, split_size)
    y = torch.split(y, split_size)
    
    return list(zip(x, y))

# collect test data
data_test = test_tensor_rdd.collect()
x_test, y_test = zip(*data_test)
x_test, y_test = torch.stack(x_test).detach(), torch.stack(y_test).detach()

iterations = 10 # 10

models = []
# initiate models
for i in range(n_models):
    model = TemporalModel(*hyperparams)
    models.append(model.state_dict())

# Sample a small subset of the labeled data. 
# All data is loaded into driver's memory.
x_l, y_l = get_labeled_subset()

print(f&quot;Training distributed ensemble of {len(models)} models&quot;)

# train using only labeled data
models = train_ensemble(n_models,
                        models,
                        split_for_ensemble(x_l, y_l),
                        hyperparams)

# evaluate on test set
with torch.no_grad():
    test_preds = evaluate_avg_on_set(models, hyperparams, x_test, n_models)
    test_mpjpe = mpjpe(test_preds, y_test)
    print(f&quot;MPJPE for test set: {test_mpjpe}&quot;)


print(&quot;Labeled training iteration finished&quot;)

test_mpjpes = []
#train_mpjpes_iteration = []
 
# train using labeled and unlabeled data
for i in range(iterations):
    
    # evaluate on test set
    test_preds = evaluate_avg_on_set(models, hyperparams, x_test, n_models)
    test_mpjpe = mpjpe(test_preds, y_test)
    test_mpjpes.append(test_mpjpe)
    
    x_ul = get_unlabeled_subset()
    
    # predict unlabeled data
    unlabeled_preds = evaluate_avg_on_set(models,
                                          hyperparams,
                                          x_ul,
                                          n_models)
    
    # Random pick a subset of trainning data
    x_l, y_l = get_labeled_subset()
    
    # concat labeled and unlabeled data
    x_cc = torch.concat([x_l, x_ul])
    y_cc = torch.concat([y_l, unlabeled_preds])
    
    # mix labeled and unlabeled data by shuffling
    idx = torch.randperm(x_cc.shape[0])
    x_cc, y_cc = x_cc[idx], y_cc[idx]
     
    print(&quot;Running semi-supervised training iteration: {}&quot;.format(i+1))
    # train using mix of labeled and pseudolabeled data
    models = train_ensemble(n_models,
                            models,
                            split_for_ensemble(x_cc, y_cc),
                            hyperparams)
    
    #train_mpjpes_iteration.append(train_mjpes)
    
    saved_models_dir = Path(&quot;saved_models/humaneva/checkpoints/semi-supervised&quot;)
    save_models(models, saved_models_dir,i)
    
    # evaluate on test set
    with torch.no_grad():
        test_preds = evaluate_avg_on_set(models, hyperparams, x_test, n_models)
        test_mpjpe = mpjpe(test_preds, y_test)
        test_mpjpes.append(test_mpjpe)
        print(f&quot;MPJPE for test set: {test_mpjpe}&quot;)

</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">%matplotlib inline
fig = plt.figure()

plt.plot(test_mpjpe)
plt.show()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">%matplotlib inline
fig = plt.figure()


for i_model, mpjpes in enumerate(zip(*train_mpjpes_list))
    plt.plot(mpjpes, label=f&quot;Ensemble {i_model}&quot;)
    
plt.legend()
plt.show()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># evaluate on test set
test_mpjpes=[]
with torch.no_grad():
    test_preds = evaluate_avg_on_set(models, hyperparams, x_test, n_models)
    test_mpjpe = mpjpe(test_preds, y_test)
    test_mpjpes.append(test_mpjpe)
    print(&quot;MPJPE for test set:&quot;)
    print(test_mpjpes)

</code></pre>
</div>
<div class="cell markdown">
<h4 id="training-loop-for-supervised-baseline"><a class="header" href="#training-loop-for-supervised-baseline">Training loop (for supervised baseline)</a></h4>
<p>We first establish the baseline, where the ensemble model is trained in a distrbuted way. Specifcially, each member of the emsemble model is trained in different work node in parallel. Besides, each member is limited to access a subset of the trainining data stored in the driver node. It is a natural idea to send the same fraction of training data to the work node. However, to avoid the scenario that the work node might no have enough space to store the subset of traning data, we set the threshold for the maximum size of the data to be stored in the work node. Pratically, the size of the subset of training data is fixed to be N=1000.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">import torch

# optional to do data partition 
def get_partitioned_rdd(input_rdd, partition_size=1000):
  
    &quot;&quot;&quot;Partition RDD

    Args:
    input_rdd: RDD to be partitioned

    Returns:
    Partitioned RDD
    &quot;&quot;&quot;
    return input_rdd.mapPartitions(lambda partition: partition_all(partition_size, partition))

n_models = 10
subset_size = 1000
n_iterations = 1000
total_size = n_models * subset_size

models_supervised = []
# initiate models
for i in range(n_models):
    model = TemporalModel(*hyperparams)
    models_supervised.append(model.state_dict())

test_mpjpes_supervised = []
train_mpjpes_iteration_supervised
# train using only labeled data
for iteration in range(n_iterations):
    x_l, y_l = get_labeled_subset()


    models_supervised, train_mjpes_supervised = train_ensemble(n_models, models_supervised, split_for_ensemble(x_l, y_l), hyperparams, n_epochs)
    
    train_mpjpes_iteration_supervised.append(train_mjpes_supervised)
    
    saved_models_dir = Path(&quot;saved_models/humaneva/checkpoints/supervised&quot;)
    save_models(models_supervised, saved_models_dir,epoch)

    # Ealuate on test set
    with torch.no_grad():
        test_preds_supervised = evaluate_avg_on_set(models_supervised, hyperparams, x_test, n_models)
        test_mpjpe_supervised = mpjpe(test_preds_supervised, y_test)
        test_mpjpes_supervised.append(test_mpjpe_supervised)
        print(&quot;MPJPE for test set (supervised baseline):&quot;)
        print(test_mpjpes_supervised)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">%matplotlib inline
fig = plt.figure()

plt.plot(test_mpjpe_supervised)
plt.show()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">%matplotlib inline
fig = plt.figure()


for i_model, mpjpes in enumerate(zip(*train_mpjpes_list_supervised))
    plt.plot(mpjpes, label=f&quot;Ensemble {i_model}&quot;)
    
plt.legend()
plt.show()
</code></pre>
</div>
<div class="cell markdown">
<h2 id="latest-from-main"><a class="header" href="#latest-from-main">Latest from Main</a></h2>
</div>
<div class="cell markdown">
<h3 id="to-do"><a class="header" href="#to-do">To Do</a></h3>
<ul>
<li>a figure shows that the test error is further reduced while incorporating the pseudo-labeled data.</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">%matplotlib inline
fig = plt.figure()


for i_model, mpjpes in enumerate(zip(*train_mpjpes_iteration_supervised)):
    plt.plot(mpjpes, label=f&quot;Ensemble {i_model}&quot;)

plt.title(&quot;Supervised&quot;)
plt.xlabel(&quot;Iteration&quot;)
plt.ylabel(&quot;MPJPE train loss&quot;)
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))
plt.show()
</code></pre>
</div>
<div class="cell markdown">
<h4 id="training-loop-for-semi-supervised-learning-1"><a class="header" href="#training-loop-for-semi-supervised-learning-1">Training loop for semi-supervised learning</a></h4>
<p>The hypothesis of training ensemble models in a distributed way is that we coud obtain better target estimation for the unllablelled. Sepcifically, the prediction of the test sample is obtained by avergaing the prediciton from each memeber. Moreover, incoporating the samples with pseudo labels predicted by ensemble models into the training data is expcted to further improve the perfromace becasue more information is contained in the training data.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">n_models = 20
subset_size = 1000
total_size = n_models * subset_size
start_unlabelled_size = 100
# collect test data
data_test = test_tensor_rdd.collect()
x_test, y_test = zip(*data_test)
x_test, y_test = torch.stack(x_test).detach(), torch.stack(y_test).detach()

iterations = 10 # 10

models = []
# initiate models
for i in range(n_models):
    model = TemporalModel(*hyperparams)
    models.append(model.state_dict())

# Sample a small subset of the labeled data. 
# All data is loaded into driver's memory.
x_l, y_l = get_labeled_subset(labeled_tensor_rdd, total_size)

print(f&quot;Training distributed ensemble of {len(models)} models&quot;)

# train using only labeled data
#models, train_mjpes = train_ensemble(n_models,
                        #models,
                        #split_for_ensemble(x_l, y_l,n_models, total_size),
                        #hyperparams)

models, train_mjpes = train_ensemble(n_models,
                            models,
                            sample_data_for_ensemble(x_l, y_l, n_models, subset_size),
                            hyperparams)
# evaluate on test set
with torch.no_grad():
    test_preds = evaluate_avg_on_set(models, hyperparams, x_test, n_models)
    test_mpjpe = mpjpe(test_preds, y_test)
    print(f&quot;MPJPE for test set: {test_mpjpe}&quot;)


print(&quot;Labeled training iteration finished&quot;)

test_mpjpes = []
train_mpjpes_iteration = []
 
# train using labeled and unlabeled data
for i in range(iterations):
    
    # evaluate on test set
    with torch.no_grad():
        test_preds = evaluate_avg_on_set(models, hyperparams, x_test, n_models)
        test_mpjpe = mpjpe(test_preds, y_test)
        test_mpjpes.append(test_mpjpe)
        print(f&quot;MPJPE for test set: {test_mpjpe}&quot;)
    # use an adaptive total size for unlablled dataste
    full_size = (i+1)* start_unlabelled_size
    x_ul = get_unlabeled_subset(unlabeled_tensor_rdd, full_size)
    
    # predict unlabeled data
    unlabeled_preds = evaluate_avg_on_set(models,
                                          hyperparams,
                                          x_ul,
                                          n_models)
    
    # Random pick a subset of trainning data
    x_l, y_l = get_labeled_subset(labeled_tensor_rdd, total_size)
    
    # concat labeled and unlabeled data
    x_cc = torch.concat([x_l, x_ul])
    y_cc = torch.concat([y_l, unlabeled_preds])
    
    # mix labeled and unlabeled data by shuffling
    idx = torch.randperm(x_cc.shape[0])
    x_cc, y_cc = x_cc[idx], y_cc[idx]
     
    print(&quot;Running semi-supervised training iteration: {}&quot;.format(i+1))
    # train using mix of labeled and pseudolabeled data
    #models, train_mjpes = train_ensemble(n_models,
                            #models,
                            #split_for_ensemble(x_cc, y_cc, n_models, total_size),
                            #hyperparams)
                
    models, train_mjpes = train_ensemble(n_models,
                            models,
                            sample_data_for_ensemble(x_cc, y_cc, n_models, subset_size),
                            hyperparams)
    
    train_mpjpes_iteration.append(train_mjpes)
    
    saved_models_dir = Path(&quot;/dbfs/VideoPose3D/saved_models/humaneva/checkpoints/semi-supervised&quot;)
    save_models(models, saved_models_dir,i)
    
# evaluate on test set
with torch.no_grad():
    test_preds = evaluate_avg_on_set(models, hyperparams, x_test, n_models)
    test_mpjpe = mpjpe(test_preds, y_test)
    test_mpjpes.append(test_mpjpe)
    print(f&quot;MPJPE for test set: {test_mpjpe}&quot;)

</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">%matplotlib inline
fig = plt.figure()

plt.plot(test_mpjpes)
plt.title(&quot;Semi-supervised using pseudotargets&quot;)
plt.xlabel(&quot;Iteration&quot;)
plt.ylabel(&quot;MPJPE test loss&quot;)
plt.show()
plt.close()
print(len(test_mpjpes))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">%matplotlib inline
fig = plt.figure()

for i_model, mpjpes in enumerate(zip(*train_mpjpes_iteration)):
    plt.plot(mpjpes, label=f&quot;Ensemble {i_model}&quot;)
    
plt.title(&quot;Semi-supervised using pseudotargets&quot;)
plt.xlabel(&quot;Iteration&quot;)
plt.ylabel(&quot;MPJPE train loss&quot;)
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))
plt.show()
</code></pre>
</div>
<div class="cell markdown">
<h4 id="function-for-asserting-that-all-elements-in-list-are-equal-not-used-right-now-but-might-be-useful-1"><a class="header" href="#function-for-asserting-that-all-elements-in-list-are-equal-not-used-right-now-but-might-be-useful-1">Function for asserting that all elements in list are equal (not used right now but might be useful)</a></h4>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def all_equal(iterable):
    g = groupby(iterable)
    return next(g, True) and not next(g, False)
    
udf_all_equal = udf(all_equal, BooleanType())

# Broadcast hyperparams of models
hyperparams_rdd = sc.broadcast(hyperparams)

model_params = []
for i in range(n_models):
    model = TemporalModel(*hyperparams)
    model_params.append(model.state_dict())
    
model_params_rdd = sc.parallelize(model_params)

def train_distributed(model_params, hyperparams):
    pass
    

def train_ensemble_distributed(model_params, data, hyperparams):
    pass

    for m in len(model_params.count()):
        data.mapParitions(lambda k: train_distributed(k, model_params, hyperparams))

  


# optional to do data partition 
def get_partitioned_rdd(input_rdd, partition_size=1000):
  
    &quot;&quot;&quot;Partition RDD

    Args:
    input_rdd: RDD to be partitioned

    Returns:
    Partitioned RDD
    &quot;&quot;&quot;
    return input_rdd.mapPartitions(lambda partition: partition_all(partition_size, partition))
</code></pre>
</div>
<div class="cell markdown">
<hr />
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def render_animation(keypoints, keypoints_metadata, poses, skeleton, fps, bitrate, azim, output, viewport,
                     limit=-1, downsample=1, size=6, input_video_path=None, input_video_skip=0):
    &quot;&quot;&quot;
    Render an animation. The supported output modes are:
     -- 'interactive': display an interactive figure
                       (also works on notebooks if associated with %matplotlib inline)
     -- 'html': render the animation as HTML5 video. Can be displayed in a notebook using HTML(...).
     -- 'filename.mp4': render and export the animation as an h264 video (requires ffmpeg).
     -- 'filename.gif': render and export the animation a gif file (requires imagemagick).
    &quot;&quot;&quot;
    plt.ioff()
    fig = plt.figure(figsize=(size*(1 + len(poses)), size))
    ax_in = fig.add_subplot(1, 1 + len(poses), 1)
    ax_in.get_xaxis().set_visible(False)
    ax_in.get_yaxis().set_visible(False)
    ax_in.set_axis_off()
    ax_in.set_title('Input')

    ax_3d = []
    lines_3d = []
    trajectories = []
    radius = 1.7
    for index, (title, data) in enumerate(poses.items()):
        ax = fig.add_subplot(1, 1 + len(poses), index+2, projection='3d')
        ax.view_init(elev=15., azim=azim)
        ax.set_xlim3d([-radius/2, radius/2])
        ax.set_zlim3d([0, radius])
        ax.set_ylim3d([-radius/2, radius/2])
        try:
            ax.set_aspect('equal')
        except NotImplementedError:
            ax.set_aspect('auto')
        ax.set_xticklabels([])
        ax.set_yticklabels([])
        ax.set_zticklabels([])
        ax.dist = 7.5
        ax.set_title(title) #, pad=35
        ax_3d.append(ax)
        lines_3d.append([])
        trajectories.append(data[:, 0, [0, 1]])
    poses = list(poses.values())

    # Decode video
    if input_video_path is None:
        # Black background
        all_frames = np.zeros((keypoints.shape[0], viewport[1], viewport[0]), dtype='uint8')
    else:
        # Load video using ffmpeg
        all_frames = []
        for f in read_video(input_video_path, skip=input_video_skip, limit=limit):
            all_frames.append(f)
        effective_length = min(keypoints.shape[0], len(all_frames))
        all_frames = all_frames[:effective_length]
        
        keypoints = keypoints[input_video_skip:] # todo remove
        for idx in range(len(poses)):
            poses[idx] = poses[idx][input_video_skip:]
        
        if fps is None:
            fps = get_fps(input_video_path)
    
    if downsample &gt; 1:
        keypoints = downsample_tensor(keypoints, downsample)
        all_frames = downsample_tensor(np.array(all_frames), downsample).astype('uint8')
        for idx in range(len(poses)):
            poses[idx] = downsample_tensor(poses[idx], downsample)
            trajectories[idx] = downsample_tensor(trajectories[idx], downsample)
        fps /= downsample

    initialized = False
    image = None
    lines = []
    points = None
    
    if limit &lt; 1:
        limit = len(all_frames)
    else:
        limit = min(limit, len(all_frames))

    parents = skeleton.parents()
    def update_video(i):
        nonlocal initialized, image, lines, points

        for n, ax in enumerate(ax_3d):
            ax.set_xlim3d([-radius/2 + trajectories[n][i, 0], radius/2 + trajectories[n][i, 0]])
            ax.set_ylim3d([-radius/2 + trajectories[n][i, 1], radius/2 + trajectories[n][i, 1]])

        # Update 2D poses
        joints_right_2d = keypoints_metadata['keypoints_symmetry'][1]
        colors_2d = np.full(keypoints.shape[1], 'black')
        colors_2d[joints_right_2d] = 'red'
        if not initialized:
            image = ax_in.imshow(all_frames[i], aspect='equal')
            
            for j, j_parent in enumerate(parents):
                if j_parent == -1:
                    continue
                    
                if len(parents) == keypoints.shape[1] and keypoints_metadata['layout_name'] != 'coco':
                    # Draw skeleton only if keypoints match (otherwise we don't have the parents definition)
                    lines.append(ax_in.plot([keypoints[i, j, 0], keypoints[i, j_parent, 0]],
                                            [keypoints[i, j, 1], keypoints[i, j_parent, 1]], color='pink'))

                col = 'red' if j in skeleton.joints_right() else 'black'
                for n, ax in enumerate(ax_3d):
                    pos = poses[n][i]
                    lines_3d[n].append(ax.plot([pos[j, 0], pos[j_parent, 0]],
                                               [pos[j, 1], pos[j_parent, 1]],
                                               [pos[j, 2], pos[j_parent, 2]], zdir='z', c=col))

            points = ax_in.scatter(*keypoints[i].T, 10, color=colors_2d, edgecolors='white', zorder=10)

            initialized = True
        else:
            image.set_data(all_frames[i])

            for j, j_parent in enumerate(parents):
                if j_parent == -1:
                    continue
                
                if len(parents) == keypoints.shape[1] and keypoints_metadata['layout_name'] != 'coco':
                    lines[j-1][0].set_data([keypoints[i, j, 0], keypoints[i, j_parent, 0]],
                                           [keypoints[i, j, 1], keypoints[i, j_parent, 1]])

                for n, ax in enumerate(ax_3d):
                    pos = poses[n][i]
                    lines_3d[n][j-1][0].set_xdata(np.array([pos[j, 0], pos[j_parent, 0]]))
                    lines_3d[n][j-1][0].set_ydata(np.array([pos[j, 1], pos[j_parent, 1]]))
                    lines_3d[n][j-1][0].set_3d_properties(np.array([pos[j, 2], pos[j_parent, 2]]), zdir='z')

            points.set_offsets(keypoints[i])
        
        print('{}/{}      '.format(i, limit), end='\r')
        

    fig.tight_layout()
    
    anim = FuncAnimation(fig, update_video, frames=np.arange(0, limit), interval=1000/fps, repeat=False)
    if output.endswith('.mp4'):
        Writer = writers['ffmpeg']
        writer = Writer(fps=fps, metadata={}, bitrate=bitrate)
        anim.save(output, writer=writer)
    elif output.endswith('.gif'):
        anim.save(output, dpi=80, writer='imagemagick')
    else:
        raise ValueError('Unsupported output format (only .mp4 and .gif are supported)')
    plt.close()
    
print('Rendering...')    
input_keypoints = keypoints[args.viz_subject][args.viz_action][args.viz_camera].copy()
ground_truth = None
if args.viz_subject in dataset.subjects() and args.viz_action in dataset[args.viz_subject]:
    if 'positions_3d' in dataset[args.viz_subject][args.viz_action]:
        ground_truth = dataset[args.viz_subject][args.viz_action]['positions_3d'][args.viz_camera].copy()
if ground_truth is None:
    print('INFO: this action is unlabeled. Ground truth will not be rendered.')

gen = UnchunkedGenerator(None,
                         None,
                         [input_keypoints],
                         pad=pad,
                         causal_shift=causal_shift,
                         augment=args.test_time_augmentation,
                         kps_left=kps_left,
                         kps_right=kps_right,
                         joints_left=joints_left,
                         joints_right=joints_right)
prediction = evaluate(gen, return_predictions=True)
if model_traj is not None and ground_truth is None:
    prediction_traj = evaluate(gen, return_predictions=True, use_trajectory_model=True)
    prediction += prediction_traj

if args.viz_export is not None:
    print('Exporting joint positions to', args.viz_export)
    # Predictions are in camera space
    np.save(args.viz_export, prediction)

if args.viz_output is not None:
    if ground_truth is not None:
        # Reapply trajectory
        trajectory = ground_truth[:, :1]
        ground_truth[:, 1:] += trajectory
        prediction += trajectory

    # Invert camera transformation
    cam = dataset.cameras()[args.viz_subject][args.viz_camera]
    if ground_truth is not None:
        prediction = camera_to_world(prediction, R=cam['orientation'], t=cam['translation'])
        ground_truth = camera_to_world(ground_truth, R=cam['orientation'], t=cam['translation'])
    else:
        # If the ground truth is not available, take the camera extrinsic params from a random subject.
        # They are almost the same, and anyway, we only need this for visualization purposes.
        for subject in dataset.cameras():
            if 'orientation' in dataset.cameras()[subject][args.viz_camera]:
                rot = dataset.cameras()[subject][args.viz_camera]['orientation']
                break
        prediction = camera_to_world(prediction, R=rot, t=0)
        # We don't have the trajectory, but at least we can rebase the height
        prediction[:, :, 2] -= np.min(prediction[:, :, 2])

    anim_output = {'Reconstruction': prediction}
    if ground_truth is not None and not args.viz_no_ground_truth:
        anim_output['Ground truth'] = ground_truth

    input_keypoints = image_coordinates(input_keypoints[..., :2], w=cam['res_w'], h=cam['res_h'])

    render_animation(input_keypoints, keypoints_metadata, anim_output,
                     dataset.skeleton(), dataset.fps(), args.viz_bitrate, cam['azimuth'], args.viz_output,
                     limit=args.viz_limit, downsample=args.viz_downsample, size=args.viz_size,
                     input_video_path=args.viz_video, viewport=(cam['res_w'], cam['res_h']),
</code></pre>
</div>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/03_Ensemble_Evaluation.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next" href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/00_introduction.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/03_Ensemble_Evaluation.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next" href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/00_introduction.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="../../../elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="../../../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->


    </body>
</html>
