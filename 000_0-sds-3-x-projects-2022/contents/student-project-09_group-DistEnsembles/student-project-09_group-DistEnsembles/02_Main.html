<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>02_Main - sds-3.x/ScaDaMaLe</title>


        <!-- Custom HTML head -->

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../../../favicon.svg">
        <link rel="shortcut icon" href="../../../favicon.png">
        <link rel="stylesheet" href="../../../css/variables.css">
        <link rel="stylesheet" href="../../../css/general.css">
        <link rel="stylesheet" href="../../../css/chrome.css">
        <link rel="stylesheet" href="../../../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../../../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../../../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../../../highlight.css">
        <link rel="stylesheet" href="../../../tomorrow-night.css">
        <link rel="stylesheet" href="../../../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../../../scroll-mdbook-outputs.css">

        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../../../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/00_Introduction.html"><strong aria-hidden="true">1.</strong> student-project-01_group-GraphOfWiki_00_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/01_DataLoading_redirectsTable.html"><strong aria-hidden="true">1.1.</strong> 01_DataLoading_redirectsTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/02_DataLoading_pagesTable.html"><strong aria-hidden="true">1.2.</strong> 02_DataLoading_pagesTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/03_DataLoading_pagelinksTable.html"><strong aria-hidden="true">1.3.</strong> 03_DataLoading_pagelinksTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/04_DataLoading_categorylinksTable.html"><strong aria-hidden="true">1.4.</strong> 04_DataLoading_categorylinksTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/05_DataLoading_categoryTable.html"><strong aria-hidden="true">1.5.</strong> 05_DataLoading_categoryTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/06_redirectRemoval.html"><strong aria-hidden="true">1.6.</strong> 06_redirectRemoval</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/07_createArticleGraph.html"><strong aria-hidden="true">1.7.</strong> 07_createArticleGraph</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/08_explorationArticleGraph.html"><strong aria-hidden="true">1.8.</strong> 08_explorationArticleGraph</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/09_fullGraphAnalysis.html"><strong aria-hidden="true">1.9.</strong> 09_fullGraphAnalysis</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/10_explorativeMotifs.html"><strong aria-hidden="true">1.10.</strong> 10_explorativeMotifs</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/11_conclusionDiscussionAndFutureWork.html"><strong aria-hidden="true">1.11.</strong> 11_conclusionDiscussionAndFutureWork</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/12_gameNotebookSetup.html"><strong aria-hidden="true">1.12.</strong> 12_gameNotebookSetup</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/13_gameNotebook.html"><strong aria-hidden="true">1.13.</strong> 13_gameNotebook</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/99_PlanningAndNotes.html"><strong aria-hidden="true">1.14.</strong> 99_PlanningAndNotes</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-02_group-DDLOfVision/student-project-02_group-DDLOfVision/00_vqa_introduction.html"><strong aria-hidden="true">2.</strong> student-project-02_group-DDLOfVision_00_vqa_introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-02_group-DDLOfVision/student-project-02_group-DDLOfVision/01_vqa_model_training.html"><strong aria-hidden="true">2.1.</strong> 01_vqa_model_training</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-02_group-DDLOfVision/student-project-02_group-DDLOfVision/02_vqa_model_inference.html"><strong aria-hidden="true">2.2.</strong> 02_vqa_model_inference</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-02_group-DDLOfVision/student-project-02_group-DDLOfVision/0y_test_mnist-pytorch.html"><strong aria-hidden="true">2.3.</strong> 0y_test_mnist-pytorch</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/00_ingest_data.html"><strong aria-hidden="true">3.</strong> student-project-03_group-WikiKG90mv2_00_ingest_data</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/01_fetch_descriptions.html"><strong aria-hidden="true">3.1.</strong> 01_fetch_descriptions</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/02_load_data.html"><strong aria-hidden="true">3.2.</strong> 02_load_data</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/03_data_exploration.html"><strong aria-hidden="true">3.3.</strong> 03_data_exploration</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/04_analysing_relation_types.html"><strong aria-hidden="true">3.4.</strong> 04_analysing_relation_types</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/05_Motif_search_defination_code.html"><strong aria-hidden="true">3.5.</strong> 05_Motif_search_defination_code</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/06_python_analysis.html"><strong aria-hidden="true">3.6.</strong> 06_python_analysis</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/08_pagerank.html"><strong aria-hidden="true">3.7.</strong> 08_pagerank</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/09_outro_discussion.html"><strong aria-hidden="true">3.8.</strong> 09_outro_discussion</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/0x_Motif_search_defination_code.html"><strong aria-hidden="true">3.9.</strong> 0x_Motif_search_defination_code</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/10_motif_mining_WikiKGv2.html"><strong aria-hidden="true">3.10.</strong> 10_motif_mining_WikiKGv2</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-04_group-FedMLMedicalApp/student-project-04_group-FedMLMedicalApp/00_Notebook_Presentation.html"><strong aria-hidden="true">4.</strong> student-project-04_group-FedMLMedicalApp_00_Notebook_Presentation</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-04_group-FedMLMedicalApp/student-project-04_group-FedMLMedicalApp/01_BrainTumorSegmentation_Centralized_Training.html"><strong aria-hidden="true">4.1.</strong> 01_BrainTumorSegmentation_Centralized_Training</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-04_group-FedMLMedicalApp/student-project-04_group-FedMLMedicalApp/02_Federated_Learning_BrainTumorSegmentation.html"><strong aria-hidden="true">4.2.</strong> 02_Federated_Learning_BrainTumorSegmentation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-04_group-FedMLMedicalApp/student-project-04_group-FedMLMedicalApp/data_upload_test.html"><strong aria-hidden="true">4.3.</strong> data_upload_test</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/00_introduction.html"><strong aria-hidden="true">5.</strong> student-project-05_group-DistOpt_00_introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/01_Bayesian_optimization.html"><strong aria-hidden="true">5.1.</strong> 01_Bayesian_optimization</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/02_Gaussian_processes.html"><strong aria-hidden="true">5.2.</strong> 02_Gaussian_processes</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/03_acquisition_functions.html"><strong aria-hidden="true">5.3.</strong> 03_acquisition_functions</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/04_scalable_Bayesian_optimization.html"><strong aria-hidden="true">5.4.</strong> 04_scalable_Bayesian_optimization</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/05_implementation_documentation.html"><strong aria-hidden="true">5.5.</strong> 05_implementation_documentation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/06_our_implementation.html"><strong aria-hidden="true">5.6.</strong> 06_our_implementation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/07_additional_code.html"><strong aria-hidden="true">5.7.</strong> 07_additional_code</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-07_group-ExpsZerOInit/student-project-07_group-ExpsZerOInit/00_introduction.html"><strong aria-hidden="true">6.</strong> student-project-07_group-ExpsZerOInit_00_introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-07_group-ExpsZerOInit/student-project-07_group-ExpsZerOInit/01_resnet.html"><strong aria-hidden="true">6.1.</strong> 01_resnet</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-08_group-WikiSearch/student-project-08_group-WikiSearch/00_Introduction.html"><strong aria-hidden="true">7.</strong> student-project-08_group-WikiSearch_00_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-08_group-WikiSearch/student-project-08_group-WikiSearch/01_InputParsing.html"><strong aria-hidden="true">7.1.</strong> 01_InputParsing</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-08_group-WikiSearch/student-project-08_group-WikiSearch/02_PageRank.html"><strong aria-hidden="true">7.2.</strong> 02_PageRank</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-08_group-WikiSearch/student-project-08_group-WikiSearch/03_QuerySearch.html"><strong aria-hidden="true">7.3.</strong> 03_QuerySearch</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/00_Introduction.html"><strong aria-hidden="true">8.</strong> student-project-09_group-DistEnsembles_00_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/01_Data.html"><strong aria-hidden="true">8.1.</strong> 01_Data</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/02_Main.html" class="active"><strong aria-hidden="true">8.2.</strong> 02_Main</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/02_non_distributed.html"><strong aria-hidden="true">8.3.</strong> 02_non_distributed</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/02a_Single_Model.html"><strong aria-hidden="true">8.4.</strong> 02a_Single_Model</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/03_Evaluation.html"><strong aria-hidden="true">8.5.</strong> 03_Evaluation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/03_RDDs.html"><strong aria-hidden="true">8.6.</strong> 03_RDDs</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/03a_Ensemble.html"><strong aria-hidden="true">8.7.</strong> 03a_Ensemble</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/041_Data_Preprocessing.html"><strong aria-hidden="true">8.8.</strong> 041_Data_Preprocessing</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/00_introduction.html"><strong aria-hidden="true">9.</strong> student-project-10_group-RDI_00_introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/01_prepare_data.html"><strong aria-hidden="true">9.1.</strong> 01_prepare_data</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/02_baseline.html"><strong aria-hidden="true">9.2.</strong> 02_baseline</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/03_single_machine.html"><strong aria-hidden="true">9.3.</strong> 03_single_machine</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/04_distributed_learning.html"><strong aria-hidden="true">9.4.</strong> 04_distributed_learning</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-11_group-CollaborativeFiltering/student-project-11_group-CollaborativeFiltering/01_Introduction.html"><strong aria-hidden="true">10.</strong> student-project-11_group-CollaborativeFiltering_01_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-11_group-CollaborativeFiltering/student-project-11_group-CollaborativeFiltering/01a_Implementation.html"><strong aria-hidden="true">10.1.</strong> 01a_Implementation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-11_group-CollaborativeFiltering/student-project-11_group-CollaborativeFiltering/02_FirstOrderMethods.html"><strong aria-hidden="true">10.2.</strong> 02_FirstOrderMethods</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../editors.html"><strong aria-hidden="true">11.</strong> Editors</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">sds-3.x/ScaDaMaLe</h1>

                    <div class="right-buttons">
                        <a href="../../../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div class="cell markdown">
<h2 id="distributed-ensemble-for-semi-supervised-learning-with-pseudolabels"><a class="header" href="#distributed-ensemble-for-semi-supervised-learning-with-pseudolabels">Distributed Ensemble for semi-supervised learning with pseudolabels</a></h2>
<p>Here we provide the necessary code for creating and training a distributed ensemble of temporal convolutional network for semi-supervised learning with pseudolabels.</p>
</div>
<div class="cell markdown">
<h3 id="imports"><a class="header" href="#imports">Imports</a></h3>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">import numpy as np
import torch

import pyspark.sql.functions as F
from pyspark.sql import Window
from pyspark.sql.functions import collect_list, size, udf
from pyspark.ml.feature import VectorAssembler
from pyspark.sql.types import BooleanType
from pyspark.sql.functions import udf
from itertools import groupby
from pyspark.rdd import PipelinedRDD

from pathlib import Path
import os
import matplotlib.pyplot as plt
</code></pre>
</div>
<div class="cell markdown">
<h3 id="data"><a class="header" href="#data">Data</a></h3>
<p>The HumanEva-I dataset are used to evaluate our distributed ensemble models. HumanEva-I consists of 4 subjects performing a set of 6 pre-defined actions including Walk, Jog, Throw/catch, Gesture, Box, and Combo. Ground-truth 3D motions are captured using a commercial motion capture system provided by ViconPeack. Videos data is recorded via using two commerical video capture systems. Originally, there are 56 sequences and approximately 80000 frames in total. 15-joint sekeleton is adopted, giving 15 keypoints, and the provided train/test split is used. Our approach of distributed ensembles can be applied on other datasets for 3D pose estimation, where locations of 2D and 3D are provided. * frame visualization should be added here.</p>
<p>HumanEva-I is pre-processed using Matlab and stored in the .CSV format, which Apache Spark supports. The train and test data are loaded as DataFrames. The columns consists of frame index, subject index, action name, camera index, and 2D and 3D positions of each joint (keypoint). For brevity, the subject indices, action names and camera indices are grouped together to a single column.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">from pyspark.sql.types import StructType, StringType, DoubleType, IntegerType

humaneva_train_path = &quot;/VideoPose3D/humaneva/humaneva15_train.csv&quot;
humaneva_test_path = &quot;/VideoPose3D/humaneva/humaneva15_test.csv&quot;

def load_data_from_csv(file_location):
    &quot;&quot;&quot;Load and preprocess HumanEva data
    Args:
        file_location: file location from which to load the data
        
    Returns:
        df: spark DataFrame
    &quot;&quot;&quot;
    file_type = &quot;csv&quot;
    infer_schema = &quot;true&quot;
    first_row_is_header = False
    delimiter = &quot;,&quot;
    
    schema = StructType() \
      .add(&quot;Idx&quot;,IntegerType(),True) \
      .add(&quot;Subject&quot;,StringType(),True) \
      .add(&quot;Action&quot;,StringType(),True) \
      .add(&quot;Camera&quot;,StringType(),True)
    for i in range(15):
        schema = schema.add(f&quot;u{i}&quot;,DoubleType(),True).add(f&quot;v{i}&quot;,DoubleType(),True)
    for i in range(15):
        schema = schema.add(f&quot;X{i}&quot;,DoubleType(),True).add(f&quot;Y{i}&quot;,DoubleType(),True).add(f&quot;Z{i}&quot;,DoubleType(),True)
    
    # Load the data from file
    df = spark.read.csv(file_location, header=True, schema=schema, sep=',')
    return df

df_train = load_data_from_csv(humaneva_train_path).withColumn(&quot;Group&quot;, F.concat_ws(', ', &quot;Subject&quot;, &quot;Action&quot;, &quot;Camera&quot;)).drop(&quot;Subject&quot;, &quot;Action&quot;, &quot;Camera&quot;)
df_test = load_data_from_csv(humaneva_test_path).withColumn(&quot;Group&quot;, F.concat_ws(', ', &quot;Subject&quot;, &quot;Action&quot;, &quot;Camera&quot;)).drop(&quot;Subject&quot;, &quot;Action&quot;, &quot;Camera&quot;)
</code></pre>
</div>
<div class="cell markdown">
<p>Visualization of train DataFrame. X, Y and Z represents the coordinate of the 3D locations; while u and v represent the coordinates of the 2D locations.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">display(df_train)
</code></pre>
</div>
<div class="cell markdown">
<h4 id="assemble-feature-and-target-columns"><a class="header" href="#assemble-feature-and-target-columns">Assemble feature and target columns</a></h4>
<p>We us VectorAssembler to transoform the 2D (feature) and 3d (target) locations to vectors.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">feature_names = []
target_names = []
n_keypoints = 15 
for i in range(n_keypoints):
    feature_names.append(&quot;u{}&quot;.format(i))
    feature_names.append(&quot;v{}&quot;.format(i))
    target_names.append(&quot;X{}&quot;.format(i))
    target_names.append(&quot;Y{}&quot;.format(i))
    target_names.append(&quot;Z{}&quot;.format(i))
    
# feature corresponds to the 2D positions.
# target corresponds to the 3D positions.
feature_assembler = VectorAssembler(inputCols=feature_names, outputCol=&quot;features&quot;) # merge u and v into a vector column.
target_assembler = VectorAssembler(inputCols=target_names, outputCol=&quot;targets&quot;)

def assemble_vectors(df):
    df = feature_assembler.transform(df)
    df = target_assembler.transform(df)
    df = df.drop(*feature_names).drop(*target_names)
    return df

df_train = assemble_vectors(df_train)
df_test = assemble_vectors(df_test)
</code></pre>
</div>
<div class="cell markdown">
<h4 id="create-receptive-fields"><a class="header" href="#create-receptive-fields">Create receptive fields</a></h4>
<p>The employed temporal convolutional network uses the temporal information, which means the 3D pose prediction of the current frame depends on the previous frame and the future frames. Since the data is provided per frame, to reduce the computational load of data pre-processing in the worker node, we first encapsulate any sequential 27 frames into one feature sequence. 27 denotes the receptive field. Each <em>feature</em> contains the 2D positions of the 15 joints (keypoints). Each <em>feature sequence</em> therefore consists of the 2D positions of 27 frames. The target of a sequence is the 3D pose of the middle frame. This data is used for training and evaluation instead of the individual positions.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">receptive_field = 27

w = Window.orderBy(&quot;Idx&quot;).partitionBy([&quot;Group&quot;]).rowsBetween(Window.currentRow-receptive_field//2, Window.currentRow+receptive_field//2)

def create_receptive_fields(df):
    df = df.withColumn(&quot;feature_sequence&quot;, collect_list(&quot;features&quot;).over(w))
    df = df.withColumn(&quot;group_sequence&quot;, collect_list(&quot;Group&quot;).over(w))
    df = df.filter(size(df.group_sequence) == receptive_field)
    return df

df_train_receptive = create_receptive_fields(df_train).drop(&quot;features&quot;)
df_test_receptive = create_receptive_fields(df_test).drop(&quot;features&quot;)
</code></pre>
</div>
<div class="cell markdown">
<p>Visualisation of receptive field data</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">display(df_train_receptive)
</code></pre>
</div>
<div class="cell markdown">
<h4 id="split-training-set-into-labeled-and-unlabeled-based-on-chunks"><a class="header" href="#split-training-set-into-labeled-and-unlabeled-based-on-chunks">Split training set into labeled and unlabeled based on chunks</a></h4>
<p>In the project, we are exploring semi-supervised learning with psuedolabels, which requires both labeled and unlabeled training data. However, the original HumanEva-I dataset does not provide pre-defined sets of labeled and unlabeled data. Therefore, we randomly split the data, with respect to the group, into an unlabeled and labeled set. To have a realistic semi-supervised setting, we assume that the unlabeled training data is slighly larger than the labeled training data. The targets are droped for the unlabeled training set.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">from random import sample, seed

## find right random seed to compensate for the different size of each chunk
seed(0) # seed 0 gives ok split
chunks = df_train_receptive.select(&quot;Group&quot;).distinct().collect()
chunks = [x[&quot;Group&quot;] for x in chunks]

num_chunks = len(chunks)
num_unlabeled = int(num_chunks*0.6)

unlabeled_chunks = sample(chunks, num_unlabeled)
labeled_chunks = [x for x in chunks if x not in unlabeled_chunks]

df_train_receptive_unlabeled = df_train_receptive.filter(df_train_receptive.Group.isin(unlabeled_chunks))
df_train_receptive_unlabeled = df_train_receptive_unlabeled.drop(&quot;targets&quot;)
df_train_receptive_labeled = df_train_receptive.filter(~df_train_receptive.Group.isin(unlabeled_chunks))
</code></pre>
</div>
<div class="cell markdown">
<h4 id="convert-dataframes-to-torch-tensors"><a class="header" href="#convert-dataframes-to-torch-tensors">Convert dataframes to torch tensors</a></h4>
<p>Here we create RDDs for training and test from the corresponding DataFrames to RDDs. Thereafter, we map the vectors to Tensor enable training using PyTorch.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">### We do not have targets for unlabelled dataset
def toTensorLabeled(x):
    fs = x[&quot;feature_sequence&quot;]
    target = x[&quot;targets&quot;]
    
    feature_tensor = []
    for f in fs:
        feature_tensor.append(f)
    
    xx = torch.tensor(feature_tensor,dtype=torch.float)
    yy = torch.tensor(target,dtype=torch.float)
    
    return xx.view(27, 15, 2), yy.view(1, 15, 3)

def toTensorUnlabeled(x):
    fs = x[&quot;feature_sequence&quot;]
    feature_tensor = []
    for f in fs:
        feature_tensor.append(f)
    
    xx = torch.tensor(feature_tensor, dtype=torch.float)
    
    return xx.view(27, 15, 2)   

labeled_tensor_rdd = df_train_receptive_labeled.rdd.map(toTensorLabeled)
unlabeled_tensor_rdd = df_train_receptive_unlabeled.rdd.map(toTensorUnlabeled)
test_tensor_rdd = df_test_receptive.rdd.map(toTensorLabeled)
</code></pre>
</div>
<div class="cell markdown">
<h4 id="dataset-split"><a class="header" href="#dataset-split">Dataset split</a></h4>
<p>Here we provide functions for * Train/Test split * Labeled/Unlabeled split * Dataset split for each member. Note that we provide two functions. The one is split<em>for</em>ensemble(), which guarantees that each member accesses the unique data. The other is sample<em>data</em>for_ensemble() simply randomly sample the same size of training data for each memeber meaning that there might be some resused data over different members.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def get_labeled_subset(labeled_tensor_rdd, full_size):
    # Data is loaded into driver's memory
    data = labeled_tensor_rdd.takeSample(True, full_size)
    x, y = zip(*data)
    return torch.stack(x), torch.stack(y)

def get_unlabeled_subset(unlabeled_tensor_rdd, full_size):
    # Data is loaded into driver's memory
    data = unlabeled_tensor_rdd.takeSample(True, full_size)
    return torch.stack(data)

def split_for_ensemble(x,y, n_models, full_size):
    '''
    Splits data so that each member acesses unique data for training
    '''
    full_size = x.shape[0]
    split_size = full_size//n_models +1  
    x = torch.split(x, split_size)
    y = torch.split(y, split_size)
    
    return list(zip(x, y))

def sample_data_for_ensemble(x,y, n_models, subset_size):
    '''
    Randomly sample a subset of training data
    '''
    x_ =[]
    y_ =[]
    
    subset_size = np.amin([subset_size, x.size(0)])
    
    for i in range(n_models):
        perm = torch.randperm(x.size(0))
        idx = perm[:subset_size]
        x_.append(x[idx])
        y_.append(y[idx])
    return list(zip(x_, y_))
</code></pre>
</div>
<div class="cell markdown">
<h3 id="distributed-ensemble-models"><a class="header" href="#distributed-ensemble-models">Distributed Ensemble Models</a></h3>
</div>
<div class="cell markdown">
<h4 id="define-model"><a class="header" href="#define-model">Define model</a></h4>
<p>Here we define the 3D pose estimation model with temporal convolutions and corresponding hyperparameters. Each ensemble will use this model to train on labeled data (including pseudolabels) and make predictions on unlabeled data.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">from torch import nn

class Args:
    # Data arguments
    num_joints = 15
    
    # Model arguments
    stride = 1    # chunk size to use during training
    epochs = 10 # 100    # number of training epochs
    batch_size = 128     # batch size in terms of predicted frames
    dropout = 0.25    # dropout probability
    learning_rate = 0.001    # initial learning rate
    lr_decay = 0.996     # learning rate decay per epoch
    data_augmentation = True # disable train-time flipping
    test_time_augmentation = True # disable test-time flipping
    architecture = '3,3,3'    # filter widths separated by comma
    channels = 1024    # number of channels in convolution layers

args = Args()
filter_widths = [int(x) for x in args.architecture.split(',')]
receptive_field = np.prod(filter_widths) # model_pos.receptive_field()
print('INFO: Receptive field: {} frames'.format(receptive_field))
pad = (receptive_field - 1) // 2 # Padding on each side
hyperparams = [args.num_joints, 2, args.num_joints, filter_widths, args.dropout, args.channels]

class TemporalModelBase(nn.Module):
    def __init__(self, num_joints_in, in_features, num_joints_out,
                 filter_widths, dropout, channels):
        super().__init__()
        # Validate input
        for fw in filter_widths:
            assert fw % 2 != 0, 'Only odd filter widths are supported'
        self.num_joints_in = num_joints_in
        self.in_features = in_features
        self.num_joints_out = num_joints_out
        self.filter_widths = filter_widths
        self.drop = nn.Dropout(dropout)
        self.relu = nn.ReLU(inplace=True)
        self.pad = [ filter_widths[0] // 2 ]
        self.expand_bn = nn.BatchNorm1d(channels, momentum=0.1)
        self.shrink = nn.Conv1d(channels, num_joints_out*3, 1)
        

    def set_bn_momentum(self, momentum):
        self.expand_bn.momentum = momentum
        for bn in self.layers_bn:
            bn.momentum = momentum
        
    def forward(self, pos2D):
        assert len(pos2D.shape) == 4 # pos2D: B x 27 x 15 x 2
        assert pos2D.shape[-2] == self.num_joints_in # 15
        assert pos2D.shape[-1] == self.in_features   # 2     
        sz = pos2D.shape[:3] # B x 27 x 15
        pos2D = pos2D.view(pos2D.shape[0], pos2D.shape[1], -1) # B x 27 x 15 * 2
        pos2D = pos2D.permute(0, 2, 1) # B x 15 * 2 x 27
        pos3D = self._forward_blocks(pos2D)
        pos3D = pos3D.permute(0, 2, 1)
        pos3D = pos3D.view(sz[0], -1, self.num_joints_out, 3)
        return pos3D

class TemporalModel(TemporalModelBase):
    def __init__(self, num_joints_in, in_features, num_joints_out,
                 filter_widths, dropout=0.25, channels=1024):
        &quot;&quot;&quot;
        Reference 3D pose estimation model with temporal convolutions.Initialize this model.
        
        Arg:
            num_joints_in -- number of input joints (i.e. 15 for HumanEva-I)
            in_features -- number of input features for each joint (typically 2 for 2D input)
            num_joints_out -- number of output joints (can be different than input)
            filter_widths -- list of convolution widths, which also determines the # of blocks and receptive field
            dropout -- dropout probability
            channels -- number of convolution channels
        &quot;&quot;&quot;
        super().__init__(num_joints_in, in_features, num_joints_out, filter_widths, dropout, channels)
        self.expand_conv = nn.Conv1d(num_joints_in*in_features, channels, filter_widths[0], bias=False)
        layers_conv = []
        layers_bn = []
        next_dilation = filter_widths[0] # 3
        for i in range(1, len(filter_widths)):
            self.pad.append((filter_widths[i] - 1)*next_dilation // 2) # [1, 3, 9]
            layers_conv.append(nn.Conv1d(channels, channels,
                                         filter_widths[i],
                                         dilation=next_dilation,
                                         bias=False))
            layers_bn.append(nn.BatchNorm1d(channels, momentum=0.1))
            layers_conv.append(nn.Conv1d(channels, channels, 1, dilation=1, bias=False))
            layers_bn.append(nn.BatchNorm1d(channels, momentum=0.1))
            next_dilation *= filter_widths[i] # 3, 9, 27
        self.layers_conv = nn.ModuleList(layers_conv)
        self.layers_bn = nn.ModuleList(layers_bn)
        
    def _forward_blocks(self, pos2D):
        # pos2D: B x 15 * 2 x 27
        x = self.drop(self.relu(self.expand_bn(self.expand_conv(pos2D)))) # B x 1024 x 25
        for i in range(len(self.pad) - 1):
            pad = self.pad[i+1] # 3, 9
            res = x[:, :, pad : x.shape[2] - pad] # B x 1024 x 19, B x 1024 x 1
            x = self.drop(self.relu(self.layers_bn[2*i](self.layers_conv[2*i](x)))) # B x 1024 x 19, B x 1024 x 1
            x = res + self.drop(self.relu(self.layers_bn[2*i + 1](self.layers_conv[2*i + 1](x))))
        pos3D = self.shrink(x) # B x 15*3 x 1
        return pos3D
    
    @staticmethod
    def from_state_dict(params, hyperparams):
        net = TemporalModel(*hyperparams)
        net.load_state_dict(params)
        return net
</code></pre>
</div>
<div class="cell markdown">
<h4 id="loss"><a class="header" href="#loss">Loss</a></h4>
<p>Here we define the loss used for training and evaluation. There are three metrics by convention. * Mean per-joint postion error (MPJPE), which is the mean Euclidean distance between predicted jlint postions and ground-truth joint postions; * The error after alignment with the ground truth in translation, rotation, and scale (P-MPJPE); * predicted poses with the ground-truth only in scale (N-MPJPE).</p>
<p>Here MPJPE is adopted for loss and evaluation.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def mpjpe(predicted, target):
    &quot;&quot;&quot;
    Mean per-joint position error (i.e. mean Euclidean distance),
    often referred to as &quot;Protocol #1&quot; in many papers.
    &quot;&quot;&quot;
    assert predicted.shape == target.shape
    return torch.mean(torch.norm(predicted - target, dim=len(target.shape)-1))
</code></pre>
</div>
<div class="cell markdown">
<h4 id="define-dataset"><a class="header" href="#define-dataset">Define dataset</a></h4>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">class DataSet(torch.utils.data.Dataset):
    def __init__(self, pos2D, pos3D):
        self.pos2D = pos2D # self.pos2D: B x 27 x 15 * 2
        self.pos3D = pos3D # self.pos3D: B x 1 x 15 * 3

    def __len__(self):
        return self.pos2D.shape[0]

    def __getitem__(self, ind):
        pos2D = self.pos2D[ind] # pos2D: B x 27 x 15 * 2 -&gt; 27 x 15 * 2
        pos3D = self.pos3D[ind] # pos2D: B x 1 x 15 * 3 -&gt; 1 x 15 * 2
        return pos2D, pos3D
</code></pre>
</div>
<div class="cell markdown">
<h4 id="train-and-predict-models"><a class="header" href="#train-and-predict-models">Train and predict models</a></h4>
<p>The train and prediction models for each member are defined here. Note that Spark enables distribute these functions on the work node automatically.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def train(params, hyperparams, pos2D, pos3D, args):
    model = TemporalModel.from_state_dict(params, hyperparams)
    model.train()
    
    lr = args.learning_rate
    lr_decay = args.lr_decay
    train_data = DataSet(pos2D, pos3D)
    dataloader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=True)
    opt = torch.optim.Adam(model.parameters(), lr=lr, amsgrad=True)
    initial_momentum = 0.1
    final_momentum = 0.001
    
    losses_3d_train = []
    for epoch in range(args.epochs):
        epoch_loss_3d_train = 0
        N = 0
        
        for batch in dataloader:
            inputs_2d, inputs_3d = batch
            if torch.cuda.is_available():
                inputs_3d = inputs_3d.cuda()
                inputs_2d = inputs_2d.cuda()
                model = model.cuda()
            inputs_3d[:, :, 0] = 0
            # Predict 3D poses
            predicted_3d_pos = model(inputs_2d)
            # Calcuclate MPJPE loss
            loss_3d_pos = mpjpe(predicted_3d_pos, inputs_3d)
            epoch_loss_3d_train += inputs_3d.shape[0]*inputs_3d.shape[1] * loss_3d_pos.item()
            N += inputs_3d.shape[0]*inputs_3d.shape[1]
            loss_total = loss_3d_pos
            opt.zero_grad()
            loss_total.backward()
            # Make one optimization step on batch
            opt.step()
        losses_3d_train.append(epoch_loss_3d_train / N)
        print('[%d] lr %f 3d_train %f' % (
                epoch + 1,
                lr,
                losses_3d_train[-1] * 1000))
        # Decay learning rate exponentially
        lr *= lr_decay
        for param_group in opt.param_groups:
            param_group['lr'] *= lr_decay
            
    err = mpjpe(model(pos2D.cuda()), pos3D.cuda())
    lossval = float(err.detach().cpu().numpy())
    return model.state_dict(), lossval


def predict(params, hyperparams, x):    
    model = TemporalModel.from_state_dict(params, hyperparams)
    model.eval()
    if torch.cuda.is_available():
        x = x.cuda()
        model.cuda() 
    return model(x).detach().cpu()
</code></pre>
</div>
<div class="cell markdown">
<h4 id="train-ensemble-models-in-parallel"><a class="header" href="#train-ensemble-models-in-parallel">Train ensemble models in parallel</a></h4>
<p>Train_ensemble() is defined to enable the training of ensemble models in parallel. Note that the training is performed in work nodes.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def train_ensemble(n_models, model_params, data, hyperparams):
    &quot;&quot;&quot;
    n_models: number of ensemble members
    model_params: number of learnable parameters 
    data:  a list of training dataset for each member
    hyperprams: the pre-defined parameters of the model
    &quot;&quot;&quot;
    
    model_data = []
    args = Args()
    assert len(model_params) == n_models
    assert len(data) == n_models, f&quot;Lenght mismatch, lenght of data is {len(data)}, while number of models are {n_models}&quot;
    
    for i, (x, y) in enumerate(data):
        model_data.append((model_params[i], hyperparams, x, y, args)) # Pairs of model parameters, hyperparamers, training data, and arguments for each member.
    
    # create an RDD
    model_data_rdd = sc.parallelize(model_data)
    # each memeber is trained using their own data
    models_trained = model_data_rdd.map(lambda t: train(*t))
    # after training,  the trained models and loss values are sent to the driver node
    models_trained = models_trained.collect()
    
    
    print(f&quot;Training losses: {[x[1] for x in models_trained]}&quot;)
    
    return [x[0] for x in models_trained],[x[1] for x in models_trained]  
        
</code></pre>
</div>
<div class="cell markdown">
<h4 id="ensemble-predictions"><a class="header" href="#ensemble-predictions">Ensemble predictions</a></h4>
<p>Note that the prediction is done in driver node.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def ensemble_predictions(models, hyperparams, test_x):
    pred_iter = _pred_models_iter(models, hyperparams, test_x)
    return pred_iter.map(lambda t: predict(*t))

def ensemble_predictions_reduced(models, hyperparams, test_x, reduce_fn):
    return ensemble_predictions(models, hyperparams, test_x).reduce(reduce_fn)

def _pred_models_iter(models, hyperparams, test_x):
    if isinstance(models, PipelinedRDD):
        return models.map(lambda model: (model, test_x))
    elif isinstance(models, list): # our case
        models_and_data = [(params, hyperparams, test_x) for params in models]
        return sc.parallelize(models_and_data)
    else:
        raise TypeError(&quot;'models' must be an RDD or a list&quot;)
        

def evaluate_avg_on_set(models, hyperparams, dataset, n_models):
    predictions_sum = ensemble_predictions_reduced(models, hyperparams, dataset, lambda x, y: x + y) # Tensor output
    predictions_avg = predictions_sum/n_models
    
    return predictions_avg  
</code></pre>
</div>
<div class="cell markdown">
<h4 id="saving-trained-models"><a class="header" href="#saving-trained-models">Saving trained models</a></h4>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def save_models(models_state_dict,save_models_dir: Path, iter: int, n_member : int) -&gt; None:
    &quot;&quot;&quot;
    Save models after training of iteration
    
    Args:
        models_state_dict: list of state dicts of pytorch nn.Module models to be saved
        save_models_dir: Path to dir where models are being saved
        iter: iteration
        n_member: number of members in the current ensemble model
    &quot;&quot;&quot;
    
    # Create saving path if it does not exist
    save_models_dir.mkdir(parents=True, exist_ok=True)
    
    for i_model, model_state_dict in enumerate(models_state_dict):
        torch.save(model_state_dict, os.path.join(save_models_dir,f&quot;{n_member}_members_ensemble{i_model}_iter{iter}.ckpt&quot;))
 
</code></pre>
</div>
<div class="cell markdown">
<h4 id="training-loop-for-supervised-baseline"><a class="header" href="#training-loop-for-supervised-baseline">Training loop (for supervised baseline)</a></h4>
<p>We first establish the baseline, where the ensemble model is trained in a distrbuted way. Specifcially, each member is trained in a different work node in parallel. The hypothesis is that the prediction should be more accurate than the single model. Moreover, each member is limited to access a subset of the trainining data stored in the driver node. It is a natural idea to send the same fraction of training data to the work node. However, to avoid the scenario that the work node might not have enough space to store the subset of traning data, we set the threshold for the maximum size of the data to be stored in the work node. Pratically, the size of the subset of training data is fixed to be N=1000.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">#n_models_set = [1 , 2, 3, 5, 10] #number of members
n_models_set = [2, 3, 5, 10]
# collect test data
data_test = test_tensor_rdd.collect()
x_test, y_test = zip(*data_test)
x_test, y_test = torch.stack(x_test).detach(), torch.stack(y_test).detach()
subset_size = 1000 # subset of training data allocated to each work node.
n_iterations = 100

for n_models in n_models_set:
    test_mpjpes_supervised = []
    train_mpjpes_iteration_supervised = []
    total_size = n_models * subset_size

    for n_models in n_models_set:
        models_supervised = []
        # initiate models
        for i in range(n_models):
            model = TemporalModel(*hyperparams)
            models_supervised.append(model.state_dict())

        # train using only labeled data
        for iteration in range(n_iterations):
            x_l, y_l = get_labeled_subset(labeled_tensor_rdd, total_size)


            models_supervised, train_mjpes_supervised = train_ensemble(n_models, models_supervised, split_for_ensemble(x_l, y_l,n_models, total_size), hyperparams)

            train_mpjpes_iteration_supervised.append(train_mjpes_supervised)

            saved_models_dir = Path(&quot;/dbfs/VideoPose3D/saved_models/humaneva/checkpoints/supervised&quot;)
            save_models(models_supervised, saved_models_dir, iteration, n_models)

            # Ealuate on test set
            with torch.no_grad():
                test_preds_supervised = evaluate_avg_on_set(models_supervised, hyperparams, x_test, n_models)
                test_mpjpe_supervised = mpjpe(test_preds_supervised, y_test)
                test_mpjpes_supervised.append(test_mpjpe_supervised)
                print(&quot;MPJPE for test set (supervised baseline):&quot;)
                print(test_mpjpes_supervised)
</code></pre>
</div>
<div class="cell markdown">
<h3 id="to-do"><a class="header" href="#to-do">To Do</a></h3>
<ul>
<li>a figure shows the MPJPE is decreasing with the increasing number of members.</li>
<li>a figure shows that the test error is further reduced while incorporating the pseudo-labeled data.</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">%matplotlib inline
fig = plt.figure()
plt.plot(test_mpjpes_supervised)
plt.title(&quot;Supervised&quot;)
plt.xlabel(&quot;Iteration&quot;)
plt.ylabel(&quot;MPJPE test loss&quot;)
plt.show()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">%matplotlib inline
fig = plt.figure()


for i_model, mpjpes in enumerate(zip(*train_mpjpes_iteration_supervised)):
    plt.plot(mpjpes, label=f&quot;Ensemble {i_model}&quot;)

plt.title(&quot;Supervised&quot;)
plt.xlabel(&quot;Iteration&quot;)
plt.ylabel(&quot;MPJPE train loss&quot;)
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))
plt.show()
</code></pre>
</div>
<div class="cell markdown">
<h4 id="training-loop-for-semi-supervised-learning"><a class="header" href="#training-loop-for-semi-supervised-learning">Training loop (for semi-supervised learning)</a></h4>
<p>The hypothesis of training ensemble models in a distributed way is that we coud obtain better target estimation for the unllablelled. Sepcifically, the prediction of the test sample is obtained by avergaing the prediciton from each memeber. Moreover, incoporating the samples with pseudo labels predicted by ensemble models into the training data is expcted to further improve the perfromace becasue more information is contained in the training data.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">n_models = 20
subset_size = 1000
total_size = n_models * subset_size
start_unlabelled_size = 100
# collect test data
data_test = test_tensor_rdd.collect()
x_test, y_test = zip(*data_test)
x_test, y_test = torch.stack(x_test).detach(), torch.stack(y_test).detach()

iterations = 10 # 10

models = []
# initiate models
for i in range(n_models):
    model = TemporalModel(*hyperparams)
    models.append(model.state_dict())

# Sample a small subset of the labeled data. 
# All data is loaded into driver's memory.
x_l, y_l = get_labeled_subset(labeled_tensor_rdd, total_size)

print(f&quot;Training distributed ensemble of {len(models)} models&quot;)

# train using only labeled data
#models, train_mjpes = train_ensemble(n_models,
                        #models,
                        #split_for_ensemble(x_l, y_l,n_models, total_size),
                        #hyperparams)

models, train_mjpes = train_ensemble(n_models,
                            models,
                            sample_data_for_ensemble(x_l, y_l, n_models, subset_size),
                            hyperparams)
# evaluate on test set
with torch.no_grad():
    test_preds = evaluate_avg_on_set(models, hyperparams, x_test, n_models)
    test_mpjpe = mpjpe(test_preds, y_test)
    print(f&quot;MPJPE for test set: {test_mpjpe}&quot;)


print(&quot;Labeled training iteration finished&quot;)

test_mpjpes = []
train_mpjpes_iteration = []
 
# train using labeled and unlabeled data
for i in range(iterations):
    
    # evaluate on test set
    with torch.no_grad():
        test_preds = evaluate_avg_on_set(models, hyperparams, x_test, n_models)
        test_mpjpe = mpjpe(test_preds, y_test)
        test_mpjpes.append(test_mpjpe)
        print(f&quot;MPJPE for test set: {test_mpjpe}&quot;)
    # use an adaptive total size for unlablled dataste
    full_size = (i+1)* start_unlabelled_size
    x_ul = get_unlabeled_subset(unlabeled_tensor_rdd, full_size)
    
    # predict unlabeled data
    unlabeled_preds = evaluate_avg_on_set(models,
                                          hyperparams,
                                          x_ul,
                                          n_models)
    
    # Random pick a subset of trainning data
    x_l, y_l = get_labeled_subset(labeled_tensor_rdd, total_size)
    
    # concat labeled and unlabeled data
    x_cc = torch.concat([x_l, x_ul])
    y_cc = torch.concat([y_l, unlabeled_preds])
    
    # mix labeled and unlabeled data by shuffling
    idx = torch.randperm(x_cc.shape[0])
    x_cc, y_cc = x_cc[idx], y_cc[idx]
     
    print(&quot;Running semi-supervised training iteration: {}&quot;.format(i+1))
    # train using mix of labeled and pseudolabeled data
    #models, train_mjpes = train_ensemble(n_models,
                            #models,
                            #split_for_ensemble(x_cc, y_cc, n_models, total_size),
                            #hyperparams)
                
    models, train_mjpes = train_ensemble(n_models,
                            models,
                            sample_data_for_ensemble(x_cc, y_cc, n_models, subset_size),
                            hyperparams)
    
    train_mpjpes_iteration.append(train_mjpes)
    
    saved_models_dir = Path(&quot;/dbfs/VideoPose3D/saved_models/humaneva/checkpoints/semi-supervised&quot;)
    save_models(models, saved_models_dir,i)
    
# evaluate on test set
with torch.no_grad():
    test_preds = evaluate_avg_on_set(models, hyperparams, x_test, n_models)
    test_mpjpe = mpjpe(test_preds, y_test)
    test_mpjpes.append(test_mpjpe)
    print(f&quot;MPJPE for test set: {test_mpjpe}&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">%matplotlib inline
fig = plt.figure()

plt.plot(test_mpjpes)
plt.title(&quot;Semi-supervised using pseudotargets&quot;)
plt.xlabel(&quot;Iteration&quot;)
plt.ylabel(&quot;MPJPE test loss&quot;)
plt.show()
plt.close()
print(len(test_mpjpes))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">%matplotlib inline
fig = plt.figure()

for i_model, mpjpes in enumerate(zip(*train_mpjpes_iteration)):
    plt.plot(mpjpes, label=f&quot;Ensemble {i_model}&quot;)
    
plt.title(&quot;Semi-supervised using pseudotargets&quot;)
plt.xlabel(&quot;Iteration&quot;)
plt.ylabel(&quot;MPJPE train loss&quot;)
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))
plt.show()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># evaluate on test set
test_mpjpes=[]
with torch.no_grad():
    test_preds = evaluate_avg_on_set(models, hyperparams, x_test, n_models)
    test_mpjpe = mpjpe(test_preds, y_test)
    test_mpjpes.append(test_mpjpe)
    print(&quot;MPJPE for test set:&quot;)
    print(test_mpjpes)

</code></pre>
</div>
<div class="cell markdown">
<h4 id="function-for-asserting-that-all-elements-in-list-are-equal-not-used-right-now-but-might-be-useful"><a class="header" href="#function-for-asserting-that-all-elements-in-list-are-equal-not-used-right-now-but-might-be-useful">Function for asserting that all elements in list are equal (not used right now but might be useful)</a></h4>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def all_equal(iterable):
    g = groupby(iterable)
    return next(g, True) and not next(g, False)
    
udf_all_equal = udf(all_equal, BooleanType())
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"> 
# Broadcast hyperparams of models
hyperparams_rdd = sc.broadcast(hyperparams)

model_params = []
for i in range(n_models):
    model = TemporalModel(*hyperparams)
    model_params.append(model.state_dict())
    
model_params_rdd = sc.parallelize(model_params)

def train_distributed(model_params, hyperparams):
    pass
    

def train_ensemble_distributed(model_params, data, hyperparams):
    pass

    for m in len(model_params.count()):
        data.mapParitions(lambda k: train_distributed(k, model_params, hyperparams))

  


# optional to do data partition 
def get_partitioned_rdd(input_rdd, partition_size=1000):
  
    &quot;&quot;&quot;Partition RDD

    Args:
    input_rdd: RDD to be partitioned

    Returns:
    Partitioned RDD
    &quot;&quot;&quot;
    return input_rdd.mapPartitions(lambda partition: partition_all(partition_size, partition))
</code></pre>
</div>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/01_Data.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next" href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/02_non_distributed.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/01_Data.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next" href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/02_non_distributed.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="../../../elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="../../../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->


    </body>
</html>
