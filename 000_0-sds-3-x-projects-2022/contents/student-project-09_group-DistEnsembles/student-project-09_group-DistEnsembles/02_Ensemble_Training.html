<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>02_Ensemble_Training - sds-3.x/ScaDaMaLe</title>


        <!-- Custom HTML head -->

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../../../favicon.svg">
        <link rel="shortcut icon" href="../../../favicon.png">
        <link rel="stylesheet" href="../../../css/variables.css">
        <link rel="stylesheet" href="../../../css/general.css">
        <link rel="stylesheet" href="../../../css/chrome.css">
        <link rel="stylesheet" href="../../../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../../../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../../../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../../../highlight.css">
        <link rel="stylesheet" href="../../../tomorrow-night.css">
        <link rel="stylesheet" href="../../../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../../../scroll-mdbook-outputs.css">

        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../../../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/00_Introduction.html"><strong aria-hidden="true">1.</strong> student-project-01_group-GraphOfWiki_00_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/01_DataLoading_redirectsTable.html"><strong aria-hidden="true">1.1.</strong> 01_DataLoading_redirectsTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/02_DataLoading_pagesTable.html"><strong aria-hidden="true">1.2.</strong> 02_DataLoading_pagesTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/03_DataLoading_pagelinksTable.html"><strong aria-hidden="true">1.3.</strong> 03_DataLoading_pagelinksTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/04_DataLoading_categorylinksTable.html"><strong aria-hidden="true">1.4.</strong> 04_DataLoading_categorylinksTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/05_DataLoading_categoryTable.html"><strong aria-hidden="true">1.5.</strong> 05_DataLoading_categoryTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/06_redirectRemoval.html"><strong aria-hidden="true">1.6.</strong> 06_redirectRemoval</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/07_createArticleGraph.html"><strong aria-hidden="true">1.7.</strong> 07_createArticleGraph</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/08_explorationArticleGraph.html"><strong aria-hidden="true">1.8.</strong> 08_explorationArticleGraph</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/09_fullGraphAnalysis.html"><strong aria-hidden="true">1.9.</strong> 09_fullGraphAnalysis</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/10_explorativeMotifs.html"><strong aria-hidden="true">1.10.</strong> 10_explorativeMotifs</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/11_conclusionDiscussionAndFutureWork.html"><strong aria-hidden="true">1.11.</strong> 11_conclusionDiscussionAndFutureWork</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/12_gameNotebookSetup.html"><strong aria-hidden="true">1.12.</strong> 12_gameNotebookSetup</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/13_gameNotebook.html"><strong aria-hidden="true">1.13.</strong> 13_gameNotebook</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-02_group-DDLOfVision/student-project-02_group-DDLOfVision/00_vqa_introduction.html"><strong aria-hidden="true">2.</strong> student-project-02_group-DDLOfVision_00_vqa_introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-02_group-DDLOfVision/student-project-02_group-DDLOfVision/01_vqa_model_training.html"><strong aria-hidden="true">2.1.</strong> 01_vqa_model_training</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG2/student-project-03_group-WikiKG2/00_ingest_data.html"><strong aria-hidden="true">3.</strong> student-project-03_group-WikiKG2_00_ingest_data</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG2/student-project-03_group-WikiKG2/01_fetch_descriptions.html"><strong aria-hidden="true">3.1.</strong> 01_fetch_descriptions</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG2/student-project-03_group-WikiKG2/02_load_data.html"><strong aria-hidden="true">3.2.</strong> 02_load_data</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG2/student-project-03_group-WikiKG2/03_data_exploration.html"><strong aria-hidden="true">3.3.</strong> 03_data_exploration</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG2/student-project-03_group-WikiKG2/04_analysing_relation_types.html"><strong aria-hidden="true">3.4.</strong> 04_analysing_relation_types</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG2/student-project-03_group-WikiKG2/05_motif_search.html"><strong aria-hidden="true">3.5.</strong> 05_motif_search</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG2/student-project-03_group-WikiKG2/06_produce_pagerank_vectors.html"><strong aria-hidden="true">3.6.</strong> 06_produce_pagerank_vectors</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG2/student-project-03_group-WikiKG2/07_pagerank_for_classification.html"><strong aria-hidden="true">3.7.</strong> 07_pagerank_for_classification</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-04_group-FedMLMedicalApp/student-project-04_group-FedMLMedicalApp/00_Notebook_Presentation.html"><strong aria-hidden="true">4.</strong> student-project-04_group-FedMLMedicalApp_00_Notebook_Presentation</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-04_group-FedMLMedicalApp/student-project-04_group-FedMLMedicalApp/01_BrainTumorSegmentation_Centralized_Training.html"><strong aria-hidden="true">4.1.</strong> 01_BrainTumorSegmentation_Centralized_Training</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-04_group-FedMLMedicalApp/student-project-04_group-FedMLMedicalApp/02_Federated_Learning_BrainTumorSegmentation.html"><strong aria-hidden="true">4.2.</strong> 02_Federated_Learning_BrainTumorSegmentation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-04_group-FedMLMedicalApp/student-project-04_group-FedMLMedicalApp/data_upload_test.html"><strong aria-hidden="true">4.3.</strong> data_upload_test</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/00_introduction.html"><strong aria-hidden="true">5.</strong> student-project-05_group-DistOpt_00_introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/01_Bayesian_optimization.html"><strong aria-hidden="true">5.1.</strong> 01_Bayesian_optimization</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/02_Gaussian_processes.html"><strong aria-hidden="true">5.2.</strong> 02_Gaussian_processes</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/03_acquisition_functions.html"><strong aria-hidden="true">5.3.</strong> 03_acquisition_functions</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/04_scalable_Bayesian_optimization.html"><strong aria-hidden="true">5.4.</strong> 04_scalable_Bayesian_optimization</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/05_implementation_documentation.html"><strong aria-hidden="true">5.5.</strong> 05_implementation_documentation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/06_our_implementation.html"><strong aria-hidden="true">5.6.</strong> 06_our_implementation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/07_additional_code.html"><strong aria-hidden="true">5.7.</strong> 07_additional_code</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-07_group-ExpsZerOInit/student-project-07_group-ExpsZerOInit/00_introduction_resnet.html"><strong aria-hidden="true">6.</strong> student-project-07_group-ExpsZerOInit_00_introduction_resnet</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-07_group-ExpsZerOInit/student-project-07_group-ExpsZerOInit/01_transformer.html"><strong aria-hidden="true">6.1.</strong> 01_transformer</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-07_group-ExpsZerOInit/student-project-07_group-ExpsZerOInit/02_ddpm.html"><strong aria-hidden="true">6.2.</strong> 02_ddpm</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-08_group-WikiSearch/student-project-08_group-WikiSearch/00_Introduction.html"><strong aria-hidden="true">7.</strong> student-project-08_group-WikiSearch_00_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-08_group-WikiSearch/student-project-08_group-WikiSearch/01_InputParsing.html"><strong aria-hidden="true">7.1.</strong> 01_InputParsing</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-08_group-WikiSearch/student-project-08_group-WikiSearch/02_PageRank.html"><strong aria-hidden="true">7.2.</strong> 02_PageRank</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-08_group-WikiSearch/student-project-08_group-WikiSearch/03_QuerySearch.html"><strong aria-hidden="true">7.3.</strong> 03_QuerySearch</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/00_Introduction.html"><strong aria-hidden="true">8.</strong> student-project-09_group-DistEnsembles_00_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/02_Ensemble_Training.html" class="active"><strong aria-hidden="true">8.1.</strong> 02_Ensemble_Training</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/03_Ensemble_Evaluation.html"><strong aria-hidden="true">8.2.</strong> 03_Ensemble_Evaluation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/099_extra_Ensemble.html"><strong aria-hidden="true">8.3.</strong> 099_extra_Ensemble</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/00_introduction.html"><strong aria-hidden="true">9.</strong> student-project-10_group-RDI_00_introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/01_prepare_data.html"><strong aria-hidden="true">9.1.</strong> 01_prepare_data</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/02_baseline.html"><strong aria-hidden="true">9.2.</strong> 02_baseline</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/03_single_machine.html"><strong aria-hidden="true">9.3.</strong> 03_single_machine</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/04_distributed_learning.html"><strong aria-hidden="true">9.4.</strong> 04_distributed_learning</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-11_group-CollaborativeFiltering/student-project-11_group-CollaborativeFiltering/01_Introduction.html"><strong aria-hidden="true">10.</strong> student-project-11_group-CollaborativeFiltering_01_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-11_group-CollaborativeFiltering/student-project-11_group-CollaborativeFiltering/02_AlgorithmsBeyondALS.html"><strong aria-hidden="true">10.1.</strong> 02_AlgorithmsBeyondALS</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-11_group-CollaborativeFiltering/student-project-11_group-CollaborativeFiltering/03_Implementation.html"><strong aria-hidden="true">10.2.</strong> 03_Implementation</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-12_group-FedLearnOpt/student-project-12_group-FedLearnOpt/01_Federated_Learning_Introduction.html"><strong aria-hidden="true">11.</strong> student-project-12_group-FedLearnOpt_01_Federated_Learning_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-12_group-FedLearnOpt/student-project-12_group-FedLearnOpt/02_Horovod_Introduction.html"><strong aria-hidden="true">11.1.</strong> 02_Horovod_Introduction</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-12_group-FedLearnOpt/student-project-12_group-FedLearnOpt/03_Implementations.html"><strong aria-hidden="true">11.2.</strong> 03_Implementations</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-13_group-DRL/student-project-13_group-DRL/00_DistributedRL.html"><strong aria-hidden="true">12.</strong> student-project-13_group-DRL_00_DistributedRL</a></li><li class="spacer"></li><li class="chapter-item expanded "><a href="../../../contents/student-project-14_group-EarthObs/student-project-14_group-EarthObs/00_Introduction.html"><strong aria-hidden="true">13.</strong> student-project-14_group-EarthObs_00_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-14_group-EarthObs/student-project-14_group-EarthObs/01_Download_data.html"><strong aria-hidden="true">13.1.</strong> 01_Download_data</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-14_group-EarthObs/student-project-14_group-EarthObs/02_Image_preprocessing.html"><strong aria-hidden="true">13.2.</strong> 02_Image_preprocessing</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-14_group-EarthObs/student-project-14_group-EarthObs/03_Model_Architecture_and_Training.html"><strong aria-hidden="true">13.3.</strong> 03_Model_Architecture_and_Training</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-14_group-EarthObs/student-project-14_group-EarthObs/04_Prediction_And_Visualisation.html"><strong aria-hidden="true">13.4.</strong> 04_Prediction_And_Visualisation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-14_group-EarthObs/student-project-14_group-EarthObs/05_Conclusions.html"><strong aria-hidden="true">13.5.</strong> 05_Conclusions</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-projects-BrIntSuSvConclusion/student-projects-BrIntSuSvConclusion/BrIntSuSv.html"><strong aria-hidden="true">14.</strong> student-projects-BrIntSuSvConclusion_BrIntSuSv</a></li><li class="spacer"></li><li class="chapter-item expanded "><a href="../../../editors.html"><strong aria-hidden="true">15.</strong> Editors</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">sds-3.x/ScaDaMaLe</h1>

                    <div class="right-buttons">
                        <a href="../../../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div class="cell markdown">
<h1 id="training-distributed-ensembles-for-3d-human-pose-estimation"><a class="header" href="#training-distributed-ensembles-for-3d-human-pose-estimation">Training Distributed Ensembles for 3D Human Pose Estimation</a></h1>
<p>In this notebook, we provide the necessary code of preparing the RDDs with pose data and creating and training a distributed ensemble of temporal CNNs for 3D pose estimation from the sequences of keypoints which will be described in more details further down.</p>
</div>
<div class="cell markdown">
<h2 id="imports"><a class="header" href="#imports">Imports</a></h2>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">import numpy as np
import torch

from random import sample, seed
import pyspark.sql.functions as F
from pyspark.sql.types import StructType, StringType, DoubleType, IntegerType, ArrayType
from pyspark.sql import Window
from pyspark.sql.functions import collect_list, size, udf
from pyspark.ml.feature import VectorAssembler
from pyspark.sql.types import BooleanType
from pyspark.sql.functions import udf
from itertools import groupby
from pyspark.rdd import PipelinedRDD
from torch import nn

from pathlib import Path
import os
import matplotlib.pyplot as plt

ROOTDIR = 'VideoPose3D'
</code></pre>
</div>
<div class="cell markdown">
<h2 id="data"><a class="header" href="#data">Data</a></h2>
</div>
<div class="cell markdown">
<h4 id="some-data-exploration"><a class="header" href="#some-data-exploration">Some data exploration</a></h4>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">humaneva_train_path = f'/{ROOTDIR}/humaneva/humaneva15_train.csv'
humaneva_test_path = f'/{ROOTDIR}/humaneva/humaneva15_test.csv'

def load_data_from_csv(file_location):
    &quot;&quot;&quot;Load and preprocess HumanEva data
    Args:
        file_location: file location from which to load the data
        
    Returns:
        df: spark DataFrame
    &quot;&quot;&quot;
    file_type = &quot;csv&quot;
    infer_schema = &quot;true&quot;
    first_row_is_header = False
    delimiter = &quot;,&quot;
    
    # Prepare a schema with column names+types
    schema = StructType() \
      .add(&quot;Idx&quot;,IntegerType(),True) \
      .add(&quot;Subject&quot;,StringType(),True) \
      .add(&quot;Action&quot;,StringType(),True) \
      .add(&quot;Camera&quot;,StringType(),True)
    for i in range(15):
        schema = schema.add(f&quot;u{i}&quot;,DoubleType(),True).add(f&quot;v{i}&quot;,DoubleType(),True)
    for i in range(15):
        schema = schema.add(f&quot;X{i}&quot;,DoubleType(),True).add(f&quot;Y{i}&quot;,DoubleType(),True).add(f&quot;Z{i}&quot;,DoubleType(),True)
    
    # Load the data from file
    df = spark.read.csv(file_location, header=True, schema=schema, sep=',')
    return df

df_train = load_data_from_csv(humaneva_train_path)
df_test = load_data_from_csv(humaneva_test_path)
</code></pre>
</div>
<div class="cell markdown">
<p>Let us take a closer look at <code>df_train</code>:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">display(df_train.head(5))
</code></pre>
</div>
<div class="cell markdown">
<p>Here, * (<code>Xi</code>, <code>Yi</code>, <code>Zi</code>) are the 3D coordinates of the <code>i</code>-th keypoint * (<code>ui</code>, <code>vi</code>) are the projected 2D coordinates of the corresponding keypoint * (<code>Subject</code>, <code>Action</code>, <code>Camera</code>) identify the same group of frames for which we will further apply a sliding window approach</p>
<p>Let's plot the distribution of the train data and test data stratified by the action type to ensure we have enough observations in both.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">@F.udf(StringType())
def first_word(s, delimeter=' '):
    &quot;&quot;&quot;Take the first word ouf of the sentence string.&quot;&quot;&quot;
    return s.split(delimeter)[0]

display(df_train.withColumn(&quot;ActionType&quot;, first_word(df_train['Action'])))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">display(df_test.withColumn(&quot;ActionType&quot;, first_word(df_test['Action'])))
</code></pre>
</div>
<div class="cell markdown">
<h4 id="assembling-features-and-targets"><a class="header" href="#assembling-features-and-targets">Assembling features and targets</a></h4>
<p>With VectorAssembler, we transoform the 3D keypoints (used as targets) into 45-dimensional vectors [<code>X0</code>, <code>Y0</code>, <code>Z0</code>,...,<code>X14</code>, <code>Y14</code>, <code>Z14</code>] and corresponding projected 2D keypoints (used as features) into 30-dimensional vectors [<code>u0</code>, <code>v0</code>,...,<code>u14</code>, <code>v14</code>].</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">df_train = df_train.withColumn(&quot;Group&quot;, F.concat_ws(', ', &quot;Subject&quot;, &quot;Action&quot;, &quot;Camera&quot;)).drop(&quot;Subject&quot;, &quot;Action&quot;, &quot;Camera&quot;)
df_test = df_test.withColumn(&quot;Group&quot;, F.concat_ws(', ', &quot;Subject&quot;, &quot;Action&quot;, &quot;Camera&quot;)).drop(&quot;Subject&quot;, &quot;Action&quot;, &quot;Camera&quot;)

feature_names = []
target_names = []
n_keypoints = 15 
for i in range(n_keypoints):
    # features correspond to 2D positions
    feature_names.append(&quot;u{}&quot;.format(i))
    feature_names.append(&quot;v{}&quot;.format(i))
    # targets correspond to 3D positions
    target_names.append(&quot;X{}&quot;.format(i))
    target_names.append(&quot;Y{}&quot;.format(i))
    target_names.append(&quot;Z{}&quot;.format(i))
    
# merge u, v into a vector column
feature_assembler = VectorAssembler(inputCols=feature_names, outputCol=&quot;features&quot;)
# merge X, Y, Z into a vector column
target_assembler = VectorAssembler(inputCols=target_names, outputCol=&quot;targets&quot;)

def assemble_vectors(df):
    df = feature_assembler.transform(df)
    df = target_assembler.transform(df)
    df = df.drop(*feature_names).drop(*target_names)
    return df

df_train_vectors = assemble_vectors(df_train)
df_test_vectors = assemble_vectors(df_test)
</code></pre>
</div>
<div class="cell markdown">
<h4 id="temporal-splits"><a class="header" href="#temporal-splits">Temporal splits</a></h4>
</div>
<div class="cell markdown">
<p>Temporal convolutional networks use convolutional layers to slide over the time axis in the input sequences. Dilations are often employed to model long-term temporal relations. In our project, we use a a temporal CNN to predict 3D human pose from a small (27 frames) sequence of 2D keypoints, further referred to as <strong>receptive field</strong>. An example of such a network with receptive field <strong>9</strong> is shown below.</p>
</div>
<div class="cell markdown">
<p><img src="https://dariopavllo.github.io/VideoPose3D/data/convolutions_anim.gif" alt="temporal-cnn" /></p>
</div>
<div class="cell markdown">
<h4 id="generating-receptive-fields"><a class="header" href="#generating-receptive-fields">Generating receptive fields</a></h4>
</div>
<div class="cell markdown">
<p>So, the employed temporal CNN uses the temporal information, in particular, the 3D pose prediction of the current frame depends on several previous frames and several future frames. In case of confusion, how can we use the future frames — with the 27 receptive field we get a 0.2sec lag which is not so bad, but it's also possible to shift the convolutions so that we could use only previous frames for real-time applications. Since the data is provided per frame, to reduce the computational load of data pre-processing in the worker node, we first encapsulate any sequential 27 frames into one feature sequence. Each <em>feature</em> contains the 2D positions of the 15 joints (keypoints). Each <em>feature sequence</em> therefore consists of the 2D positions of 27 frames. The target of a sequence is the 3D pose of the middle frame. This data is used for training and evaluation instead of the individual positions.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">receptive_field = 27

w = Window.orderBy(&quot;Idx&quot;).partitionBy([&quot;Group&quot;]).rowsBetween(Window.currentRow-receptive_field//2, Window.currentRow+receptive_field//2)

def create_receptive_fields(df):
    df = df.withColumn(&quot;feature_sequence&quot;, collect_list(&quot;features&quot;).over(w))
    df = df.withColumn(&quot;group_sequence&quot;, collect_list(&quot;Group&quot;).over(w))
    df = df.filter(size(df.group_sequence) == receptive_field)
    return df

df_train_receptive = create_receptive_fields(df_train_vectors).drop(&quot;features&quot;)
df_test_receptive = create_receptive_fields(df_test_vectors).drop(&quot;features&quot;)
</code></pre>
</div>
<div class="cell markdown">
<p>Visualisation of receptive field data</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">display(df_train_receptive)
</code></pre>
</div>
<div class="cell markdown">
<!---#### Split training set into labeled and unlabeled based on chunks

In the project, we are exploring semi-supervised learning with psuedolabels, which requires both labeled and unlabeled training data. However, the original HumanEva-I dataset does not provide pre-defined sets of labeled and unlabeled data. Therefore, we randomly split the data, with respect to the group, into an unlabeled and labeled set. To have a realistic semi-supervised setting, we assume that the unlabeled training data is slighly larger than the labeled training data. The targets are droped for the unlabeled training set.--->
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># seed 0 has been chosen because it gives an ok split in terms of chunk sizes
# split dataset to labeled and unlabeled dataset, which is used in the framerwork of semi-supervised learning
seed(0)
chunks = df_train_receptive.select(&quot;Group&quot;).distinct().collect()
chunks = [x[&quot;Group&quot;] for x in chunks]

num_chunks = len(chunks)
num_unlabeled = int(num_chunks*0.6)

unlabeled_chunks = sample(chunks, num_unlabeled)
labeled_chunks = [x for x in chunks if x not in unlabeled_chunks]

df_train_receptive_unlabeled = df_train_receptive.filter(df_train_receptive.Group.isin(unlabeled_chunks))
df_train_receptive_unlabeled = df_train_receptive_unlabeled.drop(&quot;targets&quot;)
df_train_receptive_labeled = df_train_receptive.filter(~df_train_receptive.Group.isin(unlabeled_chunks))
</code></pre>
</div>
<div class="cell markdown">
<h4 id="converting-dataframes-to-torch-tensors"><a class="header" href="#converting-dataframes-to-torch-tensors">Converting dataframes to torch tensors</a></h4>
<p>Here we create RDDs for training and test from the corresponding DataFrames to RDDs. Thereafter, we map the vectors to Tensor enable training using PyTorch.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def toTensorLabeled(x):
    fs = x[&quot;feature_sequence&quot;]
    target = x[&quot;targets&quot;]
    
    feature_tensor = []
    for f in fs:
        feature_tensor.append(f)
    
    xx = torch.tensor(feature_tensor,dtype=torch.float)
    yy = torch.tensor(target,dtype=torch.float)
    
    return xx.view(27, 15, 2), yy.view(1, 15, 3)

def toTensorUnlabeled(x):
    fs = x[&quot;feature_sequence&quot;]
    feature_tensor = []
    for f in fs:
        feature_tensor.append(f)
    
    xx = torch.tensor(feature_tensor, dtype=torch.float)
    
    return xx.view(27, 15, 2)   

labeled_tensor_rdd = df_train_receptive_labeled.rdd.map(toTensorLabeled)
unlabeled_tensor_rdd = df_train_receptive_unlabeled.rdd.map(toTensorUnlabeled)
test_tensor_rdd = df_test_receptive.rdd.map(toTensorLabeled)
</code></pre>
</div>
<div class="cell markdown">
<h4 id="dataset-splits"><a class="header" href="#dataset-splits">Dataset splits</a></h4>
<p>Here we provide functions for * Train/Test split * Labeled/Unlabeled split * Dataset split for each member. Note that we provide two functions. The one is <code>split_for_ensemble</code>, which guarantees that each member accesses the unique data. The other is <code>sample_data_for_ensemble</code> that randomly samples the same size of training data for each memeber meaning that there might be some reused data over different members.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def get_labeled_subset(labeled_tensor_rdd, full_size):
    # Data is loaded into driver's memory
    data = labeled_tensor_rdd.takeSample(True, full_size)
    x, y = zip(*data)
    return torch.stack(x), torch.stack(y)

def split_for_ensemble(x,y, n_models, full_size):
    '''
    Splits data so that each member acesses unique data for training
    '''
    full_size = x.shape[0]
    split_size = full_size//n_models if full_size % n_models == 0 else full_size//n_models + 1
    x = torch.split(x, split_size)
    y = torch.split(y, split_size)
    return list(zip(x, y))

def get_unlabeled_subset(unlabeled_tensor_rdd, full_size):
    # Data is loaded into driver's memory
    data = unlabeled_tensor_rdd.takeSample(True, full_size)
    return torch.stack(data)

def sample_data_for_ensemble(x,y, n_models, subset_size):
    '''
    Randomly sample a subset of training data
    '''
    x_ = []
    y_ = []
    
    subset_size = np.amin([subset_size, x.size(0)])
    
    for i in range(n_models):
        perm = torch.randperm(x.size(0))
        idx = perm[:subset_size]
        x_.append(x[idx])
        y_.append(y[idx])
    return list(zip(x_, y_))


class DataSet(torch.utils.data.Dataset):
    def __init__(self, pos2D, pos3D):
        self.pos2D = pos2D # self.pos2D: B x 27 x 15 * 2
        self.pos3D = pos3D # self.pos3D: B x 1 x 15 * 3

    def __len__(self):
        return self.pos2D.shape[0]

    def __getitem__(self, ind):
        pos2D = self.pos2D[ind] # pos2D: B x 27 x 15 * 2 -&gt; 27 x 15 * 2
        pos3D = self.pos3D[ind] # pos2D: B x 1 x 15 * 3 -&gt; 1 x 15 * 2
        return pos2D, pos3D
</code></pre>
</div>
<div class="cell markdown">
<h2 id="temporal-cnn-architecture"><a class="header" href="#temporal-cnn-architecture">Temporal CNN Architecture</a></h2>
</div>
<div class="cell markdown">
<h4 id="temporal-cnns-for-3d-pose-estimation"><a class="header" href="#temporal-cnns-for-3d-pose-estimation">Temporal CNNs for 3D pose estimation</a></h4>
<p>Here we define the 3D pose estimation model with temporal convolutions. All members of our ensembles use the same model architecture.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">class TemporalModelBase(nn.Module):
    def __init__(self, num_joints_in, in_features, num_joints_out,
                 filter_widths, dropout, channels):
        super().__init__()
        # Validate input
        for fw in filter_widths:
            assert fw % 2 != 0, 'Only odd filter widths are supported'
        self.num_joints_in = num_joints_in
        self.in_features = in_features
        self.num_joints_out = num_joints_out
        self.filter_widths = filter_widths
        self.drop = nn.Dropout(dropout)
        self.relu = nn.ReLU(inplace=True)
        self.pad = [ filter_widths[0] // 2 ]
        self.expand_bn = nn.BatchNorm1d(channels, momentum=0.1)
        self.shrink = nn.Conv1d(channels, num_joints_out*3, 1)
        

    def set_bn_momentum(self, momentum):
        self.expand_bn.momentum = momentum
        for bn in self.layers_bn:
            bn.momentum = momentum
        
    def forward(self, pos2D):
        assert len(pos2D.shape) == 4 # pos2D: B x 27 x 15 x 2
        assert pos2D.shape[-2] == self.num_joints_in # 15
        assert pos2D.shape[-1] == self.in_features   # 2     
        sz = pos2D.shape[:3] # B x 27 x 15
        pos2D = pos2D.view(pos2D.shape[0], pos2D.shape[1], -1) # B x 27 x 15 * 2
        pos2D = pos2D.permute(0, 2, 1) # B x 15 * 2 x 27
        pos3D = self._forward_blocks(pos2D)
        pos3D = pos3D.permute(0, 2, 1)
        pos3D = pos3D.view(sz[0], -1, self.num_joints_out, 3)
        return pos3D

class TemporalModel(TemporalModelBase):
    def __init__(self, num_joints_in, in_features, num_joints_out,
                 filter_widths, dropout=0.25, channels=1024):
        &quot;&quot;&quot;
        Reference 3D pose estimation model with temporal convolutions.Initialize this model.
        
        Arg:
            num_joints_in -- number of input joints (i.e. 15 for HumanEva-I)
            in_features -- number of input features for each joint (typically 2 for 2D input)
            num_joints_out -- number of output joints (can be different than input)
            filter_widths -- list of convolution widths, which also determines the # of blocks and receptive field
            dropout -- dropout probability
            channels -- number of convolution channels
        &quot;&quot;&quot;
        super().__init__(num_joints_in, in_features, num_joints_out, filter_widths, dropout, channels)
        self.expand_conv = nn.Conv1d(num_joints_in*in_features, channels, filter_widths[0], bias=False)
        layers_conv = []
        layers_bn = []
        next_dilation = filter_widths[0] # 3
        for i in range(1, len(filter_widths)):
            self.pad.append((filter_widths[i] - 1)*next_dilation // 2) # [1, 3, 9]
            layers_conv.append(nn.Conv1d(channels, channels,
                                         filter_widths[i],
                                         dilation=next_dilation,
                                         bias=False))
            layers_bn.append(nn.BatchNorm1d(channels, momentum=0.1))
            layers_conv.append(nn.Conv1d(channels, channels, 1, dilation=1, bias=False))
            layers_bn.append(nn.BatchNorm1d(channels, momentum=0.1))
            next_dilation *= filter_widths[i] # 3, 9, 27
        self.layers_conv = nn.ModuleList(layers_conv)
        self.layers_bn = nn.ModuleList(layers_bn)
        
    def _forward_blocks(self, pos2D):
        # pos2D: B x 15 * 2 x 27
        x = self.drop(self.relu(self.expand_bn(self.expand_conv(pos2D)))) # B x 1024 x 25
        print(x.shape)
        for i in range(len(self.pad) - 1):
            pad = self.pad[i+1] # 3, 9
            res = x[:, :, pad : x.shape[2] - pad] # B x 1024 x 19, B x 1024 x 1
            x = self.drop(self.relu(self.layers_bn[2*i](self.layers_conv[2*i](x)))) # B x 1024 x 19, B x 1024 x 1
            x = res + self.drop(self.relu(self.layers_bn[2*i + 1](self.layers_conv[2*i + 1](x))))
        pos3D = self.shrink(x) # B x 15*3 x 1
        return pos3D
    
    @staticmethod
    def from_state_dict(params, hyperparams):
        net = TemporalModel(*hyperparams)
        net.load_state_dict(params)
        return net
</code></pre>
</div>
<div class="cell markdown">
<p>Below we define the hyperparameters for the architecture and training</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">class Args:
    num_joints = 15
    stride = 1    # temporal length of the prediction to use during training
    epochs = 10   # number of training epochs
    batch_size = 128     # batch size in terms of predicted frames
    dropout = 0.25    # dropout probability
    learning_rate = 0.001    # initial learning rate
    lr_decay = 0.996     # learning rate decay per epoch
    data_augmentation = True # train-time flipping
    test_time_augmentation = True # test-time flipping
    architecture = '3,3,3'    # filter widths separated by comma
    channels = 1024    # number of channels in convolution layers

args = Args()
filter_widths = [int(x) for x in args.architecture.split(',')]
receptive_field = np.prod(filter_widths) # model_pos.receptive_field()
print('INFO: Receptive field: {} frames'.format(receptive_field))
pad = (receptive_field - 1) // 2 # Padding on each side
hyperparams = [args.num_joints, 2, args.num_joints, filter_widths, args.dropout, args.channels]
</code></pre>
</div>
<div class="cell markdown">
<h4 id="mpjpe-loss"><a class="header" href="#mpjpe-loss">MPJPE Loss</a></h4>
<p>The loss used for training and evaluation is the mean per-joint postion error (MPJPE), which is the mean Euclidean distance between predicted joint postions and ground-truth joint postions</p>
<h1>\[
\mathbf{MPJPE}(X^*, X)</h1>
<p>\sum_{k=1}^K \frac{|| X_k - X_k^*||_2}{K}
\]</p>
<p>where \(X_k \in \mathbb{R}^3\) is the predicted 3D location of the \(k\)-th keypoint and \(X_k^*\) is its corresponding ground-truth 3D location. <!-- in the camera coordinate system. --></p>
<!-- In order to evaluate $\mathbf{MPJPE}$ properly, two types of projection ambiguities have to be handled.
1) **absolute depth** ambiguity — normalize each pose by applying a translation that puts the skeleton root to the origin of the coordinate system.
2) **depth flip** ambiguity — evaluate $\mathbf{MPJPE}$ twice for the original and depth-flipped point cloud, retaining the better of the two. -->
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def mpjpe(predicted, target):
    &quot;&quot;&quot;
    Mean per-joint position error (i.e. mean Euclidean distance),
    often referred to as &quot;Protocol #1&quot; in many papers.
    &quot;&quot;&quot;
    assert predicted.shape == target.shape
    return torch.mean(torch.norm(predicted - target, dim=len(target.shape)-1))
</code></pre>
</div>
<div class="cell markdown">
<h2 id="functionality-for-ensemble-training"><a class="header" href="#functionality-for-ensemble-training">Functionality for Ensemble Training</a></h2>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">__backend_agg_display_orig = display
__backend_agg_dfs = []
def __backend_agg_display_new(df):
    __backend_agg_df_modules = [&quot;pandas.core.frame&quot;, &quot;databricks.koalas.frame&quot;, &quot;pyspark.sql.dataframe&quot;, &quot;pyspark.pandas.frame&quot;]
    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):
        __backend_agg_dfs.append(df)

display = __backend_agg_display_new

def __backend_agg_user_code_fn():
    import base64
    exec(base64.standard_b64decode(&quot;QEYudWRmKFN0cmluZ1R5cGUoKSkKZGVmIGZpcnN0X3dvcmQocywgZGVsaW1ldGVyPScgJyk6CiAgICAiIiJUYWtlIHRoZSBmaXJzdCB3b3JkIG91ZiBvZiB0aGUgc2VudGVuY2Ugc3RyaW5nLiIiIgogICAgcmV0dXJuIHMuc3BsaXQoZGVsaW1ldGVyKVswXQoKZGlzcGxheShkZl90cmFpbi53aXRoQ29sdW1uKCJBY3Rpb25UeXBlIiwgZmlyc3Rfd29yZChkZl90cmFpblsnQWN0aW9uJ10pKSk=&quot;).decode())

try:
    # run user code
    __backend_agg_user_code_fn()

    #reset display function
    display = __backend_agg_display_orig

    if len(__backend_agg_dfs) &gt; 0:
        # create a temp view
        if hasattr(__backend_agg_dfs[0], &quot;to_spark&quot;):
            # koalas dataframe
            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(&quot;DatabricksViewf358a0c&quot;)
        elif type(__backend_agg_dfs[0]).__module__ == &quot;pandas.core.frame&quot; or isinstance(__backend_agg_dfs[0], list):
            # pandas dataframe
            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(&quot;DatabricksViewf358a0c&quot;)
        else:
            __backend_agg_dfs[0].createOrReplaceTempView(&quot;DatabricksViewf358a0c&quot;)
        #run backend agg
        display(spark.sql(&quot;&quot;&quot;WITH q AS (select * from DatabricksViewf358a0c) SELECT `ActionType`,COUNT(`Idx`) `column_c7d5d6923` FROM q GROUP BY `ActionType`&quot;&quot;&quot;))
    else:
        displayHTML(&quot;dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.&quot;)


finally:
    spark.sql(&quot;drop view if exists DatabricksViewf358a0c&quot;)
    display = __backend_agg_display_orig
    del __backend_agg_display_new
    del __backend_agg_display_orig
    del __backend_agg_dfs
    del __backend_agg_user_code_fn

</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">__backend_agg_display_orig = display
__backend_agg_dfs = []
def __backend_agg_display_new(df):
    __backend_agg_df_modules = [&quot;pandas.core.frame&quot;, &quot;databricks.koalas.frame&quot;, &quot;pyspark.sql.dataframe&quot;, &quot;pyspark.pandas.frame&quot;]
    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):
        __backend_agg_dfs.append(df)

display = __backend_agg_display_new

def __backend_agg_user_code_fn():
    import base64
    exec(base64.standard_b64decode(&quot;ZGlzcGxheShkZl90ZXN0LndpdGhDb2x1bW4oIkFjdGlvblR5cGUiLCBmaXJzdF93b3JkKGRmX3Rlc3RbJ0FjdGlvbiddKSkp&quot;).decode())

try:
    # run user code
    __backend_agg_user_code_fn()

    #reset display function
    display = __backend_agg_display_orig

    if len(__backend_agg_dfs) &gt; 0:
        # create a temp view
        if hasattr(__backend_agg_dfs[0], &quot;to_spark&quot;):
            # koalas dataframe
            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(&quot;DatabricksViewb1d64fd&quot;)
        elif type(__backend_agg_dfs[0]).__module__ == &quot;pandas.core.frame&quot; or isinstance(__backend_agg_dfs[0], list):
            # pandas dataframe
            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(&quot;DatabricksViewb1d64fd&quot;)
        else:
            __backend_agg_dfs[0].createOrReplaceTempView(&quot;DatabricksViewb1d64fd&quot;)
        #run backend agg
        display(spark.sql(&quot;&quot;&quot;WITH q AS (select * from DatabricksViewb1d64fd) SELECT `ActionType`,COUNT(`Idx`) `column_542de4915` FROM q GROUP BY `ActionType`&quot;&quot;&quot;))
    else:
        displayHTML(&quot;dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.&quot;)


finally:
    spark.sql(&quot;drop view if exists DatabricksViewb1d64fd&quot;)
    display = __backend_agg_display_orig
    del __backend_agg_display_new
    del __backend_agg_display_orig
    del __backend_agg_dfs
    del __backend_agg_user_code_fn

</code></pre>
</div>
<div class="cell markdown">
<h4 id="per-model-training-predictions-pipelines"><a class="header" href="#per-model-training-predictions-pipelines">Per-model training-predictions pipelines</a></h4>
<p>The train and prediction models for each member are defined below. Note that Spark enables distribution of these functions on the work node automatically.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def train(params, hyperparams, pos2D, pos3D, args):
    &quot;&quot;&quot;
    A training pipeline that every model in an ensemble performs
    Args:
        params -- model state dict (initial parameters)
        hyperparams -- hyperparameters corresponding to the model architecture
        pos2D -- inputs -- 2D receptive fields
        pos3D -- targets -- 3D poses
        args -- training parameters
    Returns:
        trained parameters in a model state dict
        training loss value
    &quot;&quot;&quot;
    model = TemporalModel.from_state_dict(params, hyperparams)
    model.train()
    
    lr = args.learning_rate
    lr_decay = args.lr_decay
    train_data = DataSet(pos2D, pos3D)
    dataloader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=True)
    opt = torch.optim.Adam(model.parameters(), lr=lr, amsgrad=True)
    initial_momentum = 0.1
    final_momentum = 0.001
    
    losses_3d_train = []
    for epoch in range(args.epochs):
        epoch_loss_3d_train = 0
        N = 0
        for batch in dataloader:
            inputs_2d, inputs_3d = batch
            if torch.cuda.is_available():
                inputs_3d = inputs_3d.cuda()
                inputs_2d = inputs_2d.cuda()
                model = model.cuda()
            inputs_3d[:, :, 0] = 0
            # Predict 3D poses
            predicted_3d_pos = model(inputs_2d)
            # Calcuclate MPJPE loss
            loss_3d_pos = mpjpe(predicted_3d_pos, inputs_3d)
            epoch_loss_3d_train += inputs_3d.shape[0]*inputs_3d.shape[1] * loss_3d_pos.item()
            N += inputs_3d.shape[0]*inputs_3d.shape[1]
            loss_total = loss_3d_pos
            opt.zero_grad()
            loss_total.backward()
            # Make one optimization step on batch
            opt.step()
        losses_3d_train.append(epoch_loss_3d_train / N)
        print('[%d] lr %f 3d_train %f' % (
                epoch + 1,
                lr,
                losses_3d_train[-1] * 1000))
        # Decay learning rate exponentially
        lr *= lr_decay
        for param_group in opt.param_groups:
            param_group['lr'] *= lr_decay
    err = mpjpe(model(pos2D.cuda()), pos3D.cuda())
    lossval = float(err.detach().cpu().numpy())
    return model.state_dict(), lossval


def predict(params, hyperparams, pos2D):
    &quot;&quot;&quot;
    Inference pipeline that every model in an ensemble performs
    Args:
        params -- model state dict (initial parameters)
        hyperparams -- hyperparameters corresponding to the model architecture
        pos2D -- inputs -- 2D receptive fields
    &quot;&quot;&quot;
    model = TemporalModel.from_state_dict(params, hyperparams)
    model.eval()
    if torch.cuda.is_available():
        pos2D = pos2D.cuda()
        model.cuda() 
    return model(pos2D).detach().cpu()
</code></pre>
</div>
<div class="cell markdown">
<h4 id="parallel-training"><a class="header" href="#parallel-training">Parallel training</a></h4>
<p>Below we define <code>train_ensemble</code> which enables the training of ensemble models in parallel. Note that the training is performed in work nodes.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def train_ensemble(n_models, model_params, data, hyperparams):
    &quot;&quot;&quot;
    Args:
        n_models -- number of ensemble members
        model_params -- list of learnable parameters for each member
        data --  a list of training dataset for each member
        hyperparams -- hyperparameters corresponding to each member's model architecture (same for all)
    &quot;&quot;&quot;
    model_data = []
    args = Args()
    assert len(model_params) == n_models
    assert len(data) == n_models, f&quot;Lenght mismatch, lenght of data is {len(data)}, while number of models are {n_models}&quot;
    
    for i, (x, y) in enumerate(data):
        # Tuples of model parameters, hyperparamers, training data, and arguments for each member.
        model_data.append((model_params[i], hyperparams, x, y, args))
    
    # create an RDD with model data
    model_data_rdd = sc.parallelize(model_data)
    # train each memeber using their own data
    models_trained = model_data_rdd.map(lambda t: train(*t))
    # send trained models state dicts and loss values to the driver node
    models_trained = models_trained.collect()
    
    # x[0] -&gt; trained model paramteres
    # x[1] -&gt; trainig loss value
    print(f&quot;Training losses: {[x[1] for x in models_trained]}&quot;)
    
    return [x[0] for x in models_trained], [x[1] for x in models_trained]  
</code></pre>
</div>
<div class="cell markdown">
<h4 id="ensemble-based-predictions"><a class="header" href="#ensemble-based-predictions">Ensemble-based predictions</a></h4>
<p>Predictions are done on worker nodes.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def ensemble_predictions(models, hyperparams, test_x):
    pred_iter = _pred_models_iter(models, hyperparams, test_x)
    return pred_iter.map(lambda t: predict(*t))

def ensemble_predictions_reduced(models, hyperparams, test_x, reduce_fn):
    return ensemble_predictions(models, hyperparams, test_x).reduce(reduce_fn)

def _pred_models_iter(models, hyperparams, test_x):
    if isinstance(models, PipelinedRDD):
        return models.map(lambda model: (model, test_x))
    elif isinstance(models, list): # our case
        models_and_data = [(params, hyperparams, test_x) for params in models]
        return sc.parallelize(models_and_data)
    else:
        raise TypeError(&quot;'models' must be an RDD or a list&quot;)
        

def evaluate_avg_on_set(models, hyperparams, dataset, n_models):
    predictions_sum = ensemble_predictions_reduced(models, hyperparams, dataset, lambda x, y: x + y) # Tensor output
    predictions_avg = predictions_sum/n_models
    
    return predictions_avg  
</code></pre>
</div>
<div class="cell markdown">
<h4 id="saving-the-models"><a class="header" href="#saving-the-models">Saving the models</a></h4>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def save_models(models_state_dict, save_models_dir: Path, iter: int, n_member : int) -&gt; None:
    &quot;&quot;&quot;
    Save models after training of iteration
    
    Args:
        models_state_dict: list of state dicts of pytorch nn.Module models to be saved
        save_models_dir: Path to dir where models are being saved
        iter: iteration
        n_member: number of members in the current ensemble model
    &quot;&quot;&quot;
    # Create saving path if it does not exist
    save_models_dir.mkdir(parents=True, exist_ok=True)
    
    for i_model, model_state_dict in enumerate(models_state_dict):
        torch.save(model_state_dict, os.path.join(save_models_dir, f&quot;{n_member}_members_ensemble{i_model}_iter{iter}.ckpt&quot;))
    print(f'Saved iter. {iter} to {save_models_dir}')
        
def save_models_with_results(models_state_dict, train_mpjpes, test_mpjpes, save_models_dir, iter, n_member):
    save_models_dir.mkdir(parents=True, exist_ok=True)
    for i_model, (model_state_dict, train_mpjpe, test_mpjpe) in \
            enumerate(zip(models_state_dict, train_mpjpes, test_mpjpes)):
        data = {
            'model_state_dict': model_state_dict,
            'train_mpjpe': train_mpjpe,
            'test_mpjpe': test_mpjpe,
        }
        torch.save(data, os.path.join(save_models_dir, f&quot;{n_member}_members_ensemble{i_model}_iter{iter}.ckpt&quot;))
    print(f'Saved iter. {iter} to {save_models_dir}')
</code></pre>
</div>
<div class="cell markdown">
<h2 id="actual-distributed-ensemble-training"><a class="header" href="#actual-distributed-ensemble-training">Actual Distributed Ensemble Training</a></h2>
</div>
<div class="cell markdown">
<h4 id="supervised-training"><a class="header" href="#supervised-training">Supervised training</a></h4>
<p>Below we train an ensemble of models, where each model is trained in a distrbuted way. Specifically, each member is trained in a different work node in parallel. The hypothesis is that the prediction should be more accurate than the single model. Moreover, each member is limited to access a subset of the trainining data stored in the driver node. It is a natural idea to send the same fraction of training data to the work node. However, to avoid the scenario that the work node might not have enough space to store the subset of traning data, we set the threshold for the maximum size of the data to be stored in the work node. Pratically, the size of the subset of training data is fixed to be N=1000. We tried up to 11 models in an ensemble.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">saved_models_dir = Path(f&quot;/dbfs/{ROOTDIR}/saved_models/humaneva/checkpoints/supervised&quot;)
n_models_set = [1, 2, 3, 5, 10]
# collect test data
data_test = test_tensor_rdd.collect()
x_test, y_test = zip(*data_test)
x_test, y_test = torch.stack(x_test).detach(), torch.stack(y_test).detach()
subset_size = 1000 # subset of training data allocated to each work node.
n_iterations = 100

for n_models in n_models_set:
    test_mpjpes_supervised = []
    train_mpjpes_iteration_supervised = []
    total_size = n_models * subset_size

    for n_models in n_models_set:
        models_supervised = []
        # initiate models
        for i in range(n_models):
            model = TemporalModel(*hyperparams)
            models_supervised.append(model.state_dict())

        # train using only labeled data
        for iteration in range(n_iterations):
            x_l, y_l = get_labeled_subset(labeled_tensor_rdd, total_size)


            models_supervised, train_mjpes_supervised = train_ensemble(n_models, models_supervised, split_for_ensemble(x_l, y_l,n_models, total_size), hyperparams)

            train_mpjpes_iteration_supervised.append(train_mjpes_supervised)
            save_models(models_supervised, saved_models_dir, iteration, n_models)

            # Ealuate on test set
            with torch.no_grad():
                test_preds_supervised = evaluate_avg_on_set(models_supervised, hyperparams, x_test, n_models)
                test_mpjpe_supervised = mpjpe(test_preds_supervised, y_test)
                test_mpjpes_supervised.append(test_mpjpe_supervised)
                print(&quot;MPJPE for test set (supervised baseline):&quot;)
                print(test_mpjpes_supervised)

# evaluate on test set
test_mpjpes = []
with torch.no_grad():
    test_preds = evaluate_avg_on_set(models_supervised, hyperparams, x_test, n_models)
    test_mpjpe = mpjpe(test_preds, y_test)
    test_mpjpes.append(test_mpjpe)
    print(&quot;MPJPE for test set:&quot;)
    print(test_mpjpes)
</code></pre>
</div>
<div class="cell markdown">
<hr />
</div>
<div class="cell markdown">
<h4 id="semi-supervised-training"><a class="header" href="#semi-supervised-training">Semi-supervised training</a></h4>
<p>Due to the time and computational limitation, only two ensemble models including the size of memeber is 3 and 5 are conducted.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">saved_semisupervised_models_dir = Path(f&quot;/dbfs/{ROOTDIR}/saved_models/humaneva/checkpoints/semi_supervised&quot;)
saved_pre_trained_supervised_models_dir = Path(f&quot;/dbfs/{ROOTDIR}/saved_models/humaneva/checkpoints/semi_supervised/pretrained&quot;)
subset_size = 500 # it seems that we have some memory issues. Decrease the subset size helps.
start_unlabelled_size = 10
iterations = 100

# collect test data
data_test = test_tensor_rdd.collect()
x_test, y_test = zip(*data_test)
x_test, y_test = torch.stack(x_test).detach(), torch.stack(y_test).detach()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">n_models_set =[3, 5]
for n_models in n_models_set:
    total_size = n_models * subset_size
     
    ################## Initialize models ##################
    models_semisupervised = []
    for model_idx in range(n_models):
        model = TemporalModel(*hyperparams)
        models_semisupervised.append(model.state_dict())
    
     
    print(f&quot;Supervised pre-training of {len(models_semisupervised)} models started&quot;)
    for iter in range(100):
        x_l, y_l = get_labeled_subset(labeled_tensor_rdd, total_size)
        models_semisupervised, train_mjpes = train_ensemble(n_models,
                                   models_semisupervised,
                                    sample_data_for_ensemble(x_l, y_l, n_models, subset_size),
                                    hyperparams)
        save_models(models_semisupervised,saved_pre_trained_supervised_models_dir, iter,n_models)
                                     
                                     

    ################## Semi-supervised training ##################
    print(f&quot;Semi-supervised training of {len(models_semisupervised)} models started&quot;)
    train_mpjpes_iteration = []
    test_mpjpes_iteration = []
    # train using labeled and unlabeled data
    for iter in range(iterations):
        # use an adaptive total size for unlabelled dataste
        full_size = (iter+1) * start_unlabelled_size
        x_ul = get_unlabeled_subset(unlabeled_tensor_rdd, full_size)
        # predict unlabeled data
        unlabeled_preds = evaluate_avg_on_set(models_semisupervised,
                                              hyperparams,
                                              x_ul,
                                              n_models)

        # Random pick a subset of trainning data
        x_l, y_l = get_labeled_subset(labeled_tensor_rdd, total_size)

        # concat labeled and unlabeled data
        x_cc = torch.concat([x_l, x_ul])
        y_cc = torch.concat([y_l, unlabeled_preds])

        # mix labeled and unlabeled data by shuffling
        idx = torch.randperm(x_cc.shape[0])
        x_cc, y_cc = x_cc[idx], y_cc[idx]

        # train using mix of labeled and pseudolabeled data
        models_semisupervised, train_mjpes = train_ensemble(n_models,
                                models_semisupervised,
                                split_for_ensemble(x_cc, y_cc, n_models, total_size),
                                hyperparams)

        train_mpjpes_iteration.append(train_mjpes)  
        # evaluate on test set
        with torch.no_grad():
            test_preds = evaluate_avg_on_set(models_semisupervised, hyperparams, x_test, n_models)
            test_mpjpes = mpjpe(test_preds, y_test)
            test_mpjpes_iteration.append(test_mpjpes)
            print(f'Iteration: {iter+1}\ttrain MPJPE: {train_mjpes}\ttest MPJPE: {test_mpjpes}')
            save_models(models_semisupervised,
                                    saved_semisupervised_models_dir,
                                    iter,
                                    n_models)
</code></pre>
</div>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/00_Introduction.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next" href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/03_Ensemble_Evaluation.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/00_Introduction.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next" href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/03_Ensemble_Evaluation.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="../../../elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="../../../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->


    </body>
</html>
