<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>00_vqa_introduction - sds-3.x/ScaDaMaLe</title>


        <!-- Custom HTML head -->

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../../../favicon.svg">
        <link rel="shortcut icon" href="../../../favicon.png">
        <link rel="stylesheet" href="../../../css/variables.css">
        <link rel="stylesheet" href="../../../css/general.css">
        <link rel="stylesheet" href="../../../css/chrome.css">
        <link rel="stylesheet" href="../../../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../../../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../../../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../../../highlight.css">
        <link rel="stylesheet" href="../../../tomorrow-night.css">
        <link rel="stylesheet" href="../../../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../../../scroll-mdbook-outputs.css">

        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../../../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="../../../contents/student-project-02_group-DDLOfVision/student-project-02_group-DDLOfVision/00_vqa_introduction.html" class="active"><strong aria-hidden="true">1.</strong> 00_vqa_introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-02_group-DDLOfVision/student-project-02_group-DDLOfVision/01_vqa_model_training.html"><strong aria-hidden="true">1.1.</strong> 01_vqa_model_training</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-02_group-DDLOfVision/student-project-02_group-DDLOfVision/02_vqa_model_inference.html"><strong aria-hidden="true">1.2.</strong> 02_vqa_model_inference</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-02_group-DDLOfVision/student-project-02_group-DDLOfVision/0y_test_mnist-pytorch.html"><strong aria-hidden="true">1.3.</strong> 0y_test_mnist-pytorch</a></li></ol></li><li class="chapter-item expanded "><li class="part-title">student-project-03_group-WikiKG90mv2</li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/00_ingest_data.html"><strong aria-hidden="true">2.</strong> 00_ingest_data</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/02_load_data.html"><strong aria-hidden="true">2.1.</strong> 02_load_data</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/03_data_exploration.html"><strong aria-hidden="true">2.2.</strong> 03_data_exploration</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/04Motif_search_defination_code.html"><strong aria-hidden="true">2.3.</strong> 04Motif_search_defination_code</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/08_pagerank.html"><strong aria-hidden="true">2.4.</strong> 08_pagerank</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/10_motif_mining_WikiKGv2.html"><strong aria-hidden="true">2.5.</strong> 10_motif_mining_WikiKGv2</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/fetch_descriptions.html"><strong aria-hidden="true">2.6.</strong> fetch_descriptions</a></li></ol></li><li class="chapter-item expanded "><li class="part-title">student-project-04_group-FedMLMedicalApp</li><li class="chapter-item expanded "><a href="../../../contents/student-project-04_group-FedMLMedicalApp/student-project-04_group-FedMLMedicalApp/00_Notebook_Presentation.html"><strong aria-hidden="true">3.</strong> 00_Notebook_Presentation</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-04_group-FedMLMedicalApp/student-project-04_group-FedMLMedicalApp/01_BrainTumorSegmentation_Centralized_Training.html"><strong aria-hidden="true">3.1.</strong> 01_BrainTumorSegmentation_Centralized_Training</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-04_group-FedMLMedicalApp/student-project-04_group-FedMLMedicalApp/02_Federated_Learning_BrainTumorSegmentation.html"><strong aria-hidden="true">3.2.</strong> 02_Federated_Learning_BrainTumorSegmentation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-04_group-FedMLMedicalApp/student-project-04_group-FedMLMedicalApp/data_upload_test.html"><strong aria-hidden="true">3.3.</strong> data_upload_test</a></li></ol></li><li class="chapter-item expanded "><li class="part-title">student-project-05_group-DistOpt</li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/00_introduction.html"><strong aria-hidden="true">4.</strong> 00_introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/01_Bayesian_optimization.html"><strong aria-hidden="true">4.1.</strong> 01_Bayesian_optimization</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/02_Gaussian_processes.html"><strong aria-hidden="true">4.2.</strong> 02_Gaussian_processes</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/03_acquisition_functions.html"><strong aria-hidden="true">4.3.</strong> 03_acquisition_functions</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/04_TuRBO.html"><strong aria-hidden="true">4.4.</strong> 04_TuRBO</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/05_deep_kernel_learning.html"><strong aria-hidden="true">4.5.</strong> 05_deep_kernel_learning</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/06_our_implementation.html"><strong aria-hidden="true">4.6.</strong> 06_our_implementation</a></li></ol></li><li class="chapter-item expanded "><a href="../../../editors.html"><strong aria-hidden="true">5.</strong> Editors</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">sds-3.x/ScaDaMaLe</h1>

                    <div class="right-buttons">
                        <a href="../../../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div class="cell markdown">
<h1 id="visual-question-answering-using-transformers"><a class="header" href="#visual-question-answering-using-transformers">Visual Question Answering using Transformers</a></h1>
</div>
<div class="cell markdown">
<h2 id="task-description"><a class="header" href="#task-description">Task Description</a></h2>
<p>Visual Question Answering (VQA) is the task of understanding a given image and answering questions in natural language based on the image. This is a challenging task as it requires reasoning about two different data modalities (text and image) in conjunction. An additional challenge is to generate an answer to the question in natural language. Such a system enables multimodal interaction with humans and one useful application is in assistive technologies for visually challenged individuals. A general framework to solve this task involves the following steps:</p>
<ul>
<li>Image feature extraction</li>
<li>Question feature extraction</li>
<li>Relating and combining image and question features</li>
<li>Answer generation</li>
</ul>
<p>However, for this project, we simplify this problem by only considering yes/no type questions. This removes the need to train an answer generation model and the VQA task can be simply posed as a binary classification problem as follows:</p>
<ul>
<li>Image feature extraction</li>
<li>Question feature extraction</li>
<li>Classifier to predict yes/no answer</li>
</ul>
<p>The classifier performs the task of relating the image and the question to predict the most appropriate answer. A mathematical formulation of the task is:</p>
<blockquote>
<p>Given an image \(x\), a question \(q\) and answer \(a \in {0, 1}\), the task is to learn a model to predict the correct answer choice \(a = f(x, q ; \theta)\), with model parameters \(\theta\).</p>
</blockquote>
</div>
<div class="cell markdown">
<h2 id="dataset"><a class="header" href="#dataset">Dataset</a></h2>
<p>For this task, we use the VQA (Visual Question Answering) v1.0 dataset (https://visualqa.org/) [1]. This dataset was first introduced at the VQA Challenge at CVPR 2016 and it is used as a standard benchmark dataset for the VQA task. This dataset uses selected images from the COCO dataset [6] and each image can have multiple related questions. We pick the subset of the dataset that contains yes/no type questions. Then, we obtain a dataset that consists of 63317 training images and 30612 validation images. In total, there are 95302 questions in the training set and 45478 questions in the validation set. Below, we visualize a few examples from the training dataset.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-cd">/dbfs/ml/VQA
#Uncomment and run these commands to download and unzip the dataset
#!wget -nc https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/Questions_Train_mscoco.zip
#!wget -nc https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/Annotations_Train_mscoco.zip
#!wget -nc http://images.cocodataset.org/zips/train2014.zip
#!wget -nc https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/Questions_Val_mscoco.zip
#!wget -nc https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/Annotations_Val_mscoco.zip
#!wget -nc http://images.cocodataset.org/zips/val2014.zip
#!unzip -qq '*.zip'
#!ls train2014 | wc -l
#!ls val2014 | wc -l
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">from collections import namedtuple
import json
import matplotlib.pyplot as plt
import os
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">VQAVisualizationExample = namedtuple('VQAVisualizationExample', [
    'question_txt', 
    'answer_txt', 
    'img'
])

class VQADataset(Dataset):
    def __init__(self, data_dir=&quot;/dbfs/ml/VQA/&quot;, data_split=&quot;train&quot;):
        self.data_split = data_split
        self.data_dir = data_dir
        
        # Get ready the text
        if self.data_split==&quot;train&quot;:
            self.questions = json.load(open(os.path.join(self.data_dir, 'MultipleChoice_mscoco_train2014_questions.json')))['questions']
            self.answers = json.load(open(os.path.join(self.data_dir, 'mscoco_train2014_annotations.json')))['annotations']
        else:
            self.questions = json.load(open(os.path.join(self.data_dir, 'MultipleChoice_mscoco_val2014_questions.json')))['questions']
            self.answers = json.load(open(os.path.join(self.data_dir, 'mscoco_val2014_annotations.json')))['annotations']
        self.yesno_indices = [i for i, a in enumerate(self.answers) if a[&quot;answer_type&quot;] == &quot;yes/no&quot;]
        self.questions = [self.questions[i] for i in self.yesno_indices]
        self.answers = [self.answers[i] for i in self.yesno_indices]

    def __len__(self):
        return len(self.questions)
    
    def __getitem__(self, idx):
        question = self.questions[idx]
        answer = self.answers[idx]
        
        assert question['question_id'] == answer['question_id']
        
        question_txt = question['question']
        answer_txt = answer['multiple_choice_answer']
        img_id = question['image_id']
        img = Image.open(os.path.join(self.data_dir, f'{self.data_split}2014/COCO_{self.data_split}2014_{img_id:012}.jpg'))
        
        return VQAVisualizationExample(question_txt, answer_txt, img)
        
        
vqa_ds = VQADataset(data_split=&quot;train&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># Visualize few examples from the dataset
n_examples = 10
example_ids = [0, 8, 100, 200, 300, 400, 500, 600, 800, 1000]

fig, ax = plt.subplots(n_examples//2, 2, figsize=(30, 40))

for k in range(n_examples):
    j = 0 if k%2==0 else 1
    i = k // 2
    example = vqa_ds[example_ids[k]]

    question_txt = example.question_txt
    correct_answer = example.answer_txt

    ax[i][j].imshow(example.img)
    _ = ax[i][j].set_xticks([])
    _ = ax[i][j].set_yticks([])
    _ = ax[i][j].set_title(f&quot;Question: {question_txt}, \nCorrect answer: {correct_answer}&quot;, fontsize=14)
</code></pre>
</div>
<div class="cell markdown">
<h2 id="solution-idea"><a class="header" href="#solution-idea">Solution Idea</a></h2>
<p>We follow the following steps to solve this task:</p>
<ol>
<li>Image feature extraction</li>
<li>Question feature extraction</li>
<li>Classifier to pick the correct answer</li>
</ol>
<p>Given an image \(x\), a question \(q\) and answer \(a \in {0, 1}\), the VQA model can be formulated as a classifier \(p = h(g_v(x), g_l(q))\), where \(p\) is the probability of the answer being &quot;yes&quot;, \(p = P(a=1|x, q)\). Here, \(g_l(\cdot)\) is the text feature extractor and \(g_v\) is the visual feature extractor. The answer \(a\) is predicted as \(a = \mathcal{1}_{p&gt;0.5}\).</p>
<p>Inspired by the recent advances in the usage of Transformers in both vision and language representation learning, we use Transformer architectures to extract image and text features. Particularly, self-supervised pretraining on large unlabeled datasets have been shown to transfer well to new tasks. Sometimes, these self-supervised representations even surpass fully supervised training on the specific task, especially when limited labeled data is available. Hence, we choose to use publicly available self-supervised and pre-trained Transformer models for the feature extractors. For the image feature extractor, we use the Small Vision Transformer (ViT-Small/16) [5] pre-trained using DINO self-supervised learning method [2]. For the text feature extractor that is used to extract features for the questions, we use the ALBERT model [3], which is a computationally efficient version of BERT [4].</p>
<p>The Transformer feature extractors output a set of feature vectors for each pair of question and image. The feature extractors themselves are kept frozen and are not trained. We add a small trainable interaction module and allows the image and text features to interact and extract a combined set of features that is useful for answering the question. These features are processed using a 2-layer MLP to get the final classification prediction. The flowchart of our method is shown in the figure below.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">fig, ax = plt.subplots(1, 1, figsize=(10, 10))
_ = ax.imshow(Image.open(&quot;/dbfs/ml/VQA/vqa.png&quot;))
_ = ax.set_xticks([])
_ = ax.set_yticks([])
_ = ax.axis(&quot;off&quot;)
</code></pre>
</div>
<div class="cell markdown">
<h2 id="results"><a class="header" href="#results">Results</a></h2>
<p>We implemented the training of our model using the distributed data parallel method with Pytorch and Horovod. This can leverage multiple GPUs to perform scalable training of deep learning models. On the yes/no VQA task, we achieve an accuracy of x % on the validation dataset. Considering that we use a simple setup with few trainable parameters, the achieved performance looks reasonable. Deeper and more complicated interaction between the image and text features can be beneficial to improve the results.</p>
</div>
<div class="cell markdown">
<h2 id="references"><a class="header" href="#references">References</a></h2>
<p>[1] Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., &amp; Parikh, D. (2017). Making the V in VQA matter: Elevating the role of image understanding in visual question answering. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> (pp. 6904-6913).</p>
<p>[2] Caron, M., Touvron, H., Misra, I., JÃ©gou, H., Mairal, J., Bojanowski, P., &amp; Joulin, A. (2021). Emerging properties in self-supervised vision transformers. In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em> (pp. 9650-9660).</p>
<p>[3] Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., &amp; Soricut, R. (2019, September). ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. In <em>International Conference on Learning Representations</em>.</p>
<p>[4] Kenton, J. D. M. W. C., &amp; Toutanova, L. K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In <em>Proceedings of NAACL-HLT</em> (pp. 4171-4186).</p>
<p>[5] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... &amp; Houlsby, N. (2020, September). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In <em>International Conference on Learning Representations</em>.</p>
<p>[6] Lin, T. Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., ... &amp; Zitnick, C. L. (2014, September). Microsoft coco: Common objects in context. In European conference on computer vision (pp. 740-755). Springer, Cham.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->

                            <a rel="next" href="../../../contents/student-project-02_group-DDLOfVision/student-project-02_group-DDLOfVision/01_vqa_model_training.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

                    <a rel="next" href="../../../contents/student-project-02_group-DDLOfVision/student-project-02_group-DDLOfVision/01_vqa_model_training.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="../../../elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="../../../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->


    </body>
</html>
