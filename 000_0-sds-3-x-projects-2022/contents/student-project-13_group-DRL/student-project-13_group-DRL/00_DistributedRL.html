<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>student-project-13_group-DRL_00_DistributedRL - sds-3.x/ScaDaMaLe</title>


        <!-- Custom HTML head -->

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../../../favicon.svg">
        <link rel="shortcut icon" href="../../../favicon.png">
        <link rel="stylesheet" href="../../../css/variables.css">
        <link rel="stylesheet" href="../../../css/general.css">
        <link rel="stylesheet" href="../../../css/chrome.css">
        <link rel="stylesheet" href="../../../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../../../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../../../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../../../highlight.css">
        <link rel="stylesheet" href="../../../tomorrow-night.css">
        <link rel="stylesheet" href="../../../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../../../scroll-mdbook-outputs.css">

        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../../../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/00_Introduction.html"><strong aria-hidden="true">1.</strong> student-project-01_group-GraphOfWiki_00_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/01_DataLoading_redirectsTable.html"><strong aria-hidden="true">1.1.</strong> 01_DataLoading_redirectsTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/02_DataLoading_pagesTable.html"><strong aria-hidden="true">1.2.</strong> 02_DataLoading_pagesTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/03_DataLoading_pagelinksTable.html"><strong aria-hidden="true">1.3.</strong> 03_DataLoading_pagelinksTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/04_DataLoading_categorylinksTable.html"><strong aria-hidden="true">1.4.</strong> 04_DataLoading_categorylinksTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/05_DataLoading_categoryTable.html"><strong aria-hidden="true">1.5.</strong> 05_DataLoading_categoryTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/06_redirectRemoval.html"><strong aria-hidden="true">1.6.</strong> 06_redirectRemoval</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/07_createArticleGraph.html"><strong aria-hidden="true">1.7.</strong> 07_createArticleGraph</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/08_explorationArticleGraph.html"><strong aria-hidden="true">1.8.</strong> 08_explorationArticleGraph</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/09_fullGraphAnalysis.html"><strong aria-hidden="true">1.9.</strong> 09_fullGraphAnalysis</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/10_explorativeMotifs.html"><strong aria-hidden="true">1.10.</strong> 10_explorativeMotifs</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/11_conclusionDiscussionAndFutureWork.html"><strong aria-hidden="true">1.11.</strong> 11_conclusionDiscussionAndFutureWork</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/12_gameNotebookSetup.html"><strong aria-hidden="true">1.12.</strong> 12_gameNotebookSetup</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/13_gameNotebook.html"><strong aria-hidden="true">1.13.</strong> 13_gameNotebook</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-02_group-DDLOfVision/student-project-02_group-DDLOfVision/00_vqa_introduction.html"><strong aria-hidden="true">2.</strong> student-project-02_group-DDLOfVision_00_vqa_introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-02_group-DDLOfVision/student-project-02_group-DDLOfVision/01_vqa_model_training.html"><strong aria-hidden="true">2.1.</strong> 01_vqa_model_training</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-04_group-FedMLMedicalApp/student-project-04_group-FedMLMedicalApp/00_Notebook_Presentation.html"><strong aria-hidden="true">3.</strong> student-project-04_group-FedMLMedicalApp_00_Notebook_Presentation</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-04_group-FedMLMedicalApp/student-project-04_group-FedMLMedicalApp/01_BrainTumorSegmentation_Centralized_Training.html"><strong aria-hidden="true">3.1.</strong> 01_BrainTumorSegmentation_Centralized_Training</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-04_group-FedMLMedicalApp/student-project-04_group-FedMLMedicalApp/02_Federated_Learning_BrainTumorSegmentation.html"><strong aria-hidden="true">3.2.</strong> 02_Federated_Learning_BrainTumorSegmentation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-04_group-FedMLMedicalApp/student-project-04_group-FedMLMedicalApp/data_upload_test.html"><strong aria-hidden="true">3.3.</strong> data_upload_test</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/00_introduction.html"><strong aria-hidden="true">4.</strong> student-project-05_group-DistOpt_00_introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/01_Bayesian_optimization.html"><strong aria-hidden="true">4.1.</strong> 01_Bayesian_optimization</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/02_Gaussian_processes.html"><strong aria-hidden="true">4.2.</strong> 02_Gaussian_processes</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/03_acquisition_functions.html"><strong aria-hidden="true">4.3.</strong> 03_acquisition_functions</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/04_scalable_Bayesian_optimization.html"><strong aria-hidden="true">4.4.</strong> 04_scalable_Bayesian_optimization</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/05_implementation_documentation.html"><strong aria-hidden="true">4.5.</strong> 05_implementation_documentation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/06_our_implementation.html"><strong aria-hidden="true">4.6.</strong> 06_our_implementation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/07_additional_code.html"><strong aria-hidden="true">4.7.</strong> 07_additional_code</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-07_group-ExpsZerOInit/student-project-07_group-ExpsZerOInit/00_introduction_resnet.html"><strong aria-hidden="true">5.</strong> student-project-07_group-ExpsZerOInit_00_introduction_resnet</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-07_group-ExpsZerOInit/student-project-07_group-ExpsZerOInit/01_transformer.html"><strong aria-hidden="true">5.1.</strong> 01_transformer</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-07_group-ExpsZerOInit/student-project-07_group-ExpsZerOInit/02_ddpm.html"><strong aria-hidden="true">5.2.</strong> 02_ddpm</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-08_group-WikiSearch/student-project-08_group-WikiSearch/00_Introduction.html"><strong aria-hidden="true">6.</strong> student-project-08_group-WikiSearch_00_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-08_group-WikiSearch/student-project-08_group-WikiSearch/01_InputParsing.html"><strong aria-hidden="true">6.1.</strong> 01_InputParsing</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-08_group-WikiSearch/student-project-08_group-WikiSearch/02_PageRank.html"><strong aria-hidden="true">6.2.</strong> 02_PageRank</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-08_group-WikiSearch/student-project-08_group-WikiSearch/03_QuerySearch.html"><strong aria-hidden="true">6.3.</strong> 03_QuerySearch</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/00_Introduction.html"><strong aria-hidden="true">7.</strong> student-project-09_group-DistEnsembles_00_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/02_Ensemble_Training.html"><strong aria-hidden="true">7.1.</strong> 02_Ensemble_Training</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/03_Ensemble_Evaluation.html"><strong aria-hidden="true">7.2.</strong> 03_Ensemble_Evaluation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/099_extra_Ensemble.html"><strong aria-hidden="true">7.3.</strong> 099_extra_Ensemble</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/00_introduction.html"><strong aria-hidden="true">8.</strong> student-project-10_group-RDI_00_introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/01_prepare_data.html"><strong aria-hidden="true">8.1.</strong> 01_prepare_data</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/02_baseline.html"><strong aria-hidden="true">8.2.</strong> 02_baseline</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/03_single_machine.html"><strong aria-hidden="true">8.3.</strong> 03_single_machine</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/04_distributed_learning.html"><strong aria-hidden="true">8.4.</strong> 04_distributed_learning</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-11_group-CollaborativeFiltering/student-project-11_group-CollaborativeFiltering/01_Introduction.html"><strong aria-hidden="true">9.</strong> student-project-11_group-CollaborativeFiltering_01_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-11_group-CollaborativeFiltering/student-project-11_group-CollaborativeFiltering/02_AlgorithmsBeyondALS.html"><strong aria-hidden="true">9.1.</strong> 02_AlgorithmsBeyondALS</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-11_group-CollaborativeFiltering/student-project-11_group-CollaborativeFiltering/03_Implementation.html"><strong aria-hidden="true">9.2.</strong> 03_Implementation</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-12_group-FedLearnOpt/student-project-12_group-FedLearnOpt/01_Federated_Learning_Introduction.html"><strong aria-hidden="true">10.</strong> student-project-12_group-FedLearnOpt_01_Federated_Learning_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-12_group-FedLearnOpt/student-project-12_group-FedLearnOpt/02_Horovod_Introduction.html"><strong aria-hidden="true">10.1.</strong> 02_Horovod_Introduction</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-12_group-FedLearnOpt/student-project-12_group-FedLearnOpt/03_Implementations.html"><strong aria-hidden="true">10.2.</strong> 03_Implementations</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-13_group-DRL/student-project-13_group-DRL/00_DistributedRL.html" class="active"><strong aria-hidden="true">11.</strong> student-project-13_group-DRL_00_DistributedRL</a></li><li class="spacer"></li><li class="chapter-item expanded "><a href="../../../contents/student-project-14_group-EarthObs/student-project-14_group-EarthObs/00_Introduction.html"><strong aria-hidden="true">12.</strong> student-project-14_group-EarthObs_00_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-14_group-EarthObs/student-project-14_group-EarthObs/01_Download_data.html"><strong aria-hidden="true">12.1.</strong> 01_Download_data</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-14_group-EarthObs/student-project-14_group-EarthObs/02_Image_preprocessing.html"><strong aria-hidden="true">12.2.</strong> 02_Image_preprocessing</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-14_group-EarthObs/student-project-14_group-EarthObs/03_Model_Architecture_and_Training.html"><strong aria-hidden="true">12.3.</strong> 03_Model_Architecture_and_Training</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-14_group-EarthObs/student-project-14_group-EarthObs/04_Prediction_And_Visualisation.html"><strong aria-hidden="true">12.4.</strong> 04_Prediction_And_Visualisation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-14_group-EarthObs/student-project-14_group-EarthObs/05_Conclusions.html"><strong aria-hidden="true">12.5.</strong> 05_Conclusions</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-projects-BrIntSuSvConclusion/student-projects-BrIntSuSvConclusion/BrIntSuSv.html"><strong aria-hidden="true">13.</strong> student-projects-BrIntSuSvConclusion_BrIntSuSv</a></li><li class="spacer"></li><li class="chapter-item expanded "><a href="../../../editors.html"><strong aria-hidden="true">14.</strong> Editors</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">sds-3.x/ScaDaMaLe</h1>

                    <div class="right-buttons">
                        <a href="../../../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div class="cell markdown">
<h1 id="distributed-reinforcement-learning"><a class="header" href="#distributed-reinforcement-learning">Distributed Reinforcement Learning</a></h1>
<p><strong>Project members:</strong></p>
<ul>
<li>Johan Edstedt, Linköping University</li>
<li>Arvi Jonnarth, Linköping University &amp; Husqvarna Group</li>
<li>Yushan Zhang, Linköping University</li>
</ul>
<p><strong>Presentation:</strong></p>
<ul>
<li><a href="https://liuonline-my.sharepoint.com/:p:/g/personal/yuszh17_liu_se/EUpJxDxlXgtEgeBzTwhVBYkBFyvQyTwRIwyHDRQ6w-Rf8g?e=PRziyw">https://liuonline-my.sharepoint.com/:p:/g/personal/yuszh17<em>liu</em>se/EUpJxDxlXgtEgeBzTwhVBYkBFyvQyTwRIwyHDRQ6w-Rf8g?e=PRziyw</a></li>
<li><a href="https://github.com/lamastex/scalable-data-science/tree/master/images/ScaDaMaLe/2022-WASP/group-13">https://github.com/lamastex/scalable-data-science/tree/master/images/ScaDaMaLe/2022-WASP/group-13</a></li>
</ul>
</div>
<div class="cell markdown">
<p><a href="https://www.youtube.com/watch?v=dLI7rfPyC-s"><img src="http://img.youtube.com/vi/dLI7rfPyC-s/0.jpg" alt="ScaDaMaLe WASP-UU 2022 - Student Group Project 13 - Distributed Reinforcement Learning" /></a></p>
</div>
<div class="cell markdown">
<p><strong>Project description</strong></p>
<p>We extend the <a href="https://arxiv.org/pdf/1707.06347.pdf">Proximal Policy Optimization (PPO)</a> and <a href="https://arxiv.org/pdf/1801.01290.pdf">Soft Actor-Critic (SAC)</a> reinforcement learning algorithms from the <code>stable_baselines3</code> library to make them run in a distributed fashion.</p>
<p>We use <a href="https://github.com/horovod/horovod">Horovod</a>, which is a distributed deep learning training framework that supports several ML libraries, including <a href="https://pytorch.org/">PyTorch</a> through the <code>horovod.torch</code> package. But more importantly, it can be run on top of spark using <code>sparkdl.HorovodRunner</code>. The HorovodRunner launches spark jobs on training functions that implement the Horovod framework. All that is needed to utilize the Horovod framework is to wrap the original <code>torch.optim.Optimizer</code> optimizer in a <code>horovod.torch.DistributedOptimizer</code>.</p>
<p><strong>Reinforcement Learning</strong></p>
<p>Definition: Reinforcement Learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning. It is a machine learning training method based on rewarding desired behaviors and/or punishing undesired ones. In general, a reinforcement learning agent is able to perceive and interpret its environment, take actions and learn through trial and error.</p>
<p>Environment: <a href="https://www.gymlibrary.dev/">Gymnasium</a>, where the classic “agent-environment loop” is implemented. The agent performs some actions in the environment (usually by passing some control inputs to the environment, e.g. torque inputs of motors) and observes how the environment’s state changes. One such action-observation exchange is referred to as a timestep.</p>
<img src ='https://www.gymlibrary.dev/_images/AE_loop.png' width="200">
<p>Example tasks:</p>
<p><img src ='https://www.gymlibrary.dev/_images/assault.gif' width="100"> <img src ='https://www.gymlibrary.dev/_static/videos/atari/breakout.gif' width="100"> <img src ='https://www.gymlibrary.dev/_static/videos/mujoco/humanoid.gif' width="100"> <img src ='https://www.gymlibrary.dev/_static/videos/mujoco/ant.gif' width="100"> <img src ='https://www.gymlibrary.dev/_static/videos/classic_control/cart_pole.gif' width="100"> <img src ='https://www.gymlibrary.dev/_static/videos/classic_control/mountain_car.gif' width="100"> <img src ='https://www.gymlibrary.dev/_static/videos/box2d/bipedal_walker.gif' width="100"> <img src ='https://www.gymlibrary.dev/_static/videos/box2d/lunar_lander.gif' width="100"></p>
<p><strong>Stable Baselines3</strong></p>
<p><a href="https://github.com/DLR-RM/stable-baselines3">Stable Baselines3</a> is a set of reliable implementations of reinforcement learning algorithms in PyTorch. It is the latest major version of <a href="https://github.com/hill-a/stable-baselines">Stable Baselines</a>.</p>
<p>Github repository: https://github.com/DLR-RM/stable-baselines3</p>
<p>Paper: https://jmlr.org/papers/volume22/20-1364/20-1364.pdf</p>
<p><strong>PPO:</strong></p>
<p>The <a href="https://arxiv.org/abs/1707.06347">Proximal Policy Optimization</a> algorithm combines ideas from A2C (having multiple workers) and TRPO (it uses a trust region to improve the actor).</p>
<p>The main idea is that after an update, the new policy should be not too far from the old policy. For that, ppo uses clipping to avoid too large update.</p>
<p><strong>SAC:</strong></p>
<p><a href="https://spinningup.openai.com/en/latest/algorithms/sac.html">Soft Actor Critic (SAC)</a> Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.</p>
<p>SAC is the successor of <a href="https://arxiv.org/abs/1702.08165">Soft Q-Learning SQL</a> and incorporates the double Q-learning trick from TD3. A key feature of SAC, and a major difference with common RL algorithms, is that it is trained to maximize a trade-off between expected return and entropy, a measure of randomness in the policy.</p>
</div>
<div class="cell markdown">
<p>Get all necessary imports.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">import csv
import gym
import horovod.torch as hvd
import numpy as np
import os
import shutil
import torch as th
import warnings

from datetime import datetime
from gym import spaces
from matplotlib import pyplot as plt
from sparkdl import HorovodRunner
from torch.nn import functional as F
from typing import Any, Dict, List, Optional, Tuple, Type, TypeVar, Union

from stable_baselines3 import PPO, SAC
from stable_baselines3.common.buffers import ReplayBuffer
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.noise import ActionNoise
from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy
from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule
from stable_baselines3.common.utils import explained_variance, get_parameters_by_name, get_schedule_fn, polyak_update
from stable_baselines3.sac.policies import CnnPolicy, MlpPolicy, MultiInputPolicy, SACPolicy
</code></pre>
</div>
<div class="cell markdown">
<p>And global variables</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">LOG_DIR = '/dbfs/drl_logs'
</code></pre>
</div>
<div class="cell markdown">
<h2 id="distributed-proximal-policy-optimization-dppo"><a class="header" href="#distributed-proximal-policy-optimization-dppo">Distributed Proximal Policy Optimization, DPPO</a></h2>
<h3 id="theoretical-background"><a class="header" href="#theoretical-background">Theoretical Background</a></h3>
<p>PPO is a reinforcement learning algorithm, which is trained using the following objective (that should be maximized)</p>
<p>\[ L^{\text{CLIP}}(\theta) = \mathbb E_t \big[ \text{min}(r_t(\theta)A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)A_t) \big], \]</p>
<p>let's go through what this means. First, \(\pi_{\theta}(a_t|s_t)\) is the &quot;policy&quot;. This is a neural network that we want to train to estimate the &quot;best&quot; action to take in any given state. Then we have \(r_t(\theta) = \frac{\pi_{\theta}(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)}\), which is the ratio of the probability of the network choosing a certain action, compared to a previous version of the network. Then we have \(A_t = -V(s_t) + r_t + \gamma r_{t+1} ...\) which is the estimated <em>advantage</em>. Intuitively this is how much better the policy performed than expected by the value network V. The inituition is that we want to get as large advantages as possible, and it is especially important when the previous policy was unlikely to have yielded the result. However, we still want the new parameters too not go too far from the old ones. To ensure this we &quot;clip&quot; the ratio. This ensures that the policy network gets no gradients after a certain threshold epsilon. See image below:</p>
<img src="https://miro.medium.com/max/1400/1*MpPiARNoNGCxJE2a8m9itA.webp" alt="alt text" width="800"/>
<p>Here it is clear that the network will cease getting gradient after improving sufficiently on for all timesteps t. A further detail is that the value network has an additional value loss which is trained to minimize the advantage. The algorithm can be summarized as follows.</p>
<ol>
<li>Gather a &quot;dataset&quot; of state action reward pairs, given the current policy. (Hereafter referred to as rollouts)</li>
<li>Train the policy to maximize advantage while clipping, also train the value network to predict the values for accurate estimation of advantage. (Hereafter training)</li>
</ol>
<h3 id="practical-implementation"><a class="header" href="#practical-implementation">Practical Implementation</a></h3>
<p>Looking at the above algorithm it is immediately obvious that the rollouts are embarassingly parallel, since no communication is needed between nodes. However, during training, the parameters of the models are updated according to the optimizer (which requires gradients to be synched between nodes). One further caveat: The stable_baselines3 implementation uses inplace gradient clipping. This is a non-linear operation, which means that the sum of clipped gradients over the nodes is not the same as the clipped summed gradient. Hence we <strong>first</strong> need to sync (sum) the gradients, and <em>then</em> perform clipping. Horovod by default syncs the gradients at optimizer.step(), however it also permits manual syncing, which we have implemented in the code below. We summarize the communication needed between nodes during rollouts and training in the figure below:</p>
<img src="https://github.com/Parskatt/storage/releases/download/webfiles/dppo.png" alt="alt text" width="800" class="center"/>
</div>
<div class="cell markdown">
<p>We extend the original <code>PPO</code> class from <code>stable_baselines3</code>. We modify the <code>_setup_model</code> function to wrap the optimizer in a <code>hvd.DistributedOptimizer</code>. We also had to change the <code>train</code> function which performs the gradient step. This change was necessary to account for the gradient clipping, which must happen <strong>after</strong> averaging the gradients across machines.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">class DPPO(PPO):
    &quot;&quot;&quot;
    Distributed Proximal Policy Optimization (DPPO)
    &quot;&quot;&quot;

    def _setup_model(self) -&gt; None:
        super()._setup_model()
        self.policy.optimizer = hvd.DistributedOptimizer(self.policy.optimizer, named_parameters=self.policy.named_parameters())
        hvd.broadcast_parameters(self.policy.state_dict(), root_rank=0)
        hvd.broadcast_optimizer_state(self.policy.optimizer, root_rank=0)

    def train(self) -&gt; None:
        &quot;&quot;&quot;
        Update policy using the currently gathered rollout buffer.
        &quot;&quot;&quot;
        # Switch to train mode (this affects batch norm / dropout)
        self.policy.set_training_mode(True)
        # Update optimizer learning rate
        self._update_learning_rate(self.policy.optimizer)
        # Compute current clip range
        clip_range = self.clip_range(self._current_progress_remaining)
        # Optional: clip range for the value function
        if self.clip_range_vf is not None:
            clip_range_vf = self.clip_range_vf(self._current_progress_remaining)

        entropy_losses = []
        pg_losses, value_losses = [], []
        clip_fractions = []

        continue_training = True

        # train for n_epochs epochs
        for epoch in range(self.n_epochs):
            approx_kl_divs = []
            # Do a complete pass on the rollout buffer
            for rollout_data in self.rollout_buffer.get(self.batch_size):
                actions = rollout_data.actions
                if isinstance(self.action_space, spaces.Discrete):
                    # Convert discrete action from float to long
                    actions = rollout_data.actions.long().flatten()

                # Re-sample the noise matrix because the log_std has changed
                if self.use_sde:
                    self.policy.reset_noise(self.batch_size)

                values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)
                values = values.flatten()
                # Normalize advantage
                advantages = rollout_data.advantages
                # Normalization does not make sense if mini batchsize == 1, see GH issue #325
                if self.normalize_advantage and len(advantages) &gt; 1:
                    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

                # ratio between old and new policy, should be one at the first iteration
                ratio = th.exp(log_prob - rollout_data.old_log_prob)

                # clipped surrogate loss
                policy_loss_1 = advantages * ratio
                policy_loss_2 = advantages * th.clamp(ratio, 1 - clip_range, 1 + clip_range)
                policy_loss = -th.min(policy_loss_1, policy_loss_2).mean()

                # Logging
                pg_losses.append(policy_loss.item())
                clip_fraction = th.mean((th.abs(ratio - 1) &gt; clip_range).float()).item()
                clip_fractions.append(clip_fraction)

                if self.clip_range_vf is None:
                    # No clipping
                    values_pred = values
                else:
                    # Clip the difference between old and new value
                    # NOTE: this depends on the reward scaling
                    values_pred = rollout_data.old_values + th.clamp(
                        values - rollout_data.old_values, -clip_range_vf, clip_range_vf
                    )
                # Value loss using the TD(gae_lambda) target
                value_loss = F.mse_loss(rollout_data.returns, values_pred)
                value_losses.append(value_loss.item())

                # Entropy loss favor exploration
                if entropy is None:
                    # Approximate entropy when no analytical form
                    entropy_loss = -th.mean(-log_prob)
                else:
                    entropy_loss = -th.mean(entropy)

                entropy_losses.append(entropy_loss.item())

                loss = policy_loss + self.ent_coef * entropy_loss + self.vf_coef * value_loss

                # Calculate approximate form of reverse KL Divergence for early stopping
                # see issue #417: https://github.com/DLR-RM/stable-baselines3/issues/417
                # and discussion in PR #419: https://github.com/DLR-RM/stable-baselines3/pull/419
                # and Schulman blog: http://joschu.net/blog/kl-approx.html
                with th.no_grad():
                    log_ratio = log_prob - rollout_data.old_log_prob
                    approx_kl_div = th.mean((th.exp(log_ratio) - 1) - log_ratio).cpu().numpy()
                    approx_kl_divs.append(approx_kl_div)

                if self.target_kl is not None and approx_kl_div &gt; 1.5 * self.target_kl:
                    continue_training = False
                    if self.verbose &gt;= 1:
                        print(f&quot;Early stopping at step {epoch} due to reaching max kl: {approx_kl_div:.2f}&quot;)
                    break

                # Optimization step
                self.policy.optimizer.zero_grad()
                loss.backward()
                # Need to synch gradients first, to prevent non-linear gradient clipping to happen too early
                self.policy.optimizer.synchronize()
                # Clip grad norm
                th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)
                # Dont need to sync now.
                with self.policy.optimizer.skip_synchronize():
                    self.policy.optimizer.step()

            if not continue_training:
                break

        self._n_updates += self.n_epochs
        explained_var = explained_variance(self.rollout_buffer.values.flatten(), self.rollout_buffer.returns.flatten())

        # Logs
        self.logger.record(&quot;train/entropy_loss&quot;, np.mean(entropy_losses))
        self.logger.record(&quot;train/policy_gradient_loss&quot;, np.mean(pg_losses))
        self.logger.record(&quot;train/value_loss&quot;, np.mean(value_losses))
        self.logger.record(&quot;train/approx_kl&quot;, np.mean(approx_kl_divs))
        self.logger.record(&quot;train/clip_fraction&quot;, np.mean(clip_fractions))
        self.logger.record(&quot;train/loss&quot;, loss.item())
        self.logger.record(&quot;train/explained_variance&quot;, explained_var)
        if hasattr(self.policy, &quot;log_std&quot;):
            self.logger.record(&quot;train/std&quot;, th.exp(self.policy.log_std).mean().item())

        self.logger.record(&quot;train/n_updates&quot;, self._n_updates, exclude=&quot;tensorboard&quot;)
        self.logger.record(&quot;train/clip_range&quot;, clip_range)
        if self.clip_range_vf is not None:
            self.logger.record(&quot;train/clip_range_vf&quot;, clip_range_vf)
</code></pre>
</div>
<div class="cell markdown">
<p>Next, we define our distributed training function that <code>Horovod</code> will run on each process. We make it general since we want to use it for different algorithms. It takes an algorithm class <code>algo</code> as input. Furthermore, it takes the environment name, type of policy, and the number of time steps, as well as any keyword arguments to be passed to the algorithm class. We do any logging or model saving only on the main process, i.e. rank 0.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def train_hvd(algo, env_name=&quot;Pendulum-v1&quot;, policy=&quot;MlpPolicy&quot;, total_timesteps=100_000, **kwargs):
    
    # Initialize Horovod
    hvd.init()
    
    # Create environment, model, and run training
    env = gym.make(env_name)
    # log reward etc. only on the main process by wrapping the environment in a Monior
    if hvd.rank() == 0:
        env = Monitor(env, os.path.join(LOG_DIR, env_name, algo.__name__ + '-' + str(hvd.size())))
    model = algo(policy, env, **kwargs)
    model.learn(total_timesteps=total_timesteps, log_interval=1)
    
    # Save model only on process 0
    if hvd.rank() == 0:
        exp_time = datetime.now().strftime('%Y-%m-%d_%H%M%S')
        log_dir = os.path.join(LOG_DIR, exp_time)
        if not os.path.exists(log_dir):
            os.makedirs(log_dir)
        # DBFS doesnt support zip, need to create on driver and then move file to DBFS
        filename = f'{algo.__name__}_{env_name}.zip'
        model.save(filename)
        shutil.move(filename, f&quot;{log_dir}/&quot;)
    
    env.close()
</code></pre>
</div>
<div class="cell markdown">
<h3 id="single-process-ppo"><a class="header" href="#single-process-ppo">Single-process PPO</a></h3>
<p>First, let us train a PPO agent in a single process using the original implementation from <code>stable_baselines3</code> (i.e. not using our <code>DPPO</code> class or <code>train_hvd</code> function).</p>
<p>Start with some parameters:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># The environment where to test our distributed PPO algorithm
ppo_env_name = 'CartPole-v1'
#ppo_env_name = 'Pendulum-v1'
#ppo_env_name = 'MountainCarContinuous-v0'
#ppo_env_name = 'BipedalWalker-v3'

# PPO parameters
ppo_total_timesteps = 100_000
ppo_learning_rate = 1e-3 # default: 3e-4
ppo_n_steps = 4096 * 8 # default: 2048
ppo_batch_size = 4096 # default: 64

# How many processes to use for distributed training
ppo_world_sizes = [1, 2, 4, 8]
</code></pre>
</div>
<div class="cell markdown">
<p>Train the PPO agent.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># Create a gym environment and wrap it in a Monitor to track the reward
env = gym.make(ppo_env_name)
env = Monitor(env, os.path.join(LOG_DIR, ppo_env_name, 'PPO'))

# Define our PPO agent
model = PPO(
    &quot;MlpPolicy&quot;,
    env,
    learning_rate=ppo_learning_rate,
    n_steps=ppo_n_steps,
    batch_size=ppo_batch_size,
    verbose=1)

# Train our agent
model.learn(total_timesteps=ppo_total_timesteps, log_interval=1)
env.close()
</code></pre>
</div>
<div class="cell markdown">
<h3 id="multi-process-dppo"><a class="header" href="#multi-process-dppo">Multi-process DPPO</a></h3>
<p>Train our distributed DPPO algorithm for different number of processes (<code>world_size</code>). To have comparable training settings for different number of processes, we divide the per-process rollout buffer size <code>n_steps</code> with the number of processes (since the total buffer size will then be the same). We also multiply the learning rate <code>learning_rate</code> with the number of processes (since the total batch size will be larger). Note that this means that a gradient step for the multi-process case will essentially be equivalent to <code>world_size</code> number of single-process gradient steps. We will take this into account when plotting the results later to validate that the training settings actually are equivalent.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># Loop over number of processes and do an experiment for each
for world_size in ppo_world_sizes:

    # Initialize the HorovodRunner
    hr = HorovodRunner(np=world_size, driver_log_verbosity='all')

    # Launch the spark job on our training function
    hr.run(
        train_hvd,
        algo = DPPO,
        env_name = ppo_env_name,
        policy = &quot;MlpPolicy&quot;,
        total_timesteps = ppo_total_timesteps,
        learning_rate = ppo_learning_rate * world_size,
        n_steps = ppo_n_steps // world_size,
        batch_size = ppo_batch_size,
        verbose = 1
    )
</code></pre>
</div>
<div class="cell markdown">
<h3 id="ppo-and-dppo-results"><a class="header" href="#ppo-and-dppo-results">PPO and DPPO results</a></h3>
<p>We compare the different runs by plotting the reward over per-process training steps, total number of training steps, and wall time. We expect a faster convergence rate for a larger number of processes when looking at the per-process training steps, and the same reward for the same number of total training steps, since these should correspond to the same training setting. We also expect a faster convergence in wall time for more processes. We also compute the speedup for different number of processes.</p>
</div>
<div class="cell markdown">
<p>Define a function to get the logs from a run.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def get_logs(log_file, as_dict=False):
    steps, rewards, ep_lengths, times = [], [], [], []
    with open(log_file) as csv_file:
        csv_reader = csv.reader(csv_file, delimiter=',')
        for i, row in enumerate(csv_reader):
            if i &lt; 2: # skip first two rows (headers)
                continue
            rewards.append(float(row[0]))
            ep_lengths.append(int(row[1]))
            times.append(float(row[2]))
            steps.append((steps[-1] if len(steps) != 0 else 0) + ep_lengths[-1])
    if as_dict:
        return {'steps': steps,
                'rewards': rewards,
                'ep_lengths': ep_lengths,
                'times': times}
    return steps, rewards, ep_lengths, times
</code></pre>
</div>
<div class="cell markdown">
<p>Define a function to plot the reward.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def plot_results(env_name, algo_names, x_axis='steps', num_procs=None, smooth=None):
    
    # Read logs
    logs = {}
    if isinstance(algo_names, str):
        logs[algo_names] = get_logs(os.path.join(LOG_DIR, env_name, f'{algo_names}.monitor.csv'), as_dict=True)
    else: # list of str
        for algo_name in algo_names:
            logs[algo_name] = get_logs(os.path.join(LOG_DIR, env_name, f'{algo_name}.monitor.csv'), as_dict=True)
    
    # Plot reward
    for algo_name in algo_names:

        # Scale x-axis if number of processes is given
        if num_procs is not None:
            scale_by_num_proc = num_procs[algo_name]
        else:
            scale_by_num_proc = 1
        
        # Smooth and plot the reward
        reward = logs[algo_name]['rewards']
        if smooth is not None:
            reward = [reward[0]]*smooth + reward + [reward[-1]]*smooth
            reward = np.convolve(reward, [1/(smooth*2 + 1)]*(smooth*2 + 1), mode='valid')
        plt.plot(scale_by_num_proc*np.array(logs[algo_name][x_axis]), reward)
    
    plt.legend(algo_names)
    plt.title(env_name)
    plt.ylabel('reward')
    if x_axis == 'times':
        if num_procs is not None:
            plt.xlabel('total cpu time [s]')
        else:
            plt.xlabel('wall time [s]')
    elif x_axis == 'steps':
        if num_procs is not None:
            plt.xlabel('total steps')
        else:
            plt.xlabel('steps per process')
    else:
        plt.xlabel(x_axis)
    plt.gcf().set_size_inches(8, 4)
    plt.show()
</code></pre>
</div>
<div class="cell markdown">
<p>Define a function to plot the speedup.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def plot_speedup(env_name, runs):
    algo_name = runs[0]

    # Compute wall time, total steps, and speedup
    wall_times = []
    total_steps = []
    speedups = []
    nprocs = []

    # Get logs from single-process run
    logs1 = get_logs(os.path.join(LOG_DIR, env_name, f'D{algo_name}-1.monitor.csv'), as_dict=True)
    wall_time1 = logs1['times'][-1]
    total_step1 = logs1['steps'][-1]

    # Compute results for each run
    for run in runs:
        logs = get_logs(os.path.join(LOG_DIR, env_name, f'{run}.monitor.csv'), as_dict=True)
        wall_times.append(logs['times'][-1])
        total_steps.append(logs['steps'][-1])
        if '-' in run:
            nprocs.append(int(run.split('-')[1]))
            total_steps[-1] *= nprocs[-1]
            speedup = wall_time1 / wall_times[-1]
            speedup *= total_steps[-1] / total_step1 # adjust for different total steps
            speedups.append(round(speedup, 2))

    # Print run times and total steps
    print('Run times')
    for run, wall_time, total_step in zip(runs, wall_times, total_steps):
        print(f'{run}: {round(wall_time, 2)} s ({total_step} total steps)')

    # Print speedup
    print('\nSpeedups')
    for speedup, nproc in zip(speedups, nprocs):
        print(f'D{algo_name}-{nproc}: {speedup}')

    # Plot speedup as a bar plot
    x = np.arange(len(speedups))  # the label locations
    width = 0.35  # the width of the bars
    fig, ax = plt.subplots()
    rects1 = ax.bar(x - width/2, speedups, width, label='speedup')
    rects2 = ax.bar(x + width/2, nprocs, width, label='ideal')

    # Add some text for labels, title and custom x-axis tick labels, etc.
    ax.set_ylabel('Speedup')
    ax.set_xlabel('Number of processes')
    ax.set_title(f'D{algo_name} speedup on {env_name}')
    plt.xticks(x, nprocs)
    ax.legend()
    ax.bar_label(rects1, padding=3)
    ax.bar_label(rects2, padding=3)
    fig.tight_layout()
    plt.show()
</code></pre>
</div>
<div class="cell markdown">
<h4 id="reward-plots"><a class="header" href="#reward-plots">Reward plots</a></h4>
<p>Finally, we plot the reward.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">ppo_runs = ['PPO'] + [f'DPPO-{ws}' for ws in ppo_world_sizes]
ppo_nprocs = {**{'PPO': 1}, **{f'DPPO-{ws}': ws for ws in ppo_world_sizes}}

plot_results(ppo_env_name, ppo_runs, x_axis='steps', smooth=5)
plot_results(ppo_env_name, ppo_runs, x_axis='steps', num_procs=ppo_nprocs, smooth=5)
plot_results(ppo_env_name, ppo_runs, x_axis='times', smooth=5)
</code></pre>
</div>
<div class="cell markdown">
<h4 id="speedup"><a class="header" href="#speedup">Speedup</a></h4>
<p>Plot the speedup for the different number of processes.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">plot_speedup(ppo_env_name, ppo_runs)
</code></pre>
</div>
<div class="cell markdown">
<h4 id="gif"><a class="header" href="#gif">GIF</a></h4>
<p>Here is what our trained agent looks like.</p>
<p>Left: Not trained (the environment resets each time the pole falls too low)
Right: Trained with PPO</p>
<p><img src="https://github.com/Parskatt/storage/releases/download/webfiles/PPO_CartPole-v1_00000.gif" width="300"/> <img src="https://github.com/Parskatt/storage/releases/download/webfiles/PPO_CartPole-v1_80000.gif" width="300" class="center"/></p>
</div>
<div class="cell markdown">
<h2 id="distributed-soft-actor-critic-dsac"><a class="header" href="#distributed-soft-actor-critic-dsac">Distributed Soft Actor-Critic, DSAC</a></h2>
<p>SAC is an off-policy reinforcement learning algorithm. The main difference between it and PPO is the following:</p>
<ol>
<li>A larger set of networks are used, and they do not share the same optimizer.</li>
<li>Instead of &quot;rollouts&quot;, the agent takes a single &quot;step&quot; in the environment, and immediately updates the weights using a &quot;replay buffer&quot;. Note that the algorithm is called an &quot;off-policy algorithm&quot; due to the fact that state action pairs in the replay buffer may have been generated by previous policies, not the current policy.</li>
</ol>
<p>Regarding point 1., in practice this forces us to wrap multiple optimizers. Furthermore, due to quirks of horovod (see: https://github.com/horovod/horovod/issues/1417), we are forced to sync gradients explicitly after the every backwards pass.</p>
<p>Regarding point 2., SAC alternates between updating a rolling buffer and updating model weights using the buffer. We illustrate this below.</p>
<img src="https://github.com/Parskatt/storage/releases/download/webfiles/dsac.png" alt="alt text" width="800" class="center"/>
</div>
<div class="cell markdown">
<p>We extend the original <code>SAC</code> class from <code>stable_baselines3</code>. Different from DPPO, we modify the <code>__init__</code> function to wrap multiple optimizers. The wrapping part is a bit different since actor and critic networks may or may not share the feature extractor. Additionally, a third optimizer is used for an entropy coefficient (scalar), which may or may not be used. SAC does not used gradient clipping, but we still needed to modify the <code>train</code> function to syncronize the actor and critic optimizers between each other.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">class DSAC(SAC):
    &quot;&quot;&quot;
    Distributed Soft Actor-Critic (DSAC)
    &quot;&quot;&quot;

    def __init__(
        self,
        policy,
        env,
        learning_rate = 3e-4,
        buffer_size = 1_000_000,  # 1e6
        learning_starts = 100,
        batch_size = 256,
        tau = 0.005,
        gamma = 0.99,
        train_freq = 1,
        gradient_steps = 1,
        action_noise = None,
        replay_buffer_class = None,
        replay_buffer_kwargs = None,
        optimize_memory_usage = False,
        ent_coef = &quot;auto&quot;,
        target_update_interval = 1,
        target_entropy = &quot;auto&quot;,
        use_sde = False,
        sde_sample_freq = -1,
        use_sde_at_warmup = False,
        tensorboard_log = None,
        create_eval_env = False,
        policy_kwargs = None,
        verbose = 0,
        seed = None,
        device = &quot;auto&quot;,
        _init_setup_model = True,
    ):

        super().__init__(
            policy,
            env,
            learning_rate,
            buffer_size,
            learning_starts,
            batch_size,
            tau,
            gamma,
            train_freq,
            gradient_steps,
            action_noise,
            replay_buffer_class,
            replay_buffer_kwargs,
            optimize_memory_usage,
            ent_coef,
            target_update_interval,
            target_entropy,
            use_sde,
            sde_sample_freq,
            use_sde_at_warmup,
            tensorboard_log,
            create_eval_env,
            policy_kwargs,
            verbose,
            seed,
            device,
            _init_setup_model,
        )
        
        # Wrap optimizers in Horovod
        # Actor optimizer
        self.actor.optimizer = hvd.DistributedOptimizer(self.actor.optimizer, named_parameters=self.actor.named_parameters())
        hvd.broadcast_parameters(self.actor.state_dict(), root_rank=0)
        hvd.broadcast_optimizer_state(self.actor.optimizer, root_rank=0)
        
        # Critic optimizer
        if self.policy.share_features_extractor:
            critic_parameters = [(name, param) for name, param in self.critic.named_parameters() if &quot;features_extractor&quot; not in name]
        else: # used by default
            critic_parameters = self.critic.named_parameters()
        self.critic.optimizer = hvd.DistributedOptimizer(self.critic.optimizer, named_parameters=critic_parameters)
        hvd.broadcast_parameters(self.critic.state_dict(), root_rank=0)
        hvd.broadcast_optimizer_state(self.critic.optimizer, root_rank=0)
        
        # Entropy coefficient optimizer
        if self.ent_coef_optimizer is not None:
            self.ent_coef_optimizer = hvd.DistributedOptimizer(self.ent_coef_optimizer, named_parameters=[(&quot;log_ent_coef&quot;, self.log_ent_coef)])
            hvd.broadcast_parameters([self.log_ent_coef], root_rank=0)
            hvd.broadcast_optimizer_state(self.ent_coef_optimizer, root_rank=0)
    
    # Need to redefine this function to synchronize multiple optimizers
    def train(self, gradient_steps: int, batch_size: int = 64) -&gt; None:
        
        # Switch to train mode (this affects batch norm / dropout)
        self.policy.set_training_mode(True)
        # Update optimizers learning rate
        optimizers = [self.actor.optimizer, self.critic.optimizer]
        if self.ent_coef_optimizer is not None:
            optimizers += [self.ent_coef_optimizer]

        # Update learning rate according to lr schedule
        self._update_learning_rate(optimizers)

        ent_coef_losses, ent_coefs = [], []
        actor_losses, critic_losses = [], []

        for gradient_step in range(gradient_steps):
            # Sample replay buffer
            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)

            # We need to sample because `log_std` may have changed between two gradient steps
            if self.use_sde:
                self.actor.reset_noise()

            # Action by the current actor for the sampled state
            actions_pi, log_prob = self.actor.action_log_prob(replay_data.observations)
            log_prob = log_prob.reshape(-1, 1)

            ent_coef_loss = None
            if self.ent_coef_optimizer is not None:
                # Important: detach the variable from the graph
                # so we don't change it with other losses
                # see https://github.com/rail-berkeley/softlearning/issues/60
                ent_coef = th.exp(self.log_ent_coef.detach())
                ent_coef_loss = -(self.log_ent_coef * (log_prob + self.target_entropy).detach()).mean()
                ent_coef_losses.append(ent_coef_loss.item())
            else:
                ent_coef = self.ent_coef_tensor

            ent_coefs.append(ent_coef.item())

            # Optimize entropy coefficient, also called
            # entropy temperature or alpha in the paper
            if ent_coef_loss is not None:
                self.ent_coef_optimizer.zero_grad()
                ent_coef_loss.backward()
                self.ent_coef_optimizer.step()

            with th.no_grad():
                # Select action according to policy
                next_actions, next_log_prob = self.actor.action_log_prob(replay_data.next_observations)
                # Compute the next Q values: min over all critics targets
                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)
                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)
                # add entropy term
                next_q_values = next_q_values - ent_coef * next_log_prob.reshape(-1, 1)
                # td error + entropy term
                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values

            # Get current Q-values estimates for each critic network
            # using action from the replay buffer
            current_q_values = self.critic(replay_data.observations, replay_data.actions)

            # Compute critic loss
            critic_loss = 0.5 * sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)
            critic_losses.append(critic_loss.item())

            # Optimize the critic
            self.critic.optimizer.zero_grad()
            critic_loss.backward()
            self.critic.optimizer.step()
            self.actor.optimizer.synchronize() # &lt;----- diff from original function

            # Compute actor loss
            # Alternative: actor_loss = th.mean(log_prob - qf1_pi)
            # Min over all critic networks
            q_values_pi = th.cat(self.critic(replay_data.observations, actions_pi), dim=1)
            min_qf_pi, _ = th.min(q_values_pi, dim=1, keepdim=True)
            actor_loss = (ent_coef * log_prob - min_qf_pi).mean()
            actor_losses.append(actor_loss.item())

            # Optimize the actor
            self.actor.optimizer.zero_grad()
            actor_loss.backward()
            self.actor.optimizer.step()
            self.critic.optimizer.synchronize() # &lt;----- diff from original function

            # Update target networks
            if gradient_step % self.target_update_interval == 0:
                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)
                # Copy running stats, see GH issue #996
                polyak_update(self.batch_norm_stats, self.batch_norm_stats_target, 1.0)

        self._n_updates += gradient_steps

        self.logger.record(&quot;train/n_updates&quot;, self._n_updates, exclude=&quot;tensorboard&quot;)
        self.logger.record(&quot;train/ent_coef&quot;, np.mean(ent_coefs))
        self.logger.record(&quot;train/actor_loss&quot;, np.mean(actor_losses))
        self.logger.record(&quot;train/critic_loss&quot;, np.mean(critic_losses))
        if len(ent_coef_losses) &gt; 0:
            self.logger.record(&quot;train/ent_coef_loss&quot;, np.mean(ent_coef_losses))
</code></pre>
</div>
<div class="cell markdown">
<h3 id="single-process-sac"><a class="header" href="#single-process-sac">Single-process SAC</a></h3>
<p>Train a SAC agent in a single process using the original implementation from <code>stable_baselines3</code> (i.e. not using our <code>DSAC</code> class or <code>train_hvd</code> function).</p>
<p>Start with some parameters:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># The environment where to test our distributed SAC algorithm
#sac_env_name = 'CartPole-v1'
sac_env_name = 'Pendulum-v1'
#sac_env_name = 'MountainCarContinuous-v0'
#sac_env_name = 'BipedalWalker-v3'

# SAC parameters
sac_total_timesteps = 10_000
sac_learning_rate = 1e-3 # default: 3e-4
sac_buffer_size = 4096 * 8 # default: 1_000_000
sac_learning_starts = 2048 # default: 100
sac_batch_size = 2048 # default: 256

# How many processes to use for distributed training
sac_world_sizes = [1, 2, 4, 8]
</code></pre>
</div>
<div class="cell markdown">
<p>Train the SAC agent.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># Create a gym environment and wrap it in a Monitor to track the reward
env = gym.make(sac_env_name)
env = Monitor(env, os.path.join(LOG_DIR, sac_env_name, 'SAC'))

# Define the SAC agent
model = SAC(
    &quot;MlpPolicy&quot;,
    env,
    learning_rate=sac_learning_rate,
    buffer_size=sac_buffer_size,
    learning_starts=sac_learning_starts,
    batch_size=sac_batch_size,
    verbose=1)

# Train the agent
model.learn(total_timesteps=sac_total_timesteps, log_interval=1)
env.close()
</code></pre>
</div>
<div class="cell markdown">
<h3 id="multi-process-dsac"><a class="header" href="#multi-process-dsac">Multi-process DSAC</a></h3>
<p>Train our distributed DSAC algorithm for different number of processes (<code>world_size</code>). Againt, to have comparable training settings for different number of processes, we divide the per-process replay buffer size (<code>buffer_size</code>) with the number of processes, and multiply the learning rate (<code>learning_rate</code>) with the number of processes. We use the same <code>train_hvd</code> function as for PPO.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">for world_size in sac_world_sizes:
    hr = HorovodRunner(np=world_size, driver_log_verbosity='all')
    hr.run(
        train_hvd,
        algo = DSAC,
        env_name = sac_env_name,
        policy = &quot;MlpPolicy&quot;,
        total_timesteps = sac_total_timesteps,
        learning_rate = sac_learning_rate * world_size,
        buffer_size = sac_buffer_size // world_size,
        learning_starts = sac_learning_starts // world_size,
        batch_size = sac_batch_size,
        verbose = 1
    )
</code></pre>
</div>
<div class="cell markdown">
<h3 id="sacdsac-results"><a class="header" href="#sacdsac-results">SAC/DSAC results</a></h3>
<p>We compare the different runs by plotting the reward over per-process training steps, total number of training steps, and wall time.</p>
</div>
<div class="cell markdown">
<h4 id="reward-plots-1"><a class="header" href="#reward-plots-1">Reward plots</a></h4>
<p>Plot the reward.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">sac_runs = ['SAC'] + [f'DSAC-{ws}' for ws in sac_world_sizes]
sac_nprocs = {**{'SAC': 1}, **{f'DSAC-{ws}': ws for ws in sac_world_sizes}}

plot_results(sac_env_name, sac_runs, x_axis='steps')
plot_results(sac_env_name, sac_runs, x_axis='steps', num_procs=sac_nprocs)
plot_results(sac_env_name, sac_runs, x_axis='times')
</code></pre>
</div>
<div class="cell markdown">
<h4 id="speedup-1"><a class="header" href="#speedup-1">Speedup</a></h4>
<p>Plot the speedup for the different number of processes.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">plot_speedup(sac_env_name, sac_runs)
</code></pre>
</div>
<div class="cell markdown">
<h4 id="gif-1"><a class="header" href="#gif-1">GIF</a></h4>
<p>Here is what our trained agent looks like.</p>
<p>Left: Not trained (random policy)
Right: Trained with SAC</p>
<p><img src="https://github.com/Parskatt/storage/releases/download/webfiles/SAC_Pendulum-v1_00000.gif" width="300" class="center"/> <img src="https://github.com/Parskatt/storage/releases/download/webfiles/SAC_Pendulum-v1_80000.gif" width="300" class="center"/></p>
</div>
<div class="cell markdown">
<h2 id="conclusions"><a class="header" href="#conclusions">Conclusions</a></h2>
<p>Reinforcement learning is scalable as we have demonstrated with the two methods PPO and SAC. To increase the scalability, we need to reduce the communications overhead of the gradient syncronizations compared to the inter-process computations. This can be done by either increasing the batch size (together with the learning rate), or by using a larger model. The former increases the number of forward and backward passes per gradient syncronization, while the latter increases the computational cost for each pass.</p>
</div>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../../../contents/student-project-12_group-FedLearnOpt/student-project-12_group-FedLearnOpt/03_Implementations.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next" href="../../../contents/student-project-14_group-EarthObs/student-project-14_group-EarthObs/00_Introduction.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../../../contents/student-project-12_group-FedLearnOpt/student-project-12_group-FedLearnOpt/03_Implementations.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next" href="../../../contents/student-project-14_group-EarthObs/student-project-14_group-EarthObs/00_Introduction.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="../../../elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="../../../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->


    </body>
</html>
