<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>01_resnet - sds-3.x/ScaDaMaLe</title>


        <!-- Custom HTML head -->

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../../../favicon.svg">
        <link rel="shortcut icon" href="../../../favicon.png">
        <link rel="stylesheet" href="../../../css/variables.css">
        <link rel="stylesheet" href="../../../css/general.css">
        <link rel="stylesheet" href="../../../css/chrome.css">
        <link rel="stylesheet" href="../../../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../../../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../../../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../../../highlight.css">
        <link rel="stylesheet" href="../../../tomorrow-night.css">
        <link rel="stylesheet" href="../../../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../../../scroll-mdbook-outputs.css">

        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../../../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/00_Introduction.html"><strong aria-hidden="true">1.</strong> student-project-01_group-GraphOfWiki_00_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/01_DataLoading_redirectsTable.html"><strong aria-hidden="true">1.1.</strong> 01_DataLoading_redirectsTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/02_DataLoading_pagesTable.html"><strong aria-hidden="true">1.2.</strong> 02_DataLoading_pagesTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/03_DataLoading_pagelinksTable.html"><strong aria-hidden="true">1.3.</strong> 03_DataLoading_pagelinksTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/04_DataLoading_categorylinksTable.html"><strong aria-hidden="true">1.4.</strong> 04_DataLoading_categorylinksTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/05_DataLoading_categoryTable.html"><strong aria-hidden="true">1.5.</strong> 05_DataLoading_categoryTable</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/06_redirectRemoval.html"><strong aria-hidden="true">1.6.</strong> 06_redirectRemoval</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/07_createArticleGraph.html"><strong aria-hidden="true">1.7.</strong> 07_createArticleGraph</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/08_explorationArticleGraph.html"><strong aria-hidden="true">1.8.</strong> 08_explorationArticleGraph</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/09_fullGraphAnalysis.html"><strong aria-hidden="true">1.9.</strong> 09_fullGraphAnalysis</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/10_explorativeMotifs.html"><strong aria-hidden="true">1.10.</strong> 10_explorativeMotifs</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/11_conclusionDiscussionAndFutureWork.html"><strong aria-hidden="true">1.11.</strong> 11_conclusionDiscussionAndFutureWork</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/12_gameNotebookSetup.html"><strong aria-hidden="true">1.12.</strong> 12_gameNotebookSetup</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/13_gameNotebook.html"><strong aria-hidden="true">1.13.</strong> 13_gameNotebook</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-01_group-GraphOfWiki/student-project-01_group-GraphOfWiki/99_PlanningAndNotes.html"><strong aria-hidden="true">1.14.</strong> 99_PlanningAndNotes</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-02_group-DDLOfVision/student-project-02_group-DDLOfVision/00_vqa_introduction.html"><strong aria-hidden="true">2.</strong> student-project-02_group-DDLOfVision_00_vqa_introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-02_group-DDLOfVision/student-project-02_group-DDLOfVision/01_vqa_model_training.html"><strong aria-hidden="true">2.1.</strong> 01_vqa_model_training</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-02_group-DDLOfVision/student-project-02_group-DDLOfVision/02_vqa_model_inference.html"><strong aria-hidden="true">2.2.</strong> 02_vqa_model_inference</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-02_group-DDLOfVision/student-project-02_group-DDLOfVision/0y_test_mnist-pytorch.html"><strong aria-hidden="true">2.3.</strong> 0y_test_mnist-pytorch</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/00_ingest_data.html"><strong aria-hidden="true">3.</strong> student-project-03_group-WikiKG90mv2_00_ingest_data</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/01_fetch_descriptions.html"><strong aria-hidden="true">3.1.</strong> 01_fetch_descriptions</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/02_load_data.html"><strong aria-hidden="true">3.2.</strong> 02_load_data</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/03_data_exploration.html"><strong aria-hidden="true">3.3.</strong> 03_data_exploration</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/04_analysing_relation_types.html"><strong aria-hidden="true">3.4.</strong> 04_analysing_relation_types</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/05_Motif_search_defination_code.html"><strong aria-hidden="true">3.5.</strong> 05_Motif_search_defination_code</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/06_python_analysis.html"><strong aria-hidden="true">3.6.</strong> 06_python_analysis</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/08_pagerank.html"><strong aria-hidden="true">3.7.</strong> 08_pagerank</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/09_outro_discussion.html"><strong aria-hidden="true">3.8.</strong> 09_outro_discussion</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/0x_Motif_search_defination_code.html"><strong aria-hidden="true">3.9.</strong> 0x_Motif_search_defination_code</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-03_group-WikiKG90mv2/student-project-03_group-WikiKG90mv2/10_motif_mining_WikiKGv2.html"><strong aria-hidden="true">3.10.</strong> 10_motif_mining_WikiKGv2</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-04_group-FedMLMedicalApp/student-project-04_group-FedMLMedicalApp/00_Notebook_Presentation.html"><strong aria-hidden="true">4.</strong> student-project-04_group-FedMLMedicalApp_00_Notebook_Presentation</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-04_group-FedMLMedicalApp/student-project-04_group-FedMLMedicalApp/01_BrainTumorSegmentation_Centralized_Training.html"><strong aria-hidden="true">4.1.</strong> 01_BrainTumorSegmentation_Centralized_Training</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-04_group-FedMLMedicalApp/student-project-04_group-FedMLMedicalApp/02_Federated_Learning_BrainTumorSegmentation.html"><strong aria-hidden="true">4.2.</strong> 02_Federated_Learning_BrainTumorSegmentation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-04_group-FedMLMedicalApp/student-project-04_group-FedMLMedicalApp/data_upload_test.html"><strong aria-hidden="true">4.3.</strong> data_upload_test</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/00_introduction.html"><strong aria-hidden="true">5.</strong> student-project-05_group-DistOpt_00_introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/01_Bayesian_optimization.html"><strong aria-hidden="true">5.1.</strong> 01_Bayesian_optimization</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/02_Gaussian_processes.html"><strong aria-hidden="true">5.2.</strong> 02_Gaussian_processes</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/03_acquisition_functions.html"><strong aria-hidden="true">5.3.</strong> 03_acquisition_functions</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/04_scalable_Bayesian_optimization.html"><strong aria-hidden="true">5.4.</strong> 04_scalable_Bayesian_optimization</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/05_implementation_documentation.html"><strong aria-hidden="true">5.5.</strong> 05_implementation_documentation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/06_our_implementation.html"><strong aria-hidden="true">5.6.</strong> 06_our_implementation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-05_group-DistOpt/student-project-05_group-DistOpt/07_additional_code.html"><strong aria-hidden="true">5.7.</strong> 07_additional_code</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-07_group-ExpsZerOInit/student-project-07_group-ExpsZerOInit/00_introduction.html"><strong aria-hidden="true">6.</strong> student-project-07_group-ExpsZerOInit_00_introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-07_group-ExpsZerOInit/student-project-07_group-ExpsZerOInit/01_resnet.html" class="active"><strong aria-hidden="true">6.1.</strong> 01_resnet</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-08_group-WikiSearch/student-project-08_group-WikiSearch/00_Introduction.html"><strong aria-hidden="true">7.</strong> student-project-08_group-WikiSearch_00_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-08_group-WikiSearch/student-project-08_group-WikiSearch/01_InputParsing.html"><strong aria-hidden="true">7.1.</strong> 01_InputParsing</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-08_group-WikiSearch/student-project-08_group-WikiSearch/02_PageRank.html"><strong aria-hidden="true">7.2.</strong> 02_PageRank</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-08_group-WikiSearch/student-project-08_group-WikiSearch/03_QuerySearch.html"><strong aria-hidden="true">7.3.</strong> 03_QuerySearch</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/00_Introduction.html"><strong aria-hidden="true">8.</strong> student-project-09_group-DistEnsembles_00_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/01_Data.html"><strong aria-hidden="true">8.1.</strong> 01_Data</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/02_Main.html"><strong aria-hidden="true">8.2.</strong> 02_Main</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/02_non_distributed.html"><strong aria-hidden="true">8.3.</strong> 02_non_distributed</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/02a_Single_Model.html"><strong aria-hidden="true">8.4.</strong> 02a_Single_Model</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/03_Evaluation.html"><strong aria-hidden="true">8.5.</strong> 03_Evaluation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/03_RDDs.html"><strong aria-hidden="true">8.6.</strong> 03_RDDs</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/03a_Ensemble.html"><strong aria-hidden="true">8.7.</strong> 03a_Ensemble</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-09_group-DistEnsembles/student-project-09_group-DistEnsembles/041_Data_Preprocessing.html"><strong aria-hidden="true">8.8.</strong> 041_Data_Preprocessing</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/00_introduction.html"><strong aria-hidden="true">9.</strong> student-project-10_group-RDI_00_introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/01_prepare_data.html"><strong aria-hidden="true">9.1.</strong> 01_prepare_data</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/02_baseline.html"><strong aria-hidden="true">9.2.</strong> 02_baseline</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/03_single_machine.html"><strong aria-hidden="true">9.3.</strong> 03_single_machine</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-10_group-RDI/student-project-10_group-RDI/04_distributed_learning.html"><strong aria-hidden="true">9.4.</strong> 04_distributed_learning</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../contents/student-project-11_group-CollaborativeFiltering/student-project-11_group-CollaborativeFiltering/01_Introduction.html"><strong aria-hidden="true">10.</strong> student-project-11_group-CollaborativeFiltering_01_Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../contents/student-project-11_group-CollaborativeFiltering/student-project-11_group-CollaborativeFiltering/01a_Implementation.html"><strong aria-hidden="true">10.1.</strong> 01a_Implementation</a></li><li class="chapter-item expanded "><a href="../../../contents/student-project-11_group-CollaborativeFiltering/student-project-11_group-CollaborativeFiltering/02_FirstOrderMethods.html"><strong aria-hidden="true">10.2.</strong> 02_FirstOrderMethods</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="../../../editors.html"><strong aria-hidden="true">11.</strong> Editors</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">sds-3.x/ScaDaMaLe</h1>

                    <div class="right-buttons">
                        <a href="../../../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-pip">install wget
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># !wget https://raw.githubusercontent.com/pytorch/examples/main/word_language_model/data/wikitext-2/train.txt
# !wget https://raw.githubusercontent.com/pytorch/examples/main/word_language_model/data/wikitext-2/valid.txt
# !wget https://raw.githubusercontent.com/pytorch/examples/main/word_language_model/data/wikitext-2/test.txt
# !mkdir /dbfs/ml/Group_7/wikitext-2/
# !mv train.txt valid.txt test.txt /dbfs/ml/Group_7/wikitext-2/
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">import torch
from torch import nn
from scipy.linalg import hadamard
import numpy as np

def partial_identity(out_dim, in_dim):
    &quot;&quot;&quot;Return the partial identity matrix with shape `out_dim` x `in_dim`.&quot;&quot;&quot;
    if out_dim &lt; in_dim:
        I = torch.eye(out_dim)
        O = torch.zeros(out_dim, (in_dim - out_dim))

        return torch.cat((I, O), 1)
    
    elif out_dim == in_dim:
        return torch.eye(out_dim)
    
    else:
        I = torch.eye(in_dim)
        O = torch.zeros((out_dim - in_dim), in_dim)
        return torch.cat((I, O), 0)


def zerO_init_conv_layer_(weight):
    &quot;&quot;&quot;
    In-place initialise the given convolutional layer with zerO-init 
    using the following equation:
    ---------------------------------
    W[:,:,n,n] := c * I_p * H_m * I_p
    ---------------------------------
     where W:   out_dim x in_dim x n_filters
           I_p: out_dim x m  (partial identity)
           H_m: m x m        (Hadamard matrix)
           I_p: m x in_dim   (partial identity)
    &quot;&quot;&quot;
    out_dim, in_dim, k = weight.shape[:3]
    n = int(np.floor(k / 2))
    
    if out_dim == in_dim:
        weight.data[..., n, n] = torch.eye(in_dim)
    elif out_dim &lt; in_dim:
        weight.data[..., n, n] = partial_identity(out_dim, in_dim).type_as(weight)
    else:
        m = int(np.ceil(np.log2(out_dim)))
        c = 2 ** (-(m - 1) / 2)

        H = lambda dim: torch.tensor(hadamard(dim)).type_as(weight)

        weight.data = (
            c * H(2**m)[:out_dim, :in_dim]
        )
</code></pre>
</div>
<div class="cell markdown">
<p>We also apply ZerO to Transformer and evaluate it on WikiText-2 dataset (Vaswani et al., 2017). In each Transformer layer, we use ZerO to initialize both multi-head attention and feed-forward layers. Because the embedding size is fixed in the multi-head attention, we initialize the projection matrix of queries W<em>Q as identity and the projection matrices of keys and values W</em>K, W_V at zero. For the feed-forward layers, we initialize the connection matrices according to their hidden dimensions using Algorithm 1.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def zerO_init_multihead_attention(name, p):
    if name.endswith(&quot;.q_proj_weight&quot;):
        nn.init.eye_(p)
    if name.endswith(&quot;.k_proj_weight&quot;) or name.endswith(&quot;.v_proj_weight&quot;):
        nn.init.zeros_(p)

def zerO_init_model(model):
    for name, p in model.named_parameters():
        zerO_init_multihead_attention(name, p)
        
    for name, m in model.named_modules():
        if isinstance(m, nn.Linear):
            zerO_init(m.weight)
    return model
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># Modified version of MultiheadAttention. There will be three separate matrices for queries, keys and values, regardless of dimensionality.
# We need this because the queries are initialized differently than the keys and values.

from torch.nn import MultiheadAttention
from typing import Optional
from torch.nn.parameter import Parameter
from torch.nn import Linear

# From torch/nn/modules/linear.py

class NonDynamicallyQuantizableLinear(Linear):
    def __init__(self, in_features: int, out_features: int, bias: bool = True,
                 device=None, dtype=None) -&gt; None:
        super().__init__(in_features, out_features, bias=bias,
                         device=device, dtype=dtype)

# Based on torch.nn.MultiheadAttention

class ModifiedMultiheadAttention(MultiheadAttention):
    __constants__ = ['batch_first']
    bias_k: Optional[torch.Tensor]
    bias_v: Optional[torch.Tensor]

    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False,
                 kdim=None, vdim=None, batch_first=False, device=None, dtype=None) -&gt; None:
        factory_kwargs = {'device': device, 'dtype': dtype}
        super(MultiheadAttention, self).__init__()
        self.embed_dim = embed_dim
        self.kdim = kdim if kdim is not None else embed_dim
        self.vdim = vdim if vdim is not None else embed_dim
        self._qkv_same_embed_dim = False # Changed here

        self.num_heads = num_heads
        self.dropout = dropout
        self.batch_first = batch_first
        self.head_dim = embed_dim // num_heads
        assert self.head_dim * num_heads == self.embed_dim, &quot;embed_dim must be divisible by num_heads&quot;

        if not self._qkv_same_embed_dim:
            self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim), **factory_kwargs))
            self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim), **factory_kwargs))
            self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim), **factory_kwargs))
            self.register_parameter('in_proj_weight', None)
        else:
            self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim), **factory_kwargs))
            self.register_parameter('q_proj_weight', None)
            self.register_parameter('k_proj_weight', None)
            self.register_parameter('v_proj_weight', None)

        if bias:
            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim, **factory_kwargs))
        else:
            self.register_parameter('in_proj_bias', None)
        self.out_proj = NonDynamicallyQuantizableLinear(embed_dim, embed_dim, bias=bias, **factory_kwargs)

        if add_bias_kv:
            self.bias_k = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))
            self.bias_v = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))
        else:
            self.bias_k = self.bias_v = None

        self.add_zero_attn = add_zero_attn

        self._reset_parameters()
        


from typing import Union, Callable
from torch import Tensor
from torch.nn import Dropout
from torch.nn import Linear
from torch.nn import LayerNorm
from torch.nn import functional as F
from torch.nn import TransformerEncoderLayer

# From torch/nn/modules/transformer.py

def _get_activation_fn(activation: str) -&gt; Callable[[Tensor], Tensor]:
    if activation == &quot;relu&quot;:
        return F.relu
    elif activation == &quot;gelu&quot;:
        return F.gelu

    raise RuntimeError(&quot;activation should be relu/gelu, not {}&quot;.format(activation))

# Based on torch.nn.TransformerEncoderLayer

class ModifiedTransformerEncoderLayer(TransformerEncoderLayer):
    __constants__ = ['batch_first', 'norm_first']

    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1,
                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,
                 layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,
                 device=None, dtype=None, zero_init=False) -&gt; None:
        factory_kwargs = {'device': device, 'dtype': dtype}
        super(TransformerEncoderLayer, self).__init__()
        if zero_init: # Changed here
            self.self_attn = ModifiedMultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,
                                            **factory_kwargs)
        else:
            self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,
                                            **factory_kwargs)
        # Implementation of Feedforward model
        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)
        self.dropout = Dropout(dropout)
        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)

        self.norm_first = norm_first
        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)
        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)
        self.dropout1 = Dropout(dropout)
        self.dropout2 = Dropout(dropout)

        # Legacy string support for activation function.
        if isinstance(activation, str):
            activation = _get_activation_fn(activation)

        # We can't test self.activation in forward() in TorchScript,
        # so stash some information about it instead.
        if activation is F.relu or isinstance(activation, torch.nn.ReLU):
            self.activation_relu_or_gelu = 1
        elif activation is F.gelu or isinstance(activation, torch.nn.GELU):
            self.activation_relu_or_gelu = 2
        else:
            self.activation_relu_or_gelu = 0
        self.activation = activation
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">##### Source: https://github.com/pytorch/examples/tree/main/word_language_model

# data.py

import os
from io import open
import torch

class Dictionary(object):
    def __init__(self):
        self.word2idx = {}
        self.idx2word = []

    def add_word(self, word):
        if word not in self.word2idx:
            self.idx2word.append(word)
            self.word2idx[word] = len(self.idx2word) - 1
        return self.word2idx[word]

    def __len__(self):
        return len(self.idx2word)


class Corpus(object):
    def __init__(self, path):
        self.dictionary = Dictionary()
        self.train = self.tokenize(os.path.join(path, 'train.txt'))
        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))
        self.test = self.tokenize(os.path.join(path, 'test.txt'))

    def tokenize(self, path):
        &quot;&quot;&quot;Tokenizes a text file.&quot;&quot;&quot;
        assert os.path.exists(path)
        # Add words to the dictionary
        with open(path, 'r', encoding=&quot;utf8&quot;) as f:
            for line in f:
                words = line.split() + ['&lt;eos&gt;']
                for word in words:
                    self.dictionary.add_word(word)

        # Tokenize file content
        with open(path, 'r', encoding=&quot;utf8&quot;) as f:
            idss = []
            for line in f:
                words = line.split() + ['&lt;eos&gt;']
                ids = []
                for word in words:
                    ids.append(self.dictionary.word2idx[word])
                idss.append(torch.tensor(ids).type(torch.int64))
            ids = torch.cat(idss)

        return ids
    
    
    
    
# model.py (with slight modification to incorporate the changes above)
    
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl

# Temporarily leave PositionalEncoding module here. Will be moved somewhere else.
class PositionalEncoding(nn.Module):
    r&quot;&quot;&quot;Inject some information about the relative or absolute position of the tokens in the sequence.
        The positional encodings have the same dimension as the embeddings, so that the two can be summed.
        Here, we use sine and cosine functions of different frequencies.
    .. math:
        \text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))
        \text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))
        \text{where pos is the word position and i is the embed idx)
    Args:
        d_model: the embed dim (required).
        dropout: the dropout value (default=0.1).
        max_len: the max. length of the incoming sequence (default=5000).
    Examples:
        &gt;&gt;&gt; pos_encoder = PositionalEncoding(d_model)
    &quot;&quot;&quot;

    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        r&quot;&quot;&quot;Inputs of forward function
        Args:
            x: the sequence fed to the positional encoder model (required).
        Shape:
            x: [sequence length, batch size, embed dim]
            output: [sequence length, batch size, embed dim]
        Examples:
            &gt;&gt;&gt; output = pos_encoder(x)
        &quot;&quot;&quot;

        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

class TransformerModel(pl.LightningModule):
    &quot;&quot;&quot;Container module with an encoder, a recurrent or transformer module, and a decoder.&quot;&quot;&quot;
    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.2, learning_rate=20, zero_init=False):
        super(TransformerModel, self).__init__()
        try:
            from torch.nn import TransformerEncoder
        except:
            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')

        self.learning_rate = learning_rate
        self.model_type = 'Transformer'
        self.src_mask = None
        self.pos_encoder = PositionalEncoding(ninp, dropout)
        encoder_layers = ModifiedTransformerEncoderLayer(ninp, nhead, nhid, dropout, zero_init=zero_init)
        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)
        self.encoder = nn.Embedding(ntoken, ninp)
        self.ninp = ninp
        self.decoder = nn.Linear(ninp, ntoken)
        self.ntokens = ntoken
        self.init_weights()
        self.loss = nn.NLLLoss()

    def _generate_square_subsequent_mask(self, sz):
        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
        return mask

    def init_weights(self):
        initrange = 0.1
        nn.init.uniform_(self.encoder.weight, -initrange, initrange)
        nn.init.zeros_(self.decoder.bias)
        nn.init.uniform_(self.decoder.weight, -initrange, initrange)

    def forward(self, src, has_mask=True):
        if has_mask:
            device = src.device
            if self.src_mask is None or self.src_mask.size(0) != len(src):
                mask = self._generate_square_subsequent_mask(len(src)).to(device)
                self.src_mask = mask
        else:
            self.src_mask = None

        src = self.encoder(src) * math.sqrt(self.ninp)
        src = self.pos_encoder(src)
        output = self.transformer_encoder(src, self.src_mask)
        output = self.decoder(output)
        return F.log_softmax(output, dim=-1)
    
    def training_step(self, batch, batch_idx):
        data, target = batch
        data.squeeze_(0)
        target.squeeze_(0)
        output = self(data).view(-1, self.ntokens)
        loss = self.loss(output, target)
        self.log(&quot;train_loss&quot;, loss)
        if not batch_idx % 965:
            print(f&quot;EPOCH {self.trainer.current_epoch} BATCH {batch_idx} LOSS {self.trainer.callback_metrics['train_loss']}&quot;)
        return loss

   # def validation_step(self, batch, batch_idx):
#         data, target = batch
#         data.squeeze_(0)
#         target.squeeze_(0)
#         output = self(data).view(-1, self.ntokens)
#         loss = self.loss(output, target)  * len(data)

#         self.log(&quot;val_loss&quot;, loss)

#         return loss
# def on_validation_epoch_end(self):
#     print(f&quot;VALIDATION LOSS: {self.trainer.callback_metrics['val_loss']}&quot;)
    def test_step(self, batch, batch_idx):
        data, target = batch
        data.squeeze_(0)
        target.squeeze_(0)

        output = self(data).view(-1, self.ntokens)
        loss = self.loss(output, target)  * len(data)

        self.log(&quot;test_loss&quot;, loss)

        return loss

    def configure_optimizers(self):
        optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate)
        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [10], gamma=0.25)
        return [optimizer], [scheduler]
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">!wget https://raw.githubusercontent.com/pytorch/examples/main/word_language_model/data/wikitext-2/train.txt
!wget https://raw.githubusercontent.com/pytorch/examples/main/word_language_model/data/wikitext-2/valid.txt
!wget https://raw.githubusercontent.com/pytorch/examples/main/word_language_model/data/wikitext-2/test.txt
!mkdir /dbfs/ml/Group_7/wikitext-2/
!mv train.txt valid.txt test.txt /dbfs/ml/Group_7/wikitext-2/
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">import pytorch_lightning as pl
import time
###############################################################################
# Load data
###############################################################################

# Starting from sequential data, batchify arranges the dataset into columns.
# For instance, with the alphabet as the sequence and batch size 4, we'd get
# ┌ a g m s ┐
# │ b h n t │
# │ c i o u │
# │ d j p v │
# │ e k q w │
# └ f l r x ┘.
# These columns are treated as independent by the model, which means that the
# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient
# batch processing.


class WikiDataset(torch.utils.data.Dataset):
    def __init__(self, corpus_data, batch_size, bptt = 35):
        self.bptt = bptt
        self.data = self.batchify(corpus_data, batch_size)
        
    def batchify(self, data, bsz):
        # Work out how cleanly we can divide the dataset into bsz parts.
        nbatch = data.size(0) // bsz
        # Trim off any extra elements that wouldn't cleanly fit (remainders).
        data = data.narrow(0, 0, nbatch * bsz)
        # Evenly divide the data across the bsz batches.
        data = data.view(bsz, -1).t().contiguous()
        return data

    def __getitem__(self, i):  
        i *= self.bptt
        seq_len = min(self.bptt, len(self.data) - 1 - i)
        data = self.data[i:i+seq_len]
        target = self.data[i+1:i+1+seq_len].view(-1)
        return data, target  
    
    def __len__(self):
        return (len(self.data) - 1) // self.bptt

class WikiDataModule(pl.LightningDataModule):
    def __init__(self, train_batch_size, eval_batch_size = 10, bptt = 35, data_dir = &quot;./wikitext-2/&quot;):
        super().__init__()
        self.corpus = Corpus(data_dir)
        self.train_data = WikiDataset(self.corpus.train, train_batch_size, bptt)
        # self.val_data = WikiDataset(self.corpus.valid, eval_batch_size, bptt)
        self.test_data = WikiDataset(self.corpus.test, eval_batch_size, bptt)
        self.train_batch_size = train_batch_size
        self.eval_batch_size = eval_batch_size
        
    def train_dataloader(self):
        return torch.utils.data.DataLoader(self.train_data, batch_size=1, num_workers=4)

    # def val_dataloader(self):
    #     return torch.utils.data.DataLoader(self.val_data, batch_size=1, num_workers=4)

    def test_dataloader(self):
        return torch.utils.data.DataLoader(self.test_data, batch_size=1, num_workers=4)

###############################################################################
# Build the model
###############################################################################


###############################################################################
# Training code
###############################################################################

def repackage_hidden(h):
    &quot;&quot;&quot;Wraps hidden states in new Tensors, to detach them from their history.&quot;&quot;&quot;

    if isinstance(h, torch.Tensor):
        return h.detach()
    else:
        return tuple(repackage_hidden(v) for v in h)


# get_batch subdivides the source data into chunks of length args.bptt.
# If source is equal to the example output of the batchify function, with
# a bptt-limit of 2, we'd get the following two Variables for i = 0:
# ┌ a g m s ┐ ┌ b h n t ┐
# └ b h n t ┘ └ c i o u ┘
# Note that despite the name of the function, the subdivison of data is not
# done along the batch dimension (i.e. dimension 1), since that was handled
# by the batchify function. The chunks are along dimension 0, corresponding
# to the seq_len dimension in the LSTM.

from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor
from pytorch_lightning.loggers import TensorBoardLogger
import os
import wget
def get_batch(source, i):
    seq_len = min(args.bptt, len(source) - 1 - i)
    data = source[i:i+seq_len]
    target = source[i+1:i+1+seq_len].view(-1)
    return data, target


def evaluate(data_source):
    # Turn on evaluation mode which disables dropout.
    model.eval()
    total_loss = 0.
    ntokens = len(corpus.dictionary)
    with torch.no_grad():
        for i in range(0, data_source.size(0) - 1, args.bptt):
            data, targets = get_batch(data_source, i)
            
            output = model(data)
            output = output.view(-1, ntokens)
            
            total_loss += len(data) * criterion(output, targets).item()
    return total_loss / (len(data_source) - 1)


def train(emsize=200, nhead=2, nhid=200, nlayers=2, dropout=0.2, zero_init=False, max_epochs=20, learning_rate=20):
    # Turn on training mode which enables dropout.
    start_time = time.time()
    if not os.path.isfile(&quot;./wikitext-2/train.txt&quot;):
        wget.download(&quot;https://raw.githubusercontent.com/pytorch/examples/main/word_language_model/data/wikitext-2/train.txt&quot;)
        wget.download(&quot;https://raw.githubusercontent.com/pytorch/examples/main/word_language_model/data/wikitext-2/valid.txt&quot;)
        wget.download(&quot;https://raw.githubusercontent.com/pytorch/examples/main/word_language_model/data/wikitext-2/test.txt&quot;)
        os.makedirs(&quot;wikitext-2&quot;)
        os.rename(&quot;train.txt&quot;, &quot;./wikitext-2/train.txt&quot;)
        os.rename(&quot;valid.txt&quot;, &quot;./wikitext-2/valid.txt&quot;)
        os.rename(&quot;test.txt&quot;, &quot;./wikitext-2/test.txt&quot;)


    dm = WikiDataModule(train_batch_size=20)
    ntokens = len(dm.corpus.dictionary)
    model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout, learning_rate=learning_rate, zero_init=zero_init)
    logger = TensorBoardLogger(save_dir=f&quot;/dbfs/ml/Group_7/logs/transformer/{'zeroinit' if zero_init else 'default'}&quot;)

    trainer = pl.Trainer(max_epochs=20,
        strategy=&quot;horovod&quot;,
        accelerator='gpu',
        num_sanity_val_steps=0,

        devices=1,
        gradient_clip_val = 0.25,
        callbacks=[
            # EarlyStopping(monitor=&quot;val_acc&quot;, min_delta=0.00, patience=3, verbose=False, mode=&quot;max&quot;),
            # TQDMProgressBar(refresh_rate=10),
            ModelCheckpoint(monitor='val_loss', mode='min'),
            LearningRateMonitor(logging_interval='epoch'),
        ],
        logger=logger,
        enable_progress_bar=False
        )   
    trainer.fit(model, dm)

    if hvd.rank() == 0:
        trainer.test(ckpt_path='best', datamodule=dm)
        test_loss = trainer.callback_metrics[&quot;test_loss&quot;]
        with open(&quot;/dbfs/ml/Group_7/transformer_results.txt&quot;, &quot;a&quot;) as f:
            f.write(f&quot;TEST LOSS: {test_loss} with nlayers = {nlayers} and zeroinit {zero_init}&quot;)
        print(test_loss)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">import horovod.torch as hvd
from sparkdl import HorovodRunner
import json

numproc = 2
assert numproc in (1,2)
hr = HorovodRunner(np=numproc, driver_log_verbosity='all') 
hr.run(train, nlayers=2, zero_init=True, max_epochs=20//numproc, learning_rate=20//numproc)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-pip">install nvgpu
</code></pre>
</div>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../../../contents/student-project-07_group-ExpsZerOInit/student-project-07_group-ExpsZerOInit/00_introduction.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next" href="../../../contents/student-project-08_group-WikiSearch/student-project-08_group-WikiSearch/00_Introduction.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../../../contents/student-project-07_group-ExpsZerOInit/student-project-07_group-ExpsZerOInit/00_introduction.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next" href="../../../contents/student-project-08_group-WikiSearch/student-project-08_group-WikiSearch/00_Introduction.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="../../../elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="../../../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->


    </body>
</html>
