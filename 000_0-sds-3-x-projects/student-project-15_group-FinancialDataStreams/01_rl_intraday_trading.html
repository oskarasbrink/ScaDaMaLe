
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Reinforcement Learning for Intraday Trading &#8212; Scalable Data Science &amp; Distributed Machine Learning</title>
    
  <link rel="stylesheet" href="../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Reinforcement Learning for Intraday Trading - Distributed model tuning with Elephas" href="02_rl_intraday_trading_elephas.html" />
    <link rel="prev" title="Reinforcement Learning for Intraday Trading" href="00_video.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Scalable Data Science & Distributed Machine Learning</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/000_ScaDaMaLe.html">
   Introduction
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Introduction to Apache Spark Core
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/001_whySpark.html">
   Why Apache Spark?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/002_00_loginToDatabricks.html">
   databricks community edition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/002_00_loginToDatabricks.html#essentials-of-databricks-cloud-dbc-in-a-big-hurry">
   Essentials of Databricks Cloud (DBC) in a Big Hurry
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/002_00_loginToDatabricks.html#dbc-essentials-what-is-databricks-cloud">
   DBC Essentials: What is Databricks Cloud?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/002_00_loginToDatabricks.html#dbc-essentials-shard-cluster-notebook-and-dashboard">
   DBC Essentials: Shard, Cluster, Notebook and Dashboard
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/002_00_loginToDatabricks.html#dbc-essentials-team-state-collaboration-elastic-resources">
   DBC Essentials: Team, State, Collaboration, Elastic Resources
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/002_00_loginToDatabricks.html#you-should-all-have-databricks-community-edition-account-by-now">
   You Should All Have databricks community edition account by now!
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/002_00_loginToDatabricks.html#import-course-content-now">
   Import Course Content Now!
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/002_00_loginToDatabricks.html#cloud-free-computing-environment-optional-but-recommended">
   Cloud-free Computing Environment (Optional but recommended)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/002_01_multiLingualNotebooks.html">
   Notebooks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/002_01_multiLingualNotebooks.html#further-reference-homework-recurrrent-points-of-reference">
   Further Reference / Homework / Recurrrent Points of Reference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/003_00_scalaCrashCourse.html">
   Introduction to Scala through Scala Notebook
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/003_00_scalaCrashCourse.html#scala-in-your-own-computer">
   Scala in your own computer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/003_00_scalaCrashCourse.html#scala-resources">
   Scala Resources
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/003_00_scalaCrashCourse.html#introduction-to-scala">
   Introduction to Scala
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/003_00_scalaCrashCourse.html#let-s-get-our-hands-dirty-in-scala">
   Letâ€™s get our hands dirty in Scala
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/003_00_scalaCrashCourse.html#scala-types">
   Scala Types
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/003_00_scalaCrashCourse.html#expressions">
   Expressions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/003_00_scalaCrashCourse.html#blocks">
   Blocks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/003_00_scalaCrashCourse.html#functions">
   Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/003_00_scalaCrashCourse.html#methods">
   Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/003_00_scalaCrashCourse.html#classes">
   Classes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/003_00_scalaCrashCourse.html#case-classes">
   Case Classes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/003_00_scalaCrashCourse.html#objects">
   Objects
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/003_00_scalaCrashCourse.html#traits">
   Traits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/003_00_scalaCrashCourse.html#main-method">
   Main Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/003_00_scalaCrashCourse.html#what-i-try-not-do-while-learning-a-new-language">
   What I try not do while learning a new language?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/003_01_scalaCrashCourse.html">
   Scala Resources
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/003_01_scalaCrashCourse.html#let-s-continue-to-get-our-hands-dirty-in-scala">
   Letâ€™s continue to get our hands dirty in Scala
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/003_01_scalaCrashCourse.html#scala-type-hierarchy">
   Scala Type Hierarchy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/003_01_scalaCrashCourse.html#scala-collections">
   Scala Collections
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/003_01_scalaCrashCourse.html#exercise-in-functional-programming">
   Exercise in Functional Programming
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/003_01_scalaCrashCourse.html#lazy-evaluation">
   Lazy Evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/003_01_scalaCrashCourse.html#recursions">
   Recursions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/004_RDDsTransformationsActions.html">
   Introduction to Spark
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/004_RDDsTransformationsActions.html#spark-cluster-overview">
   Spark Cluster Overview:
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/005_RDDsTransformationsActionsHOMEWORK.html">
   HOMEWORK notebook - RDDs Transformations and Actions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/006a_PipedRDD.html">
   Piped RDDs and Bayesian AB Testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/006a_PipedRDD.html#from-a-local-collection">
   From a Local Collection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/006a_PipedRDD.html#glom">
   glom
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/006a_PipedRDD.html#checkpointing">
   Checkpointing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/006a_PipedRDD.html#pipe-rdds-to-system-commands">
   Pipe RDDs to System Commands
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/006a_PipedRDD.html#mappartitions">
   mapPartitions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/006a_PipedRDD.html#foreachpartition">
   foreachPartition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/006a_PipedRDD.html#numerically-rigorous-bayesian-ab-testing">
   Numerically Rigorous Bayesian AB Testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/006a_PipedRDD.html#usage-instructions-for-isit1or2coins">
   Usage instructions for IsIt1or2Coins
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/006_WordCount.html">
   Word Count on US State of the Union (SoU) Addresses
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Introduction to Apache Spark SQL
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/007_SparkSQLIntroBasics.html">
   Introduction to Spark SQL
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/007_SparkSQLIntroBasics.html#overview">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/007_SparkSQLIntroBasics.html#datasets-and-dataframes">
   Datasets and DataFrames
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/007_SparkSQLIntroBasics.html#getting-started-in-spark-2-x">
   Getting Started in Spark 2.x
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/007a_SparkSQLProgGuide_HW.html">
   Spark Sql Programming Guide
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/007b_SparkSQLProgGuide_HW.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/007b_SparkSQLProgGuide_HW.html#id1">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/007c_SparkSQLProgGuide_HW.html">
   Getting Started - Exercise
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/007d_SparkSQLProgGuide_HW.html">
   Data Sources
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/007d_SparkSQLProgGuide_HW.html#id1">
   Data Sources
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/007e_SparkSQLProgGuide_HW.html">
   Performance Tuning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/007f_SparkSQLProgGuide_HW.html">
   Distributed SQL Engine
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/007f_SparkSQLProgGuide_HW.html#id1">
   Distributed SQL Engine
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/007g_PivotInSQL.html">
   ScaDaMaLe, Scalable Data Science and Distributed Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/007g_PivotInSQL.html#sql-pivoting-since-spark-2-4">
   SQL Pivoting since Spark 2.4
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/007g_PivotInSQL.html#load-data">
   Load Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/007g_PivotInSQL.html#pivoting-in-sql">
   Pivoting in SQL
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/007g_PivotInSQL.html#pivoting-with-multiple-aggregate-expressions">
   Pivoting with Multiple Aggregate Expressions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/007g_PivotInSQL.html#pivoting-with-multiple-grouping-columns">
   Pivoting with Multiple Grouping Columns
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/007g_PivotInSQL.html#pivoting-with-multiple-pivot-columns">
   Pivoting with Multiple Pivot Columns
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/008_DiamondsPipeline_01ETLEDA.html">
   Diamonds ML Pipeline Workflow - DataFrame ETL and EDA Part
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/008_DiamondsPipeline_01ETLEDA.html#step-1-load-data-as-dataframe">
   Step 1. Load data as DataFrame
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/008_DiamondsPipeline_01ETLEDA.html#step-2-understand-the-data">
   Step 2. Understand the data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/008_DiamondsPipeline_01ETLEDA.html#let-us-run-through-some-basic-inteactive-sql-queries-next">
   Let us run through some basic inteactive SQL queries next
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/008_DiamondsPipeline_01ETLEDA.html#why-do-we-need-to-know-these-interactive-sql-queries">
   Why do we need to know these interactive SQL queries?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/008_DiamondsPipeline_01ETLEDA.html#we-will-continue-later-with-ml-pipelines-to-do-prediction-with-a-fitted-model-from-this-dataset">
   We will continue later with ML pipelines to do prediction with a fitted model from this dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/009_PowerPlantPipeline_01ETLEDA.html">
   Power Plant ML Pipeline Application - DataFrame Part
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/009_PowerPlantPipeline_01ETLEDA.html#step-1-business-understanding">
   Step 1: Business Understanding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/009_PowerPlantPipeline_01ETLEDA.html#step-2-load-your-data">
   Step 2: Load Your Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/009_PowerPlantPipeline_01ETLEDA.html#use-this-for-other-smallish-datasets-you-want-to-import-to-your-ce">
   USE THIS FOR OTHER SMALLish DataSets you want to import to your CE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/009_PowerPlantPipeline_01ETLEDA.html#step-3-explore-your-data">
   Step 3: Explore Your Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/009_PowerPlantPipeline_01ETLEDA.html#step-4-visualize-your-data">
   Step 4: Visualize Your Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/010_wikipediaClickStream_01ETLEDA.html">
   Wiki Clickstream Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/010_wikipediaClickStream_01ETLEDA.html#wikipedia-logo">
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/033_OBO_LoadExtract.html">
   Old Bailey Online Data Analysis in Apache Spark
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/033_OBO_LoadExtract.html#analysing-the-full-old-bailey-online-sessions-papers-dataset">
   Analysing the Full Old Bailey Online Sessions Papers Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/033_OBO_PipedRDD_RigorousBayesianABTesting.html">
   Piped RDDs and Bayesian AB Testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/033_OBO_PipedRDD_RigorousBayesianABTesting.html#from-a-local-collection">
   From a Local Collection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/033_OBO_PipedRDD_RigorousBayesianABTesting.html#glom">
   glom
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/033_OBO_PipedRDD_RigorousBayesianABTesting.html#checkpointing">
   Checkpointing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/033_OBO_PipedRDD_RigorousBayesianABTesting.html#pipe-rdds-to-system-commands">
   Pipe RDDs to System Commands
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/033_OBO_PipedRDD_RigorousBayesianABTesting.html#mappartitions">
   mapPartitions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/033_OBO_PipedRDD_RigorousBayesianABTesting.html#foreachpartition">
   foreachPartition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/033_OBO_PipedRDD_RigorousBayesianABTesting.html#numerically-rigorous-bayesian-ab-testing">
   Numerically Rigorous Bayesian AB Testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/033_OBO_PipedRDD_RigorousBayesianABTesting.html#usage-instructions-for-isit1or2coins">
   Usage instructions for IsIt1or2Coins
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/033_OBO_PipedRDD_RigorousBayesianABTesting.html#parsing-the-output-from-isit1or2coins">
   Parsing the output from
   <code class="docutils literal notranslate">
    <span class="pre">
     IsIt1or2Coins
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/033_OBO_PipedRDD_RigorousBayesianABTesting.html#providing-case-classes-for-input-and-output-for-easy-spark-communication">
   Providing case classes for input and output for easy spark communication
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/035_LDA_CornellMovieDialogs.html">
   Topic Modeling of Movie Dialogs with Latent Dirichlet Allocation
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Distribute Deep Learning with Horovod
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_7-sds-3-x-ddl/00_DDL_Introduction.html">
   Introduction to Distributed Deep Learning (DDL) with Horovod over Tensorflow/keras and Pytorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_7-sds-3-x-ddl/0x_mnist-tensorflow-keras.html">
   Distributed deep learning training using TensorFlow and Keras with HorovodRunner
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_7-sds-3-x-ddl/0y_mnist-pytorch.html">
   Distributed deep learning training using PyTorch with HorovodRunner for MNIST
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  WASP AI-Track Student Projects
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/035_LDA_CornellMovieDialogs.html">
   Topic Modeling of Movie Dialogs with Latent Dirichlet Allocation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-01_group-TheTwoCultures/00_download_data.html">
   The two cultures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-01_group-TheTwoCultures/01_load_data.html">
   Preprocessing and loading the relevant data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-01_group-TheTwoCultures/02_logisticregression.html">
   The two cultures - Classifying threads with logistic regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-01_group-TheTwoCultures/03_word2vec.html">
   Classification using Word2Vec
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-01_group-TheTwoCultures/04_LDA.html">
   Topic Modeling with LDA
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-02_group-LiUUmeaSceneGraphMotifs/01_Loading_GQA-JSON.html">
   Reading GQA JSON files
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-02_group-LiUUmeaSceneGraphMotifs/02_SceneGraphMotifs.html">
   Exploring the GQA Scene Graph Dataset Structure and Properties
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-02_group-LiUUmeaSceneGraphMotifs/02_SceneGraphMotifs.html#general-discussion">
   General discussion
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-03_group-GuangyiZhang/01_triads.html">
   Signed Triads in Social Media
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-04_group-DistributedLinearAlgebra/01_DistributedSVD.html">
   Distributed singular value decomposition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-04_group-DistributedLinearAlgebra/02_CollaborativeFiltering.html">
   Music Recommender System
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-05_group-LundDirichletAnalysts/01_Wikipedia_LDA_Analysis.html">
   Wikipedia analysis using Latent Dirichlet Allocation (LDA)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-05_group-LundDirichletAnalysts/01_Wikipedia_LDA_Analysis.html#folders-and-files">
   Folders and Files
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-05_group-LundDirichletAnalysts/01_Wikipedia_LDA_Analysis.html#downloading">
   Downloading
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-05_group-LundDirichletAnalysts/01_Wikipedia_LDA_Analysis.html#extracting-and-filtering">
   Extracting and Filtering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-05_group-LundDirichletAnalysts/01_Wikipedia_LDA_Analysis.html#prepare-for-lda">
   Prepare for LDA
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-05_group-LundDirichletAnalysts/01_Wikipedia_LDA_Analysis.html#lda">
   LDA
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-05_group-LundDirichletAnalysts/01_Wikipedia_LDA_Analysis.html#looking-at-the-model">
   Looking at the model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-06_group-ParticleClustering/00_Introduction.html">
   Unsupervised clustering of particle physics data with distributed training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-06_group-ParticleClustering/00_Introduction.html#introduction">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-06_group-ParticleClustering/00_Introduction.html#background">
   Background
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-06_group-ParticleClustering/00_Introduction.html#the-uclusted-algorithm">
   The UClusted algorithm
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-06_group-ParticleClustering/00_Introduction.html#motivation">
   Motivation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-06_group-ParticleClustering/00_Introduction.html#our-contribution">
   Our contribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-06_group-ParticleClustering/01_data_and_preprocessing.html">
   Important!
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-06_group-ParticleClustering/01_data_and_preprocessing.html#important-continued-from-above">
   Important (continued from above)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-06_group-ParticleClustering/04_evaluate.html">
   Evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-07_group-MathAtKTH/01_Coding_Motifs.html">
   Motif Finding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-07_group-MathAtKTH/01_Coding_Motifs.html#application">
   Application
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-07_group-MathAtKTH/02_Data_Processing.html">
   Data Processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-07_group-MathAtKTH/03_graph_string_converter.html">
   Intro
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-07_group-MathAtKTH/03_graph_string_converter.html#examples">
   Examples
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-08_group-DistributedEnsemble/00_Introduction.html">
   Lit
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-08_group-DistributedEnsemble/00_Introduction.html#docs">
   Docs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-08_group-DistributedEnsemble/model_2.html">
   MLP with auto-inferred
   <code class="docutils literal notranslate">
    <span class="pre">
     shapes
    </span>
   </code>
   param
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-08_group-DistributedEnsemble/project.html">
   ScaDaMaLe project: Distributed ensembles
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-08_group-DistributedEnsemble/project.html#imports">
   Imports
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-08_group-DistributedEnsemble/project.html#data">
   Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-08_group-DistributedEnsemble/project.html#distributed-ensemble-of-neural-networks">
   Distributed ensemble of neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-08_group-DistributedEnsemble/project.html#distributed-ensembles-prediction-api">
   Distributed ensembles prediction API
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-08_group-DistributedEnsemble/project.html#application-example-distributed-predictions">
   Application example: Distributed predictions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-08_group-DistributedEnsemble/project.html#application-example-out-of-distribution-detection">
   Application example: Out of distribution detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-09_group-TopicModeling/01_Introduction.html">
   Topic Modeling with SARS-Cov-2 Genome ðŸ§¬
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-09_group-TopicModeling/02_Data_Processing.html">
   Extract (overlapping [since yet do not know which parts corresponds to coding regions]) 3-mers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-09_group-TopicModeling/03_LDA.html">
   Load processed data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-09_group-TopicModeling/03_LDA.html#format-data-for-apache-spark-mllib-clustering-lda-adapted-from-the-lda-course-tutorial-034lda20newsgroupssmall">
   Format data for apache.spark.mllib.clustering.LDA (adapted from the LDA course tutorial â€˜034
   <em>
    LDA
   </em>
   20NewsGroupsSmallâ€™)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-09_group-TopicModeling/03_LDA.html#visualise-results">
   Visualise Results
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-09_group-TopicModeling/04_Classification_CountVector.html">
   Load processed data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-09_group-TopicModeling/04_Classification_CountVector.html#format-data">
   Format data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-09_group-TopicModeling/04_Classification_CountVector.html#classification">
   Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-09_group-TopicModeling/04_Classification_CountVector.html#evaluation">
   Evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-09_group-TopicModeling/05_Classification.html">
   Load processed data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-09_group-TopicModeling/05_Classification.html#explore-data">
   Explore data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-09_group-TopicModeling/05_Classification.html#classification">
   Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-09_group-TopicModeling/05_Classification.html#evaluation">
   Evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-09_group-TopicModeling/06_Results.html">
   Results and Conclusion
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-10_group-Geosmus/01_Introduction.html">
   Twitter Streaming Using Geolocation and Emoji Based Sentiment Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-10_group-Geosmus/02_ClusteringEmoticonsBasedOnTweets.html">
   Clustering emoticons based on tweets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-10_group-Geosmus/03_Dynamic_Tweet_Maps.html">
   Dynamic Tweet Maps
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-10_group-Geosmus/04_conclusion.html">
   Conclusion
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-10_group-Geosmus/05_appendix_get-cc-data.html">
   Notebook for collecting tweets with country codes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-10_group-Geosmus/07_a_appendix_extendedTwitterUtils2run.html">
   Note, this notebook has been edited slightly from the one supplied by Raaz.
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-10_group-Geosmus/07_a_appendix_extendedTwitterUtils2run.html#scadamale-scalable-data-science-and-distributed-machine-learning">
   ScaDaMaLe, Scalable Data Science and Distributed Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-10_group-Geosmus/07_a_appendix_extendedTwitterUtils2run.html#extended-spark-streaming-twitter-twitterutils">
   Extended spark.streaming.twitter.TwitterUtils
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-10_group-Geosmus/07_b_appendix_TTTDFfunctions.html">
   Note, this notebook has been edited slightly from the one supplied by Raaz.
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-10_group-Geosmus/07_b_appendix_TTTDFfunctions.html#scadamale-scalable-data-science-and-distributed-machine-learning">
   ScaDaMaLe, Scalable Data Science and Distributed Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-11_group-Sketchings/00_QuantileEstimation.html">
   Anomaly Detection with Iterative Quantile Estimation and T-digest
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-12_group-CovidPandemic/00_ProjectDescriptionAndPlanning.html">
   Project Description and Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-12_group-CovidPandemic/01_DownloadFilesPeriodicallyScript.html">
   Download Files Periodically
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-12_group-CovidPandemic/01_StreamToFile.html">
   Stream to parquet file
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-12_group-CovidPandemic/021_DataPreprocess_Explain.html">
   Loading data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-12_group-CovidPandemic/021_DataPreprocess_Explain.html#preprocessing">
   Preprocessing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-12_group-CovidPandemic/02_DataPreprocess.html">
   Loading data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-12_group-CovidPandemic/02_DataPreprocess.html#preprocessing">
   Preprocessing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-12_group-CovidPandemic/03_ExplosiveAnalysis.html">
   This notebook is for explosive analysis of features in data.
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-12_group-CovidPandemic/03_ExplosiveAnalysis.html#statistics-of-invariant-features">
   Statistics of invariant features
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-12_group-CovidPandemic/03_ExplosiveAnalysis.html#correlation-between-invariant-features">
   Correlation between invariant features
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-12_group-CovidPandemic/03_ExplosiveAnalysis.html#correlation-between-new-case-per-million-total-case-new-death-per-million-total-death-per-million-reproduction-rate-and-stringency-index">
   Correlation between new case per million, total case, new death per million, total death per million, reproduction rate and stringency index.
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-12_group-CovidPandemic/04_DataVisualize.html">
   Show reproduction rate of selected countries i.e. Sweden, Germany, Danmark, Finland, Norway
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-12_group-CovidPandemic/04_DataVisualize.html#visualize-total-cases-total-deaths-new-cases-and-new-deaths-during-pandemic">
   Visualize total cases, total deaths, new cases and new deaths during pandemic
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-12_group-CovidPandemic/04_DataVisualize.html#total-deaths">
   Total Deaths
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-12_group-CovidPandemic/04_DataVisualize.html#new-cases">
   New Cases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-12_group-CovidPandemic/04_DataVisualize.html#new-deaths">
   New Deaths
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-12_group-CovidPandemic/05_Clustering.html">
   Clustering of country features in the Covid 19 dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-12_group-CovidPandemic/06_DataPredicton_LR.html">
   Prediction with Linear Regression (LR) Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-12_group-CovidPandemic/07_DataPredicton_ARIMA.html">
   Prediction with Time Series model - ARIMA
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-12_group-CovidPandemic/08_DataPrediction_GP.html">
   Prediction with time series model - Gaussian Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-13_group-Genomics/01_1000genomes.html">
   Genomics Analysis with Glow and Spark
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-13_group-Genomics/01_1000genomes.html#load-libs-and-define-helper-functions">
   Load libs and define helper functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-13_group-Genomics/01_1000genomes.html#read-data">
   Read data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-13_group-Genomics/01_1000genomes.html#pca">
   PCA
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-13_group-Genomics/01_1000genomes.html#predicting-ethinicity">
   Predicting Ethinicity
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-13_group-Genomics/01_1000genomes.html#filtering-of-snps-based-on-chi-squared-test">
   Filtering of SNPs based on chi-squared test
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-14_group-NullHypothesisEvaluationCriteria/distributed_combinatorial_bandit.html">
   Distributional combinatorial bandits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="00_video.html">
   Reinforcement Learning for Intraday Trading
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Reinforcement Learning for Intraday Trading
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_rl_intraday_trading_elephas.html">
   Reinforcement Learning for Intraday Trading - Distributed model tuning with Elephas
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_rl_intraday_trading_elephas.html#elephas">
   Elephas
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_rl_intraday_trading_elephas.html#notes-to-the-elephas-training">
   Notes to the elephas training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_rl_intraday_trading_elephas.html#id1">
   Notes to the elephas training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_resources.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_resources.html#environment">
   Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_resources.html#rl-algorithms">
   RL-Algorithms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_resources.html#scalable-dl">
   Scalable DL
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_resources.html#spark-scalability-monitoring">
   Spark scalability monitoring
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_resources.html#raaz-to-group">
   Raaz to groupâ€¦
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-16_group-IntrusionDetection/00_Introduction.html">
   Problem Definition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-16_group-IntrusionDetection/00_Introduction.html#loading-and-preprocessing-data">
   Loading and Preprocessing Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-18_group-ProjectRL/Intro.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-18_group-ProjectRL/Intro.html#problem-description">
   Problem description
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-19_group-Featuring/01_FundamentalMatrix.html">
   Link to our video explaining the 1) theory, 2) preprocessing the dataset, 3) algorithm and 4) results
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-19_group-Featuring/01_FundamentalMatrix.html#https-drive-google-com-drive-folders-1zewj6jsjeuu9f8q5xy-avwxq3yj9oi7z-usp-sharing">
   https://drive.google.com/drive/folders/1zEWj6JsJEUu9f8Q5Xy_avwxQ3yJ9oI7Z?usp=sharing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-19_group-Featuring/01_FundamentalMatrix.html#problem-formulation">
   Problem formulation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-19_group-Featuring/01_FundamentalMatrix.html#results">
   Results
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-20_group-Generalization/01_Background.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-20_group-Generalization/01_Background.html#project-description">
   Project description
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-20_group-Generalization/02_CNNs.html">
   The data set
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-20_group-Generalization/02_CNNs.html#mixup-data-generator">
   MixUp data generator
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-20_group-Generalization/02_CNNs.html#training-function">
   Training function
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-20_group-Generalization/02_CNNs.html#connection-between-mixup-performance-and-generalization">
   Connection between MixUp performance and generalization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-20_group-Generalization/02_CNNs.html#directly-training-on-mixup-data">
   Directly training on MixUp data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-20_group-Generalization/02_CNNs.html#conclusions">
   Conclusions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-21_group-GraphSpectralAnalysis/00_introduction.html">
   Graph Spectral Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-21_group-GraphSpectralAnalysis/01_preprocess_data.html">
   Preprocess the data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-21_group-GraphSpectralAnalysis/02_generate_graphs.html">
   Generate random graphs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-21_group-GraphSpectralAnalysis/03_compute_rsvd.html">
   Compute RSVD
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../student-project-21_group-GraphSpectralAnalysis/04_analyse_eigenvalues.html">
   Analyse the eigenvalue spectrum
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Voluntary Student Projects
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/030_Spark_GDELT_project.html">
   Plugging into GDELT Streams - TODO - IN PROGRESS
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/030_Spark_GDELT_project.html#this-is-just-dipping-our-pinky-toe-in-this-ocean-of-information">
   This is just dipping our pinky toe in this ocean of information!
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../000_1-sds-3-x/030_Spark_GDELT_project.html#download-from-gdelt-project">
   Download from gdelt-project
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/000_0-sds-3-x-projects/student-project-15_group-FinancialDataStreams/01_rl_intraday_trading.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://lamastex.github.io/ScaDaMaLe/index.html"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://lamastex.github.io/ScaDaMaLe/index.html/issues/new?title=Issue%20on%20page%20%2F000_0-sds-3-x-projects/student-project-15_group-FinancialDataStreams/01_rl_intraday_trading.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#group-members">
   Group members:
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#brent-crude-oil-dataset">
   Brent Crude Oil Dataset
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#preparing-the-data-in-python">
     Preparing the data in Python
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#rl-environment">
   RL Environment
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dqn-algorithm">
   DQN Algorithm
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-rl-agent">
   Training RL agent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#testing-rl-agent">
   Testing RL agent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#symmary-and-future-work">
   Symmary and Future Work
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#trend-calculus">
   Trend Calculus
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="reinforcement-learning-for-intraday-trading">
<h1>Reinforcement Learning for Intraday Trading<a class="headerlink" href="#reinforcement-learning-for-intraday-trading" title="Permalink to this headline">Â¶</a></h1>
<p>In this project, our aim is to implement a Reinforcement Learning (RL)
strategy for trading stocks. Specifically, we use the DQN â€“ Deep
Q-Network â€“ algorithm to train an agent which trades Brent Crude Oil
(BCOUSD) stocks, in order to maximize long term profit. Finally,
evaluate the the results from the RL agent based on the predictions from
the TrendCalculus predictive algorithm. Note that the overall concern is
to make our implementation scalable.</p>
<p>&lt;img
src=https://www.kdnuggets.com/images/reinforcement-learning-fig1-700.jpg&gt;</p>
<div class="section" id="group-members">
<h2>Group members:<a class="headerlink" href="#group-members" title="Permalink to this headline">Â¶</a></h2>
<ul class="simple">
<li><p>Fabian Sinzinger</p></li>
<li><p>Karl BÃ¤ckstrÃ¶m</p></li>
<li><p>Rita Laezza</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">//</span> <span class="n">Scala</span> <span class="n">imports</span>
<span class="kn">import</span> <span class="nn">org.lamastex.spark.trendcalculus._</span>
<span class="kn">import</span> <span class="nn">spark.implicits._</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql._</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.functions._</span>
<span class="kn">import</span> <span class="nn">java.sql.Timestamp</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.expressions._</span>
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import org.lamastex.spark.trendcalculus._
import spark.implicits._
import org.apache.spark.sql._
import org.apache.spark.sql.functions._
import java.sql.Timestamp
import org.apache.spark.sql.expressions._
</pre></div>
</div>
</div></blockquote>
</div>
<div class="section" id="brent-crude-oil-dataset">
<h2>Brent Crude Oil Dataset<a class="headerlink" href="#brent-crude-oil-dataset" title="Permalink to this headline">Â¶</a></h2>
<p>The dataset consists of historical data starting from the <em>14th of
October 2010</em> to the <em>21st of June 2019</em>. Since the data in the first
and last day is incomplete, we remove it from the dataset. The BCUSD
data is sampled approximatly every minute with a specific timestamp and
registered in US dollars.</p>
<p>To read the BCUSD dataset, we use the same parsers provided by the
<a class="reference external" href="https://github.com/lamastex/spark-trend-calculus">TrendCalculus</a>
library. This allows us to load the FX data into a Spark Dataset. The
<strong>fx1m</strong> function returns the dataset as <strong>TickerPoint</strong> objects with
values <strong>x</strong> and <strong>y</strong>, which are <strong>time</strong> and a <strong>close</strong> values
respectively. The first consists of the name of the stock, the second is
the timestamp of the data point and the latter consists of the value of
the stock at the end of each 1 minute bin.</p>
<p>Finally we add the <strong>index</strong> column to facilitate retrieving values from
the table, since there are gaps in the data meaning that not all minutes
have an entry. Further a **diff_close** column was added, which
consists of the relative difference between the <strong>close</strong> value at the
current and the previous <strong>time</strong>. Note hat since <strong>ticker</strong> is always
the same, we remove that column.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>// Load dataset
val oilDS = spark.read.fx1m(&quot;dbfs:/FileStore/shared_uploads/fabiansi@kth.se/*csv.gz&quot;).toDF.withColumn(&quot;ticker&quot;, lit(&quot;BCOUSD&quot;)).select($&quot;ticker&quot;, $&quot;time&quot; as &quot;x&quot;, $&quot;close&quot; as &quot;y&quot;).as[TickerPoint].orderBy(&quot;time&quot;)

// Add column with difference from previous close value (expected &#39;x&#39;, &#39;y&#39; column names)
val windowSpec = Window.orderBy(&quot;x&quot;)
val oilDS1 = oilDS 
.withColumn(&quot;diff_close&quot;, $&quot;y&quot; - when((lag(&quot;y&quot;, 1).over(windowSpec)).isNull, 0).otherwise(lag(&quot;y&quot;, 1).over(windowSpec)))

// Rename variables
val oilDS2 = oilDS1.withColumnRenamed(&quot;x&quot;,&quot;time&quot;).withColumnRenamed(&quot;y&quot;,&quot;close&quot;)

// Remove incomplete data from first day (2010-11-14) and last day (2019-06-21)
val oilDS3 = oilDS2.filter(to_date(oilDS2(&quot;time&quot;)) &gt;= lit(&quot;2010-11-15&quot;) &amp;&amp; to_date(oilDS2(&quot;time&quot;)) &lt;= lit(&quot;2019-06-20&quot;))

// Add index column
val windowSpec1 = Window.orderBy(&quot;time&quot;)
val oilDS4 = oilDS3
.withColumn(&quot;index&quot;, row_number().over(windowSpec1))

// Drop ticker column
val oilDS5 = oilDS4.drop(&quot;ticker&quot;)

// Store loaded data as temp view, to be accessible in Python
oilDS5.createOrReplaceTempView(&quot;temp&quot;)
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>oilDS: org.apache.spark.sql.Dataset[org.lamastex.spark.trendcalculus.TickerPoint] = [ticker: string, x: timestamp ... 1 more field]
windowSpec: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@347f9beb
oilDS1: org.apache.spark.sql.DataFrame = [ticker: string, x: timestamp ... 2 more fields]
oilDS2: org.apache.spark.sql.DataFrame = [ticker: string, time: timestamp ... 2 more fields]
oilDS3: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ticker: string, time: timestamp ... 2 more fields]
windowSpec1: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@3c3c818a
oilDS4: org.apache.spark.sql.DataFrame = [ticker: string, time: timestamp ... 3 more fields]
oilDS5: org.apache.spark.sql.DataFrame = [time: timestamp, close: double ... 2 more fields]
</pre></div>
</div>
</div></blockquote>
<div class="section" id="preparing-the-data-in-python">
<h3>Preparing the data in Python<a class="headerlink" href="#preparing-the-data-in-python" title="Permalink to this headline">Â¶</a></h3>
<p>Because the
<a class="reference external" href="https://github.com/lamastex/spark-trend-calculus">TrendCalculus</a>
library we use is implemented in Scala and we want to do our
implementation in Python, we have to make sure that the data loaded in
Scala is correctly read in Python, before moving on. To that end, we
select the first 10 data points and show them in a table.</p>
<p>We can see that there are roughly <strong>2.5 million data points</strong> in the
BCUSD dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#Python imports</span>
<span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers.core</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Conv1D</span><span class="p">,</span> <span class="n">MaxPool1D</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">BatchNormalization</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">optimizers</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create Dataframe from temp data</span>
<span class="n">oilDF_py</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s2">&quot;temp&quot;</span><span class="p">)</span>

<span class="c1"># Select the 10 first Rows of data and print them</span>
<span class="n">ten_oilDF_py</span> <span class="o">=</span> <span class="n">oilDF_py</span><span class="o">.</span><span class="n">limit</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ten_oilDF_py</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Check number of data points</span>
<span class="n">last_index</span> <span class="o">=</span> <span class="n">oilDF_py</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of data points: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">last_index</span><span class="p">))</span>

<span class="c1"># Select the date of the last data point</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Last data point: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">oilDF_py</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">oilDF_py</span><span class="o">.</span><span class="n">index</span> <span class="o">==</span> <span class="n">last_index</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s1">&#39;time&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>+-------------------+-----+--------------------+-----+
|               time|close|          diff_close|index|
+-------------------+-----+--------------------+-----+
|2010-11-15 00:00:00| 86.6|-0.01000000000000...|    1|
|2010-11-15 00:01:00| 86.6|                 0.0|    2|
|2010-11-15 00:02:00|86.63|0.030000000000001137|    3|
|2010-11-15 00:03:00|86.61|-0.01999999999999602|    4|
|2010-11-15 00:05:00|86.61|                 0.0|    5|
|2010-11-15 00:07:00| 86.6|-0.01000000000000...|    6|
|2010-11-15 00:08:00|86.58|-0.01999999999999602|    7|
|2010-11-15 00:09:00|86.58|                 0.0|    8|
|2010-11-15 00:10:00|86.58|                 0.0|    9|
|2010-11-15 00:12:00|86.57|-0.01000000000000...|   10|
+-------------------+-----+--------------------+-----+

Number of data points: 2523078
Last data point: 2019-06-20 23:59:00
</pre></div>
</div>
</div></blockquote>
</div>
</div>
<div class="section" id="rl-environment">
<h2>RL Environment<a class="headerlink" href="#rl-environment" title="Permalink to this headline">Â¶</a></h2>
<p>In order to train RL agents, we first need to create the environment
with which the agent will interact to gather experience. In our case,
that consist of a stock market simulation which plays out historical
data from the BCUSD dataset. This is valid, under the assumption that
the trading on the part of our agent has no affect on the stock market.
An RL problem can be formally defined by a Markov Decision Process
(MDP).</p>
<p>For our application, we have the following MDP: - State, s: a window of
**diff<em>close** values for a given <strong>scope</strong>, i.e. the current value
and history leading up to it. - Action, a: either <strong>LONG</strong> for buying
stock, or <strong>SHORT</strong> for selling stock. Note that <strong>PASS</strong> is not
required, since if stock is already owned, buying means holding and if
stock is not owned then shorting means pass. - Reward, r: if
a</em>t=**LONG** r<em>t=s</em>t+1=**diff_close**; if a<em>t=<strong>SHORT</strong>
r</em>t=s_t+1=-**diff_close**. Essentially, the reward is negative if
we sell and the stock goes up or if we buy and the stock goes down in
the next timestep. Conversely, the reward is positive if we buy and the
stock goes up or if we sell and the stock goes down in the next
timestep.</p>
<p>This environment is very simplified, with only binary actions. An
alternative could be to use continuos actions to determine how much
stock to buy or sell. However, since we aim to compare to TrendCalculus
results which only predict reversals, these actions are more adequate.
For the implementation, we used OpenAI Gymâ€™s formalism, which includes a
<strong>done</strong> variable to indicate the end of an episode. In <strong>MarketEnv</strong>,
by setting the **start_date** and **end_date** atttributes, we
can select the part of the dataset we wish to use. Finally, the and
**episode_size** parameter determines the episode size. An
episodeâ€™s starting point can be sampled at random or not, which is
defined when calling <strong>reset</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Adapted from: https://github.com/kh-kim/stock_market_reinforcement_learning/blob/master/market_env.py</span>


<span class="k">class</span> <span class="nc">MarketEnv</span><span class="p">(</span><span class="n">gym</span><span class="o">.</span><span class="n">Env</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">full_data</span><span class="p">,</span> <span class="n">start_date</span><span class="p">,</span> <span class="n">end_date</span><span class="p">,</span> <span class="n">episode_size</span><span class="o">=</span><span class="mi">30</span><span class="o">*</span><span class="mi">24</span><span class="o">*</span><span class="mi">60</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="mi">60</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">episode_size</span> <span class="o">=</span> <span class="n">episode_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actions</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;LONG&quot;</span><span class="p">,</span> <span class="s2">&quot;SHORT&quot;</span><span class="p">]</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Discrete</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actions</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state_space</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Box</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">scope</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">scope</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">diff_close</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">full_data</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">full_data</span><span class="p">[</span><span class="s2">&quot;time&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">start_date</span><span class="p">)</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">full_data</span><span class="p">[</span><span class="s2">&quot;time&quot;</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">end_date</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s1">&#39;diff_close&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
        <span class="n">max_diff_close</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">diff_close</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">diff_close</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">diff_close</span><span class="o">*</span><span class="n">max_diff_close</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">close</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">full_data</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">full_data</span><span class="p">[</span><span class="s2">&quot;time&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">start_date</span><span class="p">)</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">full_data</span><span class="p">[</span><span class="s2">&quot;time&quot;</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">end_date</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s1">&#39;close&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_ticks_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">diff_close</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">scope</span> <span class="o">=</span> <span class="n">scope</span> <span class="c1"># N values to be included in a state vector</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scope</span>  <span class="c1"># start N steps in, to ensure that we have enough past values for history </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">episode_init_time</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_index</span>  <span class="c1"># initial time index of the episode</span>


    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="n">info</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;index&#39;</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">time_index</span><span class="p">),</span> <span class="s1">&#39;close&#39;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">close</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">time_index</span><span class="p">])}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_index</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">diff_close</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">time_index</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">scope</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">time_index</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span> <span class="o">-</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">action</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="p">)</span>
        
        <span class="c1"># Check if done</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_index</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">episode_init_time</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">episode_size</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">done</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_index</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">diff_close</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">scope</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">done</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">done</span><span class="p">,</span> <span class="n">info</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">random_starttime</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward</span> <span class="o">=</span> <span class="mf">0.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scope</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">diff_close</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">time_index</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">scope</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">time_index</span><span class="p">]</span>
        
        <span class="k">if</span> <span class="n">random_starttime</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">time_index</span> <span class="o">+=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_ticks_train</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">scope</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">episode_init_time</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_index</span>
        
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span>

    <span class="k">def</span> <span class="nf">seed</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">states</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">actions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">reward_sum</span> <span class="o">=</span> <span class="mf">0.</span>

<span class="c1"># Verify environment for 1 hour</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2010</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2010</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">MarketEnv</span><span class="p">(</span><span class="n">oilDF_py</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">episode_size</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">random_starttime</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
    <span class="n">states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="c1"># Take random actions</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="n">actions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
    <span class="n">reward_sum</span> <span class="o">+=</span> <span class="n">reward</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Return = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">reward_sum</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Return = 0.005
</pre></div>
</div>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot samples</span>
<span class="n">timesteps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">states</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">states</span><span class="p">))</span>
<span class="n">longs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span> <span class="o">==</span>  <span class="mi">0</span><span class="p">)</span>
<span class="n">shorts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span> <span class="o">==</span>  <span class="mi">1</span><span class="p">)</span>
<span class="n">states</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">timesteps</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;diff_close&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">timesteps</span><span class="p">[</span><span class="n">longs</span><span class="p">],</span> <span class="n">states</span><span class="p">[</span><span class="n">longs</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="s1">&#39;*g&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;long&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">timesteps</span><span class="p">[</span><span class="n">shorts</span><span class="p">],</span> <span class="n">states</span><span class="p">[</span><span class="n">shorts</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="s1">&#39;*r&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;short&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;(s,a)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Timestep&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">states</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">states</span><span class="p">),</span> <span class="mf">1.0</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">timesteps</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="s1">&#39;o-r&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;r&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Timestep&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">states</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">states</span><span class="p">),</span> <span class="mf">1.0</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">display</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="dqn-algorithm">
<h2>DQN Algorithm<a class="headerlink" href="#dqn-algorithm" title="Permalink to this headline">Â¶</a></h2>
<p>Since we have discrete actions, we can use Q-learning to train our
agent. Specifically we use the DQN algorithm with Experience Replay,
which was first described in DeepMindâ€™s: <a class="reference external" href="https://arxiv.org/pdf/1312.5602.pdf">Playing Atari with Deep
Reinforcement Learning</a>. The
algorithm is described below, where equation [3], refers to the
gradient: &lt;img src=â€https://imgur.com/eGhNC9m.pngâ€ width=650&gt;</p>
<p>&lt;img src=â€https://imgur.com/mvopoh8.pngâ€ width=800&gt;</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Adapted from: https://dbc-635ca498-e5f1.cloud.databricks.com/?o=445287446643905#notebook/4201196137758409/command/4201196137758410</span>

<span class="k">class</span> <span class="nc">ExperienceReplay</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_memory</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">discount</span><span class="o">=.</span><span class="mi">9</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_memory</span> <span class="o">=</span> <span class="n">max_memory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">discount</span> <span class="o">=</span> <span class="n">discount</span>

    <span class="k">def</span> <span class="nf">remember</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">states</span><span class="p">,</span> <span class="n">done</span><span class="p">])</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_memory</span><span class="p">:</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">get_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">len_memory</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">)</span>
        <span class="n">num_actions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">output_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">env_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">min</span><span class="p">(</span><span class="n">len_memory</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">),</span> <span class="n">env_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_actions</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">len_memory</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])):</span>
            <span class="n">state_t</span><span class="p">,</span> <span class="n">action_t</span><span class="p">,</span> <span class="n">reward_t</span><span class="p">,</span> <span class="n">state_tp1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">done</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">state_t</span>
            <span class="c1"># There should be no target values for actions not taken.</span>
            <span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">state_t</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">Q_sa</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">state_tp1</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">done</span><span class="p">:</span> <span class="c1"># if done is True</span>
                <span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">action_t</span><span class="p">]</span> <span class="o">=</span> <span class="n">reward_t</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># reward_t + gamma * max_a&#39; Q(s&#39;, a&#39;)</span>
                <span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">action_t</span><span class="p">]</span> <span class="o">=</span> <span class="n">reward_t</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">discount</span> <span class="o">*</span> <span class="n">Q_sa</span>
        <span class="k">return</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="training-rl-agent">
<h2>Training RL agent<a class="headerlink" href="#training-rl-agent" title="Permalink to this headline">Â¶</a></h2>
<p>In order to train the RL agent, we use the data from 2014 to 2018,
leaving the data from 2019 for testing. RL implementations are quite
difficult to train, due to the large amount of parameters which need to
be tuned. We have spent little time seraching for better hyperparameters
as this was beyond the scope of the course. We have picked parameters
based on a similar implementation of RL for trading, however we have
designed an new Q-network, since the state is different in our
implementation. Sine we are dealing wih sequential data, we could have
opted for an RNN, however 1-dimensional CNNs are also a common choice
which is less computationally heavy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Adapted from: https://dbc-635ca498-e5f1.cloud.databricks.com/?o=445287446643905#notebook/4201196137758409/command/4201196137758410</span>

<span class="c1"># RL parameters</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="o">.</span><span class="mi">5</span>  <span class="c1"># exploration</span>
<span class="n">min_epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">max_memory</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">discount</span> <span class="o">=</span> <span class="mf">0.8</span>

<span class="c1"># Environment parameters</span>
<span class="n">num_actions</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># [long, short]</span>
<span class="n">episodes</span> <span class="o">=</span> <span class="mi">500</span> <span class="c1"># 100000</span>
<span class="n">episode_size</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">*</span> <span class="mi">1</span> <span class="o">*</span> <span class="mi">60</span>  <span class="c1"># roughly an hour worth of data in each training episode</span>

<span class="c1"># Define state sequence scope (approx. 1 hour)</span>
<span class="n">sequence_scope</span> <span class="o">=</span> <span class="mi">60</span>
<span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_scope</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Create Q Network</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv1D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPool1D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv1D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPool1D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">num_actions</span><span class="p">))</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">)</span>

<span class="c1"># Define training interval</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2010</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2018</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">59</span><span class="p">)</span>

<span class="c1"># Initialize Environment</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">MarketEnv</span><span class="p">(</span><span class="n">oilDF_py</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">episode_size</span><span class="o">=</span><span class="n">episode_size</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="n">sequence_scope</span><span class="p">)</span>

<span class="c1"># Initialize experience replay object</span>
<span class="n">exp_replay</span> <span class="o">=</span> <span class="n">ExperienceReplay</span><span class="p">(</span><span class="n">max_memory</span><span class="o">=</span><span class="n">max_memory</span><span class="p">,</span> <span class="n">discount</span><span class="o">=</span><span class="n">discount</span><span class="p">)</span>

<span class="c1"># Train</span>
<span class="n">returns</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">episodes</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">reward_sum</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
    
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">input_t</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">sequence_scope</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> 
    
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>     
        <span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">input_tm1</span> <span class="o">=</span> <span class="n">input_t</span>
        <span class="c1"># get next action</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="n">epsilon</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">q</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">input_tm1</span><span class="p">)</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">q</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="c1"># apply action, get rewards and new state</span>
        <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">reward_sum</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="n">input_t</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">sequence_scope</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>         

        <span class="c1"># store experience</span>
        <span class="n">exp_replay</span><span class="o">.</span><span class="n">remember</span><span class="p">([</span><span class="n">input_tm1</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">input_t</span><span class="p">],</span> <span class="n">done</span><span class="p">)</span>

        <span class="c1"># adapt model</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="n">exp_replay</span><span class="o">.</span><span class="n">get_batch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="n">model</span><span class="o">.</span><span class="n">train_on_batch</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
    
    
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Episode </span><span class="si">{:03d}</span><span class="s2">/</span><span class="si">{:d}</span><span class="s2"> | Average Loss </span><span class="si">{:.4f}</span><span class="s2"> | Cumulative Reward </span><span class="si">{:.4f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">episodes</span><span class="p">,</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">counter</span><span class="p">,</span> <span class="n">reward_sum</span><span class="p">))</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">min_epsilon</span><span class="p">,</span> <span class="n">epsilon</span> <span class="o">*</span> <span class="mf">0.99</span><span class="p">)</span>
    <span class="n">returns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_sum</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Episode 001/500 | Average Loss 0.9243 | Cumulative Reward -0.0492
Episode 002/500 | Average Loss 0.0431 | Cumulative Reward -0.2952
Episode 003/500 | Average Loss 0.0102 | Cumulative Reward 0.0246
Episode 004/500 | Average Loss 0.0332 | Cumulative Reward -0.3444
Episode 005/500 | Average Loss 0.0366 | Cumulative Reward 0.4920
Episode 006/500 | Average Loss 44.9978 | Cumulative Reward -0.0984
Episode 007/500 | Average Loss 13892.3869 | Cumulative Reward -0.7626
Episode 008/500 | Average Loss 695661.5894 | Cumulative Reward 1.9434
Episode 009/500 | Average Loss 12746050.9732 | Cumulative Reward -0.2952
Episode 010/500 | Average Loss 31002932.3811 | Cumulative Reward 0.0492
Episode 011/500 | Average Loss 272621435.5164 | Cumulative Reward 0.4182
Episode 012/500 | Average Loss 171599336.8525 | Cumulative Reward 0.4674
Episode 013/500 | Average Loss 27726520.4590 | Cumulative Reward 0.1968
Episode 014/500 | Average Loss 20788127.3443 | Cumulative Reward 0.0984
Episode 015/500 | Average Loss 10797102.9508 | Cumulative Reward -0.0738
Episode 016/500 | Average Loss 4081633.1680 | Cumulative Reward -0.0984
Episode 017/500 | Average Loss 3617864.5246 | Cumulative Reward 0.5166
Episode 018/500 | Average Loss 2102176.5451 | Cumulative Reward -0.9348
Episode 019/500 | Average Loss 1819763.1424 | Cumulative Reward -0.9348
Episode 020/500 | Average Loss 525313.2244 | Cumulative Reward 0.3198
Episode 021/500 | Average Loss 590327.7751 | Cumulative Reward -0.5412
Episode 022/500 | Average Loss 643538.9570 | Cumulative Reward -1.2792
Episode 023/500 | Average Loss 506215.3632 | Cumulative Reward 0.2214
Episode 024/500 | Average Loss 229747.6570 | Cumulative Reward 0.3690
Episode 025/500 | Average Loss 118323.1301 | Cumulative Reward -0.1968
Episode 026/500 | Average Loss 90964.7006 | Cumulative Reward 0.1476
Episode 027/500 | Average Loss 47487.1318 | Cumulative Reward -0.4428
Episode 028/500 | Average Loss 90195246.1188 | Cumulative Reward -3.2718
Episode 029/500 | Average Loss 32692468.4262 | Cumulative Reward 0.1230
Episode 030/500 | Average Loss 2332925.2500 | Cumulative Reward -0.3690
Episode 031/500 | Average Loss 834450.7592 | Cumulative Reward -0.1968
Episode 032/500 | Average Loss 283082.5282 | Cumulative Reward -0.3198
Episode 033/500 | Average Loss 283707.8343 | Cumulative Reward 0.0000
Episode 034/500 | Average Loss 231921.2454 | Cumulative Reward -0.1230
Episode 035/500 | Average Loss 147320.1317 | Cumulative Reward 0.9594
Episode 036/500 | Average Loss 128691.0975 | Cumulative Reward -0.5904
Episode 037/500 | Average Loss 103319.1212 | Cumulative Reward 0.2952
Episode 038/500 | Average Loss 89891.4415 | Cumulative Reward 2.8782
Episode 039/500 | Average Loss 328345656.4238 | Cumulative Reward -0.6150
Episode 040/500 | Average Loss 10943518558.4262 | Cumulative Reward -0.6888
Episode 041/500 | Average Loss 162841831.4754 | Cumulative Reward 0.1476
Episode 042/500 | Average Loss 37689431.5000 | Cumulative Reward 0.4182
Episode 043/500 | Average Loss 72136614.4344 | Cumulative Reward 0.0738
Episode 044/500 | Average Loss 56635724.9180 | Cumulative Reward -0.2706
Episode 045/500 | Average Loss 10563238.0512 | Cumulative Reward 0.0492
Episode 046/500 | Average Loss 132264149.6066 | Cumulative Reward 1.0824
Episode 047/500 | Average Loss 41677930.5574 | Cumulative Reward 0.0246
Episode 048/500 | Average Loss 9066351.1148 | Cumulative Reward -0.1230
Episode 049/500 | Average Loss 2358803.1926 | Cumulative Reward 0.0246
Episode 050/500 | Average Loss 1814690.9365 | Cumulative Reward -4.8708
Episode 051/500 | Average Loss 1223300511.6383 | Cumulative Reward -1.4514
Episode 052/500 | Average Loss 1202551068.6885 | Cumulative Reward -0.6888
Episode 053/500 | Average Loss 48371521.6721 | Cumulative Reward -0.0738
Episode 054/500 | Average Loss 3760147.6393 | Cumulative Reward -1.7712
Episode 055/500 | Average Loss 1791821.9693 | Cumulative Reward -0.0246
Episode 056/500 | Average Loss 2677952.7643 | Cumulative Reward -0.1230
Episode 057/500 | Average Loss 4241063.5451 | Cumulative Reward 0.4674
Episode 058/500 | Average Loss 3664478.7951 | Cumulative Reward -0.3198
Episode 059/500 | Average Loss 2111481.0102 | Cumulative Reward 0.0492
Episode 060/500 | Average Loss 1312407.7039 | Cumulative Reward -0.3690
Episode 061/500 | Average Loss 1215967.5225 | Cumulative Reward -1.6974
Episode 062/500 | Average Loss 1206593.6096 | Cumulative Reward -0.8118
Episode 063/500 | Average Loss 1242360.7818 | Cumulative Reward 0.9348
Episode 064/500 | Average Loss 911372.9959 | Cumulative Reward 0.8856
Episode 065/500 | Average Loss 983090.6424 | Cumulative Reward 0.2460
Episode 066/500 | Average Loss 56758439.2228 | Cumulative Reward -0.5166
Episode 067/500 | Average Loss 1265408.1035 | Cumulative Reward -0.0246
Episode 068/500 | Average Loss 5961294.6898 | Cumulative Reward -0.1722
Episode 069/500 | Average Loss 142104936.0943 | Cumulative Reward -1.8450
Episode 070/500 | Average Loss 604659424.3279 | Cumulative Reward -1.7466
Episode 071/500 | Average Loss 2669601345.0492 | Cumulative Reward -0.7872
Episode 072/500 | Average Loss 6518834110.9508 | Cumulative Reward -0.3198
Episode 073/500 | Average Loss 660690072.6557 | Cumulative Reward 0.3936
Episode 074/500 | Average Loss 49482324.3770 | Cumulative Reward 0.2706
Episode 075/500 | Average Loss 12702479.1721 | Cumulative Reward 2.2140
Episode 076/500 | Average Loss 31520307.4426 | Cumulative Reward -0.6642
Episode 077/500 | Average Loss 30264551.5410 | Cumulative Reward 0.1230
Episode 078/500 | Average Loss 23348717.7705 | Cumulative Reward -0.8610
Episode 079/500 | Average Loss 10719680.3934 | Cumulative Reward -1.5006
Episode 080/500 | Average Loss 5350704.6926 | Cumulative Reward 0.2952
Episode 081/500 | Average Loss 4198182.6107 | Cumulative Reward 0.5412
Episode 082/500 | Average Loss 3379803.6189 | Cumulative Reward 3.6162
Episode 083/500 | Average Loss 129381317.8607 | Cumulative Reward -0.0246
Episode 084/500 | Average Loss 220882119.9344 | Cumulative Reward -0.0000
Episode 085/500 | Average Loss 66604575.5410 | Cumulative Reward -0.0738
Episode 086/500 | Average Loss 274207500.3934 | Cumulative Reward -1.1808
Episode 087/500 | Average Loss 256016848.9549 | Cumulative Reward -0.5904
Episode 088/500 | Average Loss 150811378.3607 | Cumulative Reward -0.0984
Episode 089/500 | Average Loss 64320583.2131 | Cumulative Reward 0.1476
Episode 090/500 | Average Loss 30557816.7213 | Cumulative Reward 0.3690
Episode 091/500 | Average Loss 14250694.0328 | Cumulative Reward -0.2214
Episode 092/500 | Average Loss 7108390.2541 | Cumulative Reward 0.4428
Episode 093/500 | Average Loss 3342842.4262 | Cumulative Reward 0.8610
Episode 094/500 | Average Loss 1480352.2623 | Cumulative Reward -0.1230
Episode 095/500 | Average Loss 665809.5815 | Cumulative Reward -0.1968
Episode 096/500 | Average Loss 357767.4959 | Cumulative Reward 1.1562
Episode 097/500 | Average Loss 239387.9193 | Cumulative Reward 0.1968
Episode 098/500 | Average Loss 204709.5968 | Cumulative Reward -0.7380
Episode 099/500 | Average Loss 166877.2141 | Cumulative Reward 0.5658
Episode 100/500 | Average Loss 155712.9769 | Cumulative Reward 0.9102
Episode 101/500 | Average Loss 144984.7732 | Cumulative Reward 1.2300
Episode 102/500 | Average Loss 138251.9153 | Cumulative Reward 0.2214
Episode 103/500 | Average Loss 127553.8490 | Cumulative Reward 0.3444
Episode 104/500 | Average Loss 124246.1187 | Cumulative Reward 0.9348
Episode 105/500 | Average Loss 118589.2683 | Cumulative Reward 1.5498
Episode 106/500 | Average Loss 125549.7988 | Cumulative Reward 0.6888
Episode 107/500 | Average Loss 134375.3892 | Cumulative Reward 0.1722
Episode 108/500 | Average Loss 148787.7741 | Cumulative Reward -0.1230
Episode 109/500 | Average Loss 164604.2537 | Cumulative Reward 1.2054
Episode 110/500 | Average Loss 199937.5569 | Cumulative Reward 0.0000
Episode 111/500 | Average Loss 191672.2460 | Cumulative Reward -0.3444
Episode 112/500 | Average Loss 258762.1554 | Cumulative Reward 0.1230
Episode 113/500 | Average Loss 289838.2514 | Cumulative Reward -0.5904
Episode 114/500 | Average Loss 465530.3043 | Cumulative Reward -1.7220
Episode 115/500 | Average Loss 415042.3491 | Cumulative Reward 0.0246
Episode 116/500 | Average Loss 309605.8963 | Cumulative Reward -0.3444
Episode 117/500 | Average Loss 262915.5269 | Cumulative Reward -0.0246
Episode 118/500 | Average Loss 276877.2044 | Cumulative Reward 0.1722
Episode 119/500 | Average Loss 297945.2974 | Cumulative Reward -0.8610
Episode 120/500 | Average Loss 252190.8712 | Cumulative Reward 0.3198
Episode 121/500 | Average Loss 209873.6153 | Cumulative Reward -0.0984
Episode 122/500 | Average Loss 200975.3696 | Cumulative Reward -0.4182
Episode 123/500 | Average Loss 224392.9705 | Cumulative Reward 0.6396
Episode 124/500 | Average Loss 175272.5402 | Cumulative Reward -0.1230
Episode 125/500 | Average Loss 134914.6336 | Cumulative Reward 0.0738
Episode 126/500 | Average Loss 118408.8128 | Cumulative Reward 0.9102
Episode 127/500 | Average Loss 119856.1210 | Cumulative Reward -0.1476
Episode 128/500 | Average Loss 128668.1934 | Cumulative Reward 0.4182
Episode 129/500 | Average Loss 121379.1356 | Cumulative Reward 0.3444
Episode 130/500 | Average Loss 122105.9212 | Cumulative Reward 0.8364
Episode 131/500 | Average Loss 126979.4586 | Cumulative Reward 0.4428
Episode 132/500 | Average Loss 159263.2796 | Cumulative Reward 1.4268
Episode 133/500 | Average Loss 249200.5343 | Cumulative Reward -0.3690
Episode 134/500 | Average Loss 351498.9006 | Cumulative Reward 0.2460
Episode 135/500 | Average Loss 344710.3376 | Cumulative Reward 0.4920
Episode 136/500 | Average Loss 290292.0177 | Cumulative Reward 0.3444
Episode 137/500 | Average Loss 203908.2672 | Cumulative Reward -1.1808
Episode 138/500 | Average Loss 152643.9161 | Cumulative Reward -0.3690
Episode 139/500 | Average Loss 103234.8929 | Cumulative Reward 0.3690
Episode 140/500 | Average Loss 81400.9171 | Cumulative Reward -0.8364
Episode 141/500 | Average Loss 70417.8615 | Cumulative Reward -0.0246
Episode 142/500 | Average Loss 62871.1321 | Cumulative Reward -0.1968
Episode 143/500 | Average Loss 66203.8933 | Cumulative Reward 1.3284
Episode 144/500 | Average Loss 80965.3102 | Cumulative Reward -0.7380
Episode 145/500 | Average Loss 85374.2427 | Cumulative Reward 0.0738
Episode 146/500 | Average Loss 74547.1055 | Cumulative Reward -0.2952
Episode 147/500 | Average Loss 61201.4695 | Cumulative Reward -0.0000
Episode 148/500 | Average Loss 58691.6379 | Cumulative Reward -0.7872
Episode 149/500 | Average Loss 53623.2068 | Cumulative Reward -0.1968
Episode 150/500 | Average Loss 52295.3436 | Cumulative Reward 0.5658
Episode 151/500 | Average Loss 54563.6969 | Cumulative Reward -0.7528
Episode 152/500 | Average Loss 56892.9392 | Cumulative Reward -1.5990
Episode 153/500 | Average Loss 56162.3126 | Cumulative Reward 0.1968
Episode 154/500 | Average Loss 57015.3386 | Cumulative Reward -0.1722
Episode 155/500 | Average Loss 61344.5501 | Cumulative Reward 0.2214
Episode 156/500 | Average Loss 63970.4605 | Cumulative Reward 0.1968
Episode 157/500 | Average Loss 64938.6926 | Cumulative Reward 1.3776
Episode 158/500 | Average Loss 71837.3588 | Cumulative Reward -0.4428
Episode 159/500 | Average Loss 69606.6803 | Cumulative Reward 1.1316
Episode 160/500 | Average Loss 51512.9089 | Cumulative Reward -0.4428
Episode 161/500 | Average Loss 42763.9575 | Cumulative Reward -0.6642
Episode 162/500 | Average Loss 34133.1615 | Cumulative Reward 0.0492
Episode 163/500 | Average Loss 26055.7404 | Cumulative Reward -0.7380
Episode 164/500 | Average Loss 17645.5418 | Cumulative Reward 0.3198
Episode 165/500 | Average Loss 23771.6887 | Cumulative Reward -0.6888
Episode 166/500 | Average Loss 33822.2709 | Cumulative Reward -1.4760
Episode 167/500 | Average Loss 36789.7859 | Cumulative Reward -0.3444
Episode 168/500 | Average Loss 30012.2132 | Cumulative Reward 0.3198
Episode 169/500 | Average Loss 25915.5318 | Cumulative Reward 0.0246
Episode 170/500 | Average Loss 21197.9396 | Cumulative Reward 0.4182
Episode 171/500 | Average Loss 18828.6276 | Cumulative Reward 0.6642
Episode 172/500 | Average Loss 12931.5156 | Cumulative Reward 0.1230
Episode 173/500 | Average Loss 9384.6666 | Cumulative Reward -0.5412
Episode 174/500 | Average Loss 9953.6688 | Cumulative Reward 0.0246
Episode 175/500 | Average Loss 14217.0748 | Cumulative Reward 0.0738
Episode 176/500 | Average Loss 18767.0269 | Cumulative Reward -0.2706
Episode 177/500 | Average Loss 17302.4695 | Cumulative Reward 0.4674
Episode 178/500 | Average Loss 12386.1551 | Cumulative Reward 0.0984
Episode 179/500 | Average Loss 8794.3302 | Cumulative Reward 0.9348
Episode 180/500 | Average Loss 8130.0289 | Cumulative Reward -0.2214
Episode 181/500 | Average Loss 8009.3913 | Cumulative Reward -0.3198
Episode 182/500 | Average Loss 6421.1121 | Cumulative Reward -0.0492
Episode 183/500 | Average Loss 5636.7044 | Cumulative Reward 0.1968
Episode 184/500 | Average Loss 8854.0772 | Cumulative Reward 0.4920
Episode 185/500 | Average Loss 10231.7754 | Cumulative Reward 0.9840
Episode 186/500 | Average Loss 8491.7384 | Cumulative Reward -0.1722
Episode 187/500 | Average Loss 5335.5632 | Cumulative Reward -0.8856
Episode 188/500 | Average Loss 4152.4838 | Cumulative Reward 1.1316
Episode 189/500 | Average Loss 3644.6625 | Cumulative Reward 1.3284
Episode 190/500 | Average Loss 4318.2997 | Cumulative Reward -0.2460
Episode 191/500 | Average Loss 4694.9497 | Cumulative Reward -0.0492
Episode 192/500 | Average Loss 3490.6077 | Cumulative Reward 0.4674
Episode 193/500 | Average Loss 3043.5791 | Cumulative Reward 0.2903
Episode 194/500 | Average Loss 2338.0377 | Cumulative Reward 0.4182
Episode 195/500 | Average Loss 2115.3294 | Cumulative Reward 1.5990
Episode 196/500 | Average Loss 3127.7692 | Cumulative Reward -1.0578
Episode 197/500 | Average Loss 4421.3908 | Cumulative Reward -0.2214
Episode 198/500 | Average Loss 2560.3734 | Cumulative Reward 0.2706
Episode 199/500 | Average Loss 2147.2506 | Cumulative Reward -1.5006
Episode 200/500 | Average Loss 2207.9893 | Cumulative Reward -0.5658
Episode 201/500 | Average Loss 3174.7853 | Cumulative Reward -1.4760
Episode 202/500 | Average Loss 11331.6762 | Cumulative Reward 0.3198
Episode 203/500 | Average Loss 37899.0565 | Cumulative Reward -0.6150
Episode 204/500 | Average Loss 36401.8491 | Cumulative Reward -0.1230
Episode 205/500 | Average Loss 12925.8751 | Cumulative Reward -0.2706
Episode 206/500 | Average Loss 12854.6668 | Cumulative Reward -0.3936
Episode 207/500 | Average Loss 11591.4458 | Cumulative Reward -0.1968
Episode 208/500 | Average Loss 21179.2468 | Cumulative Reward 0.4920
Episode 209/500 | Average Loss 20376.6122 | Cumulative Reward 0.1968
Episode 210/500 | Average Loss 18849.6060 | Cumulative Reward 0.0000
Episode 211/500 | Average Loss 13982.4748 | Cumulative Reward -0.2214
Episode 212/500 | Average Loss 87311.4121 | Cumulative Reward -0.8856
Episode 213/500 | Average Loss 654473.7298 | Cumulative Reward 0.0492
Episode 214/500 | Average Loss 201913.9874 | Cumulative Reward -0.2214
Episode 215/500 | Average Loss 44836.9004 | Cumulative Reward 0.1968
Episode 216/500 | Average Loss 35657.7584 | Cumulative Reward 0.1968
Episode 217/500 | Average Loss 32079.6191 | Cumulative Reward -0.0246
Episode 218/500 | Average Loss 30304.5018 | Cumulative Reward -0.0492
Episode 219/500 | Average Loss 30373.2290 | Cumulative Reward -0.1968
Episode 220/500 | Average Loss 29203.3995 | Cumulative Reward 0.0492
Episode 221/500 | Average Loss 31887.5387 | Cumulative Reward 0.4428
Episode 222/500 | Average Loss 27344.9023 | Cumulative Reward -0.2706
Episode 223/500 | Average Loss 26080.0415 | Cumulative Reward 1.2546
Episode 224/500 | Average Loss 27963.8084 | Cumulative Reward -0.2952
Episode 225/500 | Average Loss 22244.3293 | Cumulative Reward 0.4674
Episode 226/500 | Average Loss 19426.3250 | Cumulative Reward 0.1476
Episode 227/500 | Average Loss 19027.4235 | Cumulative Reward 3.0504
Episode 228/500 | Average Loss 22579.4337 | Cumulative Reward 1.3284
Episode 229/500 | Average Loss 35449.1802 | Cumulative Reward -0.0000
Episode 230/500 | Average Loss 36867.8054 | Cumulative Reward -0.3936
Episode 231/500 | Average Loss 49915.4290 | Cumulative Reward -0.3936
Episode 232/500 | Average Loss 32619.6804 | Cumulative Reward -0.6396
Episode 233/500 | Average Loss 12634.1564 | Cumulative Reward 1.6728
Episode 234/500 | Average Loss 11710.0142 | Cumulative Reward -0.2952
Episode 235/500 | Average Loss 11534.9517 | Cumulative Reward 0.0246
Episode 236/500 | Average Loss 21333.6566 | Cumulative Reward -0.1476
Episode 237/500 | Average Loss 14613.8575 | Cumulative Reward -1.2792
Episode 238/500 | Average Loss 18945.5421 | Cumulative Reward 0.1968
Episode 239/500 | Average Loss 11825.2040 | Cumulative Reward 1.4268
Episode 240/500 | Average Loss 7433.5705 | Cumulative Reward -0.9594
Episode 241/500 | Average Loss 4902.9148 | Cumulative Reward -0.4674
Episode 242/500 | Average Loss 3179.8172 | Cumulative Reward 0.6888
Episode 243/500 | Average Loss 3503.6598 | Cumulative Reward -0.6150
Episode 244/500 | Average Loss 3431.9749 | Cumulative Reward -0.7134
Episode 245/500 | Average Loss 2877.5648 | Cumulative Reward -0.0246
Episode 246/500 | Average Loss 2608.7916 | Cumulative Reward -0.3690
Episode 247/500 | Average Loss 2049.1479 | Cumulative Reward -0.1476
Episode 248/500 | Average Loss 1290.3437 | Cumulative Reward -0.0246
Episode 249/500 | Average Loss 925.7207 | Cumulative Reward 0.5412
Episode 250/500 | Average Loss 823.1600 | Cumulative Reward 0.3444
Episode 251/500 | Average Loss 1006.1303 | Cumulative Reward 0.0246
Episode 252/500 | Average Loss 1000.9536 | Cumulative Reward 0.1968
Episode 253/500 | Average Loss 910.4431 | Cumulative Reward 0.0492
Episode 254/500 | Average Loss 793.4383 | Cumulative Reward -0.0738
Episode 255/500 | Average Loss 960.7262 | Cumulative Reward -0.0492
Episode 256/500 | Average Loss 1210.5615 | Cumulative Reward 0.7626
Episode 257/500 | Average Loss 1456.9758 | Cumulative Reward -0.0984
Episode 258/500 | Average Loss 1492.6155 | Cumulative Reward -0.2706
Episode 259/500 | Average Loss 800.3773 | Cumulative Reward -0.1968
Episode 260/500 | Average Loss 663.3038 | Cumulative Reward -0.5166
Episode 261/500 | Average Loss 570.7030 | Cumulative Reward 0.5412
Episode 262/500 | Average Loss 515.7673 | Cumulative Reward -0.0246
Episode 263/500 | Average Loss 914.1635 | Cumulative Reward 0.3936
Episode 264/500 | Average Loss 635.1886 | Cumulative Reward 0.5166
Episode 265/500 | Average Loss 503.3200 | Cumulative Reward 0.3936
Episode 266/500 | Average Loss 442.9404 | Cumulative Reward 0.1722
Episode 267/500 | Average Loss 372.8969 | Cumulative Reward -0.1722
Episode 268/500 | Average Loss 342.8540 | Cumulative Reward 0.8610
Episode 269/500 | Average Loss 336.0440 | Cumulative Reward 1.1562
Episode 270/500 | Average Loss 325.4765 | Cumulative Reward 1.4022
Episode 271/500 | Average Loss 639.1140 | Cumulative Reward 0.0738
Episode 272/500 | Average Loss 467.9056 | Cumulative Reward -1.3530
Episode 273/500 | Average Loss 691.2283 | Cumulative Reward -0.8856
Episode 274/500 | Average Loss 442.4127 | Cumulative Reward -0.0246
Episode 275/500 | Average Loss 672.4676 | Cumulative Reward -0.0000
Episode 276/500 | Average Loss 373.9056 | Cumulative Reward 0.7626
Episode 277/500 | Average Loss 510.2552 | Cumulative Reward 0.0492
Episode 278/500 | Average Loss 476.9959 | Cumulative Reward -0.0246
Episode 279/500 | Average Loss 374.8663 | Cumulative Reward 0.4182
Episode 280/500 | Average Loss 285.5737 | Cumulative Reward 0.2214
Episode 281/500 | Average Loss 359.3125 | Cumulative Reward 0.0738
Episode 282/500 | Average Loss 509.2474 | Cumulative Reward 0.0738
Episode 283/500 | Average Loss 509.7669 | Cumulative Reward -0.9840
Episode 284/500 | Average Loss 848.4750 | Cumulative Reward -0.1968
Episode 285/500 | Average Loss 746.0073 | Cumulative Reward 0.6396
Episode 286/500 | Average Loss 354.4148 | Cumulative Reward 0.4920
Episode 287/500 | Average Loss 744.5271 | Cumulative Reward -1.2546
Episode 288/500 | Average Loss 1177.0523 | Cumulative Reward -0.0246
Episode 289/500 | Average Loss 912.7724 | Cumulative Reward -0.0000
Episode 290/500 | Average Loss 493.3918 | Cumulative Reward 0.7872
Episode 291/500 | Average Loss 666.1050 | Cumulative Reward -0.4428
Episode 292/500 | Average Loss 855.3613 | Cumulative Reward -2.3124
Episode 293/500 | Average Loss 558.7653 | Cumulative Reward -0.6396
Episode 294/500 | Average Loss 418.6678 | Cumulative Reward -0.6396
Episode 295/500 | Average Loss 534.7206 | Cumulative Reward 0.1722
Episode 296/500 | Average Loss 264.4277 | Cumulative Reward 0.1230
Episode 297/500 | Average Loss 389.4987 | Cumulative Reward -0.2214
Episode 298/500 | Average Loss 728.0504 | Cumulative Reward 0.7134
Episode 299/500 | Average Loss 632.7627 | Cumulative Reward -0.3198
Episode 300/500 | Average Loss 637.5207 | Cumulative Reward -0.9348
Episode 301/500 | Average Loss 1028.3571 | Cumulative Reward -0.2952
Episode 302/500 | Average Loss 1685.8779 | Cumulative Reward 0.0246
Episode 303/500 | Average Loss 1392.4933 | Cumulative Reward 0.6150
Episode 304/500 | Average Loss 788.6780 | Cumulative Reward -0.2952
Episode 305/500 | Average Loss 1226.8964 | Cumulative Reward 0.7134
Episode 306/500 | Average Loss 771.0202 | Cumulative Reward -0.1968
Episode 307/500 | Average Loss 1038.6742 | Cumulative Reward 1.4514
Episode 308/500 | Average Loss 2320.5486 | Cumulative Reward -0.5658
Episode 309/500 | Average Loss 1688.7067 | Cumulative Reward -0.8364
Episode 310/500 | Average Loss 1235.5575 | Cumulative Reward 1.0824
Episode 311/500 | Average Loss 314.3540 | Cumulative Reward 0.4920
Episode 312/500 | Average Loss 221.4066 | Cumulative Reward -0.4674
Episode 313/500 | Average Loss 149.2911 | Cumulative Reward -1.4022
Episode 314/500 | Average Loss 251.1208 | Cumulative Reward 0.2952
Episode 315/500 | Average Loss 295.4088 | Cumulative Reward 0.0984
Episode 316/500 | Average Loss 353.4387 | Cumulative Reward 0.2214
Episode 317/500 | Average Loss 471.9230 | Cumulative Reward -0.1968
Episode 318/500 | Average Loss 481.8251 | Cumulative Reward -0.0246
Episode 319/500 | Average Loss 277.8028 | Cumulative Reward -0.2706
Episode 320/500 | Average Loss 458.1981 | Cumulative Reward 0.7134
Episode 321/500 | Average Loss 554.8020 | Cumulative Reward 0.0984
Episode 322/500 | Average Loss 712.4146 | Cumulative Reward -0.1722
Episode 323/500 | Average Loss 533.7365 | Cumulative Reward 1.5252
Episode 324/500 | Average Loss 361.0181 | Cumulative Reward 0.8118
Episode 325/500 | Average Loss 447.6775 | Cumulative Reward 1.0332
Episode 326/500 | Average Loss 1355.3373 | Cumulative Reward 0.0000
Episode 327/500 | Average Loss 908.2948 | Cumulative Reward -0.0984
Episode 328/500 | Average Loss 959.7786 | Cumulative Reward -1.2054
Episode 329/500 | Average Loss 450.1697 | Cumulative Reward -0.7134
Episode 330/500 | Average Loss 1212.9733 | Cumulative Reward -0.5166
Episode 331/500 | Average Loss 5285.4580 | Cumulative Reward 0.0738
Episode 332/500 | Average Loss 3094.1890 | Cumulative Reward 0.3690
Episode 333/500 | Average Loss 4562.0196 | Cumulative Reward -0.4674
Episode 334/500 | Average Loss 8924.8034 | Cumulative Reward 1.0824
Episode 335/500 | Average Loss 30994.9012 | Cumulative Reward -1.1808
Episode 336/500 | Average Loss 534143731.8066 | Cumulative Reward -0.1476
Episode 337/500 | Average Loss 5921050444109.0820 | Cumulative Reward -0.4428
Episode 338/500 | Average Loss 4505985552.3279 | Cumulative Reward 0.1230
Episode 339/500 | Average Loss 1964492739.4754 | Cumulative Reward 0.1968
Episode 340/500 | Average Loss 1593002800.1967 | Cumulative Reward 0.5486
Episode 341/500 | Average Loss 1377590111.9344 | Cumulative Reward 0.0984
Episode 342/500 | Average Loss 1024536314.2295 | Cumulative Reward 0.3936
Episode 343/500 | Average Loss 714535699.4098 | Cumulative Reward -1.0824
Episode 344/500 | Average Loss 581171856.7541 | Cumulative Reward -0.7626
Episode 345/500 | Average Loss 1080491153.4754 | Cumulative Reward -0.3444
Episode 346/500 | Average Loss 1290483386.2951 | Cumulative Reward -0.1968
Episode 347/500 | Average Loss 638285799.5984 | Cumulative Reward 0.0492
Episode 348/500 | Average Loss 533999203.1926 | Cumulative Reward 0.1968
Episode 349/500 | Average Loss 1186640501.3566 | Cumulative Reward -0.4674
Episode 350/500 | Average Loss 836347220.6230 | Cumulative Reward 2.1402
Episode 351/500 | Average Loss 19564334.5430 | Cumulative Reward 0.2460
Episode 352/500 | Average Loss 15821917.5266 | Cumulative Reward -0.0738
Episode 353/500 | Average Loss 8794998.8955 | Cumulative Reward 0.7380
Episode 354/500 | Average Loss 16293582.7008 | Cumulative Reward -0.1722
Episode 355/500 | Average Loss 17792936.4262 | Cumulative Reward -0.7872
Episode 356/500 | Average Loss 19392693.2541 | Cumulative Reward 0.0246
Episode 357/500 | Average Loss 12951887.3934 | Cumulative Reward -0.3690
Episode 358/500 | Average Loss 21376279.0902 | Cumulative Reward -0.1968
Episode 359/500 | Average Loss 18399117.6311 | Cumulative Reward 0.3198
Episode 360/500 | Average Loss 16349954.5328 | Cumulative Reward -0.0492
Episode 361/500 | Average Loss 10212129.2971 | Cumulative Reward 0.0246
Episode 362/500 | Average Loss 9528178.3489 | Cumulative Reward 0.5166
Episode 363/500 | Average Loss 10891389.6680 | Cumulative Reward -0.2952
Episode 364/500 | Average Loss 8848364.4677 | Cumulative Reward -0.1722
Episode 365/500 | Average Loss 7967370.4139 | Cumulative Reward -0.7872
Episode 366/500 | Average Loss 10170590.0517 | Cumulative Reward 1.1808
Episode 367/500 | Average Loss 7691382.5763 | Cumulative Reward 0.6150
Episode 368/500 | Average Loss 19905810.8770 | Cumulative Reward -0.4920
Episode 369/500 | Average Loss 29691840.6557 | Cumulative Reward -0.1476
Episode 370/500 | Average Loss 26487591.1967 | Cumulative Reward 0.1230
Episode 371/500 | Average Loss 23137364.8689 | Cumulative Reward -0.3936
Episode 372/500 | Average Loss 30061004.7541 | Cumulative Reward -0.3444
Episode 373/500 | Average Loss 40822205.1148 | Cumulative Reward 0.2460
Episode 374/500 | Average Loss 31239984.0328 | Cumulative Reward 0.5166
Episode 375/500 | Average Loss 32901997.1639 | Cumulative Reward 0.0246
Episode 376/500 | Average Loss 22117612.4918 | Cumulative Reward -0.1968
Episode 377/500 | Average Loss 21721578.4795 | Cumulative Reward 0.3690
Episode 378/500 | Average Loss 19514039.5656 | Cumulative Reward -0.4428
Episode 379/500 | Average Loss 18671316.6721 | Cumulative Reward -0.2214
Episode 380/500 | Average Loss 19410190.3115 | Cumulative Reward 0.3936
Episode 381/500 | Average Loss 37436785.1475 | Cumulative Reward 0.6642
Episode 382/500 | Average Loss 12312954.6025 | Cumulative Reward 0.1476
Episode 383/500 | Average Loss 12721993.5220 | Cumulative Reward -1.1808
Episode 384/500 | Average Loss 13490703.0113 | Cumulative Reward -0.1476
Episode 385/500 | Average Loss 12027525.5820 | Cumulative Reward -0.0984
Episode 386/500 | Average Loss 8776895.7362 | Cumulative Reward 0.1722
Episode 387/500 | Average Loss 27030448.9508 | Cumulative Reward -1.2546
Episode 388/500 | Average Loss 17247655.4836 | Cumulative Reward 0.7134
Episode 389/500 | Average Loss 13761868.2480 | Cumulative Reward -0.8856
Episode 390/500 | Average Loss 15799021.5820 | Cumulative Reward -1.0332
Episode 391/500 | Average Loss 24067677.2961 | Cumulative Reward -2.0664
Episode 392/500 | Average Loss 15877324.8064 | Cumulative Reward -0.0000
Episode 393/500 | Average Loss 10898226.3576 | Cumulative Reward 0.2460
Episode 394/500 | Average Loss 11247735.4857 | Cumulative Reward 0.5904
Episode 395/500 | Average Loss 14609065.8217 | Cumulative Reward 0.1968
Episode 396/500 | Average Loss 2343010.0123 | Cumulative Reward 0.1968
Episode 397/500 | Average Loss 3205300.5410 | Cumulative Reward -0.4182
Episode 398/500 | Average Loss 8199556.5533 | Cumulative Reward 0.0492
Episode 399/500 | Average Loss 21762857.0984 | Cumulative Reward -0.3444
Episode 400/500 | Average Loss 15102527.0164 | Cumulative Reward -0.0246
Episode 401/500 | Average Loss 12123659.1414 | Cumulative Reward -0.1230
Episode 402/500 | Average Loss 2576558.2551 | Cumulative Reward 0.2706
Episode 403/500 | Average Loss 2949066.3484 | Cumulative Reward -0.0984
Episode 404/500 | Average Loss 4379467.7418 | Cumulative Reward 0.1722
Episode 405/500 | Average Loss 2939416.2049 | Cumulative Reward 1.7712
Episode 406/500 | Average Loss 10072456.5164 | Cumulative Reward -0.5412
Episode 407/500 | Average Loss 11368148.5574 | Cumulative Reward -1.6728
Episode 408/500 | Average Loss 4251800.9057 | Cumulative Reward 0.0246
Episode 409/500 | Average Loss 2425709.3094 | Cumulative Reward 1.5990
Episode 410/500 | Average Loss 1586738.5707 | Cumulative Reward -0.4920
Episode 411/500 | Average Loss 1070607.6132 | Cumulative Reward -1.2054
Episode 412/500 | Average Loss 5415716.3217 | Cumulative Reward 0.0000
Episode 413/500 | Average Loss 1737057.5220 | Cumulative Reward 0.5904
Episode 414/500 | Average Loss 1278951.9534 | Cumulative Reward 0.4182
Episode 415/500 | Average Loss 728374.3837 | Cumulative Reward 0.5658
Episode 416/500 | Average Loss 9779674.6527 | Cumulative Reward -0.8856
Episode 417/500 | Average Loss 39090349.0164 | Cumulative Reward -0.1230
Episode 418/500 | Average Loss 17068589.6311 | Cumulative Reward 3.0258
Episode 419/500 | Average Loss 19433508.8033 | Cumulative Reward 0.6396
Episode 420/500 | Average Loss 8357417.2172 | Cumulative Reward -0.3936
Episode 421/500 | Average Loss 11197790.3607 | Cumulative Reward 0.2706
Episode 422/500 | Average Loss 6756482.8443 | Cumulative Reward 1.2054
Episode 423/500 | Average Loss 3976738.0656 | Cumulative Reward -0.4182
Episode 424/500 | Average Loss 3616583.8033 | Cumulative Reward -0.2706
Episode 425/500 | Average Loss 2603585.6906 | Cumulative Reward 0.0246
Episode 426/500 | Average Loss 2082487.2746 | Cumulative Reward 0.6396
Episode 427/500 | Average Loss 2534304.0727 | Cumulative Reward -0.5658
Episode 428/500 | Average Loss 12130751.7418 | Cumulative Reward -0.2214
Episode 429/500 | Average Loss 69780277.8033 | Cumulative Reward 0.4182
Episode 430/500 | Average Loss 78495889.1803 | Cumulative Reward -0.8610
Episode 431/500 | Average Loss 45958012.8525 | Cumulative Reward -0.0984
Episode 432/500 | Average Loss 136452393.9016 | Cumulative Reward 0.1476
Episode 433/500 | Average Loss 38613835.7541 | Cumulative Reward -0.5166
Episode 434/500 | Average Loss 33883618.4098 | Cumulative Reward -1.0332
Episode 435/500 | Average Loss 41067984.8197 | Cumulative Reward -0.1230
Episode 436/500 | Average Loss 91378111.4098 | Cumulative Reward -1.4760
Episode 437/500 | Average Loss 94752215.4098 | Cumulative Reward -0.4674
Episode 438/500 | Average Loss 40647725.0492 | Cumulative Reward 0.3444
Episode 439/500 | Average Loss 35812338.5574 | Cumulative Reward -0.1230
Episode 440/500 | Average Loss 29541471.6230 | Cumulative Reward 0.3198
Episode 441/500 | Average Loss 60021648.1885 | Cumulative Reward -1.1070
Episode 442/500 | Average Loss 158118092.1967 | Cumulative Reward -0.0984
Episode 443/500 | Average Loss 59849613.7049 | Cumulative Reward 0.2706
Episode 444/500 | Average Loss 46461701.9016 | Cumulative Reward -0.2952
Episode 445/500 | Average Loss 48389924.3934 | Cumulative Reward 1.0086
Episode 446/500 | Average Loss 195580981.4754 | Cumulative Reward -0.2460
Episode 447/500 | Average Loss 1722251685.6393 | Cumulative Reward 0.2214
Episode 448/500 | Average Loss 3515392862.4262 | Cumulative Reward -0.0492
Episode 449/500 | Average Loss 306165022.4836 | Cumulative Reward 1.4760
Episode 450/500 | Average Loss 99212269.6230 | Cumulative Reward 0.0738
Episode 451/500 | Average Loss 86367019.5410 | Cumulative Reward 2.5338
Episode 452/500 | Average Loss 31133120.3689 | Cumulative Reward -0.6642
Episode 453/500 | Average Loss 21015204.2172 | Cumulative Reward 1.8942
Episode 454/500 | Average Loss 11878485.2541 | Cumulative Reward -0.9102
Episode 455/500 | Average Loss 14406326.1680 | Cumulative Reward 0.1968
Episode 456/500 | Average Loss 26474831.6639 | Cumulative Reward 0.0000
Episode 457/500 | Average Loss 20445096.5492 | Cumulative Reward -0.1476
Episode 458/500 | Average Loss 77507055.2131 | Cumulative Reward -0.9348
Episode 459/500 | Average Loss 34715110.0984 | Cumulative Reward -0.1722
Episode 460/500 | Average Loss 11601538.8443 | Cumulative Reward -0.1230
Episode 461/500 | Average Loss 2247047.2961 | Cumulative Reward 0.1230
Episode 462/500 | Average Loss 939256.9170 | Cumulative Reward -0.0246
Episode 463/500 | Average Loss 808798.3960 | Cumulative Reward 1.6728
Episode 464/500 | Average Loss 683601.6122 | Cumulative Reward -0.4182
Episode 465/500 | Average Loss 1048500.5922 | Cumulative Reward 1.2792
Episode 466/500 | Average Loss 996181.3012 | Cumulative Reward 0.9594
Episode 467/500 | Average Loss 958677.9529 | Cumulative Reward -0.1230
Episode 468/500 | Average Loss 640709.4234 | Cumulative Reward 0.2952
Episode 469/500 | Average Loss 561099.1286 | Cumulative Reward -0.4182
Episode 470/500 | Average Loss 858037.4851 | Cumulative Reward -0.0984
Episode 471/500 | Average Loss 781562.1660 | Cumulative Reward -0.4428
Episode 472/500 | Average Loss 585637.4631 | Cumulative Reward 0.2706
Episode 473/500 | Average Loss 1691876.3504 | Cumulative Reward -0.0984
Episode 474/500 | Average Loss 857916.0794 | Cumulative Reward 0.8364
Episode 475/500 | Average Loss 697805.0133 | Cumulative Reward 1.2300
Episode 476/500 | Average Loss 428738.8163 | Cumulative Reward -0.9102
Episode 477/500 | Average Loss 412427.7167 | Cumulative Reward -0.1722
Episode 478/500 | Average Loss 371060.8876 | Cumulative Reward 0.3198
Episode 479/500 | Average Loss 290708.3730 | Cumulative Reward 0.0984
Episode 480/500 | Average Loss 293991.3815 | Cumulative Reward 0.3690
Episode 481/500 | Average Loss 271680.2395 | Cumulative Reward 0.1968
Episode 482/500 | Average Loss 247069.2293 | Cumulative Reward 0.2706
Episode 483/500 | Average Loss 298824.8330 | Cumulative Reward -0.0246
Episode 484/500 | Average Loss 272823.3829 | Cumulative Reward 0.1476
Episode 485/500 | Average Loss 221586.0561 | Cumulative Reward 0.2706
Episode 486/500 | Average Loss 255791.8814 | Cumulative Reward -0.2952
Episode 487/500 | Average Loss 508487.5830 | Cumulative Reward 0.0984
Episode 488/500 | Average Loss 559980.2203 | Cumulative Reward -0.7134
Episode 489/500 | Average Loss 378515.2789 | Cumulative Reward -0.7626
Episode 490/500 | Average Loss 250586.9393 | Cumulative Reward -0.4182
Episode 491/500 | Average Loss 251211.5902 | Cumulative Reward -0.5904
Episode 492/500 | Average Loss 169529.8831 | Cumulative Reward -0.5904
Episode 493/500 | Average Loss 242881.4125 | Cumulative Reward -0.1968
Episode 494/500 | Average Loss 573974.6457 | Cumulative Reward 0.4182
Episode 495/500 | Average Loss 200285.0907 | Cumulative Reward 0.7626
Episode 496/500 | Average Loss 268848.6760 | Cumulative Reward 0.1968
Episode 497/500 | Average Loss 235332.8327 | Cumulative Reward 0.0246
Episode 498/500 | Average Loss 202253.1831 | Cumulative Reward 0.0492
Episode 499/500 | Average Loss 149803.7602 | Cumulative Reward 0.1476
</pre></div>
</div>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plotting training results</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">returns</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Return&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Episode&quot;</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We have trained our model for 500 episodes and the returns are plotted
above. Note that the loss was still quite high at the end of training,
which indicates that the algorithm hasnâ€™t converged. A possible
explanation for this is that RL algorithms typically require
significantly more steps to converge. Further, considering the size of
the tranining dataset, the neural network used is very small. Besides
that, DQN is known to be quite unstable and prone to diverge, which is
why several new versions of this algorithm have been proposed since it
was first introduced. A very common implementation consists of the
Double DQN, which introduced a target Q-network used to compute the
actions, which is updated at a lower rate than the main Q-network. In
our implementation, the max operator uses the same network both to
select and to evaluate an action. This may lead to wrongly selecting
overestimated values. Having a separate target network can help prevent
this, by decoupling the selection from the evaluation.</p>
</div>
<div class="section" id="testing-rl-agent">
<h2>Testing RL agent<a class="headerlink" href="#testing-rl-agent" title="Permalink to this headline">Â¶</a></h2>
<p>In order to test our agent, we select the whole data from the 1st of
January 2019, which wasnâ€™t included during training.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">states</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">actions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">reward_sum</span> <span class="o">=</span> <span class="mf">0.</span>

<span class="c1"># Define testing interval, January 2019</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2019</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2019</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">59</span><span class="p">)</span>

<span class="c1"># Test learned model</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">MarketEnv</span><span class="p">(</span><span class="n">oilDF_py</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">episode_size</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="n">sequence_scope</span><span class="p">)</span>
<span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">random_starttime</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">input_t</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">sequence_scope</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>    
    <span class="n">states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">input_t</span><span class="p">)</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">q</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">actions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
    <span class="n">reward_sum</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">input_t</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">sequence_scope</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>      
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Return = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">reward_sum</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Return = 0.096
</pre></div>
</div>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plotting testing results</span>
<span class="n">timesteps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">states</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">states</span><span class="p">))</span>
<span class="n">longs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span> <span class="o">==</span>  <span class="mi">0</span><span class="p">)</span>
<span class="n">shorts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span> <span class="o">==</span>  <span class="mi">1</span><span class="p">)</span>
<span class="n">states</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">timesteps</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;diff_close&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">timesteps</span><span class="p">[</span><span class="n">longs</span><span class="p">],</span> <span class="n">states</span><span class="p">[</span><span class="n">longs</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="s1">&#39;*g&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;long&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">timesteps</span><span class="p">[</span><span class="n">shorts</span><span class="p">],</span> <span class="n">states</span><span class="p">[</span><span class="n">shorts</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="s1">&#39;*r&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;short&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;(s,a)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Timestep&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">states</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">timesteps</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="s1">&#39;o-r&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;r&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Timestep&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">states</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">display</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can see that the policy converged to always shorting, meaning that
the agent never buys any stock. While abstaining from investments in
fossil fuels may good advice, the result is not very useful for our
intended application. Nevertherless, reaching a successful automatic
intraday trading bot in the short time we spent implementing this
project would be a high bar. After all, this is more or less the holy
grail of computational economy.</p>
</div>
<div class="section" id="symmary-and-future-work">
<h2>Symmary and Future Work<a class="headerlink" href="#symmary-and-future-work" title="Permalink to this headline">Â¶</a></h2>
<p>In this project we have implemented an RL agent for intraday trading.
This required some data preprocessing using</p>
</div>
<div class="section" id="trend-calculus">
<h2>Trend Calculus<a class="headerlink" href="#trend-calculus" title="Permalink to this headline">Â¶</a></h2>
<p>Taken from: <a class="reference external" href="https://github.com/lamastex/spark-trend-calculus-examples">https://github.com/lamastex/spark-trend-calculus-examples</a></p>
<p>Trend Calculus is an algorithm invented by Andrew Morgan that is used to
find trend changes in a time series ( see
<a class="reference external" href="https://github.com/bytesumo/TrendCalculus/blob/master/HowToStudyTrends_v1.03.pdf">here</a>).
It works by grouping the observations in the time series into windows
and defining a trend upwards as â€œhigher highs and higher lowsâ€ compared
to the previous window. A downwards trend is similarly defined as â€œlower
highs and lower lowsâ€.</p>
<p>&lt;img
src=https://lamastex.github.io/spark-trend-calculus-examples/notebooks/db/images/HigherHighHigherLow.png
width=300&gt;</p>
<p>If there is a higher high and lower low (or lower high and higher low),
no trend is detected. This is solved by introducing intermediate windows
that split the non-trend into two trends, ensuring that every point can
be labeled with either an up or down trend.</p>
<p>&lt;img
src=https://lamastex.github.io/spark-trend-calculus-examples/notebooks/db/images/OuterInnerBars.png
width=600&gt;</p>
<p>When the trends have been calculated for all windows, the points where
the trends change sign are labeled as reversals. If the reversal is from
up to down, the previous high is the reversal point and if the reversal
is from down to up, the previous low is the reversal. This means that
the reversals always are the appropriate extrema (maximum for up to
down, minimum for down to up).</p>
<p>&lt;img
src=https://lamastex.github.io/spark-trend-calculus-examples/notebooks/db/images/trendReversals.png
width=600&gt;</p>
<p>The output of the algorithm is a time series consisting of all the
labelled reversal points. It is therefore possible to use this as the
input for another run of the Trend Calculus algorithm, finding more long
term trends.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>val windowSize = 2
val numReversals = 1 // we look at 1 iteration of the algorithm. 

val dfWithReversals = new TrendCalculus2(oilDS, windowSize, spark).nReversalsJoinedWithMaxRev(numReversals)
display(dfWithReversals)

val windowSpec = Window.orderBy(&quot;x&quot;)
val dfWithReversalsDiff = dfWithReversals 
.withColumn(&quot;diff_close&quot;, $&quot;y&quot; - when((lag(&quot;y&quot;, 1).over(windowSpec)).isNull, 0).otherwise(lag(&quot;y&quot;, 1).over(windowSpec)))

// Store loaded data as temp view, to be accessible in Python
dfWithReversalsDiff.createOrReplaceTempView(&quot;temp&quot;)
</pre></div>
</div>
</div>
</div>
<p>[TABLE]</p>
<p>Truncated to 30 rows</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Taken from: https://lamastex.github.io/spark-trend-calculus-examples/notebooks/db/01trend-calculus-showcase.html</span>

<span class="c1"># Create Dataframe from temp data</span>
<span class="n">fullDS</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s2">&quot;temp&quot;</span><span class="p">)</span>
<span class="n">fullTS</span> <span class="o">=</span> <span class="n">fullDS</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="s2">&quot;reversal1&quot;</span><span class="p">,</span> <span class="s2">&quot;diff_close&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>

<span class="n">startDate</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2019</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="c1"># first window used as scope</span>
<span class="n">endDate</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2019</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">59</span><span class="p">)</span>
<span class="n">TS</span> <span class="o">=</span> <span class="p">[</span><span class="n">row</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">fullTS</span> <span class="k">if</span> <span class="n">startDate</span> <span class="o">&lt;=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="ow">and</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">endDate</span><span class="p">]</span>

<span class="n">allData</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">TS</span><span class="p">],</span> <span class="s1">&#39;close&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">TS</span><span class="p">],</span> <span class="s1">&#39;diff_close&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;diff_close&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">TS</span><span class="p">],</span> <span class="s1">&#39;reversal1&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;reversal1&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">TS</span><span class="p">]}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot reversals</span>
<span class="n">close</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">allData</span><span class="p">[</span><span class="s1">&#39;close&#39;</span><span class="p">])</span>
<span class="n">diff_close</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">allData</span><span class="p">[</span><span class="s1">&#39;diff_close&#39;</span><span class="p">])</span>
<span class="n">timesteps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">diff_close</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">diff_close</span><span class="p">))</span>
<span class="n">revs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">allData</span><span class="p">[</span><span class="s1">&#39;reversal1&#39;</span><span class="p">])</span>
<span class="n">pos_rev_ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">revs</span> <span class="o">==</span>  <span class="mi">1</span><span class="p">)</span>
<span class="n">neg_rev_ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">revs</span> <span class="o">==</span>  <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">timesteps</span><span class="p">,</span> <span class="n">close</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;close&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">timesteps</span><span class="p">[</span><span class="n">pos_rev_ind</span><span class="p">],</span> <span class="n">close</span><span class="p">[</span><span class="n">pos_rev_ind</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="s1">&#39;*g&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;+ reversal&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">timesteps</span><span class="p">[</span><span class="n">neg_rev_ind</span><span class="p">],</span> <span class="n">close</span><span class="p">[</span><span class="n">neg_rev_ind</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="s1">&#39;*r&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;- reversal&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;close&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Timestep&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">close</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">timesteps</span><span class="p">,</span> <span class="n">diff_close</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;diff_close&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">timesteps</span><span class="p">[</span><span class="n">pos_rev_ind</span><span class="p">],</span> <span class="n">diff_close</span><span class="p">[</span><span class="n">pos_rev_ind</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="s1">&#39;*g&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;+ reversal&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">timesteps</span><span class="p">[</span><span class="n">neg_rev_ind</span><span class="p">],</span> <span class="n">diff_close</span><span class="p">[</span><span class="n">neg_rev_ind</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="s1">&#39;*r&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;- reversal&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;diff_close&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Timestep&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">diff_close</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">display</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The TrendCalculus algorithm provides an analytical framework effective
for identifying trends in historical price data, including <a class="reference external" href="https://lamastex.github.io/spark-trend-calculus-examples/notebooks/db/01trend-calculus-showcase.html">trend
pattern
analysis</a>
and <a class="reference external" href="https://lamastex.github.io/spark-trend-calculus-examples/notebooks/db/03streamable-trend-calculus-estimators.html">prediction of trend
changes</a>.
In conjunction with TrendCalculus, a complete automatic trading pipeline
can be constructed, consisting of (i) trend analysis with TrendCalculus
(ii) time series prediction (iii) control, i.e. buy or sell.
Implementing and evaluating a pipeline such as the one outlined in the
aforementioned steps is left as a suggestion for future work, and it is
of particular interest to compare the performance of such a method to a
learning-based one.</p>
<p>Adopting a learning-based approach, in particular using RL, entails
several potential benefits. Firstly, several ML methods allow
learning-based pre-processing steps, such as convolutional layers which
enable automatic feature extraction and detection, and may be used to
focus the computation on the most relevant features. Secondly,
constructing an end-to-end learning-based pipeline makes the prediction
step implicit, and potentially reduces the problem complexity to
predicting only certain aspects or features of the time series which are
necessary for the control strategy, as opposed to attempting to predict
the exact time series values. Thirdly, an end-to-end learning-based
approach alleviates potential bounds of the step-wise modularization
that a human-designed pipeline would entail, and allows the learning
algorithm to automatically deduce the optimal strategy for utilizing any
feature signal, in order to execute the most efficient control strategy.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./000_0-sds-3-x-projects/student-project-15_group-FinancialDataStreams"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="00_video.html" title="previous page">Reinforcement Learning for Intraday Trading</a>
    <a class='right-next' id="next-link" href="02_rl_intraday_trading_elephas.html" title="next page">Reinforcement Learning for Intraday Trading - Distributed model tuning with Elephas</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By ScaDaMaLe Team<br/>
        
            &copy; Copyright 2020 Creative Commons Zero v1.0 Universal.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>