<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>sds-3.x/ScaDaMaLe</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="scroll-mdbook-outputs.css">

        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="contents/000_3-sds-3-x-st/024_SparkStreamingIntro.html">024_SparkStreamingIntro</a></li><li class="chapter-item expanded affix "><a href="contents/000_3-sds-3-x-st/025_0_getTwitterDeveloperCredentials.html">025_0_getTwitterDeveloperCredentials</a></li><li class="chapter-item expanded affix "><a href="contents/000_3-sds-3-x-st/025_a_extendedTwitterUtils2run.html">025_a_extendedTwitterUtils2run</a></li><li class="chapter-item expanded affix "><a href="contents/000_3-sds-3-x-st/025_b_TTTDFfunctions.html">025_b_TTTDFfunctions</a></li><li class="chapter-item expanded affix "><a href="contents/000_3-sds-3-x-st/025_c_extendedTwitterUtils2runWithLangs.html">025_c_extendedTwitterUtils2runWithLangs</a></li><li class="chapter-item expanded affix "><a href="contents/000_3-sds-3-x-st/026_TweetCollector.html">026_TweetCollector</a></li><li class="chapter-item expanded affix "><a href="contents/000_3-sds-3-x-st/027_TweetCollectorTrackAndFollow.html">027_TweetCollectorTrackAndFollow</a></li><li class="chapter-item expanded affix "><a href="contents/000_3-sds-3-x-st/028_TweetHashtagCount.html">028_TweetHashtagCount</a></li><li class="chapter-item expanded affix "><a href="contents/000_3-sds-3-x-st/029_TweetLanguageClassifier.html">029_TweetLanguageClassifier</a></li><li class="chapter-item expanded affix "><a href="contents/000_3-sds-3-x-st/029_Viz_0_TwitterInteractiveVizualisations.html">029_Viz_0_TwitterInteractiveVizualisations</a></li><li class="chapter-item expanded affix "><a href="contents/000_3-sds-3-x-st/029_Viz_1_GraphNetworkTimeline.html">029_Viz_1_GraphNetworkTimeline</a></li><li class="chapter-item expanded affix "><a href="contents/000_3-sds-3-x-st/029_Viz_2_CoSENSE_EmotionalDimensions.html">029_Viz_2_CoSENSE_EmotionalDimensions</a></li><li class="chapter-item expanded affix "><a href="contents/000_3-sds-3-x-st/029_Viz_x_VizGraphFunction.html">029_Viz_x_VizGraphFunction</a></li><li class="chapter-item expanded affix "><a href="contents/000_3-sds-3-x-st/029_Viz_x_VizNetworkFunction.html">029_Viz_x_VizNetworkFunction</a></li><li class="chapter-item expanded affix "><a href="contents/000_3-sds-3-x-st/029_Viz_x_VizTimelineFunction.html">029_Viz_x_VizTimelineFunction</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">sds-3.x/ScaDaMaLe</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="introduction-to-spark-streaming"><a class="header" href="#introduction-to-spark-streaming">Introduction to Spark Streaming</a></h1>
<p>Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams.</p>
<p>This is a walk-through of excerpts from the following resources:</p>
<ul>
<li>the Databricks Guide:
<ul>
<li><a href="https://docs.databricks.com/spark/latest/rdd-streaming/index.html">Spark Streaming - RDD-based</a> and</li>
<li><a href="https://docs.databricks.com/spark/latest/structured-streaming/index.html#structured-streaming">Structured Streaming - DatFrame/Dataset-Based</a></li>
</ul>
</li>
<li>Spark programming guide:
<ul>
<li><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">http://spark.apache.org/docs/latest/streaming-programming-guide.html</a></li>
<li><a href="http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html">http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html</a></li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams.</p>
<p>Data can be ingested from many sources like</p>
<ul>
<li><a href="http://kafka.apache.org/documentation.html#introduction">Kafka</a>,</li>
<li><a href="https://flume.apache.org/">Flume</a>,</li>
<li><a href="https://twitter.com/">Twitter</a> <a href="https://dev.twitter.com/streaming/overview">Streaming</a> and <a href="https://dev.twitter.com/rest/public">REST</a> APIs,</li>
<li><a href="http://zeromq.org/">ZeroMQ</a>,</li>
<li><a href="https://aws.amazon.com/kinesis/streams/">Amazon Kinesis</a>, or</li>
<li><a href="http://www.gnu.org/software/mit-scheme/documentation/mit-scheme-ref/TCP-Sockets.html">TCP sockets</a>,</li>
<li>etc</li>
</ul>
<p>and can be processed using complex algorithms expressed with high-level functions like <code>map</code>, <code>reduce</code>, <code>join</code> and <code>window</code>.</p>
<p>Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark's</p>
<ul>
<li><a href="http://spark.apache.org/docs/latest/mllib-guide.html">machine learning</a> and</li>
<li><a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">graph processing</a> algorithms on data streams.</li>
</ul>
<p><img src="http://spark.apache.org/docs/latest/img/streaming-arch.png" alt="Spark Streaming architecture" /></p>
<p><strong>Internally, it works as follows:</strong></p>
<ul>
<li>Spark Streaming receives live input data streams and</li>
<li>divides the data into batches,</li>
<li>which are then processed by the Spark engine</li>
<li>to generate the final stream of results in batches.</li>
</ul>
<p><img src="http://spark.apache.org/docs/latest/img/streaming-flow.png" alt="Spark Streaming data flow" /></p>
<h2 id="discretized-streams-dstreams"><a class="header" href="#discretized-streams-dstreams">Discretized Streams (DStreams)</a></h2>
<p><strong>Discretized Stream</strong> or <strong>DStream</strong> is the basic abstraction provided by Spark Streaming. It represents a continuous stream of data, either the input data stream received from source (for eg. Kafka, Flume, and Kinesis) or the processed data stream generated by transforming the input stream. Internally, a DStream is represented by a continuous series of RDDs, which is Spark's abstraction of an immutable, distributed dataset (see <a href="http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds">Spark Programming Guide</a> for more details). Each RDD in a DStream contains data from a certain interval, as shown in the following figure.</p>
<p><img src="http://spark.apache.org/docs/latest/img/streaming-dstream.png" alt="Spark Streaming" title="Spark Streaming data flow" /></p>
<hr />
<p>This guide shows you how to start writing Spark Streaming programs with DStreams. You can write Spark Streaming programs in Scala, Java or Python (introduced in Spark 1.2), all of which are presented in this <a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">guide</a>.</p>
<p>Here, we will focus on Streaming in Scala.</p>
<hr />
<p><strong>Spark Streaming</strong> is a near-real-time micro-batch stream processing engine as opposed to other real-time stream processing frameworks like <a href="http://storm.apache.org/">Apache Storm</a>. Typically 'near-real-time' in Spark Streaming can be in the order of seconds as opposed to milliseconds, for example.</p>
</div>
<div class="cell markdown">
<h1 id="three-quick-examples"><a class="header" href="#three-quick-examples">Three Quick Examples</a></h1>
<p>Before we go into the details of how to write your own Spark Streaming program, let us take a quick look at what a simple Spark Streaming program looks like.</p>
<p>We will choose the first two examples in Databricks notebooks below.</p>
</div>
<div class="cell markdown">
<p><strong>Spark Streaming Hello World Examples</strong></p>
<p>These are adapted from several publicly available Databricks Notebooks</p>
<ol>
<li>Streaming Word Count (Scala)</li>
</ol>
<ul>
<li>Tweet Collector for Capturing Live Tweets</li>
<li>Twitter Hashtag Count (Scala)</li>
</ul>
<p>Other examples we won't try here:</p>
<ul>
<li>Kinesis Word Count (Scala)</li>
<li>Kafka Word Count (Scala)</li>
<li>FileStream Word Count (Python)</li>
<li>etc.</li>
</ul>
</div>
<div class="cell markdown">
<ol>
<li>Streaming Word Count</li>
</ol>
<hr />
<p>This is a <em>hello world</em> example of Spark Streaming which counts words on 1 second batches of streaming data.</p>
<p>It uses an in-memory string generator as a dummy source for streaming data.</p>
<h3 id="setting-up-a-streaming-source"><a class="header" href="#setting-up-a-streaming-source">Setting up a streaming source</a></h3>
</div>
<div class="cell markdown">
<p><strong>Configurations</strong></p>
<p>Configurations that control the streaming app in the notebook</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// === Configuration to control the flow of the application ===
val stopActiveContext = true	 
// &quot;true&quot;  = stop if any existing StreamingContext is running;              
// &quot;false&quot; = dont stop, and let it run undisturbed, but your latest code may not be used

// === Configurations for Spark Streaming ===
val batchIntervalSeconds = 1 
val eventsPerSecond = 1000    // For the dummy source

// Verify that the attached Spark cluster is 1.4.0+
require(sc.version.replace(&quot;.&quot;, &quot;&quot;).toInt &gt;= 140, &quot;Spark 1.4.0+ is required to run this notebook. Please attach it to a Spark 1.4.0+ cluster.&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>stopActiveContext: Boolean = true
batchIntervalSeconds: Int = 1
eventsPerSecond: Int = 1000
</code></pre>
</div>
</div>
<div class="cell markdown">
<p><strong>Imports</strong></p>
<p>Import all the necessary libraries. If you see any error here, you have to make sure that you have attached the necessary libraries to the attached cluster.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark._
import org.apache.spark.storage._
import org.apache.spark.streaming._
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import org.apache.spark._
import org.apache.spark.storage._
import org.apache.spark.streaming._
</code></pre>
</div>
</div>
<div class="cell markdown">
<p><strong>Setup: Define the function that sets up the StreamingContext</strong></p>
<p>In this we will do two things.</p>
<ul>
<li>
<p>Define a custom receiver as the dummy source (no need to understand this)</p>
<ul>
<li>this custom receiver will have lines that end with a random number between 0 and 9 and read:</li>
</ul>
<!-- -->
<pre><code>I am a dummy source 2
I am a dummy source 8
...
</code></pre>
</li>
</ul>
</div>
<div class="cell markdown">
<p>This is the dummy source implemented as a custom receiver. <strong>No need to understand this now.</strong></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// This is the dummy source implemented as a custom receiver. No need to fully understand this.

import scala.util.Random
import org.apache.spark.streaming.receiver._

class DummySource(ratePerSec: Int) extends Receiver[String](StorageLevel.MEMORY_AND_DISK_2) {

  def onStart() {
    // Start the thread that receives data over a connection
    new Thread(&quot;Dummy Source&quot;) {
      override def run() { receive() }
    }.start()
  }

  def onStop() {
   // There is nothing much to do as the thread calling receive()
   // is designed to stop by itself isStopped() returns false
  }

  /** Create a socket connection and receive data until receiver is stopped */
  private def receive() {
    while(!isStopped()) {      
      store(&quot;I am a dummy source &quot; + Random.nextInt(10))
      Thread.sleep((1000.toDouble / ratePerSec).toInt)
    }
  }
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import scala.util.Random
import org.apache.spark.streaming.receiver._
defined class DummySource
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="transforming-and-acting-on-the-dstream-of-lines"><a class="header" href="#transforming-and-acting-on-the-dstream-of-lines">Transforming and Acting on the DStream of lines</a></h3>
<p>Any operation applied on a DStream translates to operations on the underlying RDDs. For converting a stream of lines to words, the <code>flatMap</code> operation is applied on each RDD in the <code>lines</code> DStream to generate the RDDs of the <code>wordStream</code> DStream. This is shown in the following figure.</p>
<p><img src="http://spark.apache.org/docs/latest/img/streaming-dstream-ops.png" alt="Spark Streaming" title="Spark Streaming data flow" /></p>
<p>These underlying RDD transformations are computed by the Spark engine. The DStream operations hide most of these details and provide the developer with a higher-level API for convenience.</p>
<p>Next <code>reduceByKey</code> is used to get <code>wordCountStream</code> that counts the words in <code>wordStream</code>.</p>
<p>Finally, this is registered as a temporary table for each RDD in the DStream.</p>
</div>
<div class="cell markdown">
<p>Let's try to understand the following <code>creatingFunc</code> to create a new StreamingContext and setting it up for word count and registering it as temp table for each batch of 1000 lines per second in the stream.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">batchIntervalSeconds
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res1: Int = 1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">var newContextCreated = false      // Flag to detect whether new context was created or not

// Function to create a new StreamingContext and set it up
def creatingFunc(): StreamingContext = {
    
  // Create a StreamingContext - starting point for a Spark Streaming job
  val ssc = new StreamingContext(sc, Seconds(batchIntervalSeconds))
  
  // Create a stream that generates 1000 lines per second
  val stream = ssc.receiverStream(new DummySource(eventsPerSecond))  
  
  // Split the lines into words, and then do word count
  val wordStream = stream.flatMap { _.split(&quot; &quot;)  }
  val wordCountStream = wordStream.map(word =&gt; (word, 1)).reduceByKey(_ + _)

  // Create temp table at every batch interval
  wordCountStream.foreachRDD { rdd =&gt; 
    rdd.toDF(&quot;word&quot;, &quot;count&quot;).createOrReplaceTempView(&quot;batch_word_count&quot;)    
  }
  
  stream.foreachRDD { rdd =&gt;
    System.out.println(&quot;# events = &quot; + rdd.count())
    System.out.println(&quot;\t &quot; + rdd.take(10).mkString(&quot;, &quot;) + &quot;, ...&quot;)
  }
  
  ssc.remember(Minutes(1))  // To make sure data is not deleted by the time we query it interactively
  
  println(&quot;Creating function called to create new StreamingContext&quot;)
  newContextCreated = true  
  ssc
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>newContextCreated: Boolean = false
creatingFunc: ()org.apache.spark.streaming.StreamingContext
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="start-streaming-job"><a class="header" href="#start-streaming-job">Start Streaming Job</a></h3>
<p>First it is important to <strong>stop existing StreamingContext if any</strong> and then start/restart the new one.</p>
<p>Here we are going to use the configurations at the top of the notebook to decide whether to stop any existing StreamingContext, and start a new one, or recover one from existing checkpoints.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Stop any existing StreamingContext 
// The getActive function is proviced by Databricks to access active Streaming Contexts
if (stopActiveContext) {	
  StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) }
} 

// Get or create a streaming context
val ssc = StreamingContext.getActiveOrCreate(creatingFunc)
if (newContextCreated) {
  println(&quot;New context created from currently defined creating function&quot;) 
} else {
  println(&quot;Existing context running or recovered from checkpoint, may not be running currently defined creating function&quot;)
}

// Start the streaming context in the background.
ssc.start()

// This is to ensure that we wait for some time before the background streaming job starts. This will put this cell on hold for 5 times the batchIntervalSeconds.
ssc.awaitTerminationOrTimeout(batchIntervalSeconds * 5 * 1000)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Creating function called to create new StreamingContext
New context created from currently defined creating function
# events = 0
	 , ...
# events = 803
	 I am a dummy source 6, I am a dummy source 6, I am a dummy source 5, I am a dummy source 3, I am a dummy source 7, I am a dummy source 2, I am a dummy source 8, I am a dummy source 1, I am a dummy source 7, I am a dummy source 5, ...
# events = 879
	 I am a dummy source 7, I am a dummy source 5, I am a dummy source 3, I am a dummy source 0, I am a dummy source 2, I am a dummy source 3, I am a dummy source 1, I am a dummy source 2, I am a dummy source 7, I am a dummy source 7, ...
# events = 888
	 I am a dummy source 9, I am a dummy source 0, I am a dummy source 1, I am a dummy source 2, I am a dummy source 7, I am a dummy source 3, I am a dummy source 3, I am a dummy source 0, I am a dummy source 1, I am a dummy source 9, ...
# events = 883
	 I am a dummy source 6, I am a dummy source 5, I am a dummy source 4, I am a dummy source 6, I am a dummy source 4, I am a dummy source 2, I am a dummy source 3, I am a dummy source 6, I am a dummy source 1, I am a dummy source 6, ...
ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@50c3571
res2: Boolean = false
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="interactive-querying"><a class="header" href="#interactive-querying">Interactive Querying</a></h3>
<p>Now let's try querying the table. You can run this command again and again, you will find the numbers changing.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">select * from batch_word_count
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>word</th>
<th>count</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>8</td>
<td>107.0</td>
</tr>
<tr class="even">
<td>0</td>
<td>100.0</td>
</tr>
<tr class="odd">
<td>dummy</td>
<td>890.0</td>
</tr>
<tr class="even">
<td>a</td>
<td>890.0</td>
</tr>
<tr class="odd">
<td>I</td>
<td>890.0</td>
</tr>
<tr class="even">
<td>9</td>
<td>70.0</td>
</tr>
<tr class="odd">
<td>1</td>
<td>89.0</td>
</tr>
<tr class="even">
<td>2</td>
<td>87.0</td>
</tr>
<tr class="odd">
<td>source</td>
<td>890.0</td>
</tr>
<tr class="even">
<td>3</td>
<td>91.0</td>
</tr>
<tr class="odd">
<td>4</td>
<td>79.0</td>
</tr>
<tr class="even">
<td>am</td>
<td>890.0</td>
</tr>
<tr class="odd">
<td>5</td>
<td>96.0</td>
</tr>
<tr class="even">
<td>6</td>
<td>82.0</td>
</tr>
<tr class="odd">
<td>7</td>
<td>89.0</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell markdown">
<p>Try again for current table.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">select * from batch_word_count 
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>word</th>
<th>count</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>8</td>
<td>109.0</td>
</tr>
<tr class="even">
<td>0</td>
<td>77.0</td>
</tr>
<tr class="odd">
<td>dummy</td>
<td>888.0</td>
</tr>
<tr class="even">
<td>a</td>
<td>888.0</td>
</tr>
<tr class="odd">
<td>I</td>
<td>888.0</td>
</tr>
<tr class="even">
<td>9</td>
<td>84.0</td>
</tr>
<tr class="odd">
<td>1</td>
<td>85.0</td>
</tr>
<tr class="even">
<td>2</td>
<td>89.0</td>
</tr>
<tr class="odd">
<td>source</td>
<td>888.0</td>
</tr>
<tr class="even">
<td>3</td>
<td>86.0</td>
</tr>
<tr class="odd">
<td>4</td>
<td>91.0</td>
</tr>
<tr class="even">
<td>am</td>
<td>888.0</td>
</tr>
<tr class="odd">
<td>5</td>
<td>74.0</td>
</tr>
<tr class="even">
<td>6</td>
<td>84.0</td>
</tr>
<tr class="odd">
<td>7</td>
<td>109.0</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell markdown">
<h3 id="go-to-spark-ui-now-and-see-streaming-job-running"><a class="header" href="#go-to-spark-ui-now-and-see-streaming-job-running">Go to Spark UI now and see Streaming job running</a></h3>
</div>
<div class="cell markdown">
<p>Finally, if you want stop the StreamingContext, you can uncomment and execute the following code:</p>
<p><code>StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) }</code></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) } // please do this if you are done!
</code></pre>
</div>
<div class="cell markdown">
<p>Next two examples Spark Streaming is with live tweets.</p>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<p>Dear Researcher,</p>
<p>The main form of assessment for day 01 of module 01 and both days of module 03 is most likely going to be a fairly open-ended project in groups of appropriate sizes (with some structure TBD) that is close to your research interests. The exact details will be given at the end of day 02 of module 03. There will most likely be dedicated Office Hours after module 03 to support you with the project (admin, infrastructure, etc).</p>
<p>Towards this, as one possibility for project, I strongly encourage you to apply for Twitter Developer Account (you need a Twitter user account first). This process can take couple of weeks. With Twitter developer account you can do your own experiments in Twitter and it would be an interesting application of streaming.</p>
<p>The instructions are roughly as follows (Twitter will ask different questions to different users... rules keep evolving... just make it clear you are just wanting credentials for learning Spark streaming. Keep it simple.): https://lamastex.github.io/scalable-data-science/sds/basics/instructions/getTwitterDevCreds/ (Links to an external site.)</p>
<p>You can still follow the lab/lectures without your own Twitter Developer credentials IF I/we dynamically decide to go through Twitter examples of Streaming (provided at least one of you has applied for the developer credentials and is interested in such experiments), but then you will not be able to do your own experiments live in Twitter as I will be doing mine.</p>
<p>Twitter can be a fun source of interesting projects from uniformly sampled social media interactions in the world (but there are many other projects, especially those coming from your own research questions).</p>
<p>Cheers!</p>
<p>Raaz</p>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="twitter-utility-functions"><a class="header" href="#twitter-utility-functions">Twitter Utility Functions</a></h1>
<p>Here we develop a few notebooks starting with <code>025_*</code> to help us with Twitter experiments.</p>
<h2 id="extended-twitterutils"><a class="header" href="#extended-twitterutils">Extended TwitterUtils</a></h2>
<p>We extend twitter utils from Spark to allow for filtering by user-ids using <code>.follow</code> and strings in the tweet using <code>.track</code> method of <code>twitter4j</code>.</p>
<p>This is part of <em>Project MEP: Meme Evolution Programme</em> and supported by databricks, AWS and a Swedish VR grant.</p>
<p>The analysis is available in the following databricks notebook: * <a href="http://lamastex.org/lmse/mep/src/extendedTwitterUtil.html">http://lamastex.org/lmse/mep/src/extendedTwitterUtils.html</a></p>
<pre><code>Copyright 2016-2020 Ivan Sadikov and Raazesh Sainudiin

Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import twitter4j._
import twitter4j.auth.Authorization
import twitter4j.conf.ConfigurationBuilder
import twitter4j.auth.OAuthAuthorization

import org.apache.spark.streaming._
import org.apache.spark.streaming.dstream._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.receiver.Receiver
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import twitter4j._
import twitter4j.auth.Authorization
import twitter4j.conf.ConfigurationBuilder
import twitter4j.auth.OAuthAuthorization
import org.apache.spark.streaming._
import org.apache.spark.streaming.dstream._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.receiver.Receiver
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="twitter-receiver-and-stream"><a class="header" href="#twitter-receiver-and-stream">Twitter receiver and stream</a></h3>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">class ExtendedTwitterReceiver(
    twitterAuth: Authorization,
    filters: Seq[String],
    userFilters: Seq[Long],
    storageLevel: StorageLevel
  ) extends Receiver[Status](storageLevel) {

  @volatile private var twitterStream: TwitterStream = _
  @volatile private var stopped = false

  def onStart() {
    try {
      val newTwitterStream = new TwitterStreamFactory().getInstance(twitterAuth)
      newTwitterStream.addListener(new StatusListener {
        def onStatus(status: Status): Unit = {
          store(status)
        }
        // Unimplemented
        def onDeletionNotice(statusDeletionNotice: StatusDeletionNotice) {}
        def onTrackLimitationNotice(i: Int) {}
        def onScrubGeo(l: Long, l1: Long) {}
        def onStallWarning(stallWarning: StallWarning) {}
        def onException(e: Exception) {
          if (!stopped) {
            restart(&quot;Error receiving tweets&quot;, e)
          }
        }
      })

      // do filtering only when filters are available
      if (filters.nonEmpty || userFilters.nonEmpty) {
        val query = new FilterQuery()
        if (filters.nonEmpty) {
          query.track(filters.mkString(&quot;,&quot;))
        }

        if (userFilters.nonEmpty) {
          query.follow(userFilters: _*)
        }
        
        newTwitterStream.filter(query)
      } else {
        newTwitterStream.sample()
      }
      setTwitterStream(newTwitterStream)
      println(&quot;Twitter receiver started&quot;)
      stopped = false
    } catch {
      case e: Exception =&gt; restart(&quot;Error starting Twitter stream&quot;, e)
    }
  }

  def onStop() {
    stopped = true
    setTwitterStream(null)
    println(&quot;Twitter receiver stopped&quot;)
  }

  private def setTwitterStream(newTwitterStream: TwitterStream) = synchronized {
    if (twitterStream != null) {
      twitterStream.shutdown()
    }
    twitterStream = newTwitterStream
  }
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined class ExtendedTwitterReceiver
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">class ExtendedTwitterInputDStream(
    ssc_ : StreamingContext,
    twitterAuth: Option[Authorization],
    filters: Seq[String],
    userFilters: Seq[Long],
    storageLevel: StorageLevel
  ) extends ReceiverInputDStream[Status](ssc_)  {

  private def createOAuthAuthorization(): Authorization = {
    new OAuthAuthorization(new ConfigurationBuilder().build())
  }

  private val authorization = twitterAuth.getOrElse(createOAuthAuthorization())

  override def getReceiver(): Receiver[Status] = {
    new ExtendedTwitterReceiver(authorization, filters, userFilters, storageLevel)
  }
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined class ExtendedTwitterInputDStream
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="extended-twitter-utils"><a class="header" href="#extended-twitter-utils">Extended twitter utils</a></h3>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import twitter4j.Status
import twitter4j.auth.Authorization
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.dstream.{ReceiverInputDStream, DStream}

object ExtendedTwitterUtils {
  def createStream(
      ssc: StreamingContext,
      twitterAuth: Option[Authorization],
      filters: Seq[String] = Nil,
      userFilters: Seq[Long] = Nil,
      storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2
    ): ReceiverInputDStream[Status] = {
    new ExtendedTwitterInputDStream(ssc, twitterAuth, filters, userFilters, storageLevel)
  }
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import twitter4j.Status
import twitter4j.auth.Authorization
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.dstream.{ReceiverInputDStream, DStream}
defined module ExtendedTwitterUtils
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(&quot;done running the extendedTwitterUtils2run notebook - ready to stream from twitter&quot;)
</code></pre>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h2 id="tweet-transmission-tree-function"><a class="header" href="#tweet-transmission-tree-function">Tweet Transmission Tree Function</a></h2>
<p>This is part of <em>Project MEP: Meme Evolution Programme</em> and supported by databricks, AWS and a Swedish VR grant.</p>
<p>Please see the following notebook to understand the rationale for the Tweet Transmission Tree Functions: * <a href="http://lamastex.org/lmse/mep/src/TweetAnatomyAndTransmissionTree.htmll">http://lamastex.org/lmse/mep/src/TweetAnatomyAndTransmissionTree.html</a></p>
<pre><code>Copyright 2016-2020 Akinwande Atanda and Raazesh Sainudiin

Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.sql.types.{StructType, StructField, StringType};
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.spark.sql.ColumnName
import org.apache.spark.sql.DataFrame

spark.sql(&quot;set spark.sql.legacy.timeParserPolicy=LEGACY&quot;)

def fromParquetFile2DF(InputDFAsParquetFilePatternString: String): DataFrame = {
      sqlContext.
        read.parquet(InputDFAsParquetFilePatternString)
}

def tweetsJsonStringDF2TweetsDF(tweetsAsJsonStringInputDF: DataFrame): DataFrame = {
      sqlContext
        .read
        .json(tweetsAsJsonStringInputDF.map({case Row(val1: String) =&gt; val1}))
      }

def tweetsIDLong_JsonStringPairDF2TweetsDF(tweetsAsIDLong_JsonStringInputDF: DataFrame): DataFrame = {
      sqlContext
        .read
        .json(tweetsAsIDLong_JsonStringInputDF.map({case Row(val0:Long, val1: String) =&gt; val1}))
      }

def tweetsDF2TTTDF(tweetsInputDF: DataFrame): DataFrame = {
 tweetsInputDF.select(
  unix_timestamp($&quot;createdAt&quot;, &quot;&quot;&quot;MMM dd, yyyy hh:mm:ss a&quot;&quot;&quot;).cast(TimestampType).as(&quot;CurrentTweetDate&quot;),
  $&quot;id&quot;.as(&quot;CurrentTwID&quot;),
  $&quot;lang&quot;.as(&quot;lang&quot;),
  $&quot;geoLocation.latitude&quot;.as(&quot;lat&quot;),
  $&quot;geoLocation.longitude&quot;.as(&quot;lon&quot;),
  unix_timestamp($&quot;retweetedStatus.createdAt&quot;, &quot;&quot;&quot;MMM dd, yyyy hh:mm:ss a&quot;&quot;&quot;).cast(TimestampType).as(&quot;CreationDateOfOrgTwInRT&quot;), 
  $&quot;retweetedStatus.id&quot;.as(&quot;OriginalTwIDinRT&quot;),  
  unix_timestamp($&quot;quotedStatus.createdAt&quot;, &quot;&quot;&quot;MMM dd, yyyy hh:mm:ss a&quot;&quot;&quot;).cast(TimestampType).as(&quot;CreationDateOfOrgTwInQT&quot;), 
  $&quot;quotedStatus.id&quot;.as(&quot;OriginalTwIDinQT&quot;), 
  $&quot;inReplyToStatusId&quot;.as(&quot;OriginalTwIDinReply&quot;), 
  $&quot;user.id&quot;.as(&quot;CPostUserId&quot;),
  unix_timestamp($&quot;user.createdAt&quot;, &quot;&quot;&quot;MMM dd, yyyy hh:mm:ss a&quot;&quot;&quot;).cast(TimestampType).as(&quot;userCreatedAtDate&quot;),
  $&quot;retweetedStatus.user.id&quot;.as(&quot;OPostUserIdinRT&quot;), 
  $&quot;quotedStatus.user.id&quot;.as(&quot;OPostUserIdinQT&quot;),
  $&quot;inReplyToUserId&quot;.as(&quot;OPostUserIdinReply&quot;),
  $&quot;user.name&quot;.as(&quot;CPostUserName&quot;), 
  $&quot;retweetedStatus.user.name&quot;.as(&quot;OPostUserNameinRT&quot;), 
  $&quot;quotedStatus.user.name&quot;.as(&quot;OPostUserNameinQT&quot;), 
  $&quot;user.screenName&quot;.as(&quot;CPostUserSN&quot;), 
  $&quot;retweetedStatus.user.screenName&quot;.as(&quot;OPostUserSNinRT&quot;), 
  $&quot;quotedStatus.user.screenName&quot;.as(&quot;OPostUserSNinQT&quot;),
  $&quot;inReplyToScreenName&quot;.as(&quot;OPostUserSNinReply&quot;),
  $&quot;user.favouritesCount&quot;,
  $&quot;user.followersCount&quot;,
  $&quot;user.friendsCount&quot;,
  $&quot;user.isVerified&quot;,
  $&quot;user.isGeoEnabled&quot;,
  $&quot;text&quot;.as(&quot;CurrentTweet&quot;), 
  $&quot;retweetedStatus.userMentionEntities.id&quot;.as(&quot;UMentionRTiD&quot;), 
  $&quot;retweetedStatus.userMentionEntities.screenName&quot;.as(&quot;UMentionRTsN&quot;), 
  $&quot;quotedStatus.userMentionEntities.id&quot;.as(&quot;UMentionQTiD&quot;), 
  $&quot;quotedStatus.userMentionEntities.screenName&quot;.as(&quot;UMentionQTsN&quot;), 
  $&quot;userMentionEntities.id&quot;.as(&quot;UMentionASiD&quot;), 
  $&quot;userMentionEntities.screenName&quot;.as(&quot;UMentionASsN&quot;)
 ).withColumn(&quot;TweetType&quot;,
    when($&quot;OriginalTwIDinRT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinQT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; === -1,
      &quot;Original Tweet&quot;)
    .when($&quot;OriginalTwIDinRT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinQT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; &gt; -1,
      &quot;Reply Tweet&quot;)
    .when($&quot;OriginalTwIDinRT&quot;.isNotNull &amp;&amp;$&quot;OriginalTwIDinQT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; === -1,
      &quot;ReTweet&quot;)
    .when($&quot;OriginalTwIDinRT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinQT&quot;.isNotNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; === -1,
      &quot;Quoted Tweet&quot;)
    .when($&quot;OriginalTwIDinRT&quot;.isNotNull &amp;&amp; $&quot;OriginalTwIDinQT&quot;.isNotNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; === -1,
      &quot;Retweet of Quoted Tweet&quot;)
    .when($&quot;OriginalTwIDinRT&quot;.isNotNull &amp;&amp; $&quot;OriginalTwIDinQT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; &gt; -1,
      &quot;Retweet of Reply Tweet&quot;)
    .when($&quot;OriginalTwIDinRT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinQT&quot;.isNotNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; &gt; -1,
      &quot;Reply of Quoted Tweet&quot;)
    .when($&quot;OriginalTwIDinRT&quot;.isNotNull &amp;&amp; $&quot;OriginalTwIDinQT&quot;.isNotNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; &gt; -1,
      &quot;Retweet of Quoted Rely Tweet&quot;)
      .otherwise(&quot;Unclassified&quot;))
.withColumn(&quot;MentionType&quot;, 
    when($&quot;UMentionRTid&quot;.isNotNull &amp;&amp; $&quot;UMentionQTid&quot;.isNotNull, &quot;RetweetAndQuotedMention&quot;)
    .when($&quot;UMentionRTid&quot;.isNotNull &amp;&amp; $&quot;UMentionQTid&quot;.isNull, &quot;RetweetMention&quot;)
    .when($&quot;UMentionRTid&quot;.isNull &amp;&amp; $&quot;UMentionQTid&quot;.isNotNull, &quot;QuotedMention&quot;)
    .when($&quot;UMentionRTid&quot;.isNull &amp;&amp; $&quot;UMentionQTid&quot;.isNull, &quot;AuthoredMention&quot;)
    .otherwise(&quot;NoMention&quot;))
.withColumn(&quot;Weight&quot;, lit(1L))
}

def tweetsDF2TTTDFWithURLsAndHashtags(tweetsInputDF: DataFrame): DataFrame = {
 tweetsInputDF.select(
  unix_timestamp($&quot;createdAt&quot;, &quot;&quot;&quot;MMM dd, yyyy hh:mm:ss a&quot;&quot;&quot;).cast(TimestampType).as(&quot;CurrentTweetDate&quot;),
  $&quot;id&quot;.as(&quot;CurrentTwID&quot;),
  $&quot;lang&quot;.as(&quot;lang&quot;),
  $&quot;geoLocation.latitude&quot;.as(&quot;lat&quot;),
  $&quot;geoLocation.longitude&quot;.as(&quot;lon&quot;),
  unix_timestamp($&quot;retweetedStatus.createdAt&quot;, &quot;&quot;&quot;MMM dd, yyyy hh:mm:ss a&quot;&quot;&quot;).cast(TimestampType).as(&quot;CreationDateOfOrgTwInRT&quot;), 
  $&quot;retweetedStatus.id&quot;.as(&quot;OriginalTwIDinRT&quot;),  
  unix_timestamp($&quot;quotedStatus.createdAt&quot;, &quot;&quot;&quot;MMM dd, yyyy hh:mm:ss a&quot;&quot;&quot;).cast(TimestampType).as(&quot;CreationDateOfOrgTwInQT&quot;), 
  $&quot;quotedStatus.id&quot;.as(&quot;OriginalTwIDinQT&quot;), 
  $&quot;inReplyToStatusId&quot;.as(&quot;OriginalTwIDinReply&quot;), 
  $&quot;user.id&quot;.as(&quot;CPostUserId&quot;),
  unix_timestamp($&quot;user.createdAt&quot;, &quot;&quot;&quot;MMM dd, yyyy hh:mm:ss a&quot;&quot;&quot;).cast(TimestampType).as(&quot;userCreatedAtDate&quot;),
  $&quot;retweetedStatus.user.id&quot;.as(&quot;OPostUserIdinRT&quot;), 
  $&quot;quotedStatus.user.id&quot;.as(&quot;OPostUserIdinQT&quot;),
  $&quot;inReplyToUserId&quot;.as(&quot;OPostUserIdinReply&quot;),
  $&quot;user.name&quot;.as(&quot;CPostUserName&quot;), 
  $&quot;retweetedStatus.user.name&quot;.as(&quot;OPostUserNameinRT&quot;), 
  $&quot;quotedStatus.user.name&quot;.as(&quot;OPostUserNameinQT&quot;), 
  $&quot;user.screenName&quot;.as(&quot;CPostUserSN&quot;), 
  $&quot;retweetedStatus.user.screenName&quot;.as(&quot;OPostUserSNinRT&quot;), 
  $&quot;quotedStatus.user.screenName&quot;.as(&quot;OPostUserSNinQT&quot;),
  $&quot;inReplyToScreenName&quot;.as(&quot;OPostUserSNinReply&quot;),
  $&quot;user.favouritesCount&quot;,
  $&quot;user.followersCount&quot;,
  $&quot;user.friendsCount&quot;,
  $&quot;user.isVerified&quot;,
  $&quot;user.isGeoEnabled&quot;,
  $&quot;text&quot;.as(&quot;CurrentTweet&quot;), 
  $&quot;retweetedStatus.userMentionEntities.id&quot;.as(&quot;UMentionRTiD&quot;), 
  $&quot;retweetedStatus.userMentionEntities.screenName&quot;.as(&quot;UMentionRTsN&quot;), 
  $&quot;quotedStatus.userMentionEntities.id&quot;.as(&quot;UMentionQTiD&quot;), 
  $&quot;quotedStatus.userMentionEntities.screenName&quot;.as(&quot;UMentionQTsN&quot;), 
  $&quot;userMentionEntities.id&quot;.as(&quot;UMentionASiD&quot;), 
  $&quot;userMentionEntities.screenName&quot;.as(&quot;UMentionASsN&quot;),
  $&quot;urlEntities.expandedURL&quot;.as(&quot;URLs&quot;),
  $&quot;hashtagEntities.text&quot;.as(&quot;hashTags&quot;)
 ).withColumn(&quot;TweetType&quot;,
    when($&quot;OriginalTwIDinRT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinQT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; === -1,
      &quot;Original Tweet&quot;)
    .when($&quot;OriginalTwIDinRT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinQT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; &gt; -1,
      &quot;Reply Tweet&quot;)
    .when($&quot;OriginalTwIDinRT&quot;.isNotNull &amp;&amp;$&quot;OriginalTwIDinQT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; === -1,
      &quot;ReTweet&quot;)
    .when($&quot;OriginalTwIDinRT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinQT&quot;.isNotNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; === -1,
      &quot;Quoted Tweet&quot;)
    .when($&quot;OriginalTwIDinRT&quot;.isNotNull &amp;&amp; $&quot;OriginalTwIDinQT&quot;.isNotNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; === -1,
      &quot;Retweet of Quoted Tweet&quot;)
    .when($&quot;OriginalTwIDinRT&quot;.isNotNull &amp;&amp; $&quot;OriginalTwIDinQT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; &gt; -1,
      &quot;Retweet of Reply Tweet&quot;)
    .when($&quot;OriginalTwIDinRT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinQT&quot;.isNotNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; &gt; -1,
      &quot;Reply of Quoted Tweet&quot;)
    .when($&quot;OriginalTwIDinRT&quot;.isNotNull &amp;&amp; $&quot;OriginalTwIDinQT&quot;.isNotNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; &gt; -1,
      &quot;Retweet of Quoted Rely Tweet&quot;)
      .otherwise(&quot;Unclassified&quot;))
.withColumn(&quot;MentionType&quot;, 
    when($&quot;UMentionRTid&quot;.isNotNull &amp;&amp; $&quot;UMentionQTid&quot;.isNotNull, &quot;RetweetAndQuotedMention&quot;)
    .when($&quot;UMentionRTid&quot;.isNotNull &amp;&amp; $&quot;UMentionQTid&quot;.isNull, &quot;RetweetMention&quot;)
    .when($&quot;UMentionRTid&quot;.isNull &amp;&amp; $&quot;UMentionQTid&quot;.isNotNull, &quot;QuotedMention&quot;)
    .when($&quot;UMentionRTid&quot;.isNull &amp;&amp; $&quot;UMentionQTid&quot;.isNull, &quot;AuthoredMention&quot;)
    .otherwise(&quot;NoMention&quot;))
.withColumn(&quot;Weight&quot;, lit(1L))
}

println(&quot;&quot;&quot;USAGE: val df = tweetsDF2TTTDF(tweetsJsonStringDF2TweetsDF(fromParquetFile2DF(&quot;parquetFileName&quot;)))
                  val df = tweetsDF2TTTDF(tweetsIDLong_JsonStringPairDF2TweetsDF(fromParquetFile2DF(&quot;parquetFileName&quot;)))
                  &quot;&quot;&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// try to modify the function tweetsDF2TTTDF so some fields are not necessarily assumed to be available
// there are better ways - https://stackoverflow.com/questions/35904136/how-do-i-detect-if-a-spark-dataframe-has-a-column

def tweetsDF2TTTDFLightWeight(tweetsInputDF: DataFrame): DataFrame = {
 tweetsInputDF.select(
  unix_timestamp($&quot;createdAt&quot;, &quot;&quot;&quot;MMM dd, yyyy hh:mm:ss a&quot;&quot;&quot;).cast(TimestampType).as(&quot;CurrentTweetDate&quot;),
  $&quot;id&quot;.as(&quot;CurrentTwID&quot;),
  $&quot;lang&quot;.as(&quot;lang&quot;),
  //$&quot;geoLocation.latitude&quot;.as(&quot;lat&quot;),
  //$&quot;geoLocation.longitude&quot;.as(&quot;lon&quot;),
  unix_timestamp($&quot;retweetedStatus.createdAt&quot;, &quot;&quot;&quot;MMM dd, yyyy hh:mm:ss a&quot;&quot;&quot;).cast(TimestampType).as(&quot;CreationDateOfOrgTwInRT&quot;), 
  $&quot;retweetedStatus.id&quot;.as(&quot;OriginalTwIDinRT&quot;),  
  unix_timestamp($&quot;quotedStatus.createdAt&quot;, &quot;&quot;&quot;MMM dd, yyyy hh:mm:ss a&quot;&quot;&quot;).cast(TimestampType).as(&quot;CreationDateOfOrgTwInQT&quot;), 
  $&quot;quotedStatus.id&quot;.as(&quot;OriginalTwIDinQT&quot;), 
  $&quot;inReplyToStatusId&quot;.as(&quot;OriginalTwIDinReply&quot;), 
  $&quot;user.id&quot;.as(&quot;CPostUserId&quot;),
  unix_timestamp($&quot;user.createdAt&quot;, &quot;&quot;&quot;MMM dd, yyyy hh:mm:ss a&quot;&quot;&quot;).cast(TimestampType).as(&quot;userCreatedAtDate&quot;),
  $&quot;retweetedStatus.user.id&quot;.as(&quot;OPostUserIdinRT&quot;), 
  $&quot;quotedStatus.user.id&quot;.as(&quot;OPostUserIdinQT&quot;),
  $&quot;inReplyToUserId&quot;.as(&quot;OPostUserIdinReply&quot;),
  $&quot;user.name&quot;.as(&quot;CPostUserName&quot;), 
  $&quot;retweetedStatus.user.name&quot;.as(&quot;OPostUserNameinRT&quot;), 
  $&quot;quotedStatus.user.name&quot;.as(&quot;OPostUserNameinQT&quot;), 
  $&quot;user.screenName&quot;.as(&quot;CPostUserSN&quot;), 
  $&quot;retweetedStatus.user.screenName&quot;.as(&quot;OPostUserSNinRT&quot;), 
  $&quot;quotedStatus.user.screenName&quot;.as(&quot;OPostUserSNinQT&quot;),
  $&quot;inReplyToScreenName&quot;.as(&quot;OPostUserSNinReply&quot;),
  $&quot;user.favouritesCount&quot;,
  $&quot;user.followersCount&quot;,
  $&quot;user.friendsCount&quot;,
  $&quot;user.isVerified&quot;,
  $&quot;user.isGeoEnabled&quot;,
  $&quot;text&quot;.as(&quot;CurrentTweet&quot;), 
  $&quot;retweetedStatus.userMentionEntities.id&quot;.as(&quot;UMentionRTiD&quot;), 
  $&quot;retweetedStatus.userMentionEntities.screenName&quot;.as(&quot;UMentionRTsN&quot;), 
  $&quot;quotedStatus.userMentionEntities.id&quot;.as(&quot;UMentionQTiD&quot;), 
  $&quot;quotedStatus.userMentionEntities.screenName&quot;.as(&quot;UMentionQTsN&quot;), 
  $&quot;userMentionEntities.id&quot;.as(&quot;UMentionASiD&quot;), 
  $&quot;userMentionEntities.screenName&quot;.as(&quot;UMentionASsN&quot;)
 ).withColumn(&quot;TweetType&quot;,
    when($&quot;OriginalTwIDinRT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinQT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; === -1,
      &quot;Original Tweet&quot;)
    .when($&quot;OriginalTwIDinRT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinQT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; &gt; -1,
      &quot;Reply Tweet&quot;)
    .when($&quot;OriginalTwIDinRT&quot;.isNotNull &amp;&amp;$&quot;OriginalTwIDinQT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; === -1,
      &quot;ReTweet&quot;)
    .when($&quot;OriginalTwIDinRT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinQT&quot;.isNotNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; === -1,
      &quot;Quoted Tweet&quot;)
    .when($&quot;OriginalTwIDinRT&quot;.isNotNull &amp;&amp; $&quot;OriginalTwIDinQT&quot;.isNotNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; === -1,
      &quot;Retweet of Quoted Tweet&quot;)
    .when($&quot;OriginalTwIDinRT&quot;.isNotNull &amp;&amp; $&quot;OriginalTwIDinQT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; &gt; -1,
      &quot;Retweet of Reply Tweet&quot;)
    .when($&quot;OriginalTwIDinRT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinQT&quot;.isNotNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; &gt; -1,
      &quot;Reply of Quoted Tweet&quot;)
    .when($&quot;OriginalTwIDinRT&quot;.isNotNull &amp;&amp; $&quot;OriginalTwIDinQT&quot;.isNotNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; &gt; -1,
      &quot;Retweet of Quoted Rely Tweet&quot;)
      .otherwise(&quot;Unclassified&quot;))
.withColumn(&quot;MentionType&quot;, 
    when($&quot;UMentionRTid&quot;.isNotNull &amp;&amp; $&quot;UMentionQTid&quot;.isNotNull, &quot;RetweetAndQuotedMention&quot;)
    .when($&quot;UMentionRTid&quot;.isNotNull &amp;&amp; $&quot;UMentionQTid&quot;.isNull, &quot;RetweetMention&quot;)
    .when($&quot;UMentionRTid&quot;.isNull &amp;&amp; $&quot;UMentionQTid&quot;.isNotNull, &quot;QuotedMention&quot;)
    .when($&quot;UMentionRTid&quot;.isNull &amp;&amp; $&quot;UMentionQTid&quot;.isNull, &quot;AuthoredMention&quot;)
    .otherwise(&quot;NoMention&quot;))
.withColumn(&quot;Weight&quot;, lit(1L))
}

def tweetsDF2TTTDFWithURLsAndHashtagsLightWeight(tweetsInputDF: DataFrame): DataFrame = {
 tweetsInputDF.select(
  unix_timestamp($&quot;createdAt&quot;, &quot;&quot;&quot;MMM dd, yyyy hh:mm:ss a&quot;&quot;&quot;).cast(TimestampType).as(&quot;CurrentTweetDate&quot;),
  $&quot;id&quot;.as(&quot;CurrentTwID&quot;),
  $&quot;lang&quot;.as(&quot;lang&quot;),
  //$&quot;geoLocation.latitude&quot;.as(&quot;lat&quot;),
  //$&quot;geoLocation.longitude&quot;.as(&quot;lon&quot;),
  unix_timestamp($&quot;retweetedStatus.createdAt&quot;, &quot;&quot;&quot;MMM dd, yyyy hh:mm:ss a&quot;&quot;&quot;).cast(TimestampType).as(&quot;CreationDateOfOrgTwInRT&quot;), 
  $&quot;retweetedStatus.id&quot;.as(&quot;OriginalTwIDinRT&quot;),  
  unix_timestamp($&quot;quotedStatus.createdAt&quot;, &quot;&quot;&quot;MMM dd, yyyy hh:mm:ss a&quot;&quot;&quot;).cast(TimestampType).as(&quot;CreationDateOfOrgTwInQT&quot;), 
  $&quot;quotedStatus.id&quot;.as(&quot;OriginalTwIDinQT&quot;), 
  $&quot;inReplyToStatusId&quot;.as(&quot;OriginalTwIDinReply&quot;), 
  $&quot;user.id&quot;.as(&quot;CPostUserId&quot;),
  unix_timestamp($&quot;user.createdAt&quot;, &quot;&quot;&quot;MMM dd, yyyy hh:mm:ss a&quot;&quot;&quot;).cast(TimestampType).as(&quot;userCreatedAtDate&quot;),
  $&quot;retweetedStatus.user.id&quot;.as(&quot;OPostUserIdinRT&quot;), 
  $&quot;quotedStatus.user.id&quot;.as(&quot;OPostUserIdinQT&quot;),
  $&quot;inReplyToUserId&quot;.as(&quot;OPostUserIdinReply&quot;),
  $&quot;user.name&quot;.as(&quot;CPostUserName&quot;), 
  $&quot;retweetedStatus.user.name&quot;.as(&quot;OPostUserNameinRT&quot;), 
  $&quot;quotedStatus.user.name&quot;.as(&quot;OPostUserNameinQT&quot;), 
  $&quot;user.screenName&quot;.as(&quot;CPostUserSN&quot;), 
  $&quot;retweetedStatus.user.screenName&quot;.as(&quot;OPostUserSNinRT&quot;), 
  $&quot;quotedStatus.user.screenName&quot;.as(&quot;OPostUserSNinQT&quot;),
  $&quot;inReplyToScreenName&quot;.as(&quot;OPostUserSNinReply&quot;),
  $&quot;user.favouritesCount&quot;,
  $&quot;user.followersCount&quot;,
  $&quot;user.friendsCount&quot;,
  $&quot;user.isVerified&quot;,
  $&quot;user.isGeoEnabled&quot;,
  $&quot;text&quot;.as(&quot;CurrentTweet&quot;), 
  $&quot;retweetedStatus.userMentionEntities.id&quot;.as(&quot;UMentionRTiD&quot;), 
  $&quot;retweetedStatus.userMentionEntities.screenName&quot;.as(&quot;UMentionRTsN&quot;), 
  $&quot;quotedStatus.userMentionEntities.id&quot;.as(&quot;UMentionQTiD&quot;), 
  $&quot;quotedStatus.userMentionEntities.screenName&quot;.as(&quot;UMentionQTsN&quot;), 
  $&quot;userMentionEntities.id&quot;.as(&quot;UMentionASiD&quot;), 
  $&quot;userMentionEntities.screenName&quot;.as(&quot;UMentionASsN&quot;),
  $&quot;urlEntities.expandedURL&quot;.as(&quot;URLs&quot;),
  $&quot;hashtagEntities.text&quot;.as(&quot;hashTags&quot;)
 ).withColumn(&quot;TweetType&quot;,
    when($&quot;OriginalTwIDinRT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinQT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; === -1,
      &quot;Original Tweet&quot;)
    .when($&quot;OriginalTwIDinRT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinQT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; &gt; -1,
      &quot;Reply Tweet&quot;)
    .when($&quot;OriginalTwIDinRT&quot;.isNotNull &amp;&amp;$&quot;OriginalTwIDinQT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; === -1,
      &quot;ReTweet&quot;)
    .when($&quot;OriginalTwIDinRT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinQT&quot;.isNotNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; === -1,
      &quot;Quoted Tweet&quot;)
    .when($&quot;OriginalTwIDinRT&quot;.isNotNull &amp;&amp; $&quot;OriginalTwIDinQT&quot;.isNotNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; === -1,
      &quot;Retweet of Quoted Tweet&quot;)
    .when($&quot;OriginalTwIDinRT&quot;.isNotNull &amp;&amp; $&quot;OriginalTwIDinQT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; &gt; -1,
      &quot;Retweet of Reply Tweet&quot;)
    .when($&quot;OriginalTwIDinRT&quot;.isNull &amp;&amp; $&quot;OriginalTwIDinQT&quot;.isNotNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; &gt; -1,
      &quot;Reply of Quoted Tweet&quot;)
    .when($&quot;OriginalTwIDinRT&quot;.isNotNull &amp;&amp; $&quot;OriginalTwIDinQT&quot;.isNotNull &amp;&amp; $&quot;OriginalTwIDinReply&quot; &gt; -1,
      &quot;Retweet of Quoted Rely Tweet&quot;)
      .otherwise(&quot;Unclassified&quot;))
.withColumn(&quot;MentionType&quot;, 
    when($&quot;UMentionRTid&quot;.isNotNull &amp;&amp; $&quot;UMentionQTid&quot;.isNotNull, &quot;RetweetAndQuotedMention&quot;)
    .when($&quot;UMentionRTid&quot;.isNotNull &amp;&amp; $&quot;UMentionQTid&quot;.isNull, &quot;RetweetMention&quot;)
    .when($&quot;UMentionRTid&quot;.isNull &amp;&amp; $&quot;UMentionQTid&quot;.isNotNull, &quot;QuotedMention&quot;)
    .when($&quot;UMentionRTid&quot;.isNull &amp;&amp; $&quot;UMentionQTid&quot;.isNull, &quot;AuthoredMention&quot;)
    .otherwise(&quot;NoMention&quot;))
.withColumn(&quot;Weight&quot;, lit(1L))
}
</code></pre>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h2 id="extended-twitterutils-with-language"><a class="header" href="#extended-twitterutils-with-language">Extended TwitterUtils with Language</a></h2>
<p>We extend twitter utils from Spark to allow for filtering by user-ids using <code>.follow</code> and strings in the tweet using <code>.track</code> method of <code>twitter4j</code>.</p>
<p>This notebook is mainly left to show how one can adapt one of the exsiting notebooks for twitter experiemnts to cahnge the design of the experiment itself. Similarly, one can extend further into the twitter4j functions and methods we are wrapping into Scala here.</p>
<p>This is part of <em>Project MEP: Meme Evolution Programme</em> and supported by databricks academic partners program.</p>
<p>The analysis is available in the following databricks notebook: * <a href="http://lamastex.org/lmse/mep/src/extendedTwitterUtil.html">http://lamastex.org/lmse/mep/src/extendedTwitterUtils.html</a></p>
<pre><code>Copyright 2016 Ivan Sadikov and Raazesh Sainudiin

Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import twitter4j._
import twitter4j.auth.Authorization
import twitter4j.conf.ConfigurationBuilder
import twitter4j.auth.OAuthAuthorization

import org.apache.spark.streaming._
import org.apache.spark.streaming.dstream._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.receiver.Receiver
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import twitter4j._
import twitter4j.auth.Authorization
import twitter4j.conf.ConfigurationBuilder
import twitter4j.auth.OAuthAuthorization
import org.apache.spark.streaming._
import org.apache.spark.streaming.dstream._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.receiver.Receiver
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="twitter-receiver-and-stream-1"><a class="header" href="#twitter-receiver-and-stream-1">Twitter receiver and stream</a></h3>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">class ExtendedTwitterReceiver(
    twitterAuth: Authorization,
    filters: Seq[String],
    userFilters: Seq[Long],
    langFilters: Seq[String],
    storageLevel: StorageLevel
  ) extends Receiver[Status](storageLevel) {

  @volatile private var twitterStream: TwitterStream = _
  @volatile private var stopped = false

  def onStart() {
    try {
      val newTwitterStream = new TwitterStreamFactory().getInstance(twitterAuth)
      newTwitterStream.addListener(new StatusListener {
        def onStatus(status: Status): Unit = {
          store(status)
        }
        // Unimplemented
        def onDeletionNotice(statusDeletionNotice: StatusDeletionNotice) {}
        def onTrackLimitationNotice(i: Int) {}
        def onScrubGeo(l: Long, l1: Long) {}
        def onStallWarning(stallWarning: StallWarning) {}
        def onException(e: Exception) {
          if (!stopped) {
            restart(&quot;Error receiving tweets&quot;, e)
          }
        }
      })

      // do filtering only when filters are available
      if (filters.nonEmpty || userFilters.nonEmpty || langFilters.nonEmpty) {
        val query = new FilterQuery()
        if (filters.nonEmpty) {
          query.track(filters.mkString(&quot;,&quot;))
        }

        if (userFilters.nonEmpty) {
          query.follow(userFilters: _*)
        }
        
        if (langFilters.nonEmpty) {
          query.language(langFilters.mkString(&quot;,&quot;))
        }
        
        newTwitterStream.filter(query)
      } else {
        newTwitterStream.sample()
      }
      setTwitterStream(newTwitterStream)
      println(&quot;Twitter receiver started&quot;)
      stopped = false
    } catch {
      case e: Exception =&gt; restart(&quot;Error starting Twitter stream&quot;, e)
    }
  }

  def onStop() {
    stopped = true
    setTwitterStream(null)
    println(&quot;Twitter receiver stopped&quot;)
  }

  private def setTwitterStream(newTwitterStream: TwitterStream) = synchronized {
    if (twitterStream != null) {
      twitterStream.shutdown()
    }
    twitterStream = newTwitterStream
  }
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined class ExtendedTwitterReceiver
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">class ExtendedTwitterInputDStream(
    ssc_ : StreamingContext,
    twitterAuth: Option[Authorization],
    filters: Seq[String],
    userFilters: Seq[Long],
    langFilters: Seq[String],
    storageLevel: StorageLevel
  ) extends ReceiverInputDStream[Status](ssc_)  {

  private def createOAuthAuthorization(): Authorization = {
    new OAuthAuthorization(new ConfigurationBuilder().build())
  }

  private val authorization = twitterAuth.getOrElse(createOAuthAuthorization())

  override def getReceiver(): Receiver[Status] = {
    new ExtendedTwitterReceiver(authorization, filters, userFilters, langFilters, storageLevel)
  }
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined class ExtendedTwitterInputDStream
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="extended-twitter-utils-1"><a class="header" href="#extended-twitter-utils-1">Extended twitter utils</a></h3>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import twitter4j.Status
import twitter4j.auth.Authorization
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.dstream.{ReceiverInputDStream, DStream}

object ExtendedTwitterUtils {
  def createStream(
      ssc: StreamingContext,
      twitterAuth: Option[Authorization],
      filters: Seq[String] = Nil,
      userFilters: Seq[Long] = Nil,
      langFilters: Seq[String] = Nil,
      storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2
    ): ReceiverInputDStream[Status] = {
    new ExtendedTwitterInputDStream(ssc, twitterAuth, filters, userFilters, langFilters, storageLevel)
  }
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import twitter4j.Status
import twitter4j.auth.Authorization
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.dstream.{ReceiverInputDStream, DStream}
defined object ExtendedTwitterUtils
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(&quot;done running the extendedTwitterUtils2run notebook - ready to stream from twitter by filtering on strings, users and language&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>done running the extendedTwitterUtils2run notebook - ready to stream from twitter by filtering on strings, users and language
</code></pre>
</div>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="tweet-streaming-collector"><a class="header" href="#tweet-streaming-collector">Tweet Streaming Collector</a></h1>
<p>Let us build a system to collect live tweets using Spark streaming.</p>
<p>Here are the main steps in this notebook:</p>
<ul>
<li>let's collect from the public twitter stream and write to DBFS as json strings in a boiler-plate manner to understand the componets better.</li>
<li>Then we will turn the collector into a function and use it</li>
<li>Finally we will use some DataFrame-based pipelines to convert the raw tweets into other structured content.</li>
</ul>
<p>Note that capturing tweets from the public streams for free using Twitter's Streaming API has some caveats. We are supposed to have access to a uniformly random sample of roughly 1% of all Tweets across the globe, but what's exactly available in the sample from the full twitter social media network, i.e. <em>all</em> status updates in the planet, for such free collection is not exactly known in terms of sub-sampling strategies like starification layers, etc. This latter is Twitter's proprietary information. However, we are supposed to be able to assume that it is indeed a random sample of roughly 1% of all tweets.</p>
</div>
<div class="cell markdown">
<p>We will call extendedTwitterUtils notebook from here.</p>
<p>But <strong>first install</strong> the following libraries from maven central and attach to this cluster:</p>
<ul>
<li>gson with maven coordinates <code>com.google.code.gson:gson:2.8.6</code></li>
<li>twitter4j-examples with maven coordinates <code>org.twitter4j:twitter4j-examples:4.0.7</code></li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-run">&quot;./025_a_extendedTwitterUtils2run&quot;
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import twitter4j._
import twitter4j.auth.Authorization
import twitter4j.conf.ConfigurationBuilder
import twitter4j.auth.OAuthAuthorization
import org.apache.spark.streaming._
import org.apache.spark.streaming.dstream._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.receiver.Receiver
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Go to SparkUI and see if a streaming job is already running. If so you need to terminate it before starting a new streaming job. Only one streaming job can be run on the DB CE.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined class ExtendedTwitterReceiver
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined class ExtendedTwitterInputDStream
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// this will make sure all streaming job in the cluster are stopped
StreamingContext.getActive.foreach{ _.stop(stopSparkContext = false) }
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import twitter4j.Status
import twitter4j.auth.Authorization
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.dstream.{ReceiverInputDStream, DStream}
defined object ExtendedTwitterUtils
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>done running the extendedTwitterUtils2run notebook - ready to stream from twitter
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let's create a directory in dbfs for storing tweets in the cluster's distributed file system.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val outputDirectoryRoot = &quot;/datasets/tweetsStreamTmp&quot; // output directory
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>outputDirectoryRoot: String = /datasets/tweetsStreamTmp
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">dbutils.fs.mkdirs(&quot;/datasets/tweetsStreamTmp&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res2: Boolean = true
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//display(dbutils.fs.ls(outputDirectoryRoot))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// to remove a pre-existing directory and start from scratch uncomment next line and evaluate this cell
dbutils.fs.rm(&quot;/datasets/tweetsStreamTmp&quot;, true) 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res4: Boolean = true
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Capture tweets in every sliding window of <code>slideInterval</code> many milliseconds.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val slideInterval = new Duration(1 * 1000) // 1 * 1000 = 1000 milli-seconds = 1 sec
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>slideInterval: org.apache.spark.streaming.Duration = 1000 ms
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Recall that <strong>Discretized Stream</strong> or <strong>DStream</strong> is the basic abstraction provided by Spark Streaming. It represents a continuous stream of data, either the input data stream received from source, or the processed data stream generated by transforming the input stream. Internally, a DStream is represented by a continuous series of RDDs, which is Spark?s abstraction of an immutable, distributed dataset (see <a href="http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds">Spark Programming Guide</a> for more details). Each RDD in a DStream contains data from a certain interval, as shown in the following figure.</p>
<p><img src="http://spark.apache.org/docs/latest/img/streaming-dstream.png" alt="Spark Streaming" title="Spark Streaming data flow" /></p>
</div>
<div class="cell markdown">
<p>Let's import google's json library next.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import com.google.gson.Gson 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import com.google.gson.Gson
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Our goal is to take each RDD in the twitter DStream and write it as a json file in our dbfs.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Create a Spark Streaming Context.
val ssc = new StreamingContext(sc, slideInterval)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@7a8fbb86
</code></pre>
</div>
</div>
<div class="cell markdown">
<h2 id="caution"><a class="header" href="#caution">CAUTION</a></h2>
<p>Extracting knowledge from tweets is &quot;easy&quot; using techniques shown here, but one has to take legal responsibility for the use of this knowledge and conform to the rules and policies linked below.</p>
<p>Remeber that the use of twitter itself comes with various strings attached. Read:</p>
<ul>
<li><a href="https://twitter.com/rules">Twitter Rules</a></li>
</ul>
<p>Crucially, the use of the content from twitter by you (as done in this worksheet) comes with some strings. Read:</p>
<ul>
<li><a href="https://dev.twitter.com/overview/terms/agreement-and-policy">Developer Agreement &amp; Policy Twitter Developer Agreement</a></li>
</ul>
<h3 id="enter-your-own-twitter-api-credentials"><a class="header" href="#enter-your-own-twitter-api-credentials">Enter your own Twitter API Credentials.</a></h3>
<ul>
<li>Go to https://apps.twitter.com and look up your Twitter API Credentials, or create an app to create them.</li>
<li>Get your own Twitter API Credentials: <code>consumerKey</code>, <code>consumerSecret</code>, <code>accessToken</code> and <code>accessTokenSecret</code> and enter them in the cell below.</li>
</ul>
<h3 id="ethicallegal-aspects"><a class="header" href="#ethicallegal-aspects">Ethical/Legal Aspects</a></h3>
<p>See Background Readings/Viewings in Project MEP:</p>
<ul>
<li><a href="https://lamastex.github.io/scalable-data-science/sds/research/mep/">https://lamastex.github.io/scalable-data-science/sds/research/mep/</a></li>
</ul>
</div>
<div class="cell markdown">
<h2 id="tweet-collector"><a class="header" href="#tweet-collector">Tweet Collector</a></h2>
<p>There are several steps to make a streaming twitter collector. We will do them one by one so you learn all the components. In the sequel we will make a function out of the various steps.</p>
<h3 id="1-twitter-credentials"><a class="header" href="#1-twitter-credentials">1. Twitter Credentials</a></h3>
<p>First step towards doing your own experiments in twitter is to enter your Twitter API credentials.</p>
<ul>
<li>Go to https://apps.twitter.com and look up your Twitter API Credentials, or create an app to create them.</li>
<li>Run the code in a cell to Enter your own credentials.</li>
</ul>
<pre><code class="language-%scala">// put your own twitter developer credentials below instead of xxx
// instead of the '%run &quot;.../secrets/026_secret_MyTwitterOAuthCredentials&quot;' below
// you need to copy-paste the following code-block with your own Twitter credentials replacing XXXX


// put your own twitter developer credentials below 

import twitter4j.auth.OAuthAuthorization
import twitter4j.conf.ConfigurationBuilder


// These have been regenerated!!! - need to chane them

def myAPIKey       = &quot;XXXX&quot; // APIKey 
def myAPISecret    = &quot;XXXX&quot; // APISecretKey
def myAccessToken          = &quot;XXXX&quot; // AccessToken
def myAccessTokenSecret    = &quot;XXXX&quot; // AccessTokenSecret


System.setProperty(&quot;twitter4j.oauth.consumerKey&quot;, myAPIKey)
System.setProperty(&quot;twitter4j.oauth.consumerSecret&quot;, myAPISecret)
System.setProperty(&quot;twitter4j.oauth.accessToken&quot;, myAccessToken)
System.setProperty(&quot;twitter4j.oauth.accessTokenSecret&quot;, myAccessTokenSecret)

println(&quot;twitter OAuth Credentials loaded&quot;)
</code></pre>
<p>The cell-below will not expose my Twitter API Credentials: <code>myAPIKey</code>, <code>myAPISecret</code>, <code>myAccessToken</code> and <code>myAccessTokenSecret</code>. Use the code above to enter your own credentials in a scala cell.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-run">&quot;Users/raazesh.sainudiin@math.uu.se/scalable-data-science/secrets/026_secret_MyTwitterOAuthCredentials&quot;
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Create a Twitter Stream for the input source. 
val auth = Some(new OAuthAuthorization(new ConfigurationBuilder().build()))
val twitterStream = ExtendedTwitterUtils.createStream(ssc, auth)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>auth: Some[twitter4j.auth.OAuthAuthorization] = Some(OAuthAuthorization{consumerKey='8uN0N9RTLT1viaR811yyG7xwk', consumerSecret='******************************************', oauthToken=AccessToken{screenName='null', userId=4173723312}})
twitterStream: org.apache.spark.streaming.dstream.ReceiverInputDStream[twitter4j.Status] = ExtendedTwitterInputDStream@7f635e6d
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="2-mapping-tweets-to-json"><a class="header" href="#2-mapping-tweets-to-json">2. Mapping Tweets to JSON</a></h3>
<p>Let's map the tweets into <a href="https://en.wikipedia.org/wiki/JSON">JSON</a> formatted string (one tweet per line). We will use Google GSON library for this.</p>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://en.wikipedia.org/wiki/JSON"
 width="95%" height="400">
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val twitterStreamJson = twitterStream.map(
                                            x =&gt; { val gson = new Gson();
                                                 val xJson = gson.toJson(x)
                                                 xJson
                                                 }
                                          ) 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>twitterStreamJson: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.MappedDStream@b874a46
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>twitter OAuth Credentials loaded
import twitter4j.auth.OAuthAuthorization
import twitter4j.conf.ConfigurationBuilder
import twitter4j.auth.OAuthAuthorization
import twitter4j.conf.ConfigurationBuilder
myAPIKey: String
myAPISecret: String
myAccessToken: String
myAccessTokenSecret: String
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">outputDirectoryRoot
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res8: String = /datasets/tweetsStreamTmp
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">var numTweetsCollected = 0L // track number of tweets collected
val partitionsEachInterval = 1 // This tells the number of partitions in each RDD of tweets in the DStream.

twitterStreamJson.foreachRDD( 
  (rdd, time) =&gt; { // for each RDD in the DStream
      val count = rdd.count()
      if (count &gt; 0) {
        val outputRDD = rdd.repartition(partitionsEachInterval) // repartition as desired
        outputRDD.saveAsTextFile(outputDirectoryRoot + &quot;/tweets_&quot; + time.milliseconds.toString) // save as textfile
        numTweetsCollected += count // update with the latest count
      }
  }
)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>numTweetsCollected: Long = 0
partitionsEachInterval: Int = 1
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="3-start-the-stream"><a class="header" href="#3-start-the-stream">3. Start the Stream</a></h3>
<p>Nothing has actually happened yet.</p>
<p>Let's start the spark streaming context we have created next.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">ssc.start()
</code></pre>
</div>
<div class="cell markdown">
<p>Let's look at the spark UI now and monitor the streaming job in action! Go to <code>Clusters</code> on the left and click on <code>UI</code> and then <code>Streaming</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">numTweetsCollected // number of tweets collected so far
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res11: Long = 468
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let's try seeing again in a few seconds how many tweets have been collected up to now.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">numTweetsCollected // number of tweets collected so far
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res12: Long = 859
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="4-stop-the-stream"><a class="header" href="#4-stop-the-stream">4. Stop the Stream</a></h3>
<p>Note that you could easilt fill up disk space!!!</p>
<p>So let's stop the streaming job next.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">ssc.stop(stopSparkContext = false) // gotto stop soon!!!
</code></pre>
</div>
<div class="cell markdown">
<p>Let's make sure that the <code>Streaming</code> UI is not active in the <code>Clusters</code> <code>UI</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) } // extra cautious stopping of all active streaming contexts
</code></pre>
</div>
<div class="cell markdown">
<h3 id="5-examine-collected-tweets"><a class="header" href="#5-examine-collected-tweets">5. Examine Collected Tweets</a></h3>
<p>Next let's examine what was saved in dbfs.</p>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-scala">display(dbutils.fs.ls(outputDirectoryRoot))
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>path</th>
<th>name</th>
<th>size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867067000/</td>
<td>tweets_1605867067000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867068000/</td>
<td>tweets_1605867068000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867069000/</td>
<td>tweets_1605867069000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867070000/</td>
<td>tweets_1605867070000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867071000/</td>
<td>tweets_1605867071000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867072000/</td>
<td>tweets_1605867072000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867073000/</td>
<td>tweets_1605867073000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867074000/</td>
<td>tweets_1605867074000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867075000/</td>
<td>tweets_1605867075000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867076000/</td>
<td>tweets_1605867076000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867077000/</td>
<td>tweets_1605867077000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867078000/</td>
<td>tweets_1605867078000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867079000/</td>
<td>tweets_1605867079000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867080000/</td>
<td>tweets_1605867080000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867081000/</td>
<td>tweets_1605867081000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867082000/</td>
<td>tweets_1605867082000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867083000/</td>
<td>tweets_1605867083000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867084000/</td>
<td>tweets_1605867084000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867085000/</td>
<td>tweets_1605867085000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867086000/</td>
<td>tweets_1605867086000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867087000/</td>
<td>tweets_1605867087000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867088000/</td>
<td>tweets_1605867088000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867089000/</td>
<td>tweets_1605867089000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867090000/</td>
<td>tweets_1605867090000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867091000/</td>
<td>tweets_1605867091000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867092000/</td>
<td>tweets_1605867092000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867093000/</td>
<td>tweets_1605867093000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867094000/</td>
<td>tweets_1605867094000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867095000/</td>
<td>tweets_1605867095000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867096000/</td>
<td>tweets_1605867096000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867097000/</td>
<td>tweets_1605867097000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867098000/</td>
<td>tweets_1605867098000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867099000/</td>
<td>tweets_1605867099000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867100000/</td>
<td>tweets_1605867100000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867101000/</td>
<td>tweets_1605867101000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867102000/</td>
<td>tweets_1605867102000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867103000/</td>
<td>tweets_1605867103000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867104000/</td>
<td>tweets_1605867104000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867105000/</td>
<td>tweets_1605867105000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867106000/</td>
<td>tweets_1605867106000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867107000/</td>
<td>tweets_1605867107000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867108000/</td>
<td>tweets_1605867108000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867109000/</td>
<td>tweets_1605867109000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867110000/</td>
<td>tweets_1605867110000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867111000/</td>
<td>tweets_1605867111000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867112000/</td>
<td>tweets_1605867112000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867113000/</td>
<td>tweets_1605867113000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867114000/</td>
<td>tweets_1605867114000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867115000/</td>
<td>tweets_1605867115000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867116000/</td>
<td>tweets_1605867116000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867117000/</td>
<td>tweets_1605867117000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867118000/</td>
<td>tweets_1605867118000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867119000/</td>
<td>tweets_1605867119000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867120000/</td>
<td>tweets_1605867120000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867121000/</td>
<td>tweets_1605867121000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867122000/</td>
<td>tweets_1605867122000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867123000/</td>
<td>tweets_1605867123000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867124000/</td>
<td>tweets_1605867124000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867125000/</td>
<td>tweets_1605867125000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867126000/</td>
<td>tweets_1605867126000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867127000/</td>
<td>tweets_1605867127000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867128000/</td>
<td>tweets_1605867128000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867129000/</td>
<td>tweets_1605867129000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867130000/</td>
<td>tweets_1605867130000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867131000/</td>
<td>tweets_1605867131000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867132000/</td>
<td>tweets_1605867132000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867133000/</td>
<td>tweets_1605867133000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867134000/</td>
<td>tweets_1605867134000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867135000/</td>
<td>tweets_1605867135000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867136000/</td>
<td>tweets_1605867136000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867137000/</td>
<td>tweets_1605867137000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867138000/</td>
<td>tweets_1605867138000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867139000/</td>
<td>tweets_1605867139000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867140000/</td>
<td>tweets_1605867140000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867141000/</td>
<td>tweets_1605867141000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867142000/</td>
<td>tweets_1605867142000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867143000/</td>
<td>tweets_1605867143000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867144000/</td>
<td>tweets_1605867144000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867145000/</td>
<td>tweets_1605867145000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867146000/</td>
<td>tweets_1605867146000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867147000/</td>
<td>tweets_1605867147000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867148000/</td>
<td>tweets_1605867148000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867149000/</td>
<td>tweets_1605867149000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867150000/</td>
<td>tweets_1605867150000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867151000/</td>
<td>tweets_1605867151000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867152000/</td>
<td>tweets_1605867152000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867153000/</td>
<td>tweets_1605867153000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867154000/</td>
<td>tweets_1605867154000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867155000/</td>
<td>tweets_1605867155000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867156000/</td>
<td>tweets_1605867156000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867157000/</td>
<td>tweets_1605867157000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867158000/</td>
<td>tweets_1605867158000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867159000/</td>
<td>tweets_1605867159000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867160000/</td>
<td>tweets_1605867160000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867161000/</td>
<td>tweets_1605867161000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867162000/</td>
<td>tweets_1605867162000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867163000/</td>
<td>tweets_1605867163000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867164000/</td>
<td>tweets_1605867164000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867165000/</td>
<td>tweets_1605867165000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867166000/</td>
<td>tweets_1605867166000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867167000/</td>
<td>tweets_1605867167000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867168000/</td>
<td>tweets_1605867168000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867169000/</td>
<td>tweets_1605867169000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867170000/</td>
<td>tweets_1605867170000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867171000/</td>
<td>tweets_1605867171000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867172000/</td>
<td>tweets_1605867172000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867173000/</td>
<td>tweets_1605867173000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867174000/</td>
<td>tweets_1605867174000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867175000/</td>
<td>tweets_1605867175000/</td>
<td>0.0</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val tweetsDir = outputDirectoryRoot+&quot;/tweets_1605867068000/&quot; // use an existing file, may have to rename folder based on output above!
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>tweetsDir: String = /datasets/tweetsStreamTmp/tweets_1605867068000/
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(tweetsDir)) 
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>path</th>
<th>name</th>
<th>size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867068000/_SUCCESS</td>
<td>_SUCCESS</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/tweets_1605867068000/part-00000</td>
<td>part-00000</td>
<td>122395.0</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sc.textFile(tweetsDir+&quot;part-00000&quot;).count()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res17: Long = 31
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val outJson = sqlContext.read.json(tweetsDir+&quot;part-00000&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>outJson: org.apache.spark.sql.DataFrame = [contributorsIDs: array&lt;string&gt;, createdAt: string ... 26 more fields]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-scala">outJson.printSchema()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>root
 |-- contributorsIDs: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- createdAt: string (nullable = true)
 |-- currentUserRetweetId: long (nullable = true)
 |-- displayTextRangeEnd: long (nullable = true)
 |-- displayTextRangeStart: long (nullable = true)
 |-- favoriteCount: long (nullable = true)
 |-- hashtagEntities: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- end: long (nullable = true)
 |    |    |-- start: long (nullable = true)
 |    |    |-- text: string (nullable = true)
 |-- id: long (nullable = true)
 |-- inReplyToScreenName: string (nullable = true)
 |-- inReplyToStatusId: long (nullable = true)
 |-- inReplyToUserId: long (nullable = true)
 |-- isFavorited: boolean (nullable = true)
 |-- isPossiblySensitive: boolean (nullable = true)
 |-- isRetweeted: boolean (nullable = true)
 |-- isTruncated: boolean (nullable = true)
 |-- lang: string (nullable = true)
 |-- mediaEntities: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- displayURL: string (nullable = true)
 |    |    |-- end: long (nullable = true)
 |    |    |-- expandedURL: string (nullable = true)
 |    |    |-- id: long (nullable = true)
 |    |    |-- mediaURL: string (nullable = true)
 |    |    |-- mediaURLHttps: string (nullable = true)
 |    |    |-- sizes: struct (nullable = true)
 |    |    |    |-- 0: struct (nullable = true)
 |    |    |    |    |-- height: long (nullable = true)
 |    |    |    |    |-- resize: long (nullable = true)
 |    |    |    |    |-- width: long (nullable = true)
 |    |    |    |-- 1: struct (nullable = true)
 |    |    |    |    |-- height: long (nullable = true)
 |    |    |    |    |-- resize: long (nullable = true)
 |    |    |    |    |-- width: long (nullable = true)
 |    |    |    |-- 2: struct (nullable = true)
 |    |    |    |    |-- height: long (nullable = true)
 |    |    |    |    |-- resize: long (nullable = true)
 |    |    |    |    |-- width: long (nullable = true)
 |    |    |    |-- 3: struct (nullable = true)
 |    |    |    |    |-- height: long (nullable = true)
 |    |    |    |    |-- resize: long (nullable = true)
 |    |    |    |    |-- width: long (nullable = true)
 |    |    |-- start: long (nullable = true)
 |    |    |-- type: string (nullable = true)
 |    |    |-- url: string (nullable = true)
 |    |    |-- videoAspectRatioHeight: long (nullable = true)
 |    |    |-- videoAspectRatioWidth: long (nullable = true)
 |    |    |-- videoDurationMillis: long (nullable = true)
 |    |    |-- videoVariants: array (nullable = true)
 |    |    |    |-- element: string (containsNull = true)
 |-- quotedStatus: struct (nullable = true)
 |    |-- contributorsIDs: array (nullable = true)
 |    |    |-- element: string (containsNull = true)
 |    |-- createdAt: string (nullable = true)
 |    |-- currentUserRetweetId: long (nullable = true)
 |    |-- displayTextRangeEnd: long (nullable = true)
 |    |-- displayTextRangeStart: long (nullable = true)
 |    |-- favoriteCount: long (nullable = true)
 |    |-- hashtagEntities: array (nullable = true)
 |    |    |-- element: string (containsNull = true)
 |    |-- id: long (nullable = true)
 |    |-- inReplyToScreenName: string (nullable = true)
 |    |-- inReplyToStatusId: long (nullable = true)
 |    |-- inReplyToUserId: long (nullable = true)
 |    |-- isFavorited: boolean (nullable = true)
 |    |-- isPossiblySensitive: boolean (nullable = true)
 |    |-- isRetweeted: boolean (nullable = true)
 |    |-- isTruncated: boolean (nullable = true)
 |    |-- lang: string (nullable = true)
 |    |-- mediaEntities: array (nullable = true)
 |    |    |-- element: struct (containsNull = true)
 |    |    |    |-- displayURL: string (nullable = true)
 |    |    |    |-- end: long (nullable = true)
 |    |    |    |-- expandedURL: string (nullable = true)
 |    |    |    |-- id: long (nullable = true)
 |    |    |    |-- mediaURL: string (nullable = true)
 |    |    |    |-- mediaURLHttps: string (nullable = true)
 |    |    |    |-- sizes: struct (nullable = true)
 |    |    |    |    |-- 0: struct (nullable = true)
 |    |    |    |    |    |-- height: long (nullable = true)
 |    |    |    |    |    |-- resize: long (nullable = true)
 |    |    |    |    |    |-- width: long (nullable = true)
 |    |    |    |    |-- 1: struct (nullable = true)
 |    |    |    |    |    |-- height: long (nullable = true)
 |    |    |    |    |    |-- resize: long (nullable = true)
 |    |    |    |    |    |-- width: long (nullable = true)
 |    |    |    |    |-- 2: struct (nullable = true)
 |    |    |    |    |    |-- height: long (nullable = true)
 |    |    |    |    |    |-- resize: long (nullable = true)
 |    |    |    |    |    |-- width: long (nullable = true)
 |    |    |    |    |-- 3: struct (nullable = true)
 |    |    |    |    |    |-- height: long (nullable = true)
 |    |    |    |    |    |-- resize: long (nullable = true)
 |    |    |    |    |    |-- width: long (nullable = true)
 |    |    |    |-- start: long (nullable = true)
 |    |    |    |-- type: string (nullable = true)
 |    |    |    |-- url: string (nullable = true)
 |    |    |    |-- videoAspectRatioHeight: long (nullable = true)
 |    |    |    |-- videoAspectRatioWidth: long (nullable = true)
 |    |    |    |-- videoDurationMillis: long (nullable = true)
 |    |    |    |-- videoVariants: array (nullable = true)
 |    |    |    |    |-- element: string (containsNull = true)
 |    |-- place: struct (nullable = true)
 |    |    |-- boundingBoxCoordinates: array (nullable = true)
 |    |    |    |-- element: array (containsNull = true)
 |    |    |    |    |-- element: struct (containsNull = true)
 |    |    |    |    |    |-- latitude: double (nullable = true)
 |    |    |    |    |    |-- longitude: double (nullable = true)
 |    |    |-- boundingBoxType: string (nullable = true)
 |    |    |-- country: string (nullable = true)
 |    |    |-- countryCode: string (nullable = true)
 |    |    |-- fullName: string (nullable = true)
 |    |    |-- id: string (nullable = true)
 |    |    |-- name: string (nullable = true)
 |    |    |-- placeType: string (nullable = true)
 |    |    |-- url: string (nullable = true)
 |    |-- quotedStatusId: long (nullable = true)
 |    |-- retweetCount: long (nullable = true)
 |    |-- source: string (nullable = true)
 |    |-- symbolEntities: array (nullable = true)
 |    |    |-- element: string (containsNull = true)
 |    |-- text: string (nullable = true)
 |    |-- urlEntities: array (nullable = true)
 |    |    |-- element: string (containsNull = true)
 |    |-- user: struct (nullable = true)
 |    |    |-- createdAt: string (nullable = true)
 |    |    |-- description: string (nullable = true)
 |    |    |-- descriptionURLEntities: array (nullable = true)
 |    |    |    |-- element: string (containsNull = true)
 |    |    |-- favouritesCount: long (nullable = true)
 |    |    |-- followersCount: long (nullable = true)
 |    |    |-- friendsCount: long (nullable = true)
 |    |    |-- id: long (nullable = true)
 |    |    |-- isContributorsEnabled: boolean (nullable = true)
 |    |    |-- isDefaultProfile: boolean (nullable = true)
 |    |    |-- isDefaultProfileImage: boolean (nullable = true)
 |    |    |-- isFollowRequestSent: boolean (nullable = true)
 |    |    |-- isGeoEnabled: boolean (nullable = true)
 |    |    |-- isProtected: boolean (nullable = true)
 |    |    |-- isVerified: boolean (nullable = true)
 |    |    |-- listedCount: long (nullable = true)
 |    |    |-- location: string (nullable = true)
 |    |    |-- name: string (nullable = true)
 |    |    |-- profileBackgroundColor: string (nullable = true)
 |    |    |-- profileBackgroundImageUrl: string (nullable = true)
 |    |    |-- profileBackgroundImageUrlHttps: string (nullable = true)
 |    |    |-- profileBackgroundTiled: boolean (nullable = true)
 |    |    |-- profileBannerImageUrl: string (nullable = true)
 |    |    |-- profileImageUrl: string (nullable = true)
 |    |    |-- profileImageUrlHttps: string (nullable = true)
 |    |    |-- profileLinkColor: string (nullable = true)
 |    |    |-- profileSidebarBorderColor: string (nullable = true)
 |    |    |-- profileSidebarFillColor: string (nullable = true)
 |    |    |-- profileTextColor: string (nullable = true)
 |    |    |-- profileUseBackgroundImage: boolean (nullable = true)
 |    |    |-- screenName: string (nullable = true)
 |    |    |-- showAllInlineMedia: boolean (nullable = true)
 |    |    |-- statusesCount: long (nullable = true)
 |    |    |-- translator: boolean (nullable = true)
 |    |    |-- url: string (nullable = true)
 |    |    |-- utcOffset: long (nullable = true)
 |    |-- userMentionEntities: array (nullable = true)
 |    |    |-- element: struct (containsNull = true)
 |    |    |    |-- end: long (nullable = true)
 |    |    |    |-- id: long (nullable = true)
 |    |    |    |-- name: string (nullable = true)
 |    |    |    |-- screenName: string (nullable = true)
 |    |    |    |-- start: long (nullable = true)
 |-- quotedStatusId: long (nullable = true)
 |-- quotedStatusPermalink: struct (nullable = true)
 |    |-- displayURL: string (nullable = true)
 |    |-- end: long (nullable = true)
 |    |-- expandedURL: string (nullable = true)
 |    |-- start: long (nullable = true)
 |    |-- url: string (nullable = true)
 |-- retweetCount: long (nullable = true)
 |-- retweetedStatus: struct (nullable = true)
 |    |-- contributorsIDs: array (nullable = true)
 |    |    |-- element: string (containsNull = true)
 |    |-- createdAt: string (nullable = true)
 |    |-- currentUserRetweetId: long (nullable = true)
 |    |-- displayTextRangeEnd: long (nullable = true)
 |    |-- displayTextRangeStart: long (nullable = true)
 |    |-- favoriteCount: long (nullable = true)
 |    |-- hashtagEntities: array (nullable = true)
 |    |    |-- element: struct (containsNull = true)
 |    |    |    |-- end: long (nullable = true)
 |    |    |    |-- start: long (nullable = true)
 |    |    |    |-- text: string (nullable = true)
 |    |-- id: long (nullable = true)
 |    |-- inReplyToScreenName: string (nullable = true)
 |    |-- inReplyToStatusId: long (nullable = true)
 |    |-- inReplyToUserId: long (nullable = true)
 |    |-- isFavorited: boolean (nullable = true)
 |    |-- isPossiblySensitive: boolean (nullable = true)
 |    |-- isRetweeted: boolean (nullable = true)
 |    |-- isTruncated: boolean (nullable = true)
 |    |-- lang: string (nullable = true)
 |    |-- mediaEntities: array (nullable = true)
 |    |    |-- element: struct (containsNull = true)
 |    |    |    |-- displayURL: string (nullable = true)
 |    |    |    |-- end: long (nullable = true)
 |    |    |    |-- expandedURL: string (nullable = true)
 |    |    |    |-- id: long (nullable = true)
 |    |    |    |-- mediaURL: string (nullable = true)
 |    |    |    |-- mediaURLHttps: string (nullable = true)
 |    |    |    |-- sizes: struct (nullable = true)
 |    |    |    |    |-- 0: struct (nullable = true)
 |    |    |    |    |    |-- height: long (nullable = true)
 |    |    |    |    |    |-- resize: long (nullable = true)
 |    |    |    |    |    |-- width: long (nullable = true)
 |    |    |    |    |-- 1: struct (nullable = true)
 |    |    |    |    |    |-- height: long (nullable = true)
 |    |    |    |    |    |-- resize: long (nullable = true)
 |    |    |    |    |    |-- width: long (nullable = true)
 |    |    |    |    |-- 2: struct (nullable = true)
 |    |    |    |    |    |-- height: long (nullable = true)
 |    |    |    |    |    |-- resize: long (nullable = true)
 |    |    |    |    |    |-- width: long (nullable = true)
 |    |    |    |    |-- 3: struct (nullable = true)
 |    |    |    |    |    |-- height: long (nullable = true)
 |    |    |    |    |    |-- resize: long (nullable = true)
 |    |    |    |    |    |-- width: long (nullable = true)
 |    |    |    |-- start: long (nullable = true)
 |    |    |    |-- type: string (nullable = true)
 |    |    |    |-- url: string (nullable = true)
 |    |    |    |-- videoAspectRatioHeight: long (nullable = true)
 |    |    |    |-- videoAspectRatioWidth: long (nullable = true)
 |    |    |    |-- videoDurationMillis: long (nullable = true)
 |    |    |    |-- videoVariants: array (nullable = true)
 |    |    |    |    |-- element: string (containsNull = true)
 |    |-- quotedStatusId: long (nullable = true)
 |    |-- retweetCount: long (nullable = true)
 |    |-- source: string (nullable = true)
 |    |-- symbolEntities: array (nullable = true)
 |    |    |-- element: string (containsNull = true)
 |    |-- text: string (nullable = true)
 |    |-- urlEntities: array (nullable = true)
 |    |    |-- element: struct (containsNull = true)
 |    |    |    |-- displayURL: string (nullable = true)
 |    |    |    |-- end: long (nullable = true)
 |    |    |    |-- expandedURL: string (nullable = true)
 |    |    |    |-- start: long (nullable = true)
 |    |    |    |-- url: string (nullable = true)
 |    |-- user: struct (nullable = true)
 |    |    |-- createdAt: string (nullable = true)
 |    |    |-- description: string (nullable = true)
 |    |    |-- descriptionURLEntities: array (nullable = true)
 |    |    |    |-- element: string (containsNull = true)
 |    |    |-- favouritesCount: long (nullable = true)
 |    |    |-- followersCount: long (nullable = true)
 |    |    |-- friendsCount: long (nullable = true)
 |    |    |-- id: long (nullable = true)
 |    |    |-- isContributorsEnabled: boolean (nullable = true)
 |    |    |-- isDefaultProfile: boolean (nullable = true)
 |    |    |-- isDefaultProfileImage: boolean (nullable = true)
 |    |    |-- isFollowRequestSent: boolean (nullable = true)
 |    |    |-- isGeoEnabled: boolean (nullable = true)
 |    |    |-- isProtected: boolean (nullable = true)
 |    |    |-- isVerified: boolean (nullable = true)
 |    |    |-- listedCount: long (nullable = true)
 |    |    |-- location: string (nullable = true)
 |    |    |-- name: string (nullable = true)
 |    |    |-- profileBackgroundColor: string (nullable = true)
 |    |    |-- profileBackgroundImageUrl: string (nullable = true)
 |    |    |-- profileBackgroundImageUrlHttps: string (nullable = true)
 |    |    |-- profileBackgroundTiled: boolean (nullable = true)
 |    |    |-- profileBannerImageUrl: string (nullable = true)
 |    |    |-- profileImageUrl: string (nullable = true)
 |    |    |-- profileImageUrlHttps: string (nullable = true)
 |    |    |-- profileLinkColor: string (nullable = true)
 |    |    |-- profileSidebarBorderColor: string (nullable = true)
 |    |    |-- profileSidebarFillColor: string (nullable = true)
 |    |    |-- profileTextColor: string (nullable = true)
 |    |    |-- profileUseBackgroundImage: boolean (nullable = true)
 |    |    |-- screenName: string (nullable = true)
 |    |    |-- showAllInlineMedia: boolean (nullable = true)
 |    |    |-- statusesCount: long (nullable = true)
 |    |    |-- translator: boolean (nullable = true)
 |    |    |-- url: string (nullable = true)
 |    |    |-- utcOffset: long (nullable = true)
 |    |-- userMentionEntities: array (nullable = true)
 |    |    |-- element: struct (containsNull = true)
 |    |    |    |-- end: long (nullable = true)
 |    |    |    |-- id: long (nullable = true)
 |    |    |    |-- name: string (nullable = true)
 |    |    |    |-- screenName: string (nullable = true)
 |    |    |    |-- start: long (nullable = true)
 |-- source: string (nullable = true)
 |-- symbolEntities: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- text: string (nullable = true)
 |-- urlEntities: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- displayURL: string (nullable = true)
 |    |    |-- end: long (nullable = true)
 |    |    |-- expandedURL: string (nullable = true)
 |    |    |-- start: long (nullable = true)
 |    |    |-- url: string (nullable = true)
 |-- user: struct (nullable = true)
 |    |-- createdAt: string (nullable = true)
 |    |-- description: string (nullable = true)
 |    |-- descriptionURLEntities: array (nullable = true)
 |    |    |-- element: string (containsNull = true)
 |    |-- favouritesCount: long (nullable = true)
 |    |-- followersCount: long (nullable = true)
 |    |-- friendsCount: long (nullable = true)
 |    |-- id: long (nullable = true)
 |    |-- isContributorsEnabled: boolean (nullable = true)
 |    |-- isDefaultProfile: boolean (nullable = true)
 |    |-- isDefaultProfileImage: boolean (nullable = true)
 |    |-- isFollowRequestSent: boolean (nullable = true)
 |    |-- isGeoEnabled: boolean (nullable = true)
 |    |-- isProtected: boolean (nullable = true)
 |    |-- isVerified: boolean (nullable = true)
 |    |-- listedCount: long (nullable = true)
 |    |-- location: string (nullable = true)
 |    |-- name: string (nullable = true)
 |    |-- profileBackgroundColor: string (nullable = true)
 |    |-- profileBackgroundImageUrl: string (nullable = true)
 |    |-- profileBackgroundImageUrlHttps: string (nullable = true)
 |    |-- profileBackgroundTiled: boolean (nullable = true)
 |    |-- profileBannerImageUrl: string (nullable = true)
 |    |-- profileImageUrl: string (nullable = true)
 |    |-- profileImageUrlHttps: string (nullable = true)
 |    |-- profileLinkColor: string (nullable = true)
 |    |-- profileSidebarBorderColor: string (nullable = true)
 |    |-- profileSidebarFillColor: string (nullable = true)
 |    |-- profileTextColor: string (nullable = true)
 |    |-- profileUseBackgroundImage: boolean (nullable = true)
 |    |-- screenName: string (nullable = true)
 |    |-- showAllInlineMedia: boolean (nullable = true)
 |    |-- statusesCount: long (nullable = true)
 |    |-- translator: boolean (nullable = true)
 |    |-- url: string (nullable = true)
 |    |-- utcOffset: long (nullable = true)
 |-- userMentionEntities: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- end: long (nullable = true)
 |    |    |-- id: long (nullable = true)
 |    |    |-- name: string (nullable = true)
 |    |    |-- screenName: string (nullable = true)
 |    |    |-- start: long (nullable = true)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//outJson.select(&quot;id&quot;,&quot;text&quot;).show(false) // output not displayed to comply with Twitter Developer rules
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//display(outJson)  // output not displayed to comply with Twitter Developer rules
</code></pre>
</div>
<div class="cell markdown">
<p>Now, let's be good at house-keeping and clean-up the unnecessary data in dbfs, our distributed file system (in databricks).</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// to remove a pre-existing directory and start from scratch uncomment next line and evaluate this cell
dbutils.fs.rm(outputDirectoryRoot, true) 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res24: Boolean = true
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Clearly there is a lot one can do with tweets!</p>
<p>Enspecially, after you can get a few more primitives under your belt from the following areas:</p>
<ul>
<li>Natural Language Processing (MLlib, beyond word counts of course),</li>
<li>Distributed vertex programming (Graph Frames, which you already know), and</li>
<li>Scalable geospatial computing with location data on open street maps (roughly a third of tweets are geo-enabled with Latitude and Longitude of the tweet location) - we will get into this.</li>
</ul>
</div>
<div class="cell markdown">
<h2 id="method-for-spark-streaming-collector"><a class="header" href="#method-for-spark-streaming-collector">Method for Spark Streaming Collector</a></h2>
<p>Let's try to throw the bits and bobs of code in the above 5 steps into a method called <code>streamFunc</code> for simplicity and modularity.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import com.google.gson.Gson 
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

val outputDirectoryRoot = &quot;/datasets/tweetsStreamTmp&quot; // output directory
val batchInterval = 1 // in minutes
val timeoutJobLength =  batchInterval * 5

var newContextCreated = false
var numTweetsCollected = 0L // track number of tweets collected
//val conf = new SparkConf().setAppName(&quot;TrackedTweetCollector&quot;).setMaster(&quot;local&quot;)
// This is the function that creates the SteamingContext and sets up the Spark Streaming job.
def streamFunc(): StreamingContext = {
  // Create a Spark Streaming Context.
  val ssc = new StreamingContext(sc, Minutes(batchInterval))
  // Create the OAuth Twitter credentials 
  val auth = Some(new OAuthAuthorization(new ConfigurationBuilder().build()))
  // Create a Twitter Stream for the input source.  
  val twitterStream = ExtendedTwitterUtils.createStream(ssc, auth)
  // Transform the discrete RDDs into JSON
  val twitterStreamJson = twitterStream.map(x =&gt; { val gson = new Gson();
                                                 val xJson = gson.toJson(x)
                                                 xJson
                                               }) 
  // take care
  val partitionsEachInterval = 1 // This tells the number of partitions in each RDD of tweets in the DStream.
  
  // what we want done with each discrete RDD tuple: (rdd, time)
  twitterStreamJson.foreachRDD((rdd, time) =&gt; { // for each filtered RDD in the DStream
      val count = rdd.count()
      if (count &gt; 0) {
        val outputRDD = rdd.repartition(partitionsEachInterval) // repartition as desired
        // to write to parquet directly in append mode in one directory per 'time'------------       
        val outputDF = outputRDD.toDF(&quot;tweetAsJsonString&quot;)
        // get some time fields from current `.Date()`
        val year = (new java.text.SimpleDateFormat(&quot;yyyy&quot;)).format(new java.util.Date())
        val month = (new java.text.SimpleDateFormat(&quot;MM&quot;)).format(new java.util.Date())
        val day = (new java.text.SimpleDateFormat(&quot;dd&quot;)).format(new java.util.Date())
        val hour = (new java.text.SimpleDateFormat(&quot;HH&quot;)).format(new java.util.Date())
        // write to a file with a clear time-based hierarchical directory structure for example
        outputDF.write.mode(SaveMode.Append)
                .parquet(outputDirectoryRoot+ &quot;/&quot;+ year + &quot;/&quot; + month + &quot;/&quot; + day + &quot;/&quot; + hour + &quot;/&quot; + time.milliseconds) 
        // end of writing as parquet file-------------------------------------
        numTweetsCollected += count // update with the latest count
      }
  })
  newContextCreated = true
  ssc
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import com.google.gson.Gson
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
outputDirectoryRoot: String = /datasets/tweetsStreamTmp
batchInterval: Int = 1
timeoutJobLength: Int = 5
newContextCreated: Boolean = false
numTweetsCollected: Long = 0
streamFunc: ()org.apache.spark.streaming.StreamingContext
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Now just use the function to create a Spark Streaming Context
val ssc = StreamingContext.getActiveOrCreate(streamFunc)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@400b6153
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// you only need one of these to start
ssc.start()
//ssc.awaitTerminationOrTimeout(timeoutJobLength)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// this will make sure all streaming job in the cluster are stopped
// but let' run it for a few minutes before stopping it
//StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) } 
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(&quot;/datasets/tweetsStreamTmp/2020/11/20/10&quot;)) // outputDirectoryRoot
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>path</th>
<th>name</th>
<th>size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/2020/11/20/10/1605867660000/</td>
<td>1605867660000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/2020/11/20/10/1605867720000/</td>
<td>1605867720000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/2020/11/20/10/1605867780000/</td>
<td>1605867780000/</td>
<td>0.0</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell markdown">
<h2 id="tweet-transmission-tree-tables"><a class="header" href="#tweet-transmission-tree-tables">Tweet Transmission Tree Tables</a></h2>
<p>Next, let us take a quick peek at the notebook <code>./025_b_TTTDFfunctions</code> to see how we have pipelined the JSON tweets into tabular or structured data as DataFrames.</p>
<p>Please see <a href="http://lamastex.org/lmse/mep/src/TweetAnatomyAndTransmissionTree.html">http://lamastex.org/lmse/mep/src/TweetAnatomyAndTransmissionTree.html</a> to understand more deeply.</p>
<p>Note that the fundamental issue here is that we need to define what we exactly mean by a particular type of status update, i.e.:</p>
<ul>
<li>How do we categorize a particular status update, based on the data contained in it, as one of the following?
<ul>
<li>Original Tweet</li>
<li>Retweet</li>
<li>Quoted tweet</li>
<li>retweet of a Quoted Tweet</li>
<li>etc.</li>
</ul>
</li>
</ul>
<p>Answers to the above question are exactly answered by the methods in <code>./025_b_TTTDFfunctions</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-run">&quot;./025_b_TTTDFfunctions&quot;
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>USAGE: val df = tweetsDF2TTTDF(tweetsJsonStringDF2TweetsDF(fromParquetFile2DF(&quot;parquetFileName&quot;)))
                  val df = tweetsDF2TTTDF(tweetsIDLong_JsonStringPairDF2TweetsDF(fromParquetFile2DF(&quot;parquetFileName&quot;)))
                  
import org.apache.spark.sql.types.{StructType, StructField, StringType}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.spark.sql.ColumnName
import org.apache.spark.sql.DataFrame
fromParquetFile2DF: (InputDFAsParquetFilePatternString: String)org.apache.spark.sql.DataFrame
tweetsJsonStringDF2TweetsDF: (tweetsAsJsonStringInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
tweetsIDLong_JsonStringPairDF2TweetsDF: (tweetsAsIDLong_JsonStringInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
tweetsDF2TTTDF: (tweetsInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
tweetsDF2TTTDFWithURLsAndHashtags: (tweetsInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>tweetsDF2TTTDFLightWeight: (tweetsInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
tweetsDF2TTTDFWithURLsAndHashtagsLightWeight: (tweetsInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rawDF = fromParquetFile2DF(&quot;/datasets/tweetsStreamTmp/2020/11/*/*/*/*&quot;) //.cache()
val TTTsDF = tweetsDF2TTTDF(tweetsJsonStringDF2TweetsDF(rawDF)).cache()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rawDF: org.apache.spark.sql.DataFrame = [tweetAsJsonString: string]
TTTsDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CurrentTweetDate: timestamp, CurrentTwID: bigint ... 35 more fields]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">TTTsDF.count()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res34: Long = 14686
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//display(TTTsDF)  // output not displayed to comply with Twitter Developer rules
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-scala">TTTsDF.printSchema
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>root
 |-- CurrentTweetDate: timestamp (nullable = true)
 |-- CurrentTwID: long (nullable = true)
 |-- lang: string (nullable = true)
 |-- lat: double (nullable = true)
 |-- lon: double (nullable = true)
 |-- CreationDateOfOrgTwInRT: timestamp (nullable = true)
 |-- OriginalTwIDinRT: long (nullable = true)
 |-- CreationDateOfOrgTwInQT: timestamp (nullable = true)
 |-- OriginalTwIDinQT: long (nullable = true)
 |-- OriginalTwIDinReply: long (nullable = true)
 |-- CPostUserId: long (nullable = true)
 |-- userCreatedAtDate: timestamp (nullable = true)
 |-- OPostUserIdinRT: long (nullable = true)
 |-- OPostUserIdinQT: long (nullable = true)
 |-- OPostUserIdinReply: long (nullable = true)
 |-- CPostUserName: string (nullable = true)
 |-- OPostUserNameinRT: string (nullable = true)
 |-- OPostUserNameinQT: string (nullable = true)
 |-- CPostUserSN: string (nullable = true)
 |-- OPostUserSNinRT: string (nullable = true)
 |-- OPostUserSNinQT: string (nullable = true)
 |-- OPostUserSNinReply: string (nullable = true)
 |-- favouritesCount: long (nullable = true)
 |-- followersCount: long (nullable = true)
 |-- friendsCount: long (nullable = true)
 |-- isVerified: boolean (nullable = true)
 |-- isGeoEnabled: boolean (nullable = true)
 |-- CurrentTweet: string (nullable = true)
 |-- UMentionRTiD: array (nullable = true)
 |    |-- element: long (containsNull = true)
 |-- UMentionRTsN: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- UMentionQTiD: array (nullable = true)
 |    |-- element: long (containsNull = true)
 |-- UMentionQTsN: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- UMentionASiD: array (nullable = true)
 |    |-- element: long (containsNull = true)
 |-- UMentionASsN: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- TweetType: string (nullable = false)
 |-- MentionType: string (nullable = false)
 |-- Weight: long (nullable = false)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(TTTsDF.groupBy($&quot;tweetType&quot;).count().orderBy($&quot;count&quot;.desc))
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>tweetType</th>
<th>count</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ReTweet</td>
<td>5956.0</td>
</tr>
<tr class="even">
<td>Reply Tweet</td>
<td>4187.0</td>
</tr>
<tr class="odd">
<td>Original Tweet</td>
<td>3326.0</td>
</tr>
<tr class="even">
<td>Quoted Tweet</td>
<td>754.0</td>
</tr>
<tr class="odd">
<td>Retweet of Quoted Tweet</td>
<td>416.0</td>
</tr>
<tr class="even">
<td>Reply of Quoted Tweet</td>
<td>47.0</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// this will make sure all streaming job in the cluster are stopped
StreamingContext.getActive.foreach{ _.stop(stopSparkContext = false) } 
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// this will delete what we collected to keep the disk usage tight and tidy
dbutils.fs.rm(outputDirectoryRoot, true) 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res40: Boolean = true
</code></pre>
</div>
</div>
<div class="cell markdown">
<h2 id="writing-to-public-clouds"><a class="header" href="#writing-to-public-clouds">Writing to public clouds</a></h2>
<p>Next, let's write the tweets into a scalable commercial cloud storage system in AWS (similarly into Azure blobstore, Google cloud storage, etc ).</p>
<p>We will make sure to write the tweets to AWS's simple storage service or S3, a scalable storage system in the cloud. See <a href="https://aws.amazon.com/s3/">https://aws.amazon.com/s3/</a>.</p>
<p><strong>skip this if you don't have an AWS account</strong>.</p>
<p>But all the main syntactic bits are here for your future convenience :)</p>
<pre><code>// Replace with your AWS S3 credentials
//
// NOTE: Set the access to this notebook appropriately to protect the security of your keys.
// Or you can delete this cell after you run the mount command below once successfully.

val AccessKey = getArgument(&quot;1. ACCESS_KEY&quot;, &quot;REPLACE_WITH_YOUR_ACCESS_KEY&quot;)
val SecretKey = getArgument(&quot;2. SECRET_KEY&quot;, &quot;REPLACE_WITH_YOUR_SECRET_KEY&quot;)
val EncodedSecretKey = SecretKey.replace(&quot;/&quot;, &quot;%2F&quot;)
val AwsBucketName = getArgument(&quot;3. S3_BUCKET&quot;, &quot;REPLACE_WITH_YOUR_S3_BUCKET&quot;)
val MountName = getArgument(&quot;4. MNT_NAME&quot;, &quot;REPLACE_WITH_YOUR_MOUNT_NAME&quot;)
val s3Filename = &quot;tweetDump&quot;
</code></pre>
<p>Now just mount s3 as follows:</p>
<pre><code>dbutils.fs.mount(s&quot;s3a://$AccessKey:$EncodedSecretKey@$AwsBucketName&quot;, s&quot;/mnt/$MountName&quot;)
</code></pre>
<p>Now you can use the <code>dbutils</code> commands freely to access data in the mounted S3.</p>
<pre><code>dbutils.fs.help()
</code></pre>
<p>copying:</p>
<pre><code>// to copy all the tweets to s3
dbutils.fs.cp(&quot;dbfs:/rawTweets&quot;,s&quot;/mnt/$MountName/rawTweetsInS3/&quot;,recurse=true) 
</code></pre>
<p>deleting:</p>
<pre><code>// to remove all the files from s3
dbutils.fs.rm(s&quot;/mnt/$MountName/rawTweetsInS3&quot;,recurse=true) 
</code></pre>
<p>unmounting:</p>
<pre><code>// finally unmount when done - IMPORTANT!
dbutils.fs.unmount(s&quot;/mnt/$MountName&quot;) 
</code></pre>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="tweet-streaming-collector---track--follow"><a class="header" href="#tweet-streaming-collector---track--follow">Tweet Streaming Collector - Track &amp; Follow</a></h1>
<p>In the previous notebook we were capturing tweets from the public streams under the assumption that it is a random sample of roughly 1% of all tweets.</p>
<p>In this notebook, we can modify the collector to focus on specific communications of interest to us. Specifically, by including:</p>
<ul>
<li>a list of strings to track and</li>
<li>a list of twitter user-IDs of interest to follow.</li>
</ul>
<p>For this we will first <code>%run</code> the <code>ExtendedTwitterUtils</code> and <code>TTTDFfunctions</code> notebooks.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-run">&quot;./025_a_extendedTwitterUtils2run&quot;
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-run">&quot;./025_b_TTTDFfunctions&quot;
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import twitter4j._
import twitter4j.auth.Authorization
import twitter4j.conf.ConfigurationBuilder
import twitter4j.auth.OAuthAuthorization
import org.apache.spark.streaming._
import org.apache.spark.streaming.dstream._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.receiver.Receiver
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Go to SparkUI and see if a streaming job is already running. If so you need to terminate it before starting a new streaming job. Only one streaming job can be run on the DB CE.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined class ExtendedTwitterReceiver
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined class ExtendedTwitterInputDStream
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// this will make sure all streaming job in the cluster are stopped
StreamingContext.getActive.foreach{ _.stop(stopSparkContext = false) } 
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>USAGE: val df = tweetsDF2TTTDF(tweetsJsonStringDF2TweetsDF(fromParquetFile2DF(&quot;parquetFileName&quot;)))
                  val df = tweetsDF2TTTDF(tweetsIDLong_JsonStringPairDF2TweetsDF(fromParquetFile2DF(&quot;parquetFileName&quot;)))
                  
import org.apache.spark.sql.types.{StructType, StructField, StringType}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.spark.sql.ColumnName
import org.apache.spark.sql.DataFrame
fromParquetFile2DF: (InputDFAsParquetFilePatternString: String)org.apache.spark.sql.DataFrame
tweetsJsonStringDF2TweetsDF: (tweetsAsJsonStringInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
tweetsIDLong_JsonStringPairDF2TweetsDF: (tweetsAsIDLong_JsonStringInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
tweetsDF2TTTDF: (tweetsInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
tweetsDF2TTTDFWithURLsAndHashtags: (tweetsInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import twitter4j.Status
import twitter4j.auth.Authorization
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.dstream.{ReceiverInputDStream, DStream}
defined object ExtendedTwitterUtils
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>done running the extendedTwitterUtils2run notebook - ready to stream from twitter
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>tweetsDF2TTTDFLightWeight: (tweetsInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
tweetsDF2TTTDFWithURLsAndHashtagsLightWeight: (tweetsInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Load your twitter credentials (secretly!).</p>
</div>
<div class="cell markdown">
<p><strong>Enter your Twitter API Credentials.</strong></p>
<ul>
<li>Go to https://apps.twitter.com and look up your Twitter API Credentials, or create an app to create them.</li>
<li>Run the code in a cell to Enter your own credentials.</li>
</ul>
<pre><code class="language-%scala">// put your own twitter developer credentials below instead of xxx
// instead of the '%run &quot;.../secrets/026_secret_MyTwitterOAuthCredentials&quot;' below
// you need to copy-paste the following code-block with your own Twitter credentials replacing XXXX


// put your own twitter developer credentials below 

import twitter4j.auth.OAuthAuthorization
import twitter4j.conf.ConfigurationBuilder


// These have been regenerated!!! - need to chane them

def myAPIKey       = &quot;XXXX&quot; // APIKey 
def myAPISecret    = &quot;XXXX&quot; // APISecretKey
def myAccessToken          = &quot;XXXX&quot; // AccessToken
def myAccessTokenSecret    = &quot;XXXX&quot; // AccessTokenSecret


System.setProperty(&quot;twitter4j.oauth.consumerKey&quot;, myAPIKey)
System.setProperty(&quot;twitter4j.oauth.consumerSecret&quot;, myAPISecret)
System.setProperty(&quot;twitter4j.oauth.accessToken&quot;, myAccessToken)
System.setProperty(&quot;twitter4j.oauth.accessTokenSecret&quot;, myAccessTokenSecret)

println(&quot;twitter OAuth Credentials loaded&quot;)
</code></pre>
<p>The cell-below will not expose my Twitter API Credentials: <code>myAPIKey</code>, <code>myAPISecret</code>, <code>myAccessToken</code> and <code>myAccessTokenSecret</code>. Use the code above to enter your own credentials in a scala cell.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-run">&quot;Users/raazesh.sainudiin@math.uu.se/scalable-data-science/secrets/026_secret_MyTwitterOAuthCredentials&quot;
</code></pre>
</div>
<div class="cell markdown">
<h2 id="using-twitter-rest-api"><a class="header" href="#using-twitter-rest-api">Using Twitter REST API</a></h2>
<p>Next we import and instantiate for Twitter REST API, which allows us to obtain data from Twitter that is not in the live stream but in Twitter's storage layers containing archives of historcial events (including past status updates).</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// SOME IMPORTTS
import scala.collection.mutable.ArrayBuffer
import twitter4j._
import twitter4j.conf._
import scala.collection.JavaConverters._ 

import org.apache.spark.sql.Row;
import org.apache.spark.sql.types.{StructType, StructField, StringType};
import twitter4j.RateLimitStatus;
import twitter4j.ResponseList;
import com.google.gson.Gson
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import com.google.gson.Gson
import org.apache.spark.sql.DataFrame

val cb = new ConfigurationBuilder()       

val twitter = {
  val c = new ConfigurationBuilder
    c.setDebugEnabled(false)
    .setOAuthConsumerKey(myAPIKey)
    .setOAuthConsumerSecret(myAPISecret)
    .setOAuthAccessToken(myAccessToken)
    .setOAuthAccessTokenSecret(myAccessTokenSecret);

  new TwitterFactory(c.build()).getInstance()
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import scala.collection.mutable.ArrayBuffer
import twitter4j._
import twitter4j.conf._
import scala.collection.JavaConverters._
import org.apache.spark.sql.Row
import org.apache.spark.sql.types.{StructType, StructField, StringType}
import twitter4j.RateLimitStatus
import twitter4j.ResponseList
import com.google.gson.Gson
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import com.google.gson.Gson
import org.apache.spark.sql.DataFrame
cb: twitter4j.conf.ConfigurationBuilder = twitter4j.conf.ConfigurationBuilder@4559b2a7
twitter: twitter4j.Twitter = TwitterImpl{INCLUDE_MY_RETWEET=HttpParameter{name='include_my_retweet', value='true', jsonObject=null, file=null, fileBody=null}}
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="testing-rest-api"><a class="header" href="#testing-rest-api">Testing REST API</a></h3>
<p>Let's quickly test that the REST API calls can be made.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">twitter.showUser(&quot;@raazozone&quot;).getId() // quick test that REST API works - should get 4173723312
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res4: Long = 4173723312
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">twitter.showUser(&quot;@realDonaldTrump&quot;).getId() // quick test that REST API works - should get 25073877
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res5: Long = 25073877
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">twitter.showUser(&quot;@WASP_Research&quot;).getId() // quick test that REST API works - should get ?
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res6: Long = 1124265687755755520
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="creating-screennames-of-interest"><a class="header" href="#creating-screennames-of-interest">Creating ScreenNames of Interest</a></h3>
<p>Let's import a list of twitterIDS of interest to us...</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val screenNamesOfInterest = List(&quot;raazozone&quot;,&quot;realDonaldTrump&quot;,&quot;WASP_Research&quot;) // could be done from a large list of up to 4,000 or so accounts
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>screenNamesOfInterest: List[String] = List(raazozone, realDonaldTrump, WASP_Research)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def lookupUserSNs(Retweeterids:Seq[String])={
  val grouped=Retweeterids.grouped(100).toList 
  for {group&lt;-grouped  
       users=twitter.lookupUsers(group:_*)
       user&lt;-users.asScala 
   } yield user     
}// we loose some suspended accounts...

def lookupUsers(Retweeterids:Seq[Long])={
  val grouped=Retweeterids.grouped(100).toList 
  for {group&lt;-grouped  
       users=twitter.lookupUsers(group:_*)
       user&lt;-users.asScala 
   } yield user     
}// we loose some suspended accounts...
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>lookupUserSNs: (Retweeterids: Seq[String])List[twitter4j.User]
lookupUsers: (Retweeterids: Seq[Long])List[twitter4j.User]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val twitterUsersOfInterest = lookupUserSNs(screenNamesOfInterest) // not displayed
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(twitterUsersOfInterest.size, screenNamesOfInterest.size) // we could lose users due to suspended accounts etc...
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>(3,3)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// just get their IDs
val seedIDs = twitterUsersOfInterest.map(u =&gt; u.getId()).toSet.toSeq.filter(_.isValidLong) // just get the IDs of the seed users who are valid
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>seedIDs: Seq[Long] = Vector(4173723312, 25073877, 1124265687755755520)
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="extending-streamfunc-to-track--follow"><a class="header" href="#extending-streamfunc-to-track--follow">Extending <code>streamFunc</code> to Track &amp; Follow</a></h3>
<p>Now, let's extend our function to be able to track and follow.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import com.google.gson.Gson 
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

val outputDirectoryRoot = &quot;/datasets/tweetsStreamTmp&quot; // output directory
val batchInterval = 1 // in minutes
val timeoutJobLength =  batchInterval * 5

var newContextCreated = false
var numTweetsCollected = 0L // track number of tweets collected
//val conf = new SparkConf().setAppName(&quot;TrackedTweetCollector&quot;).setMaster(&quot;local&quot;)
// This is the function that creates the SteamingContext and sets up the Spark Streaming job.
def streamFunc(): StreamingContext = {
  // Create a Spark Streaming Context.
  val ssc = new StreamingContext(sc, Minutes(batchInterval))
  // Create the OAuth Twitter credentials 
  val auth = Some(new OAuthAuthorization(new ConfigurationBuilder().build()))
  
  val track = List(&quot;WASP rules!&quot;, &quot;#MakeDataGreatAgain&quot;,&quot;sds-3-x rules!&quot;)//, &quot;Hi&quot;)// just added for some live tests
  //val track = List.empty // if you do not want to track by any string
  
  val follow = seedIDs // who to follow in Twitter
  //val follow = List.empty // if you do not want to folow any specific twitter user
  
  // Create a Twitter Stream for the input source.  
  val twitterStream = ExtendedTwitterUtils.createStream(ssc, auth, track, follow)
  // Transform the discrete RDDs into JSON
  val twitterStreamJson = twitterStream.map(x =&gt; { val gson = new Gson();
                                                 val xJson = gson.toJson(x)
                                                 xJson
                                               }) 
  // take care
  val partitionsEachInterval = 1 // This tells the number of partitions in each RDD of tweets in the DStream.
  
  // what we want done with each discrete RDD tuple: (rdd, time)
  twitterStreamJson.foreachRDD((rdd, time) =&gt; { // for each filtered RDD in the DStream
      val count = rdd.count()
      if (count &gt; 0) {
        val outputRDD = rdd.repartition(partitionsEachInterval) // repartition as desired
        // to write to parquet directly in append mode in one directory per 'time'------------       
        val outputDF = outputRDD.toDF(&quot;tweetAsJsonString&quot;)
        // get some time fields from current `.Date()`
        val year = (new java.text.SimpleDateFormat(&quot;yyyy&quot;)).format(new java.util.Date())
        val month = (new java.text.SimpleDateFormat(&quot;MM&quot;)).format(new java.util.Date())
        val day = (new java.text.SimpleDateFormat(&quot;dd&quot;)).format(new java.util.Date())
        val hour = (new java.text.SimpleDateFormat(&quot;HH&quot;)).format(new java.util.Date())
        // write to a file with a clear time-based hierarchical directory structure for example
        outputDF.write.mode(SaveMode.Append)
                .parquet(outputDirectoryRoot+ &quot;/&quot;+ year + &quot;/&quot; + month + &quot;/&quot; + day + &quot;/&quot; + hour + &quot;/&quot; + time.milliseconds) 
        // end of writing as parquet file-------------------------------------
        numTweetsCollected += count // update with the latest count
      }
  })
  newContextCreated = true
  ssc
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import com.google.gson.Gson
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
outputDirectoryRoot: String = /datasets/tweetsStreamTmp
batchInterval: Int = 1
timeoutJobLength: Int = 5
newContextCreated: Boolean = false
numTweetsCollected: Long = 0
streamFunc: ()org.apache.spark.streaming.StreamingContext
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>twitter OAuth Credentials loaded
import twitter4j.auth.OAuthAuthorization
import twitter4j.conf.ConfigurationBuilder
import twitter4j.auth.OAuthAuthorization
import twitter4j.conf.ConfigurationBuilder
myAPIKey: String
myAPISecret: String
myAccessToken: String
myAccessTokenSecret: String
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="collect-verify-and-explore"><a class="header" href="#collect-verify-and-explore">Collect, Verify and Explore</a></h3>
<p>Let us collect data, store it and then explore it to see how our new experimental design changes to the Tweet collector actually performs.</p>
<p>You should ideally be doing live Tweets using your Twitter accounts as you are doing the experiment so you can confirm that your collector actually captures what you inted to. For example, retweet a userID in the list of userIDs you are following to check if this retweet ends up in your collector as expected. You can also tweet string of interest in the list of strings you are tracking.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val ssc = StreamingContext.getActiveOrCreate(streamFunc)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@54fb3d50
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">ssc.start()
//ssc.awaitTerminationOrTimeout(timeoutJobLength) // you only need one of these to start
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(&quot;/datasets/tweetsStreamTmp/2020/11/20/10/&quot;))
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>path</th>
<th>name</th>
<th>size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/2020/11/20/10/1605868860000/</td>
<td>1605868860000/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/2020/11/20/10/1605868920000/</td>
<td>1605868920000/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/2020/11/20/10/1605868980000/</td>
<td>1605868980000/</td>
<td>0.0</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(outputDirectoryRoot+&quot;/2020/11/20/10/1605868860000/&quot;)) // keep adding sub-dirs and descent into time-tree'd directory hierarchy
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>path</th>
<th>name</th>
<th>size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/2020/11/20/10/1605868860000/_SUCCESS</td>
<td>_SUCCESS</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/2020/11/20/10/1605868860000/_committed_7377493726042826738</td>
<td>_committed_7377493726042826738</td>
<td>125.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/2020/11/20/10/1605868860000/_started_7377493726042826738</td>
<td>_started_7377493726042826738</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/2020/11/20/10/1605868860000/part-00000-tid-7377493726042826738-363f9903-0ada-41ff-92fd-c87b1349727d-6412-1-c000.snappy.parquet</td>
<td>part-00000-tid-7377493726042826738-363f9903-0ada-41ff-92fd-c87b1349727d-6412-1-c000.snappy.parquet</td>
<td>95863.0</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rawDF = fromParquetFile2DF(outputDirectoryRoot+&quot;/2020/11/*/*/*/*&quot;) //.cache()
rawDF.count
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rawDF: org.apache.spark.sql.DataFrame = [tweetAsJsonString: string]
res15: Long = 437
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val TTTsDF = tweetsDF2TTTDF(tweetsJsonStringDF2TweetsDF(rawDF)).cache()
</code></pre>
</div>
<div class="cell markdown">
<p>Collect for a few minutes so all fields are availabale in the Tweets... otherwise you will get errors like this (which may be unavidable if what you are tracking has no <code>geoLocation</code> information for example, only a small fraction of Tweets have this information):</p>
<blockquote>
<p>&quot;org.apache.spark.sql.AnalysisException: cannot resolve '<code>geoLocation.latitude</code>' given input columns: [contributorsIDs, createdAt, currentUserRetweetId, displayTextRangeEnd, displayTextRangeStart, favoriteCount, hashtagEntities, id, inReplyToScreenName, inReplyToStatusId, inReplyToUserId, isFavorited, isPossiblySensitive, isRetweeted, isTruncated, lang, mediaEntities, place, quotedStatus, quotedStatusId, quotedStatusPermalink, retweetCount, retweetedStatus, source, symbolEntities, text, urlEntities, user, userMentionEntities, withheldInCountries];;&quot;</p>
</blockquote>
<p>We can parse more robustly... but let's go and modify the function so it does not look for the missing fields in these tweeets...</p>
<p><strong>NOTE</strong> In certain experiments all the user-IDs being tracked may not have enabled <code>geoLocation</code> information. In such cases, you can use the <code>tweetsDF2TTTDFLightWeight</code> to obtain the TTTDF from the raw tweets.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-run">&quot;./025_b_TTTDFfunctions&quot;
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val TTTsDF = tweetsDF2TTTDFLightWeight(tweetsJsonStringDF2TweetsDF(rawDF)).cache()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>TTTsDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CurrentTweetDate: timestamp, CurrentTwID: bigint ... 33 more fields]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(TTTsDF)  // output not displayed to comply with Twitter Developer rules
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// this will make sure all streaming job in the cluster are stopped 
StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) } 
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>USAGE: val df = tweetsDF2TTTDF(tweetsJsonStringDF2TweetsDF(fromParquetFile2DF(&quot;parquetFileName&quot;)))
                  val df = tweetsDF2TTTDF(tweetsIDLong_JsonStringPairDF2TweetsDF(fromParquetFile2DF(&quot;parquetFileName&quot;)))
                  
import org.apache.spark.sql.types.{StructType, StructField, StringType}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.spark.sql.ColumnName
import org.apache.spark.sql.DataFrame
fromParquetFile2DF: (InputDFAsParquetFilePatternString: String)org.apache.spark.sql.DataFrame
tweetsJsonStringDF2TweetsDF: (tweetsAsJsonStringInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
tweetsIDLong_JsonStringPairDF2TweetsDF: (tweetsAsIDLong_JsonStringInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
tweetsDF2TTTDF: (tweetsInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
tweetsDF2TTTDFWithURLsAndHashtags: (tweetsInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val a = TTTsDF.filter($&quot;CurrentTweet&quot; contains &quot;WASP rules!&quot;)//.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>a: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CurrentTweetDate: timestamp, CurrentTwID: bigint ... 33 more fields]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>tweetsDF2TTTDFLightWeight: (tweetsInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
tweetsDF2TTTDFWithURLsAndHashtagsLightWeight: (tweetsInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(a)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val b = TTTsDF.filter($&quot;CurrentTweet&quot; contains &quot;#MakeDataGreatAgain&quot;)//.collect()
display(b) // output not displayed to comply with Twitter Developer rules
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// this will make sure all streaming job in the cluster are stopped - raaz
StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) } 
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// this will delete what we collected to keep the disk usage tight and tidy
dbutils.fs.rm(outputDirectoryRoot, true) 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res25: Boolean = true
</code></pre>
</div>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="twitter-hashtag-count"><a class="header" href="#twitter-hashtag-count">Twitter Hashtag Count</a></h1>
<p>Using Twitter Streaming is a great way to learn Spark Streaming if you don't have your streaming datasource and want a great rich input dataset to try Spark Streaming transformations on.</p>
<p>In this example, we show how to calculate the top hashtags seen in the last X window of time every Y time unit.</p>
<h2 id="top-hashtags-in-4-easy-steps"><a class="header" href="#top-hashtags-in-4-easy-steps">Top Hashtags in 4 Easy Steps</a></h2>
<p>We will now show quickly how to compute the top hashtags in a few easy steps after running some utility functions and importing needed libraries.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-run">&quot;./025_a_extendedTwitterUtils2run&quot;
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark._
import org.apache.spark.storage._
import org.apache.spark.streaming._


import twitter4j.auth.OAuthAuthorization
import twitter4j.conf.ConfigurationBuilder
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import org.apache.spark._
import org.apache.spark.storage._
import org.apache.spark.streaming._
import twitter4j.auth.OAuthAuthorization
import twitter4j.conf.ConfigurationBuilder
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell markdown">
<p><strong>Step 1: Enter your Twitter API Credentials.</strong></p>
<ul>
<li>Go to https://apps.twitter.com and look up your Twitter API Credentials, or create an app to create them.</li>
<li>Run the code in a cell to Enter your own credentials.</li>
</ul>
<pre><code class="language-%scala">// put your own twitter developer credentials below instead of xxx
// instead of the '%run &quot;.../secrets/026_secret_MyTwitterOAuthCredentials&quot;' below
// you need to copy-paste the following code-block with your own Twitter credentials replacing XXXX


// put your own twitter developer credentials below 

import twitter4j.auth.OAuthAuthorization
import twitter4j.conf.ConfigurationBuilder


// These have been regenerated!!! - need to chane them

def myAPIKey       = &quot;XXXX&quot; // APIKey 
def myAPISecret    = &quot;XXXX&quot; // APISecretKey
def myAccessToken          = &quot;XXXX&quot; // AccessToken
def myAccessTokenSecret    = &quot;XXXX&quot; // AccessTokenSecret


System.setProperty(&quot;twitter4j.oauth.consumerKey&quot;, myAPIKey)
System.setProperty(&quot;twitter4j.oauth.consumerSecret&quot;, myAPISecret)
System.setProperty(&quot;twitter4j.oauth.accessToken&quot;, myAccessToken)
System.setProperty(&quot;twitter4j.oauth.accessTokenSecret&quot;, myAccessTokenSecret)

println(&quot;twitter OAuth Credentials loaded&quot;)
</code></pre>
<p>The cell-below will not expose my Twitter API Credentials: <code>myAPIKey</code>, <code>myAPISecret</code>, <code>myAccessToken</code> and <code>myAccessTokenSecret</code>. Use the code above to enter your own credentials in a scala cell.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import twitter4j._
import twitter4j.auth.Authorization
import twitter4j.conf.ConfigurationBuilder
import twitter4j.auth.OAuthAuthorization
import org.apache.spark.streaming._
import org.apache.spark.streaming.dstream._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.receiver.Receiver
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-run">&quot;Users/raazesh.sainudiin@math.uu.se/scalable-data-science/secrets/026_secret_MyTwitterOAuthCredentials&quot;
</code></pre>
</div>
<div class="cell markdown">
<p><strong>Step 2: Configure where to output the top hashtags and how often to compute them.</strong></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val outputDirectory = &quot;/datasets/tweetsStreamTmp&quot; // output directory

//Recompute the top hashtags every N seconds. N=1
val slideInterval = new Duration(10 * 1000) // 1000 milliseconds is 1 second!

//Compute the top hashtags for the last M seconds. M=5
val windowLength = new Duration(30 * 1000)

// Wait W seconds before stopping the streaming job. W=100
val timeoutJobLength = 20 * 1000
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>outputDirectory: String = /datasets/tweetsStreamTmp
slideInterval: org.apache.spark.streaming.Duration = 10000 ms
windowLength: org.apache.spark.streaming.Duration = 30000 ms
timeoutJobLength: Int = 20000
</code></pre>
</div>
</div>
<div class="cell markdown">
<p><strong>Step 3: Run the Twitter Streaming job.</strong></p>
</div>
<div class="cell markdown">
<p>Go to SparkUI and see if a streaming job is already running. If so you need to terminate it before starting a new streaming job. Only one streaming job can be run on the DB CE.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// this will make sure all streaming job in the cluster are stopped
StreamingContext.getActive.foreach{ _.stop(stopSparkContext = false) } 
</code></pre>
</div>
<div class="cell markdown">
<p>Clean up any old files.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">dbutils.fs.mkdirs(&quot;dbfs:/datasets/tweetsStreamTmp/&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res3: Boolean = true
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(&quot;dbfs:/datasets/tweetsStreamTmp/&quot;))
</code></pre>
</div>
<div class="cell markdown">
<p>Let us write the function that creates the Streaming Context and sets up the streaming job.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">var newContextCreated = false
var num = 0

// This is a helper class used for ordering by the second value in a (String, Int) tuple
import scala.math.Ordering
object SecondValueOrdering extends Ordering[(String, Int)] {
  def compare(a: (String, Int), b: (String, Int)) = {
    a._2 compare b._2
  }
}

// This is the function that creates the SteamingContext and sets up the Spark Streaming job.
def creatingFunc(): StreamingContext = {
  // Create a Spark Streaming Context.
  val ssc = new StreamingContext(sc, slideInterval)
  // Create a Twitter Stream for the input source. 
  val auth = Some(new OAuthAuthorization(new ConfigurationBuilder().build()))
  val twitterStream = ExtendedTwitterUtils.createStream(ssc, auth)
  
  // Parse the tweets and gather the hashTags.
  val hashTagStream = twitterStream.map(_.getText).flatMap(_.split(&quot; &quot;)).filter(_.startsWith(&quot;#&quot;))
  
  // Compute the counts of each hashtag by window.
  // reduceByKey on a window of length windowLength
  // Once this is computed, slide the window by slideInterval and calculate reduceByKey again for the second window
  val windowedhashTagCountStream = hashTagStream.map((_, 1)).reduceByKeyAndWindow((x: Int, y: Int) =&gt; x + y, windowLength, slideInterval)

  // For each window, calculate the top hashtags for that time period.
  windowedhashTagCountStream.foreachRDD(hashTagCountRDD =&gt; {
    val topEndpoints = hashTagCountRDD.top(20)(SecondValueOrdering)
    dbutils.fs.put(s&quot;${outputDirectory}/top_hashtags_${num}&quot;, topEndpoints.mkString(&quot;\n&quot;), true)
    println(s&quot;------ TOP HASHTAGS For window ${num}&quot;)
    println(topEndpoints.mkString(&quot;\n&quot;))
    num = num + 1
  })
  
  newContextCreated = true
  ssc
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>newContextCreated: Boolean = false
num: Int = 0
import scala.math.Ordering
defined object SecondValueOrdering
creatingFunc: ()org.apache.spark.streaming.StreamingContext
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Create the StreamingContext using getActiveOrCreate, as required when starting a streaming job in Databricks.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val ssc = StreamingContext.getActiveOrCreate(creatingFunc)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@3961e605
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Start the Spark Streaming Context and return when the Streaming job exits or return with the specified timeout.</p>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-scala">ssc.start()
ssc.awaitTerminationOrTimeout(timeoutJobLength)
ssc.stop(stopSparkContext = false)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Wrote 498 bytes.
------ TOP HASHTAGS For window 0
(#1212SHOPEESTRAYKIDS
MAU,8)
(#PS5,2)
(#LifeGoesOnWithBTS,2)
(#BTS_BE,2)
(#,1)
(#rechtsterrorismus,1)
(#EXO,1)
(#Pourlamesse,,1)
(#MeralAkener,1)
(#,1)
(#
#

,1)
(#SoundCloud,1)
(#weareoneEXO
#
#,1)
(#kilicdaroglu,1)
(#Pride2020,1)
(#icisleribakanl,1)
(#Vialli?,1)
(#Cuma,1)
(#,1)
(#FridayVibes,1)
Wrote 423 bytes.
------ TOP HASHTAGS For window 1
(#1212SHOPEESTRAYKIDS
MAU,8)
(#Monster
MONSTER,7)
(#FridayLivestream

SB19,4)
(#BTS_BE,3)
(#LifeGoesOnWithBTS,3)
(#EXO,2)
(#Unlock_GOLIVEINLIFE,2)
(#PS5,2)
(#redvelvet,2)
(#SUNGHOON,1)
(#omniscient_reader,1)
(#5,1)
(#takasbkcxhanthos
#GLYHO,1)
(#FCKNZS,1)
(#DAMI,1)
(#CHOICE,1)
(#fahrettinkoca,1)
(#FridayThoughts,1)
(#Lausanne.,1)
(#
@treasuremembers

https://t.co/lo6QhtLCBB,1)
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Check out the Clusters <code>Streaming</code> UI as the job is running.</p>
</div>
<div class="cell markdown">
<p>It should automatically stop the streaming job after <code>timeoutJobLength</code>.</p>
<p>If not, then stop any active Streaming Contexts, but don't stop the spark contexts they are attached to using the following command.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) }
</code></pre>
</div>
<div class="cell markdown">
<p><strong>Step 4: View the Results.</strong></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(outputDirectory))
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>path</th>
<th>name</th>
<th>size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/top_hashtags_0</td>
<td>top_hashtags_0</td>
<td>498.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/tweetsStreamTmp/top_hashtags_1</td>
<td>top_hashtags_1</td>
<td>423.0</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell markdown">
<p>There should be 100 intervals for each second and the top hashtags for each of them should be in the file <code>top_hashtags_N</code> for <code>N</code> in 0,1,2,...,99 and the top hashtags should be based on the past 5 seconds window.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">dbutils.fs.head(s&quot;${outputDirectory}/top_hashtags_0&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res8: String =
(#1212SHOPEESTRAYKIDS
MAU,8)
(#PS5,2)
(#LifeGoesOnWithBTS,2)
(#BTS_BE,2)
(#,1)
(#rechtsterrorismus,1)
(#EXO,1)
(#Pourlamesse,,1)
(#MeralAkener,1)
(#,1)
(#
#

,1)
(#SoundCloud,1)
(#weareoneEXO
#
#,1)
(#kilicdaroglu,1)
(#Pride2020,1)
(#icisleribakanl,1)
(#Vialli?,1)
(#Cuma,1)
(#,1)
(#FridayVibes,1)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined class ExtendedTwitterReceiver
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined class ExtendedTwitterInputDStream
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import twitter4j.Status
import twitter4j.auth.Authorization
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.dstream.{ReceiverInputDStream, DStream}
defined object ExtendedTwitterUtils
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>twitter OAuth Credentials loaded
import twitter4j.auth.OAuthAuthorization
import twitter4j.conf.ConfigurationBuilder
import twitter4j.auth.OAuthAuthorization
import twitter4j.conf.ConfigurationBuilder
myAPIKey: String
myAPISecret: String
myAccessToken: String
myAccessTokenSecret: String
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>done running the extendedTwitterUtils2run notebook - ready to stream from twitter
</code></pre>
</div>
</div>
<div class="cell markdown">
<h2 id="lets-brainstorm"><a class="header" href="#lets-brainstorm">Let's brainstorm</a></h2>
<p>What could you do with this type of streaming capability?</p>
<ul>
<li>marketing?</li>
<li>pharmaceutical vigilance?</li>
<li>linking twitter activity to mass media activity?</li>
<li>data quality and integrity measures...</li>
</ul>
<p>Note that there are various Spark Streaming ML algorithms that one could easily throw at such <code>reduceByKeyAndWindow</code> tweet streams:</p>
<ul>
<li><a href="https://spark.apache.org/docs/latest/mllib-frequent-pattern-mining.html">Frequent Pattern Mining</a></li>
<li><a href="https://databricks.com/blog/2015/01/28/introducing-streaming-k-means-in-spark-1-2.html">Streaming K-Means</a></li>
<li><a href="https://spark.apache.org/docs/latest/ml-clustering.html#latent-dirichlet-allocation-lda">Latent Dirichlet Allocation - Topic Modeling</a></li>
</ul>
<p>Student Project or Volunteer for next Meetup - let's check it out now:</p>
<p>HOME-WORK:</p>
<ul>
<li><a href="https://databricks.gitbooks.io/databricks-spark-reference-applications/content/twitter_classifier/index.html">Twitter Streaming Language Classifier</a></li>
</ul>
</div>
<div class="cell markdown">
<h2 id="responsible-experiments"><a class="header" href="#responsible-experiments">Responsible Experiments</a></h2>
<p>Extracting knowledge from tweets is &quot;easy&quot; using techniques shown here, but one has to take responsibility for the use of this knowledge and conform to the rules and policies linked below.</p>
<p>Remeber that the use of twitter itself comes with various strings attached. Read:</p>
<ul>
<li><a href="https://twitter.com/rules">Twitter Rules</a></li>
</ul>
<p>Crucially, the use of the content from twitter by you (as done in this worksheet) comes with some strings. Read: - <a href="https://dev.twitter.com/overview/terms/agreement-and-policy">Developer Agreement &amp; Policy Twitter Developer Agreement</a></p>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="twitter-streaming-language-classifier"><a class="header" href="#twitter-streaming-language-classifier">Twitter Streaming Language Classifier</a></h1>
<p>This is a databricksification of <a href="https://databricks.gitbooks.io/databricks-spark-reference-applications/content/twitter_classifier/index.html">https://databricks.gitbooks.io/databricks-spark-reference-applications/content/twitter_classifier/index.html</a> by Amendra Shreshta.</p>
<p>Note that you need to change the fields in background notebook <code>025_a_extendedTwitterUtils2run</code> so more fields for <code>lang</code> in Tweets are exposed. This is a good example of how one has to go deeper into the java code of <code>twitter4j</code> as new needs arise.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-run">&quot;./025_c_extendedTwitterUtils2runWithLangs&quot;
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import twitter4j._
import twitter4j.auth.Authorization
import twitter4j.conf.ConfigurationBuilder
import twitter4j.auth.OAuthAuthorization
import org.apache.spark.streaming._
import org.apache.spark.streaming.dstream._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.receiver.Receiver
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined class ExtendedTwitterReceiver
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark._
import org.apache.spark.storage._
import org.apache.spark.streaming._

import scala.math.Ordering

import twitter4j.auth.OAuthAuthorization
import twitter4j.conf.ConfigurationBuilder
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import org.apache.spark._
import org.apache.spark.storage._
import org.apache.spark.streaming._
import scala.math.Ordering
import twitter4j.auth.OAuthAuthorization
import twitter4j.conf.ConfigurationBuilder
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined class ExtendedTwitterInputDStream
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell markdown">
<p><strong>Enter your Twitter API Credentials.</strong> * Go to https://apps.twitter.com and look up your Twitter API Credentials, or create an app to create them. * Run the code in a cell to Enter your own credentials.</p>
<pre><code class="language-%scala">// put your own twitter developer credentials below instead of xxx
// instead of the '%run &quot;.../secrets/026_secret_MyTwitterOAuthCredentials&quot;' below
// you need to copy-paste the following code-block with your own Twitter credentials replacing XXXX


// put your own twitter developer credentials below 

import twitter4j.auth.OAuthAuthorization
import twitter4j.conf.ConfigurationBuilder


// These have been regenerated!!! - need to chane them

def myAPIKey       = &quot;XXXX&quot; // APIKey 
def myAPISecret    = &quot;XXXX&quot; // APISecretKey
def myAccessToken          = &quot;XXXX&quot; // AccessToken
def myAccessTokenSecret    = &quot;XXXX&quot; // AccessTokenSecret


System.setProperty(&quot;twitter4j.oauth.consumerKey&quot;, myAPIKey)
System.setProperty(&quot;twitter4j.oauth.consumerSecret&quot;, myAPISecret)
System.setProperty(&quot;twitter4j.oauth.accessToken&quot;, myAccessToken)
System.setProperty(&quot;twitter4j.oauth.accessTokenSecret&quot;, myAccessTokenSecret)

println(&quot;twitter OAuth Credentials loaded&quot;)
</code></pre>
<p>The cell-below will not expose my Twitter API Credentials: <code>myAPIKey</code>, <code>myAPISecret</code>, <code>myAccessToken</code> and <code>myAccessTokenSecret</code>. Use the code above to enter your own credentials in a scala cell.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import twitter4j.Status
import twitter4j.auth.Authorization
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.dstream.{ReceiverInputDStream, DStream}
defined object ExtendedTwitterUtils
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>done running the extendedTwitterUtils2run notebook - ready to stream from twitter by filtering on strings, users and language
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-run">&quot;Users/raazesh.sainudiin@math.uu.se/scalable-data-science/secrets/026_secret_MyTwitterOAuthCredentials&quot;
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>twitter OAuth Credentials loaded
import twitter4j.auth.OAuthAuthorization
import twitter4j.conf.ConfigurationBuilder
import twitter4j.auth.OAuthAuthorization
import twitter4j.conf.ConfigurationBuilder
myAPIKey: String
myAPISecret: String
myAccessToken: String
myAccessTokenSecret: String
</code></pre>
</div>
</div>
<div class="cell markdown">
<h2 id="step-1-collect-data"><a class="header" href="#step-1-collect-data">Step 1. Collect Data</a></h2>
<p>Start downloading tweets in order to start building a simple model for language classification.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// ## Let's create a directory in dbfs for storing tweets in the cluster's distributed file system.
val outputDirectoryRoot = &quot;/datasets/tweetsStreamTmp&quot; // output directory
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>outputDirectoryRoot: String = /datasets/tweetsStreamTmp
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// to remove a pre-existing directory and start from scratch uncomment next line and evaluate this cell
dbutils.fs.rm(outputDirectoryRoot, true) 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res2: Boolean = true
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// ## Capture tweets in every sliding window of slideInterval many milliseconds.
val slideInterval = new Duration(1 * 1000) // 1 * 1000 = 1000 milli-seconds = 1 sec
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>slideInterval: org.apache.spark.streaming.Duration = 1000 ms
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Our goal is to take each RDD in the twitter DStream and write it as a json file in our dbfs.
// Create a Spark Streaming Context.
val ssc = new StreamingContext(sc, slideInterval)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@466128fb
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Create a Twitter Stream for the input source. 
val auth = Some(new OAuthAuthorization(new ConfigurationBuilder().build()))
val twitterStream = ExtendedTwitterUtils.createStream(ssc, auth)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>auth: Some[twitter4j.auth.OAuthAuthorization] = Some(OAuthAuthorization{consumerKey='8uN0N9RTLT1viaR811yyG7xwk', consumerSecret='******************************************', oauthToken=AccessToken{screenName='null', userId=4173723312}})
twitterStream: org.apache.spark.streaming.dstream.ReceiverInputDStream[twitter4j.Status] = ExtendedTwitterInputDStream@206a254d
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Let's import google's json library next.
import com.google.gson.Gson 
//Let's map the tweets into json formatted string (one tweet per line).
val twitterStreamJson = twitterStream.map(
                                            x =&gt; { val gson = new Gson();
                                                 val xJson = gson.toJson(x)
                                                 xJson
                                                 }
                                          ) 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import com.google.gson.Gson
twitterStreamJson: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.MappedDStream@20405bce
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val partitionsEachInterval = 1 

val batchInterval = 1 // in minutes
val timeoutJobLength =  batchInterval * 5

var newContextCreated = false
var numTweetsCollected = 0L // track number of tweets collected

twitterStreamJson.foreachRDD((rdd, time) =&gt; { // for each filtered RDD in the DStream
      val count = rdd.count()
      if (count &gt; 0) {
        val outputRDD = rdd.repartition(partitionsEachInterval) // repartition as desired
        // to write to parquet directly in append mode in one directory per 'time'------------       
        val outputDF = outputRDD.toDF(&quot;tweetAsJsonString&quot;)
        // get some time fields from current `.Date()`
        val year = (new java.text.SimpleDateFormat(&quot;yyyy&quot;)).format(new java.util.Date())
        val month = (new java.text.SimpleDateFormat(&quot;MM&quot;)).format(new java.util.Date())
        val day = (new java.text.SimpleDateFormat(&quot;dd&quot;)).format(new java.util.Date())
        val hour = (new java.text.SimpleDateFormat(&quot;HH&quot;)).format(new java.util.Date())
        // write to a file with a clear time-based hierarchical directory structure for example
        outputDF.write.mode(SaveMode.Append)
                .parquet(outputDirectoryRoot+ &quot;/&quot;+ year + &quot;/&quot; + month + &quot;/&quot; + day + &quot;/&quot; + hour + &quot;/&quot; + time.milliseconds) 
        // end of writing as parquet file-------------------------------------
        numTweetsCollected += count // update with the latest count
      }
  })
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>partitionsEachInterval: Int = 1
batchInterval: Int = 1
timeoutJobLength: Int = 5
newContextCreated: Boolean = false
numTweetsCollected: Long = 0
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// ## Let's start the spark streaming context we have created next.
ssc.start()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// total tweets downloaded
numTweetsCollected
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res14: Long = 6936
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// ## Go to SparkUI and see if a streaming job is already running. If so you need to terminate it before starting a new streaming job. Only one streaming job can be run on the DB CE.
// #  let's stop the streaming job next.
ssc.stop(stopSparkContext = false) 
StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) } 
</code></pre>
</div>
<div class="cell markdown">
<h2 id="step-2-explore-data"><a class="header" href="#step-2-explore-data">Step 2: Explore Data</a></h2>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-run">&quot;./025_b_TTTDFfunctions&quot;
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// #Let's examine what was saved in dbfs
display(dbutils.fs.ls(outputDirectoryRoot))
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>path</th>
<th>name</th>
<th>size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dbfs:/datasets/tweetsStreamTmp/2020/</td>
<td>2020/</td>
<td>0.0</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Replace the date with current date
val date = &quot;/2020/11/*&quot;
val rawDF = fromParquetFile2DF(outputDirectoryRoot + date +&quot;/*/*&quot;) //.cache()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>date: String = /2020/11/*
rawDF: org.apache.spark.sql.DataFrame = [tweetAsJsonString: string]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val TTTsDF = tweetsDF2TTTDF(tweetsJsonStringDF2TweetsDF(rawDF)).cache()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>TTTsDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CurrentTweetDate: timestamp, CurrentTwID: bigint ... 35 more fields]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Creating SQL table 
TTTsDF.createOrReplaceTempView(&quot;tbl_tweet&quot;)
</code></pre>
</div>
<div class="cell markdown">
<h2 id="step-3-build-model"><a class="header" href="#step-3-build-model">Step 3. Build Model</a></h2>
<p>Let us use the structured data in <code>tbl_tweet</code> to build a simple classifier of the language using K-means.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sqlContext.sql(&quot;SELECT lang, CPostUserName, CurrentTweet FROM tbl_tweet LIMIT 10&quot;).collect.foreach(println)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>[ja,,#hour_14#]
[ja,,]
[ja,,]
[en,HourGMT5,It's November 20, 2020 at 06:01AM!]
[fr, ,Meghan kali et bts ont drop leurs albums ]
[en,Mika,Another hour! It's November 20, 2020 at 08:01PM]
[ja,,RT @Sdan_sanrio: #5 https://t.co/Wnel4BWKAR]
[pt,TRIGGERED LOLI,@ItachinMr Opa como t]
[th,ING. ,   ]
[tr, ,@soudadesky hadi gelin]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-scala">// Checking the language of tweets
sqlContext.sql(
    &quot;SELECT lang, COUNT(*) as cnt FROM tbl_tweet &quot; +
    &quot;GROUP BY lang ORDER BY cnt DESC limit 1000&quot;)
    .collect.foreach(println)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>[ja,1985]
[en,1868]
[und,532]
[th,467]
[ko,338]
[in,325]
[es,281]
[pt,280]
[ar,279]
[tr,200]
[fr,160]
[tl,109]
[ru,57]
[pl,45]
[de,42]
[hi,41]
[fa,31]
[it,31]
[nl,23]
[ur,22]
[et,20]
[zh,18]
[ht,11]
[el,9]
[ta,9]
[iw,8]
[ro,6]
[cs,6]
[eu,5]
[lt,5]
[ca,5]
[da,4]
[sr,4]
[gu,4]
[no,4]
[is,3]
[uk,3]
[vi,3]
[sl,3]
[fi,3]
[te,2]
[sv,2]
[cy,2]
[bg,2]
[mr,1]
[or,1]
[sd,1]
[kn,1]
[km,1]
[bn,1]
[ne,1]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// extracting just tweets from the table and converting it to String
val texts = sqlContext
      .sql(&quot;SELECT CurrentTweet from tbl_tweet&quot;)
      .map(_.toString)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>texts: org.apache.spark.sql.Dataset[String] = [value: string]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.mllib.clustering.KMeans
import org.apache.spark.mllib.linalg.{Vector, Vectors}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import org.apache.spark.mllib.clustering.KMeans
import org.apache.spark.mllib.linalg.{Vector, Vectors}
</code></pre>
</div>
</div>
<div class="cell markdown">
<p><strong>Featurize as bigrams</strong></p>
<p>Create feature vectors by turning each tweet into bigrams of characters (an n-gram model) and then hashing those to a length-1000 feature vector that we can pass to MLlib.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def featurize(s: String): Vector = {
  val n = 1000
  val result = new Array[Double](n)
  val bigrams = s.sliding(2).toArray
  for (h &lt;- bigrams.map(_.hashCode % n)) {
    result(h) += 1.0 / bigrams.length
  }
  Vectors.sparse(n, result.zipWithIndex.filter(_._1 != 0).map(_.swap))
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>featurize: (s: String)org.apache.spark.mllib.linalg.Vector
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//Cache the vectors RDD since it will be used for all the KMeans iterations.
val vectors = texts.rdd
      .map(featurize)
      .cache()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>vectors: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[3608] at map at command-2972105651607107:3
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// cache is lazy so count will force the data to store in memory
vectors.count()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res22: Long = 7264
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">vectors.first()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res23: org.apache.spark.mllib.linalg.Vector = (1000,[189,227,263,313,335,344,351,358,372,382,389,411,439,500,501,529,558,565,568,571,629,647,658,741,804,856,872,892,994],[0.03333333333333333,0.03333333333333333,0.03333333333333333,0.03333333333333333,0.03333333333333333,0.03333333333333333,0.03333333333333333,0.03333333333333333,0.03333333333333333,0.03333333333333333,0.03333333333333333,0.03333333333333333,0.03333333333333333,0.03333333333333333,0.03333333333333333,0.03333333333333333,0.03333333333333333,0.03333333333333333,0.03333333333333333,0.03333333333333333,0.03333333333333333,0.03333333333333333,0.03333333333333333,0.03333333333333333,0.03333333333333333,0.03333333333333333,0.06666666666666667,0.03333333333333333,0.03333333333333333])
</code></pre>
</div>
</div>
<div class="cell markdown">
<p><strong>K-Means model</strong> trained with 10 clusters and 10 iterations.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Training model with 10 cluster and 10 iteration
val model = KMeans.train(vectors, k=10, maxIterations = 10)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>model: org.apache.spark.mllib.clustering.KMeansModel = org.apache.spark.mllib.clustering.KMeansModel@61f49262
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-scala">// Sample 100 of the original set
val some_tweets = texts.take(100)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>some_tweets: Array[String] =
Array([#hour_14#], [], [], [It's November 20, 2020 at 06:01AM!], [Meghan kali et bts ont drop leurs albums ], [Another hour! It's November 20, 2020 at 08:01PM], [RT @Sdan_sanrio: #5 https://t.co/Wnel4BWKAR], [@ItachinMr Opa como t], [   ], [@soudadesky hadi gelin], [RT @p8h33: 5000   
1000 
1000 
1000 
1000 
1000 
       ], [RT @doktongchangyed:     ], [RT @selahattingrkn: Ho geldik evet nk hoa geldik.

Biz hizmet retmeye, gnllere dokunmaya geldik.

Biz bu ehrin her kar topran], [@OmarNassar01 ], [12  ], [RT @ministop_fan: RT






&amp;RT
1,000], [@Kero_kero_pad ()], [@pradverse Mera no. ni aane wala ab], [voltar a assistir spn s pra saber o final], [RT @aztvresmi: Azrbaycan Ordusunun blmlri Adam rayonuna daxil olub 

Mdafi Nazirliyi https://t.co/OGXPqdoalB], [OK # #], [RT @_G3WP3R:      https://t.co/ibjFPqHBCM], [RT @mkodchannel: 
@hikakin 
https://t.co/47YXJxeaNp https://t.co/fjsNWCCUI9], [RT @istopeverything:  ], [RT @TheDeverakonda: I told you it comes with full approval 

Good opportunity to do something together with your entire family - get tog], [RT @merlinx26:  ], [@Sarashina_BB277 ], [@steph_sunandar HAHAHA gue ud ep 7 tp gue tim han ji pyeong bgt sun], [RT @anuxaaaaa: oi mudei o cabelo  https://t.co/u3KKQ8rhAS], [RT @OficialSala12: Flamengo precisando de gol:

Michael: https://t.co/FdEyuUbqzZ], [RT @c_emre11: https://t.co/JijOPNwfJH], [RT @bts_bighit: [#]    ..
   ..
   Live Goes On!
  

#USB], [Its possible you'll fall head over heels for a new lover or f... More for Cancer https://t.co/y5ZArDaO0u], [@uwith_b      55555555555555555555], [RT @telugufilmnagar: All Smiles 
@urstrulymahesh shares a lovely picture wishing #ShilpaShirodkar on her birthday!

#MaheshBabu #Namrat], [RT @parkxminxmin:    ~ &gt;&lt;
https://t.co/9NjzwRziRP

#BTS #JIMIN # @BTS_twt https://t.co/TYBgv3lmAP], [RT @tarouame96:  https://t.co/gSmqJRrd7i], [     -    .  https://t.co/8VHbtT8edk], [RT @Anime_ABEMA:  #
2!

 1127() 1030


 #KENTHE390  (@KENTHE390 )


ABEMA], [@jam_cauilan Opo ate], [free hunter icons
https://t.co/tAZ5loOKbZ], [@omanai_on                    ], [RT @pineapplebreads: Here, I summarized the #Supernatural finale so you don't have to watch it https://t.co/LJ9jDHFbSm], [RT @WinterBbang: No one:
Literally no one:
Bit and Hangyul: (ability of thhe other members that you wanna have)
DOHYON'S HEIGHT https://t.], [RT @WaseemPatriot: #SaveChildrenOfIIOJK
During this phase of the armed conflict, the scale of the violence made documentation difficult. A], [RT @Olanmoveforward: #EndSARS
#EndNorthBanditry
#EndSarsNow
#EndBadGovernmentinNIGERIA
#LekkiMassacre
#LekkiTollGateShooting
#EndPoliceB], [@thatginamiller https://t.co/uQTIWSzELG], [RT @deryloops: his reaction  https://t.co/aDAGeYOuOf], [RT @Promise_Royalz: @AlbumTalksHQ @yemialadee https://t.co/e8mwpyQmK2], [RT @bleksip: 2020 masih takut mencari pertolongan untuk masalah kesehatan mentalmu?

Emang ngapain sih kalau ke psikiater?
Apa akan disetru], [RT @JorgeMoruno: Al margen de que su abuelo era un nazi, el mengele espaol, me ha recordado a esto. Cuando los millonarios empiezan de cer], [Aku giniin sendiri kok s*nge yaa, apakah ada yg salah (?) wkwk], [RT @SNOOPYZ3US: jungwoo preview for vogue

a thread: https://t.co/v7xiKXvsTG], [Vraiment  mme mon entourage ne va pas me croire ], [RT @aespasmiling: music bank interview!
#aespa # https://t.co/TSmezo1OX3], [Nigerian politicians], [mass indoctrination day24
#PureDoctrineOfChrist], [RT @Primacaque:  

  

], [Coronavirus vaccine: Moderna completes phase 3 of vaccine study https://t.co/YAylsaf8uq via @YouTube], [             . https://t.co/fcG9s71RR3            ], [Was it not a few weeks ago that the TL was up in arms and ready to protect young girlhood after that awful video surfaced?

Why am I now seeing justifications for a grown men sleeping with teenagers?], [consegui dormir e acordar cedo pela primeira vez em meses e t exatamente assim agora https://t.co/ATcfUH8CK9], [https://t.co/HyTgIpFsF0 https://t.co/gjCnoludHB], [ :   ,   .
X   

 

[   ]
 3    8 

 8  1,  8
 1 1 (1 1 X)

    
https://t.co/NUpv3GZqpc https://t.co/7WW7kNKjFQ], [Soluciones prueba 8: JK  https://t.co/jXVheWj8YO], [@Pkhetarpal1 @junkie_for_news @ANI Madar chod], [Weitere angekndigte Features fr #Twitter um die Plattform konkurrenzfhig zu halten:

-In Zukunft nur noch quadratische Fotos
-Tweets verschwinden nach einmaligem Ansehen
-Verifikationshaken wechselt Farbe je nach Followerzahl
-Videotweets (max 12s) ab 2021 standard], [5-1



2

# https://t.co/x8nT9lG88V], [In the light of the appalling abuse of human rights perpetrated on the people of Hong Kong, surely Essex should consider closing down all relationship with China?&quot;

https://t.co/8SRDUUvDPn], [
     
 https://t.co/ZV0gosDPji], [ https://t.co/OxqUo4l98T], [ #PORRALEGA | Acierta resultado y goleadores pepineros del #MlagaLegans y entra en el sorteo de una mascarilla oficial . Suerte a todos! https://t.co/qrEHXyjs4w], [Woman miraculously survives being shoved onto train tracks by crazed NYC subway rider https://t.co/xt80mV7VQr], [Sevimli miniklerle elence devam ediyor!

#ocuktanAlHaberi yeni blmyle yarn 17.00'de Show TV'de! @cocuktnalshowtv https://t.co/jdZZ6f8gix], [], [
10@atjam_hashimoto





/
 https://t.co/BXnn2YScdF
# # #  # #22 #21 https://t.co/lnJvxwExzA], [135( @2m5k2 )

# #lovelive # # # # https://t.co/CqkUUaZB1Q], [
I_1I_2
I_2R_2V_2



# https://t.co/RFYRwrxjUS], [=TEAM 51% VOTING=

ONCEs! Here's the mass voting schedule. We are in need of more voters. Encourage more ONCEs in different SNS and close the gap!

Please follow your regional fanbase and @billboard_twice for more information.

@JYPETWICE #TWICE #MAMAVOTE #51_Percent https://t.co/ZklwziCFXo], [How does Coronavirus compare to Ebola  SARS  etc  https://t.co/AT484Gd9kc https://t.co/klm1KUQQC7], [Arhiepiscopia Trgovitei anun aciuni caritabile de 1 milion de lei n preajma Crciunului.

https://t.co/o9QwTPhoua https://t.co/lIJS9iHk4W], [ ,     ], [##



(`) https://t.co/yVSos0wW8d], [On 21 November 1920, Michael Feery and Jerome O'Leary attended a football match in Croke Park. They never came home. Michael and Jerome were two of 14 people who were killed on Bloody Sunday.

#B100dySunday - the GAA remembers.

Learn more at https://t.co/0YyrIyygwQ https://t.co/ufmb7aZk9B], [



https://t.co/i9pTilGJxk
#YouTube # # # # # https://t.co/wQYljzmi3E], [En sus ltimos lanzamientos le podamos ver de la mano de @omarmontesSr o @lolaindigomusic, pero ahora @Rvfvrxiz reclama protagonismo en solitario con nuevo material bajo el brazo.

Descbrelo en https://t.co/4qpCcEh0rw https://t.co/SW6eylT1mR], [@Nefie26 (*`*)], [Register https://t.co/EhcY9eoAUb https://t.co/AbgyyqMErV], [Playing chess and a few other things  https://t.co/iEOqPaysO1], [@miinyan_hanabi ], [TIC y educacin en Tweeted Times @lmggr https://t.co/4u0k8Q7DKG], [@dc_maru_ ], [RT @narendramodi: Neutralising of 4 terrorists belonging to Pakistan-based terrorist organisation Jaish-e-Mohammed and the presence of larg], [no #Gears5], [@NestT4ku ], [Allahm sen utandrma gnlmden geeni hakkmda hayrl eyle bu son olsun en gzeli olsun], [RT @ATEEZofficial: [#]  
#ATEEZ # https://t.co/EivLdPVLit], [@_itsbunnie @GivingAnimeboo doneee], [RT @IHK_FFM_INT: Die AHK #Peru  ldt Sie zu den 3. Deutsch-Peruanischen Wirtschaftstagen unter dem Motto &quot;Gemeinsame Ziele vereinen&quot; ein.], [RT @LeireOlmeda: C's dice que la M-50 la pague Rivas, por rojos o algo as. Y el PP que palmaditas en la espalda las que hagan falta, pero])
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-scala">// iterate through the 100 samples and show which cluster they are in
for (i &lt;- 0 until 10) {
  println(s&quot;\nCLUSTER $i:&quot;)
  some_tweets.foreach { t =&gt;
    if (model.predict(featurize(t)) == i) {
      println(t)
    }
  }
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>CLUSTER 0:
[RT @mkodchannel: 
@hikakin 
https://t.co/47YXJxeaNp https://t.co/fjsNWCCUI9]
[RT @anuxaaaaa: oi mudei o cabelo  https://t.co/u3KKQ8rhAS]
[RT @c_emre11: https://t.co/JijOPNwfJH]
[RT @parkxminxmin:    ~ &gt;&lt;
https://t.co/9NjzwRziRP

#BTS #JIMIN # @BTS_twt https://t.co/TYBgv3lmAP]
[RT @tarouame96:  https://t.co/gSmqJRrd7i]
[free hunter icons
https://t.co/tAZ5loOKbZ]
[@thatginamiller https://t.co/uQTIWSzELG]
[RT @deryloops: his reaction  https://t.co/aDAGeYOuOf]
[RT @Promise_Royalz: @AlbumTalksHQ @yemialadee https://t.co/e8mwpyQmK2]
[https://t.co/HyTgIpFsF0 https://t.co/gjCnoludHB]
[Soluciones prueba 8: JK  https://t.co/jXVheWj8YO]
[
     
 https://t.co/ZV0gosDPji]
[ https://t.co/OxqUo4l98T]
[How does Coronavirus compare to Ebola  SARS  etc  https://t.co/AT484Gd9kc https://t.co/klm1KUQQC7]
[##



(`) https://t.co/yVSos0wW8d]
[



https://t.co/i9pTilGJxk
#YouTube # # # # # https://t.co/wQYljzmi3E]
[Register https://t.co/EhcY9eoAUb https://t.co/AbgyyqMErV]
[RT @ATEEZofficial: [#]  
#ATEEZ # https://t.co/EivLdPVLit]

CLUSTER 1:
[It's November 20, 2020 at 06:01AM!]
[Meghan kali et bts ont drop leurs albums ]
[Another hour! It's November 20, 2020 at 08:01PM]
[RT @TheDeverakonda: I told you it comes with full approval  

Good opportunity to do something together with your entire family - get tog]
[@steph_sunandar HAHAHA gue ud ep 7 tp gue tim han ji pyeong bgt sun]
[RT @telugufilmnagar: All Smiles 
@urstrulymahesh shares a lovely picture wishing #ShilpaShirodkar on her birthday!

#MaheshBabu #Namrat]
[RT @WinterBbang: No one:
Literally no one:
Bit and Hangyul: (ability of thhe other members that you wanna have) 
DOHYON'S HEIGHT https://t.]
[RT @WaseemPatriot: #SaveChildrenOfIIOJK 
During this phase of the armed conflict, the scale of the violence made documentation difficult. A]
[mass indoctrination day24
#PureDoctrineOfChrist]
[Was it not a few weeks ago that the TL was up in arms and ready to protect young girlhood after that awful video surfaced?

Why am I now seeing justifications for a grown men sleeping with teenagers?]
[Weitere angekndigte Features fr #Twitter um die Plattform konkurrenzfhig zu halten: 

-In Zukunft nur noch quadratische Fotos
-Tweets verschwinden nach einmaligem Ansehen
-Verifikationshaken wechselt Farbe je nach Followerzahl
-Videotweets (max 12s) ab 2021 standard]
[In the light of the appalling abuse of human rights perpetrated on the people of Hong Kong, surely Essex should consider closing down all relationship with China?&quot;

https://t.co/8SRDUUvDPn]
[=TEAM 51% VOTING=

ONCEs! Here's the mass voting schedule. We are in need of more voters. Encourage more ONCEs in different SNS and close the gap! 

Please follow your regional fanbase and @billboard_twice for more information.

@JYPETWICE #TWICE #MAMAVOTE #51_Percent https://t.co/ZklwziCFXo]
[RT @narendramodi: Neutralising of 4 terrorists belonging to Pakistan-based terrorist organisation Jaish-e-Mohammed and the presence of larg]
[RT @IHK_FFM_INT: Die AHK #Peru  ldt Sie zu den 3. Deutsch-Peruanischen Wirtschaftstagen unter dem Motto &quot;Gemeinsame Ziele vereinen&quot; ein.]

CLUSTER 2:
[#hour_14#]
[]
[]
[RT @Sdan_sanrio: #5 https://t.co/Wnel4BWKAR]
[   ]
[RT @p8h33: 5000   
1000 
1000 
1000 
1000 
1000 
       ]
[RT @doktongchangyed:     ]
[12  ]
[RT @ministop_fan: RT






&amp;RT
1,000]
[OK # #]
[RT @_G3WP3R:      https://t.co/ibjFPqHBCM]
[RT @istopeverything:  ]
[RT @merlinx26:  ]
[RT @bts_bighit: [#]    ..
   ..
   Live Goes On!
  

#USB]
[     -    .  https://t.co/8VHbtT8edk]
[RT @Anime_ABEMA:  # 
2!

 1127() 1030


 #KENTHE390  (@KENTHE390 ) 


ABEMA]
[@omanai_on                    ]
[RT @Olanmoveforward: #EndSARS 
#EndNorthBanditry 
#EndSarsNow 
#EndBadGovernmentinNIGERIA
#LekkiMassacre
#LekkiTollGateShooting
#EndPoliceB]
[RT @Primacaque:  

  

]
[             . https://t.co/fcG9s71RR3            ]
[ :   ,   . 
X    

 

[   ]
 3    8  

 8  1,  8 
 1 1 (1 1 X) 

    
https://t.co/NUpv3GZqpc https://t.co/7WW7kNKjFQ]
[5-1



2

# https://t.co/x8nT9lG88V]
[]
[
10@atjam_hashimoto





/
 https://t.co/BXnn2YScdF
# # #  # #22 #21 https://t.co/lnJvxwExzA]
[135( @2m5k2 )

# #lovelive # # # # https://t.co/CqkUUaZB1Q]
[
I_1I_2
I_2R_2V_2



# https://t.co/RFYRwrxjUS]
[ ,     ]

CLUSTER 3:
[@soudadesky hadi gelin]
[RT @selahattingrkn: Ho geldik evet nk hoa geldik.

Biz hizmet retmeye, gnllere dokunmaya geldik.

Biz bu ehrin her kar topran]
[@OmarNassar01 ]
[@Kero_kero_pad ()]
[@Sarashina_BB277 ]
[@jam_cauilan Opo ate]
[Nigerian politicians]
[@Pkhetarpal1 @junkie_for_news @ANI Madar chod]
[@Nefie26 (*`*)]
[@miinyan_hanabi ]
[@dc_maru_ ]
[@NestT4ku ]
[@_itsbunnie @GivingAnimeboo doneee]

CLUSTER 4:
[@ItachinMr Opa como t]
[@pradverse Mera no. ni aane wala ab]
[voltar a assistir spn s pra saber o final]
[RT @bleksip: 2020 masih takut mencari pertolongan untuk masalah kesehatan mentalmu?

Emang ngapain sih kalau ke psikiater?
Apa akan disetru]
[RT @JorgeMoruno: Al margen de que su abuelo era un nazi, el mengele espaol, me ha recordado a esto. Cuando los millonarios empiezan de cer]
[Aku giniin sendiri kok s*nge yaa, apakah ada yg salah (?) wkwk]
[Vraiment  mme mon entourage ne va pas me croire ]
[consegui dormir e acordar cedo pela primeira vez em meses e t exatamente assim agora https://t.co/ATcfUH8CK9]
[ #PORRALEGA | Acierta resultado y goleadores pepineros del #MlagaLegans y entra en el sorteo de una mascarilla oficial . Suerte a todos! https://t.co/qrEHXyjs4w]
[En sus ltimos lanzamientos le podamos ver de la mano de @omarmontesSr o @lolaindigomusic, pero ahora @Rvfvrxiz reclama protagonismo en solitario con nuevo material bajo el brazo.

Descbrelo en https://t.co/4qpCcEh0rw https://t.co/SW6eylT1mR]
[no #Gears5]
[Allahm sen utandrma gnlmden geeni hakkmda hayrl eyle bu son olsun en gzeli olsun]
[RT @LeireOlmeda: C's dice que la M-50 la pague Rivas, por rojos o algo as. Y el PP que palmaditas en la espalda las que hagan falta, pero]

CLUSTER 5:
[@uwith_b      55555555555555555555]

CLUSTER 6:
[RT @aztvresmi: Azrbaycan Ordusunun blmlri Adam rayonuna daxil olub 

Mdafi Nazirliyi https://t.co/OGXPqdoalB]
[RT @OficialSala12: Flamengo precisando de gol:

Michael: https://t.co/FdEyuUbqzZ]
[Its possible you'll fall head over heels for a new lover or f... More for Cancer https://t.co/y5ZArDaO0u]
[RT @pineapplebreads: Here, I summarized the #Supernatural finale so you don't have to watch it https://t.co/LJ9jDHFbSm]
[RT @SNOOPYZ3US: jungwoo preview for vogue

a thread: https://t.co/v7xiKXvsTG]
[RT @aespasmiling: music bank interview! 
#aespa # https://t.co/TSmezo1OX3]
[Coronavirus vaccine: Moderna completes phase 3 of vaccine study https://t.co/YAylsaf8uq via @YouTube]
[Woman miraculously survives being shoved onto train tracks by crazed NYC subway rider https://t.co/xt80mV7VQr]
[Sevimli miniklerle elence devam ediyor!

#ocuktanAlHaberi yeni blmyle yarn 17.00'de Show TV'de! @cocuktnalshowtv https://t.co/jdZZ6f8gix]
[Arhiepiscopia Trgovitei anun aciuni caritabile de 1 milion de lei n preajma Crciunului.

https://t.co/o9QwTPhoua https://t.co/lIJS9iHk4W]
[On 21 November 1920, Michael Feery and Jerome O'Leary attended a football match in Croke Park. They never came home. Michael and Jerome were two of 14 people who were killed on Bloody Sunday.

#B100dySunday - the GAA remembers.

Learn more at https://t.co/0YyrIyygwQ https://t.co/ufmb7aZk9B]
[Playing chess and a few other things  https://t.co/iEOqPaysO1]
[TIC y educacin en Tweeted Times @lmggr https://t.co/4u0k8Q7DKG]

CLUSTER 7:

CLUSTER 8:

CLUSTER 9:
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">dbutils.fs.ls(&quot;/datasets/model/&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res25: Seq[com.databricks.backend.daemon.dbutils.FileInfo] = WrappedArray(FileInfo(dbfs:/datasets/model/_SUCCESS, _SUCCESS, 0), FileInfo(dbfs:/datasets/model/part-00000, part-00000, 8266), FileInfo(dbfs:/datasets/model/part-00001, part-00001, 8266), FileInfo(dbfs:/datasets/model/part-00002, part-00002, 8266), FileInfo(dbfs:/datasets/model/part-00003, part-00003, 16282), FileInfo(dbfs:/datasets/model/part-00004, part-00004, 8266), FileInfo(dbfs:/datasets/model/part-00005, part-00005, 8266), FileInfo(dbfs:/datasets/model/part-00006, part-00006, 8266), FileInfo(dbfs:/datasets/model/part-00007, part-00007, 16282))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// to remove a pre-existing model and start from scratch
dbutils.fs.rm(&quot;/datasets/model&quot;, true) 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res26: Boolean = true
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// save the model
sc.makeRDD(model.clusterCenters).saveAsObjectFile(&quot;/datasets/model&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.mllib.clustering.KMeans
import org.apache.spark.mllib.linalg.{Vector, Vectors}
import org.apache.spark.mllib.clustering.KMeansModel
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import org.apache.spark.mllib.clustering.KMeans
import org.apache.spark.mllib.linalg.{Vector, Vectors}
import org.apache.spark.mllib.clustering.KMeansModel
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>USAGE: val df = tweetsDF2TTTDF(tweetsJsonStringDF2TweetsDF(fromParquetFile2DF(&quot;parquetFileName&quot;)))
                  val df = tweetsDF2TTTDF(tweetsIDLong_JsonStringPairDF2TweetsDF(fromParquetFile2DF(&quot;parquetFileName&quot;)))
                  
import org.apache.spark.sql.types.{StructType, StructField, StringType}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.spark.sql.ColumnName
import org.apache.spark.sql.DataFrame
fromParquetFile2DF: (InputDFAsParquetFilePatternString: String)org.apache.spark.sql.DataFrame
tweetsJsonStringDF2TweetsDF: (tweetsAsJsonStringInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
tweetsIDLong_JsonStringPairDF2TweetsDF: (tweetsAsIDLong_JsonStringInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
tweetsDF2TTTDF: (tweetsInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
tweetsDF2TTTDFWithURLsAndHashtags: (tweetsInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Checking if the model works
val clusterNumber = 5

val modelFile = &quot;/datasets/model&quot;

val model: KMeansModel = new KMeansModel(sc.objectFile[Vector](modelFile).collect)
model.predict(featurize(&quot;        &quot;)) == clusterNumber
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>clusterNumber: Int = 5
modelFile: String = /datasets/model
model: org.apache.spark.mllib.clustering.KMeansModel = org.apache.spark.mllib.clustering.KMeansModel@45bb929a
res28: Boolean = false
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>tweetsDF2TTTDFLightWeight: (tweetsInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">model.predict(featurize(&quot;&quot;)) == 2
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res29: Boolean = true
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">model.predict(featurize(&quot;        &quot;)) == 2
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res30: Boolean = true
</code></pre>
</div>
</div>
<div class="cell markdown">
<p><strong>Loading model and printing tweets that matched the desired cluster.</strong></p>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-scala">var newContextCreated = false
var num = 0

// Create a Spark Streaming Context.
@transient val ssc = new StreamingContext(sc, slideInterval)
// Create a Twitter Stream for the input source. 
@transient val auth = Some(new OAuthAuthorization(new ConfigurationBuilder().build()))
@transient val twitterStream = ExtendedTwitterUtils.createStream(ssc, auth)

//Replace the cluster number as you desire between 0 to 9
val clusterNumber = 2

//model location
val modelFile = &quot;/datasets/model&quot;

// Get tweets from twitter
val Tweet = twitterStream.map(_.getText)
//Tweet.print()

println(&quot;Initalizaing the the KMeans model...&quot;)
val model: KMeansModel = new KMeansModel(sc.objectFile[Vector](modelFile).collect)

//printing tweets that match our choosen cluster
Tweet.foreachRDD(rdd =&gt; {  
rdd.collect().foreach(i =&gt;
    {
       val record = i
       if (model.predict(featurize(record)) == clusterNumber) {
       println(record)
    }
    })
})

// Start the streaming computation
println(&quot;Initialization complete.&quot;)
ssc.start()
ssc.awaitTermination()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Initalizaing the the KMeans model...
Initialization complete.

RT @chieri_kakyoin: ((( _(_'')_ ((( _(_'')_

()
RT @1_yanny: Feliz Viernes
Activos
#InnovacinYSoberanaTecnolgica
@ElizTorrres @kerny70
@Puerto462 @BaenaDegas @luiscarrillo66 @jdmend
  .
RT @crn_ru:  ?        https://t.co/1AXuN336dY    
RT @FilipeRet: B O M   D I A   !
#__        . #_ #G20
1
RT @mechocola:  Gen.G  

 ... 
@yuu294 

@FzWWCPMYu1k0EKr 

RT @gsushma55:     .. 
    .. 

       ..
    .. !! 

#Sush!
@BTS_History613 @BTS_twt YES
@euyuule 
RT @Rmlove09127: &quot;73 22,117,000 13 22,753,094 10  636,094    515 &quot;
23 1      
RT @meniichigo: 2


RT @ArmyYourselfBR: | Todas as msicas do lbum BE esto no Top 30 do iTunes Brasil. 

#1 - #LifeGoesOn  
#5 - Blue &amp; Grey
#6 - Fly to M
RT @BFidr: 70400

https://t.co/8ChsUGqX4M
RT @Hyuga_aipara: 

#2020 
#2020 
# https://t.co/biygw0ExzT



 
 
 
 3
# #

https://t.co/dssTEuSiGL
RT @DONNYSQUADPH: Trust in You...                   amidst the   
                                            darkness.

#WalkWithYouMV htt
@shashikant88857        #   ..... 
  #_     ..!!!

       
#    !!!!

#___

#___ 

#____ 

RT @monsterzmate: 

 ...... 


https://t.co/Hl3N5ByO4Y

( @maimoto_k )
RT @KharkhariAmit:    @RahulGandhi    ?        ,    
RT @bluesherbet_:  IMF  KPOP  ()
https://t.co
 













 https://t.co/ypNVkpElUv
()DM


DMDM 

()
@BTS_LS0618 
@ippy1pandshot73 
RT @m0momochan: 7
@lfsswt @0llyfyb Yes.
@pc3589_ 

@Poketoru0820 

   





RT @furukawa_staff: Special Live



2
RT @wdmnss:  
# # https://t.co
RT @fernhaeforY:   100   BMW 






#shindanmaker
https://t.co/ZhVMhNijiW
...

DM

RT @ayln69: nfazdaAyrm lmeTerketmek



#
RT @usagi_kitty_: GoTo







@KARMARALONSO Muchas gracias!!!
@smile_slow_life 
@Q97455    
@LoveOrPoor 
@yellowkitten_   *  
@n_aygod 

@O9XnISYteURdtUX 

@FonproyA LISA LALISA MANOBAN
 
I vote for #LISA from @BLACKPINK for #BousnidStars2020 #Bousnid #4thBousnidStars2020

@shionsiphon0530 
@ferruh270370 - Hele hele   DI KRESEL GLERE SATILMI  DARBECLERE ARKA DURANLARLA H M OLMAZ. BYLE BR GRNTNN YANINDA OLMAM..PARTLERLE MENFAAT LKS H  YAAMADIM..!
RT @LLinWood: TRUTH.

@sumokyokai 15
16
RT @AsreVishwakarma:              @yadavakhilesh @mulayamyadav @proframg
#

    !
www
@Devendara_            .  
RT @eh12__: -     &quot;    &quot;
@chukwudigamer Cry
  ;
@moemii 
RT @ffluffyb:  mv  https://t.co/UQ5PvuNqA9
&amp;&amp;RT
RTRT
@KanittaK5 
RT @OfficialMonstaX: 
# #
# BEAUTY+ 
12  !

@beautyplmania
#MONSTAX #MONSTA_X
#SHOWNU #beautyplmagazine https://t.co/R87
@richan__xx 
@x9r1WdRq4O9malO 
^ - ^
RT @NOS_PROJECT: 
3


@luvxiaos ITS CURSED GN
RT @Fushigur0Megumi: &quot;..&quot;

 .. https://t.co/evkvTJAt
RT @Nassr_Almai: @coach__nawaf 
    !!!!!



100



ADOROO ADOORO ADOROOO LA NUEVA CANCION DE MILEY CYRUS Y DUA LIPAA QUE REINASSSS  #PRISONER #DuaLipa #MileyDuaPrisioner #MileyCyrus https://t.co/jWwtyfGcfb
RT @DELTAGG058: \\1//

1

 Apexpro TKL


@DELTAGG058  
RT



mk


@Avokado62228 -
RT @yupe_1123: @choco6rabi ( '-'  )

RT @kanintr_:       5555555555555
@haluokun 
@soda_rafinsky  
RT @7AHaWClulx8cjzM:            
          
#_
RT @heicat_movie_jp: MTJJ
https://t.co/s
RT @amocafe_ikeb01: AMOCAFE30



RT @boonlert1512: &quot;&quot;
#GulfKanawut 
# https://t.co/XvCPioPNyE
RT @neuron1204: Zepp
@saaalsheikh 
@iili4p @Mllk_11    

RT @coscos_makeup: 
HD N03




@coscos_makeup RT30
RT @megido72: 

# #72 https://t.co/JS4LJmPRwm
RT @ma1ahot:          ...         ...             
15
3-5
2

Twitter
DM

           .. ..
   
        
       
       
       
     
      
      
       

#__
           

-   
-  
-   
-    

-    wow
Em11 
 
https://t.co/gfAGQfd8pL
-      
     https://t.co/MxCyFmaRVV




RT @bint_alshmrii:                      .
RT @DUCKYWORLD_KR:    RT 
   10 
-

11 20~11 26

   
 : 10
 : 11 26
-
RT @aitcsudip:   - !    ,          @BJ
RT @ssarawatlismm: @Tine__chicchic 
RT @hypnosismic_RA: BDDVD2

3
2021/6/13


4
2021/7/3
 

100
BDD
@atiab_alia1 ..
   
@Feel_free_     



 ....      ! ~~~
RT @SerieTV46: #BTS &quot;BE&quot; songs on MelOn hits at 20:00 KST:

#1 Dynamite (=)
#15 Life Goes On (+6)
#44 Fly To My Room (+10)
#50 Blue &amp; Grey
#_


 #DAY  #80 11/22 https://t.co/6HvM0OfZbn @manga_okoku
RT @vminggukx: SPRING DAY                  LIFE GOES ON

#LifeGoesOnWithBTS #BETODAY https://t.co/zSBS0uCQnA
RT @queblick: 11/7HUL OVER
4th SingleSAIENfeat./JUU (UZMK)

MV


Tomorrowbytogether   

@AhmedEl93905010  
Yieee
   400 
RT @Kehlani: im so iceeeee
RT @dj_kiko28: 
1 Shiny Kung-fu Revival
2 FIRE FIRE
3 snow storm
?

# # # # #
          ...
RT @ancle7chill: Going crazy  
Orange 
@Kalaage_27 !!
RT @GoncharenkoUa:    ,          ?   , ,   
RT @Moonlover_Miyu:  190     https://t.c
@TOGth_Dowon   
RT @zlem88369215: @Zeynepe37572607 @AYMBASKANLIGI @NumanKurtulmus nfazdaAyrm lmeTerketmek
RT @ministop_fan: RT






&amp;RT
1,000
@ucpzwDpHuIwso9d _(:3)_
@tausy72 ------
RT @bts_bighit: [#]    ..
   ..
   Live Goes On!
  

#USB
@BKF94 (^o^)
()
RT @BLINKVotingPage: DAILY REMINDER for Social50:

@BLACKPINK #BLACKPINK # #JISOO # #JENNIE # #ROS # #LISA #
@magda9666     

@DPY332820 Nice
RT @mthai:   . 

*** WARNING: skipped 561051281 bytes of output ***


@nctyarchive 90's Love MV

#RESONANCE_PT2
#NCT_RESONANCE
#90sLove @NCTsmtown

                                                                                      https://t.co/TISDfkDBIh
 https://t.co/8SNHgEd9FR @YouTube







 https://t.co/36M6iGI7Sb

#
RT @UncleCafe:         
RT @GZB7IVtxmjcW26b:     

              

   
RT @jxmoods:  
@qnnP1PaLYz35uEo 
RT @BaisleyKendra: 







RTDM
()
RT @lololo46925975:    
@halup_o ,,,,  .,,!!!   !!!!
RT @_satoreika: 
#
RT @norystch: MV     
@sunagimotter 
RT @FIFAQ8_:     50$ :
 @huda_fut  
*  5   https://t.co/8C3FgGipI5

11/24()12
# 
0120-022-767(10:0019:00/)
@0pUhczppMfMPsuI 
RT @NCTsmtown: #2020ENQUETE20

#WINWIN #JUNGWOO #LUCAS #NCT_MARK
#NCT #RESONANCE
#RESONANCE_Pt2
#NCT_RESONANCE https://t.co/fGq0tXQ4AQ
       
RT @reiou0106: 


@BobaKaTea_ Doneeepo



RT @shima_s2: N
@mood_sakura 
RT @bIazedmark: mark lee being                  but also
cute as hell                  what the hell https://t.co/Zb6sSqVokI
T






RT @bellechans:    
RT @CaramelCorn1971: 




 https://t.co/5WA

@Sakura_050218 LINE
@t_o_e_m_u 


1

RT @ern_konpeito: 


 https://t.co/lADfcMmaTd
RT @AlMosahf:         
RT @topazine:  slut-shaming, body-shaming  
@sari_111 @Hamedallah_9    
@1009_1126 
 https://t.co/rgltA1e6VF #

@JINF4IRY  ALE TO JA NIE WYSLALAM ZALOSNE
RT @25hmm__:       ...
RT @rysyrys: 



23


()

#23 #23 # 
# #
RT @wanghabits: @gyeommine BREATH OUT NOW
#GOT7_BreathRelease
#_Breath_6 #GOT7_Breath @GOT7official
@jjaejaepeach 90's Love MV

#RESONANCE_PT2
#NCT_RESONANCE
#90sLove @NCTsmtown
RT @dztp: EDEDT
RT @thelittle_CB:  : 
% :50%
 : 
 : 168
% : 40%
emoji : 
@Kristin99350413  #JusticeForJohnnyDepp #ThankYouDior
sis same #GOT7_BreathRelease
#GOT7_Breath #GOT7# @GOT7Official
 https://t.co/cYkgXPpbVM
wwwwwwww
 #
RT @imyounumber1fan:  

RT @0949_LQS:    

1123 18:30 1830

  @0949_LQS 



15002 ( a ) 1
 
 (  )
Follow @0949_LQS RT
RT @topazine:  slut-shaming, body-shaming  
RT @beingarun28:             

      .. 

#BanNetflix
1124 # 3 #500  #
RT @yamanow: 22522### https://t.co
RT @mankai_company: HAPPY BIRTHDAY


@xponochka         
@t_Ace_ZERO 
RT @Casiosusu: 
@Nozu_Rato ~()
@S_standflower 
# 
                                       
RT @heymooooooom:   // #GOT7_Breath https://t.co/zCLNc5kHgc
,      ,          ,    ,   ,   jet lag.
,         
@AJELNEWS24     

  

  

 

  

  

       

    


RT @seoulcuteclub:  Watsons  Biore  Aqua rich  Blooming Blossom 347  
RT @Littleminn: SM  Departure ver.  
                       https://t.co/dZPKo0k3aH
RLP           Rlp      RTO               rlp            @hanumanbeniwal  @UmmedaRamBaytu  @NarayanBeniwal7 https://t.co/KvmZauT4fw
A BEYBLADE FUCKS UP A DISH AND KAI TRYIN TO ACT LIKE THAT DONT SCARE HIM LIKE BOIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII https://t.co/y5R7mk3cr9

                  
---



 #
YDAYDA YDA[ https://t.co/OR1S0iiVse]
RT @yukiyuki0_0r6s: 100

2Switch lite1

PayPay



@LemoGoBo1118 
@ore_1919 
3RT

3
100
@chry142w 

MY EVERYTHING IS SO AAAAAAAAAAA IM GONNA CRY RIGHT NOW
RT @star_isshhh:  '   -   '   Presents &quot; FANTOPIA &quot;  Impact Arena &amp; Challenge
22
RT @Ammume02:  
#GOT7_Breath https://t.co/zxi5Li9kzJ
RT @JOOXTH: #GOT7_BreathRelease 
 

 'Breath'  Breath of Love : Last Piece  #J
RT @alwaysmendex: GO WATCH THE DOCUMENTARY #InWonderWatchParty
@TuTi8HiThuZi WAOSOULEATER
RT @BTS_twt: 


RT
RT @sfkkfs_:  
RT @taeyongelvisx:    
@haruhidemasaki 
(`)(`)(`)
()
@shoki_jazz_tp 
@re_1230_ 
@FragileHeart96              
RT @Mohamed34461331:     
              
  
RT @na2key_: 



 https://t.co/6PrKC0pO8a
@smtakemymoney      .. 
@143Viceyy HAHAHHAHA KYOT!!! NAKAUWI KA NA?? INGAT KA HAA?? WUV U
RT @LOH_information:    

       .

      .

   12 1() 00:00 ( )  h
@kaede7728 wwwww
RT @sourcemusicFS: #

 # #GFRIEND ONLINE CONCERT
[ GFRIEND C:ON ]  (2)   

 &lt;G C:ON  &gt;  
   



RT @monamourjd: @BUNNYMYE0N @weareoneEXO omg

WE ARE ONE

#AMAs  #AMAsTNT EXO #EXOL 
@weareoneEXO
Diego Alves, Rafinha, Rodrigo Caio, Pablo Mari, Filipe Luis, Willian Aro, Gerson, Everton Ribeiro, Arrascaeta, Bruno Henrique, Gabriel Barbosa.
Jorge Jesus. 

NINGUM ME CONTOU, EU VI.

#1AnoDaGloriaEterna
()
RT @immalilbbb: rt; # clip-0:34sec: #imgfor # #imgfor # #
//
 



#
&gt;RT


@icxrea ,    
RT @I3amBam1A: !! #GOT7_BreathRelease #GOT7_Breath  #GOT7 # @GOT7Official
RT @8AIcTgxKbubUGD2: 

@j_myHOPE218 

w

RT @NCTsmtown: NCT  The 2nd Album RESONANCE Pt.2        . 

https://t.co/xwRFkCCxbA
RT @nn_ss99a: 500





DM() https://t.co/VbYjvIXvBB
@M98cocco w
@mae_saeng2      ..
@Ging10622755 @LukasZackeff .. . ..     ?
RT @retokani: 
IFBLOOD

IF    https://t.co/ztJg0hGI
RT @AA55388: *    *
*      .&quot;*

     https://t.co/0MubGIxAUA
RT
BRAZIL LOVES YOU
RT @yeonsek498: https://t.co/SAWnTQNtSo

   ,   &gt;     ,     .      ''   
RT @Shin_Kurose: 

 https://t.co
RT @AAairty:  555555
@lirx111  
@DCulee  
 

I vote for #Taehyung #BTSV from #BTS @BTS_twt for #BousnidStars2020 #Bousnid #4thBousnidStars2020
      !!!!!! #GOT7 # @GOT7Official #IGOT7 # #GOT7_BreathofLove_LastPiece #GOT7_Breath #GOT7_LastPiece
#JacksongWang https://t.co/z3mJAqb3fq
JOIN NOW DELAY IS DANGEROUS
CONTINUE DOUBTING WE CONTINUE WINNING
 https://t.co/ATfhHSLWnC

#
      .
RT @mymtuan__:     
RT @Riaqx_: #

30002

RT

@Riaqx_  @order_a_ll  @RuruaZeros  @meruxg 
RT
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// ## Go to SparkUI and see if a streaming job is already running. If so you need to terminate it before starting a new streaming job. Only one streaming job can be run on the DB CE.
// #  let's stop the streaming job next.
ssc.stop(stopSparkContext = false) 
StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) } 
</code></pre>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="visualizations-of-status-updates-in-twitterverse"><a class="header" href="#visualizations-of-status-updates-in-twitterverse">Visualizations of Status Updates in Twitterverse</a></h1>
<ul>
<li>https://github.com/olofbjorck/twitterVisualizations
<ul>
<li>The notebook <code>029_Viz_1_GraphNetworkTimeline</code> has three visualizations of Tweet Transmission Trees.</li>
</ul>
</li>
<li>The notebook <code>029_Viz_2_...</code> has some possiblilities by leveraging work from a hackathon in 2020.</li>
</ul>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="interactive-exploration-in-twitter-as-graph-network-and-timeline"><a class="header" href="#interactive-exploration-in-twitter-as-graph-network-and-timeline">Interactive Exploration in Twitter as Graph, Network and Timeline</a></h1>
<p>by Olof Bjrck, Joakim Johansson, Rania Sahioun, Raazesh Sainudiin and Ivan Sadikov</p>
<p>This is part of <em>Project MEP: Meme Evolution Programme</em> and supported by databricks, AWS and a Swedish VR grant.</p>
<pre><code>Copyright 2016-2020 Olof Bjrck, Joakim Johansson, Rania Sahioun, Ivan Sadikov and Raazesh Sainudiin

Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
</code></pre>
</div>
<div class="cell markdown">
<h2 id="run-notebooks"><a class="header" href="#run-notebooks">Run notebooks</a></h2>
<p>These notebooks have needed variables and functions.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-run">./025_b_TTTDFfunctions
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell markdown">
<h2 id="read-twitter-data-as-tttdf"><a class="header" href="#read-twitter-data-as-tttdf">Read twitter data as TTTDF</a></h2>
<p>We cache and count the data as a TTTDF or Twwet-Transmission-Tree dataFrame.</p>
<p>Make sure the count is not too big as the driver may crash. One can do a time-varying version that truncates the visualized data to an appropriate finite number.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>USAGE: val df = tweetsDF2TTTDF(tweetsJsonStringDF2TweetsDF(fromParquetFile2DF(&quot;parquetFileName&quot;)))
                  val df = tweetsDF2TTTDF(tweetsIDLong_JsonStringPairDF2TweetsDF(fromParquetFile2DF(&quot;parquetFileName&quot;)))
                  
import org.apache.spark.sql.types.{StructType, StructField, StringType}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.spark.sql.ColumnName
import org.apache.spark.sql.DataFrame
fromParquetFile2DF: (InputDFAsParquetFilePatternString: String)org.apache.spark.sql.DataFrame
tweetsJsonStringDF2TweetsDF: (tweetsAsJsonStringInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
tweetsIDLong_JsonStringPairDF2TweetsDF: (tweetsAsIDLong_JsonStringInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
tweetsDF2TTTDF: (tweetsInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
tweetsDF2TTTDFWithURLsAndHashtags: (tweetsInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>tweetsDF2TTTDFLightWeight: (tweetsInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
tweetsDF2TTTDFWithURLsAndHashtagsLightWeight: (tweetsInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Get the TTT  
val rawTweetsDir = &quot;PATH_To_TWEETS_From_StreamingJOB&quot;
val TTTDF = tweetsDF2TTTDFWithURLsAndHashtagsLightWeight(tweetsJsonStringDF2TweetsDF(fromParquetFile2DF(rawTweetsDir))).cache()
TTTDF.count // Check how much we got (need to check size and auto-truncate to not crash driver program... all real for now)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>TTTDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CurrentTweetDate: timestamp, CurrentTwID: bigint ... 35 more fields]
res2: Long = 14451
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">TTTDF.rdd.getNumPartitions
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res14: Int = 8
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Next we first write the TTTDF to a parquet file and then read it into the visualizer. This will usually be done via a fast database.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">TTTDF.write.mode(&quot;overwrite&quot;)
     .parquet(&quot;PATH_TO_PARQUET_TTTDF.parquet&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val TTTDFpq = spark.read.parquet(&quot;PATH_TO_PARQUET_TTTDF.parquet&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>TTTDFpq: org.apache.spark.sql.DataFrame = [CurrentTweetDate: timestamp, CurrentTwID: bigint ... 35 more fields]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">TTTDFpq.rdd.getNumPartitions
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res16: Int = 2
</code></pre>
</div>
</div>
<div class="cell markdown">
<h2 id="visualize-a-twitter-graph"><a class="header" href="#visualize-a-twitter-graph">Visualize a Twitter Graph</a></h2>
<p>The graph shows the twitter users in the collection organized by their popularity in terms of various status updates.</p>
<p>See the function <code>visualizeGraph</code> for details. More sophisticated D3 interactive graphs are possible with the TTTDF, especially when filtered by time intervals or interactions of interest.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-run">./029_Viz_x_VizGraphFunction
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">visualizeGraph(TTTDFpq, 0, 700, 700)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>*** WARNING: ***

THERE IS NO SIZE CHECKING! Driver might crash as DataFrame.collect is used.
Also, the display can only handle so and so many items at once. Too large
dataset and the visualization might be very laggy or crash.

***  USAGE:  ***

visualizeGraph(TTTDF)

****************

import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._
visualizeGraph: (TTTDF: org.apache.spark.sql.DataFrame, tupleWeightCutOff: Int, width: Int, height: Int)Unit
</code></pre>
</div>
</div>
<div class="cell markdown">
<h2 id="visualizing-retweet-network"><a class="header" href="#visualizing-retweet-network">Visualizing Retweet Network</a></h2>
<p>Let us interactively explore the retweet network to identify tweets in terms of the number of retweets versus the number of its unique retweeeters.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-run">./029_Viz_x_VizNetworkFunction
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>*** WARNING: ***

THERE IS NO SIZE CHECKING! Driver might crash as DataFrame.collect is used.
Also, the display can only handle so and so many items at once. Too large
dataset and the visualization might be very laggy or crash.

***  USAGE:  ***

visualizeNetwork(TTTDF)

****************

import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._
visualizeNetwork: (TTTDF: org.apache.spark.sql.DataFrame, width: Int, height: Int)Unit
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">visualizeNetwork(TTTDFpq)
</code></pre>
</div>
<div class="cell markdown">
<p><img src="https://github.com/lamastex/scalable-data-science/raw/master/images/ScaDaMaLe/000_3-sds-3-x-st/visualizenetwork029_Viz_1.png" alt="" /></p>
</div>
<div class="cell markdown">
<h2 id="interactively-visualize-a-twitter-timeline"><a class="header" href="#interactively-visualize-a-twitter-timeline">Interactively Visualize a twitter Timeline</a></h2>
<p>Search through Tweets over a timeline.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-run">./029_Viz_x_VizTimelineFunction
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell code" execution_count="1" scrolled="false">
<div class="output execute_result plain_result" execution_count="1">
<pre><code>*** WARNING: ***

BUG: The first Tweet in the dataset doesn't display for some reason.

THERE IS NO SIZE CHECKING! Driver might crash as DataFrame.collect is used.
Also, the display can only handle so and so many items at once. Too large
dataset and the visualization might be very laggy or crash.

***  USAGE:  ***

visualizeTimeline(TTTDF)
 
****************

import org.apache.spark.sql.DataFrame
visualizeTimeline: (TTTDF: org.apache.spark.sql.DataFrame, width: Int, height: Int)Unit
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">visualizeTimeline(TTTDFpq) 
</code></pre>
</div>
<div class="cell markdown">
<p><img src="https://github.com/lamastex/scalable-data-science/raw/master/images/ScaDaMaLe/000_3-sds-3-x-st/visualizeTimeline029_Viz_1.png" alt="" /></p>
</div>
<div class="cell markdown">
<h2 id="what-next"><a class="header" href="#what-next">What next?</a></h2>
<p>Ideally the TTTDF and its precursor raw tweets are in silver and bronze delta.io tables and one can then use an interactive dashboard such as:</p>
<ul>
<li><a href="https://superset.apache.org/">Apache Superset Overview</a></li>
<li><a href="https://superset.apache.org/gallery">Apache Superset Gallery</a></li>
</ul>
<blockquote>
<p>Superset is fast, lightweight, intuitive, and loaded with options that make it easy for users of all skill sets to explore and visualize their data, from simple line charts to highly detailed geospatial charts.</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://superset.apache.org/gallery"
 width="95%" height="600">
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="visualizations-of-status-updates-in-twitterverse-as-emotional-dimensions"><a class="header" href="#visualizations-of-status-updates-in-twitterverse-as-emotional-dimensions">Visualizations of Status Updates in Twitterverse as Emotional Dimensions</a></h1>
<p>The following codes can be leveraged to obtain emotional dimensions on <code>display_HTML</code> in this notebook and later for dashboards, say via Apache Superset or others.</p>
<ul>
<li>https://github.com/lamastex/CoSENSE
<ul>
<li>Video Presented to hackthecrisis.se</li>
<li><a href="https://www.youtube.com/watch?v=OTQ0j6UcMb8&amp;rel=0&amp;autoplay=1&amp;modestbranding=1&amp;start=1"><img src="http://img.youtube.com/vi/OTQ0j6UcMb8/0.jpg" alt="CoSENSE Presentation to hackthecrisis.se" /></a></li>
</ul>
</li>
</ul>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="function-for-visualizing-a-twitter-graph"><a class="header" href="#function-for-visualizing-a-twitter-graph">Function for visualizing a Twitter graph</a></h1>
<p>by Olof Bjrck, Joakim Johansson, Rania Sahioun, Raazesh Sainudiin and Ivan Sadikov</p>
<p>This is part of <em>Project MEP: Meme Evolution Programme</em> and supported by databricks, AWS and a Swedish VR grant.</p>
<pre><code>Copyright 2016-2020 Olof Bjrck, Joakim Johansson, Rania Sahioun, Ivan Sadikov and Raazesh Sainudiin

Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._

/** Displays an interactive visualization of a Twitter graph.
 *
 *  This function takes a DataFrame from TTTDFfunctions function tweetsDF2TTTDF()
 *  and displays it in an interactive visualization. The DataFrame content should
 *  be a collection of Twitter users. 
 *  
 *  Visualization description:
 *
 *  This is a graph visualization of a Twitter graph. Each circle represents a 
 *  Twitter user (that is, a unique Twitter account). The radius of the user circle 
 *  represents how many Retweets the user has within this specific Twitter network 
 *  from small radius (fewest Retweets) to large radius (most Retweets). If you 
 *  hover a user circle, the user screen name (as specified in the data) will appear 
 *  by the user circle. The line thickness connecting user circles represents how 
 *  many Retweets the user circle tuple tuple has between each other in total.
 *  Retweet direction is not accounted for. Connecting lines only appear if the user 
 *  circle tuple has at least tupleWeightCutOff Retweets.
 *  
 *  @param TTTDF A DataFrame from TTTDFfunctions function tweetsDF2TTTDF() containing
 *               the data to be displayed.
 *  @param tupleWeightCutOff The display width in pixels.
 *  @param width The display width in pixels.
 *  @param height The display height in pixels.
 */
def visualizeGraph(TTTDF: DataFrame, tupleWeightCutOff: Int = 15, width: Int = 900, height: Int = 400): Unit = {

  // Get user info from the &quot;primary&quot; users in our network. They are the users we've specified.
  val usersInfo = TTTDF
                  .select(&quot;CPostUserID&quot;, &quot;CPostUserSN&quot;, &quot;followersCount&quot;)
                  .groupBy(&quot;CPostUserID&quot;, &quot;CPostUserSN&quot;)
                  .agg(max(&quot;followersCount&quot;).as(&quot;followersCount&quot;))
                  .toDF(Seq(&quot;id&quot;, &quot;ScreenName&quot;, &quot;followersCount&quot;): _*)
                  .select(&quot;id&quot;, &quot;ScreenName&quot;)
  
  // Get all retweets and their weights (weights == 1 as we haven't done anything yet)
  val RTNetworkDF = TTTDF
                    .filter($&quot;TweetType&quot;===&quot;ReTweet&quot;) // TweetType PARAM
                    .select(&quot;OPostUserSNinRT&quot;, &quot;CPostUserSN&quot;, &quot;OPostUserIdinRT&quot;,&quot;CPostUserId&quot;,&quot;Weight&quot;)
  
  // Get all (directed) retweet tuples // PARAM - case by case depending on the TweetType PARAM since SN RT QT etc are Akin's TTTDF SQL names
  val weightedRTNetworkDF = RTNetworkDF
                            .groupBy(&quot;OPostUserSNinRT&quot;, &quot;CPostUserSN&quot;, &quot;OPostUserIdinRT&quot;,&quot;CPostUserId&quot;)
                            .agg(sum(&quot;Weight&quot;).as(&quot;Weight&quot;))
                            .orderBy($&quot;Weight&quot;.desc)
                            .toDF(&quot;source&quot;, &quot;target&quot;, &quot;sourceId&quot;, &quot;targetId&quot;, &quot;weight&quot;)
  
  // Get the out degree of each user. That is, get only the number of times a user is retweeted. We're now losing the tuples.
  val outDegreeOfOPostUserIdinRTNetwork = weightedRTNetworkDF
                                          .select(&quot;sourceId&quot;,&quot;weight&quot;)
                                          .groupBy(&quot;sourceId&quot;)
                                          .agg(sum(&quot;weight&quot;).as(&quot;weight&quot;))
                                          .orderBy($&quot;weight&quot;.desc)
                                          .toDF(Seq(&quot;id&quot;, &quot;weight&quot;): _*)

  // Get the nodes
  val nodes = usersInfo
              .join(outDegreeOfOPostUserIdinRTNetwork, Seq(&quot;id&quot;)).withColumn(&quot;group&quot;, lit(1))
              .toDF(Seq(&quot;idNr&quot;, &quot;id&quot;, &quot;weight&quot;, &quot;group&quot;): _*)
              .where(&quot;id is not null&quot;)
              .toDF(Seq(&quot;idNr&quot;, &quot;id&quot;, &quot;weight&quot;, &quot;group&quot;): _*)

  // Get links
  val linksSource = nodes.select(&quot;idNr&quot;)
                    .join(weightedRTNetworkDF, $&quot;idNr&quot; === $&quot;sourceId&quot;, &quot;left_outer&quot;)
                    .select(&quot;source&quot;, &quot;target&quot;, &quot;sourceId&quot;, &quot;targetId&quot;, &quot;weight&quot;)
  val links = nodes.select(&quot;idNr&quot;)
              .join(linksSource, $&quot;idNr&quot; === $&quot;targetId&quot;, &quot;left_outer&quot;)
              .select(&quot;source&quot;, &quot;target&quot;, &quot;weight&quot;)
              .where(&quot;source is not null&quot;)
              .where($&quot;weight&quot; &gt;= tupleWeightCutOff)
              .toDF(Seq(&quot;source&quot;, &quot;target&quot;, &quot;weight&quot;): _*)
 
  // Get JSON data
  val nodesData = nodes.toJSON.collect;
  val linksData = links.toJSON.collect;

  // CSS code
  val visualizeGraphCSS: String = 
  s&quot;&quot;&quot;
  /*
  General styling
  */

  body {
      font-family: Sans-Serif;
      width: 100%;
      height: 100%;
      margin: 0;
      margin-bottom: 100px;
      background-color: #FFF;
      overflow: scroll;
  }

  /*
  Page content
  */

  div.start {
      text-align: center;
  }

  ul.start {
      margin-top: 0;
      display: inline-block;
      text-align: left;
  }

  div.text {
      text-align: left;
      margin-left: 19.1%;
      margin-right: 19.1%;
  }

  /*
  For visualizations
  */

  .visualization {
      display: block;
      position: relative;
      align: center;
  }

  .hidden {
      visibility: hidden;
  }

  .visible {
      visibility: visible;
  }

  .zoom {
      cursor: move;
      fill: none;
      pointer-events: fill;
  }

  #graphVisualizationDiv {
      width: auto;
      height: auto;
      position: relative;
      align: center;
  }

  .links line {
      stroke: #999;
      stroke-opacity: 1;
  }

  .nodes circle {
      fill: black;
  }

  circle.clicked {
      fill: #1da1f2;
  }

  #tweet {
      z-index: 100;
      position: absolute;
      left: 80%;
      height: auto;
  }
  &quot;&quot;&quot;
  
  // JavaScript code
  val visualizeGraphJS: String = 
  s&quot;&quot;&quot;
  /*******************************************************************************

  This visualisation is a Twitter graph.

  *******************************************************************************/

  var circleEnlargeConstant = 2;
  var circleClickedStrokeWidth = 5;
  var maxRadius = 10;
  
  // Specify display sizes.
  var width = ${width}, 
      height = ${height},
      margin = {
                top: 0.1 * height,
                right: 0 * width, 
                bottom: 0.1 * height,
                left: 0 * width
            };
            
  width = width - margin.left - margin.right;
  height = height - margin.top - margin.bottom;

  // Get div
  var div = d3.select(&quot;#graphVisualizationDiv&quot;);

  // Create svg
  var svg = div.append(&quot;svg&quot;)
      .attr(&quot;class&quot;, &quot;visualization&quot;)
      .attr(&quot;width&quot;, width + margin.left + margin.right)
      .attr(&quot;height&quot;, height + margin.top + margin.bottom)
      .append(&quot;g&quot;)
          .attr(&quot;transform&quot;,
              &quot;translate(&quot; + margin.left + &quot;,&quot; + margin.top + &quot;)&quot;);

  // Create zoom
  var zoom = d3.zoom()
      .on(&quot;zoom&quot;, zoomed);

  // Create zoomable area
  var zoomView = svg.append(&quot;rect&quot;)
      .attr(&quot;class&quot;, &quot;zoom&quot;)
      .attr(&quot;width&quot;, width)
      .attr(&quot;height&quot;, height)
      .on(&quot;click&quot;, clickView)
      .call(zoom)

  // Create simulation
  var simulation = d3.forceSimulation()
      .force(&quot;charge&quot;, d3.forceManyBody().strength(-10))
      .force(&quot;center&quot;, d3.forceCenter(width / 2, height / 2))

  // Create loading text
  var loading = svg.append(&quot;text&quot;)
      .attr(&quot;y&quot;, height / 2)
      .attr(&quot;x&quot;, width / 2)
      .attr(&quot;text-anchor&quot;, &quot;middle&quot;)
      .text(&quot;Loading graph... Takes a couple of seconds&quot;);
 
  var nodes = ${nodesData.mkString(&quot;[&quot;, &quot;,\n&quot;, &quot;]&quot;)};
  var links = ${linksData.mkString(&quot;[&quot;, &quot;,\n&quot;, &quot;]&quot;)};

  // Create links
  var link = svg.append(&quot;g&quot;)
      .attr(&quot;class&quot;, &quot;links&quot;)
      .selectAll(&quot;line&quot;)
      .data(links)
      .enter().append(&quot;line&quot;)
          .attr(&quot;stroke-width&quot;, function(d) { return Math.sqrt(d.weight / 1000); });

   // Create nodes
   var node = svg.append(&quot;g&quot;)
       .attr(&quot;class&quot;, &quot;nodes&quot;)
       .selectAll(&quot;circle&quot;)
       .data(nodes)
       .enter().append(&quot;circle&quot;)
           //.attr(&quot;fill&quot;, function(d) { return color(d.group); })
           .attr(&quot;r&quot;, function(d) { return Math.sqrt(d.weight / 100) + 2 })
           .on(&quot;mouseover&quot;, mouseoverCircle)
           .on(&quot;mouseout&quot;, mouseoutCircle)
           .on(&quot;click&quot;, clickCircle)
           .call(d3.drag()
               .on(&quot;start&quot;, dragstarted)
               .on(&quot;drag&quot;, dragged)
               .on(&quot;end&quot;, dragended));

    // Add title as child to circle
    node.append(&quot;title&quot;)
        .text(function(d) { return d.id; });

    // Link nodes and links to the simulation
    simulation
        .nodes(nodes)
        .on(&quot;tick&quot;, ticked)
        .force('link', d3.forceLink(links).id(function(d) { return d.id; }));

    // Updates for each simulation tick
    function ticked() {
        link
            .attr(&quot;x1&quot;, function(d) { return d.source.x; })
            .attr(&quot;y1&quot;, function(d) { return d.source.y; })
            .attr(&quot;x2&quot;, function(d) { return d.target.x; })
            .attr(&quot;y2&quot;, function(d) { return d.target.y; });

         node
            .attr(&quot;cx&quot;, function(d) { return d.x; })
            .attr(&quot;cy&quot;, function(d) { return d.y; })
     }

     // Compute several steps before rendering
     loading.remove(); // Remove loading text
     for (var i = 0, n = 150; i &lt; n; ++i) {
         simulation.tick();
     }

  /**
   * Handle mouse hover on circle. Enlarge circle.
   */
  function mouseoverCircle() {

      // Get circle
      var circle = d3.select(this);

      // Display activated circle
      circle.attr(&quot;r&quot;, circle.attr(&quot;r&quot;) * circleEnlargeConstant);
  }

  /**
   * Handle mouse out on circle. Resize circle.
   */
  function mouseoutCircle() {

      // Get circle
      var circle = d3.select(this);

      // Display idle circle
      circle.attr(&quot;r&quot;, circle.attr(&quot;r&quot;) / circleEnlargeConstant);

  }

  /**
   * Handle circle drag start.
   */
  function dragstarted(d) {
      if (!d3.event.active) simulation.alphaTarget(0.3).restart();
      d.fx = d.x;
      d.fy = d.y;
  }

  /**
   * Handle circle drag.
   */
  function dragged(d) {
      d.fx = d3.event.x;
      d.fy = d3.event.y;
  }

  /**
   * Handle circle drag end.
   */
  function dragended(d) {
      if (!d3.event.active) simulation.alphaTarget(0);
      d.fx = null;
      d.fy = null;
  }

  /**
   * Handle zoom. Zoom both x-axis and y-axis.
   */
  function zoomed() {
      d3.selectAll(&quot;.nodes&quot;).attr(&quot;transform&quot;, d3.event.transform)
      d3.selectAll(&quot;.links&quot;).attr(&quot;transform&quot;, d3.event.transform)
  }

  /**
   * Handle click on zoomable area. That is, handle click outside a node which
   * is considered a deselecting click =&gt; deselect previously clicked node
   * and remove displayed tweets.
   */
  function clickView() {

      // Remove clicked status on clicked nodes
      d3.selectAll(&quot;.clicked&quot;)
          .attr(&quot;stroke-width&quot;, &quot;0&quot;)
          .classed(&quot;clicked&quot;, false)

      // Remove timeline
      document.getElementById(&quot;tweet&quot;).innerHTML = &quot;&quot;
  }

  /**
   * Handle click on a tweet circle. Display the clicked tweet and let the tweet
   * appear selected by adding a stroke to it.
   */
  function clickCircle(d) {

      // Remove results from old click
      clickView();

      // Add stroke width and set clicked class
      d3.select(this)
          .attr(&quot;stroke-width&quot;, circleClickedStrokeWidth)
          .classed(&quot;clicked&quot;, true);

      // Display tweet
      twttr.widgets.createTimeline(
          {
              sourceType: &quot;profile&quot;,
              userId: d.idNr
          },
          document.getElementById(&quot;tweet&quot;), // Tweet div
          {
              height: height
          }
      )
  }
  &quot;&quot;&quot;

  // HTML code
  displayHTML(s&quot;&quot;&quot;
  &lt;!DOCTYPE html&gt;
  &lt;html lang=&quot;en&quot;&gt;

      &lt;head&gt;

          &lt;meta charset=&quot;utf-8&quot;&gt;
          &lt;meta name=&quot;author&quot; content=&quot;Olof Bjrck&quot;&gt;
          &lt;title&gt;Graph&lt;/title&gt;

          &lt;style&gt;
            ${visualizeGraphCSS}
          &lt;/style&gt;

      &lt;/head&gt;

      &lt;body&gt;

          &lt;div id=&quot;graphVisualizationDiv&quot; class=&quot;visualization&quot; align=&quot;center&quot;&gt;
              &lt;div id=&quot;tweet&quot;&gt;&lt;/div&gt;
          &lt;/div&gt;

          &lt;script sync src=&quot;https://platform.twitter.com/widgets.js&quot;&gt;&lt;/script&gt;
          &lt;script src=&quot;https://d3js.org/d3.v4.min.js&quot;&gt;&lt;/script&gt;
          &lt;script&gt;
            ${visualizeGraphJS}
          &lt;/script&gt;

      &lt;/body&gt;
  &lt;/html&gt;
&quot;&quot;&quot;)
}

// Info
println(&quot;&quot;&quot;
*** WARNING: ***

THERE IS NO SIZE CHECKING! Driver might crash as DataFrame.collect is used.
Also, the display can only handle so and so many items at once. Too large
dataset and the visualization might be very laggy or crash.

***  USAGE:  ***

visualizeGraph(TTTDF)

****************
&quot;&quot;&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>*** WARNING: ***

THERE IS NO SIZE CHECKING! Driver might crash as DataFrame.collect is used.
Also, the display can only handle so and so many items at once. Too large
dataset and the visualization might be very laggy or crash.

***  USAGE:  ***

visualizeGraph(TTTDF)

****************

import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._
visualizeGraph: (TTTDF: org.apache.spark.sql.DataFrame, tupleWeightCutOff: Int, width: Int, height: Int)Unit
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="function-for-visualizing-a-twitter-network"><a class="header" href="#function-for-visualizing-a-twitter-network">Function for visualizing a Twitter network</a></h1>
<p>by Olof Bjrck, Joakim Johansson, Rania Sahioun, Raazesh Sainudiin and Ivan Sadikov</p>
<p>This is part of <em>Project MEP: Meme Evolution Programme</em> and supported by databricks, AWS and a Swedish VR grant.</p>
<pre><code>Copyright 2016-2020 Olof Bjrck, Joakim Johansson, Rania Sahioun, Ivan Sadikov and Raazesh Sainudiin

Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._

/** Displays an interactive visualization of a Twitter network.
 *
 *  This function takes a DataFrame from TTTDFfunctions function tweetsDF2TTTDF()
 *  and displays it in an interactive visualization. The DataFrame content should
 *  be a collection of Twitter users.
 *  
 *  Visualization description:
 *
 *  This is a visualization of a Twitter network. Each circle represents a Twitter 
 *  user (that is, a unique Twitter account). The color of the user circle represents 
 *  how many followers the user has from blue (fewest) to red (most). The radius of 
 *  the user circle also represents how many followers the user has from small radius 
 *  (fewest) to large radius (most) on a log scale. The x-axis shows the number of 
 *  Retweets of a user within the Twitter network (that is, the number of times the 
 *  user was Retweeted by users in this specific Twitter network). The y-axis shows 
 *  the number of unique Retweeters within the Twitter network (that is, the number 
 *  of unique users in this specific Twitter network that Retweeted a user). If you 
 *  hover over a user circle, the user screen name (as specified in the data) will 
 *  appear by the user circle and the user circle you're hovering will become enlarged.
 *  
 *  @param TTTDF A DataFrame from TTTDFfunctions function tweetsDF2TTTDF() containing
 *               the data to be displayed.
 *  @param width The display width in pixels.
 *  @param height The display height in pixels.
 */
def visualizeNetwork(TTTDF: DataFrame, width: Int = 900, height: Int = 400): Unit = {
  
  // Get all retweets and their weights (weights == 1 as we haven't done anything yet)
  val RTNetworkDF = TTTDF.filter($&quot;TweetType&quot;===&quot;ReTweet&quot;).select(&quot;OPostUserIdinRT&quot;,&quot;CPostUserId&quot;,&quot;Weight&quot;)
  
  // Get all (directed) retweet tuples
  val weightedRTNetworkDF = RTNetworkDF
                            .groupBy(&quot;OPostUserIdinRT&quot;,&quot;CPostUserId&quot;)
                            .agg(sum(&quot;Weight&quot;).as(&quot;Weight&quot;))
                            .orderBy($&quot;Weight&quot;.desc)
  
  // Get the out degree of each user. That is, get only the number of times a user is retweeted. We're now losing the tuples.
  val outDegreeOfOPostUserIdinRTNetwork = weightedRTNetworkDF
                                          .select(&quot;OPostUserIdinRT&quot;,&quot;Weight&quot;)
                                          .groupBy(&quot;OPostUserIdinRT&quot;)
                                          .agg(sum(&quot;Weight&quot;).as(&quot;Weight&quot;))
                                          .orderBy($&quot;Weight&quot;.desc)
  
  // Get number of unique retweeters (out degree neighborhood):
  // Use weightedRTNetwork, drop weight, add new weight of 1. 
  // We got all the retweet tuples with a weight of 1, then group by UserID to count number of unique retweeters
  val outNgbhdOfOPostUserIdinRTNetwork = weightedRTNetworkDF
                                          .drop(&quot;Weight&quot;)
                                          .withColumn(&quot;Weight&quot;,lit(1L))
                                          .groupBy(&quot;OPostUserIdinRT&quot;)
                                          .agg(sum(&quot;Weight&quot;).as(&quot;Weight&quot;))
                                          .orderBy($&quot;Weight&quot;.desc) 
  
  // Combine retweets and retweeters count and rename columns
  val tweetsAndRetweets = outDegreeOfOPostUserIdinRTNetwork
                          .toDF(Seq(&quot;UserID&quot;, &quot;NrOfRetweets&quot;): _*)
                          .join(outNgbhdOfOPostUserIdinRTNetwork
                                .toDF(Seq(&quot;UserID&quot;, &quot;NrOfRetweeters&quot;): _*), &quot;UserID&quot;
                               ).orderBy($&quot;NrOfRetweets&quot;.desc)
  
  // Get user info from the &quot;primary&quot; users in our network. They are the users we've specified.
  val usersInfo = TTTDF.filter($&quot;TweetType&quot;===&quot;ReTweet&quot;)
                  .select(&quot;CPostUserID&quot;, &quot;CPostUserSN&quot;, &quot;followersCount&quot;)
                  .groupBy(&quot;CPostUserID&quot;, &quot;CPostUserSN&quot;)
                  .agg(max(&quot;followersCount&quot;).as(&quot;followersCount&quot;))
                  .toDF(Seq(&quot;UserID&quot;, &quot;ScreenName&quot;, &quot;followersCount&quot;): _*)
  
  // Get the final data to visualize
  val data = usersInfo.join(tweetsAndRetweets, Seq(&quot;UserID&quot;)).distinct.toJSON.collect
  
  // CSS code
  val visualizeNetworkCSS: String = 
  s&quot;&quot;&quot;
  /*
  General styling
  */

  body {
      font-family: Sans-Serif;
      width: 100%;
      height: 100%;
      margin: 0;
      margin-bottom: 100px;
      background-color: #FFF;
      overflow: scroll;
  }

  /*
  Page content
  */

  div.start {
      text-align: center;
  }

  ul.start {
      margin-top: 0;
      display: inline-block;
      text-align: left;
  }

  div.text {
      text-align: left;
      margin-left: 19.1%;
      margin-right: 19.1%;
  }

  /*
  For visualizations
  */

  .visualization {
      display: block;
      position: relative;
      align: center;
  }

  .hidden {
      visibility: hidden;
  }

  .visible {
      visibility: visible;
  }

  .zoom {
      cursor: move;
      fill: none;
      pointer-events: fill;
  }
  
  .title {
      fill: black;
      font-family: sans-serif;
      text-anchor: middle;
      text-align: center;
      font-size: 1.5em;
  }

  #tweet {
      position: absolute;
      z-index: 100;
      left: 80%;
      height: auto;
  }
  &quot;&quot;&quot;;
  
  // JavaScript code
  val visualizeNetworkJS: String = 
  s&quot;&quot;&quot;
  /*******************************************************************************

  This visualization is an overview of a Twitter network.

  Contained in the visualisation is, for each user:

   - Number of retweets
   - Number of individual retweeters
   - Number of followers

  *******************************************************************************/

  // TODO: Add search option

  /*
  Create accessors that specify data from the csv-file
  */
  function x(d) { return d.NrOfRetweets; }
  function y(d) { return d.NrOfRetweeters; }
  function radius(d) { return d.followersCount; }
  function color(d) { return d.followersCount; } // What to do here?
  function name(d) { return d.ScreenName; }
  function id(d) { return d.UserID; }

  /*
  Create id-functions
  */
  function getCircleId(d) { return &quot;circ&quot; + id(d); }
  function getTextId(d) { return &quot;text&quot; + id(d); }

  /*
  Specify circle constants
  */
  var circleMaxRadius = 8;
  var circleMinRadius = 3;
  var circleEnlargeConstant = 2;
  var circleIdleOpacity = 0.2;
  var circleActiveOpacity = 1;
  var circleClickedStrokeWidth = 4;

  /*
  Create svg and specify display sizes
  */
      
  // Specify display sizes.
  var width = ${width}, 
      height = ${height},
      margin = {
                top: 0.1 * height,
                right: 0.05 * width, 
                bottom: 0.1 * height,
                left: 0.05 * width
            };
            
  width = width - margin.left - margin.right;
  height = height - margin.top - margin.bottom;

  // Get div
  var div = d3.select(&quot;#treeVisualizationDiv&quot;);

  // Create svg
  var svg = div.append('svg')
      .attr(&quot;class&quot;, &quot;visualization&quot;)
      .attr('width', width + margin.left + margin.right)
      .attr('height', height + margin.top + margin.bottom)
      .append(&quot;g&quot;).attr(&quot;id&quot;, &quot;inner-space&quot;)
          .attr(&quot;transform&quot;,
              &quot;translate(&quot; + margin.left + &quot;,&quot; + margin.top + &quot;)&quot;);

  /*
  Create title
  */
  var title = svg.append(&quot;text&quot;)
      .attr(&quot;class&quot;, &quot;title&quot;) // style in css
      .attr(&quot;x&quot;, width / 2)
      .attr(&quot;y&quot;, 0)
      .text(&quot;Twitter network&quot;);

  /*
  Create x-axis
  */

  // Create x-scale
  var xScale = d3.scaleLog()
      .range([0, width]);

  // Create x-axis
  var xAxis = d3.axisBottom(xScale)
      .ticks(5, d3.format(&quot;,d&quot;))

  // Create &quot;g&quot; for displaying of x-axis
  var gXAxis = svg.append(&quot;g&quot;)
      .attr(&quot;class&quot;, &quot;x axis&quot;)
      // Position at bottom
      .attr(&quot;transform&quot;, &quot;translate(&quot; + 0 + &quot;,&quot; + height + &quot;)&quot;)

  // Create x-axis label.
  var xAxisLabel = svg.append(&quot;text&quot;)
      .attr(&quot;class&quot;, &quot;x label&quot;)
      .attr(&quot;text-anchor&quot;, &quot;end&quot;)
      .attr(&quot;x&quot;, width)
      .attr(&quot;y&quot;, height - 6)
      .text(&quot;Number of retweets&quot;);

  /*
  Create y-axis
  */

  // Create y-scale
  var yScale = d3.scaleLinear()
      .range([height, 0]);

  // Create y-axis
  var yAxis = d3.axisLeft(yScale)

  // Create &quot;g&quot; for displaying of y-axis
  var gYAxis = svg.append(&quot;g&quot;)
      .attr(&quot;class&quot;, &quot;y axis&quot;)

  // Create y-axis label
  var yAxisLabel = svg.append(&quot;text&quot;)
      .attr(&quot;class&quot;, &quot;y label&quot;)
      .attr(&quot;text-anchor&quot;, &quot;end&quot;)
      .attr(&quot;y&quot;, 6)
      .attr(&quot;dy&quot;, &quot;.75em&quot;)
      .attr(&quot;transform&quot;, &quot;rotate(-90)&quot;)
      .text(&quot;Number of unique retweeters&quot;);


  /*
  Create scale for radius
  */
  var radiusScale = d3.scaleLog()
      .base(10)
      .range([circleMinRadius, circleMaxRadius])

  /*
  Create scale for color
  */
  var colorScale = d3.scaleLinear()
      .range([&quot;blue&quot;, &quot;red&quot;])

  /*
  Create zoom
  */
  var zoom = d3.zoom()
      .scaleExtent([0.5, Infinity])
      .on(&quot;zoom&quot;, zoomed);

  // Create zoomable area
  var zoomView = svg.append(&quot;rect&quot;)
      .attr(&quot;class&quot;, &quot;zoom&quot;)
      .attr(&quot;width&quot;, width)
      .attr(&quot;height&quot;, height)
      .call(zoom)
      .on(&quot;click&quot;, clickView)

  // Add data. Each row represented as a &quot;g&quot; of class &quot;node&quot; inside the svg.
  var data = ${data.mkString(&quot;[&quot;, &quot;,\n&quot;, &quot;]&quot;)};

  xScale.domain([1, getMaxX(data)])
  gXAxis.call(xAxis)
  yScale.domain([0, getMaxY(data)])
  gYAxis.call(yAxis)
  radiusScale.domain([1, getMaxRadius(data)])
  colorScale.domain([1, getMaxColor(data)])

  // Enter the data
  var nodes = svg.append(&quot;g&quot;).selectAll(&quot;g&quot;)
      .data(data)
      .enter()

  // Create circles to display the data
  nodes.append(&quot;circle&quot;)
      .call(setCircleAttributes)
      .call(setCircleMouseEvents)
      .sort(orderLargestBelow)
  // Create tooltip that shows username
  nodes.append(&quot;text&quot;)
      .call(setTextAttributes)

  /**
   * Set attributes for circles (Twitter account nodes).
   */
  function setCircleAttributes(circle) {
      circle
          .attr(&quot;class&quot;, &quot;nodeCircle&quot;)
          .attr(&quot;data-id&quot;, id)
          .attr(&quot;id&quot;, getCircleId)
          .attr(&quot;opacity&quot;, circleIdleOpacity)
          .attr(&quot;fill&quot;, function(d) { return colorScale(color(d)); })
          .attr(&quot;stroke&quot;, &quot;black&quot;)
          .attr(&quot;stroke-width&quot;, 0)
          .attr(&quot;r&quot;, function(d) { return radiusScale(radius(d)); })
          .attr(&quot;cx&quot;, function(d) { return xScale(x(d)); })
          .attr(&quot;cy&quot;, function(d) { return yScale(y(d)); })
  }

  /**
   * Set mouse events for circles.
   */
  function setCircleMouseEvents(circle) {
      circle
          // Add tooltip and enlarge circle on mouse hover
          .on(&quot;mouseover&quot;, mouseoverCircle)
          // Remove tooltip and restore circle on mouseout
          .on(&quot;mouseout&quot;, mouseoutCircle)
          // Display timeline on click
          .on(&quot;click&quot;, clickCircle)
  }

  /**
   * Set attributes for tooltip (showing screen name) text.
   */
  function setTextAttributes(text) {
      text
          .attr(&quot;class&quot;, &quot;hidden nodeText&quot;) // Set class to hidden upon creation
          .attr(&quot;data-id&quot;, id)
          .attr(&quot;id&quot;, getTextId)
          .attr(&quot;x&quot;, function(d) { return xScale(x(d)); })
          .attr(&quot;y&quot;, function(d) { return yScale(y(d)); })
          .attr(&quot;dy&quot;, function(d) { return - (circleMaxRadius * circleEnlargeConstant * 1.5); })
          .attr(&quot;text-anchor&quot;, &quot;beginning&quot;)
          .text(function(d) { return name(d); })
  }

  /**
   * Order so that largest circle gets placed deepest.
   */
  function orderLargestBelow(a, b) {
      return radius(b) - radius(a);
  }

  /**
   * Handle mouse hover on circle. Display circle's screen name.
   */
  function mouseoverCircle() {

      // Get circle
      var circle = d3.select(this);

      // Display activated circle
      circle.attr(&quot;r&quot;, circle.attr(&quot;r&quot;) * circleEnlargeConstant);
      circle.attr(&quot;opacity&quot;, circleActiveOpacity);

      // Set text class to visible
      svg.select(&quot;#text&quot; + circle.attr(&quot;data-id&quot;))
          .classed(&quot;hidden&quot;, false)
          .classed(&quot;visible&quot;, true)

  }

  /**
   * Handle zoom. Zoom both x-axis and y-axis.
   */
  function mouseoutCircle() {

      // Get circle
      var circle = d3.select(this);

      // Display idle circle
      circle.attr(&quot;r&quot;, circle.attr(&quot;r&quot;) / circleEnlargeConstant);
      circle.attr(&quot;opacity&quot;, circleIdleOpacity);

      // Set text class to hidden
      svg.select(&quot;#text&quot; + circle.attr(&quot;data-id&quot;))
          .classed(&quot;visible&quot;, false)
          .classed(&quot;hidden&quot;, true)

  }

  /**
   * Handle zoom. Zoom both x-axis and y-axis.
   */
  function zoomed() {

      // Create new x- and y-scale
      var new_xScale = d3.event.transform.rescaleX(xScale);
      var new_yScale = d3.event.transform.rescaleY(yScale);

      // Display new axes
      gXAxis.call(xAxis.scale(new_xScale));
      gYAxis.call(yAxis.scale(new_yScale));

      // Reposition circles
      d3.selectAll(&quot;.nodeCircle&quot;)
          .attr(&quot;cx&quot;, function(d) { return new_xScale(x(d)); })
          .attr(&quot;cy&quot;, function(d) { return new_yScale(y(d)); })
          // To force constant circle radius on zoom:
          //.attr(&quot;r&quot;, function(d) { return d3.event.transform.scale(radiusScale(radius(d))).k; })

      // Reposition texts
      d3.selectAll(&quot;.nodeText&quot;)
          .attr(&quot;x&quot;, function(d) { return new_xScale(x(d)); })
          .attr(&quot;y&quot;, function(d) { return new_yScale(y(d)); })

  };

  /**
   * Handle click on zoomable area. That is, handle click outside a node which
   * is considered a deselecting click =&gt; deselect previously clicked node
   * and remove displayed tweets.
   */
  function clickView() {

      // Remove clicked status on clicked nodes
      d3.selectAll(&quot;.clicked&quot;)
          .attr(&quot;stroke-width&quot;, &quot;0&quot;)
          .classed(&quot;clicked&quot;, false)

      // Remove timeline
      document.getElementById(&quot;tweet&quot;).innerHTML = &quot;&quot;
  }

  /**
   * Handle click on a tweet circle. Display the clicked tweet and let the tweet
   * appear selected by adding a stroke to it.
   */
  function clickCircle(d) {

      // Remove results from old click
      clickView();

      // Add stroke width and set clicked class
      d3.select(this)
          .attr(&quot;stroke-width&quot;, circleClickedStrokeWidth)
          .classed(&quot;clicked&quot;, true);

      // Display tweet
      twttr.widgets.createTimeline(
          {
              sourceType: &quot;profile&quot;,
              userId: id(d)
          },
          document.getElementById(&quot;tweet&quot;), // Tweet div
          {
              height: height
          }
      )
  }

  /**
   * Returns the largest x-value in the data.
   */
  function getMaxX(data) {

      return Math.max(...data.map(d =&gt; x(d)));
  }

  /**
   * Returns the largest y-value in the data.
   */
  function getMaxY(data) {

      return Math.max(...data.map(d =&gt; y(d)));
  }

  /**
   * Returns the largest radius in the data.
   */
  function getMaxRadius(data) {

      return Math.max(...data.map(d =&gt; radius(d)));
  }

  /**
   * Returns the &quot;largest&quot; color in the data.
   */
  function getMaxColor(data) {

      var maxColor = Math.max(...data.map(d =&gt; color(d)));
      var cutOff = 10000;

      if (maxColor &gt; cutOff) return cutOff;
      return maxColor;
  }

  &quot;&quot;&quot;
  
  // HTML code
  displayHTML(s&quot;&quot;&quot;
  &lt;!DOCTYPE html&gt;
  &lt;html lang=&quot;en&quot;&gt;

      &lt;head&gt;

          &lt;meta charset=&quot;utf-8&quot;&gt;
          &lt;meta name=&quot;author&quot; content=&quot;Olof Bjrck&quot;&gt;
          &lt;title&gt;Network&lt;/title&gt;

          &lt;style&gt;
            ${visualizeNetworkCSS}
          &lt;/style&gt;

      &lt;/head&gt;

      &lt;body&gt;

          &lt;div id=&quot;treeVisualizationDiv&quot; class=&quot;visualization&quot; align=&quot;center&quot;&gt;
              &lt;div id=&quot;tweet&quot;&gt;&lt;/div&gt;
          &lt;/div&gt;

          &lt;script sync src=&quot;https://platform.twitter.com/widgets.js&quot;&gt;&lt;/script&gt;
          &lt;script src=&quot;https://d3js.org/d3.v4.min.js&quot;&gt;&lt;/script&gt;
          &lt;script&gt;
            ${visualizeNetworkJS}
          &lt;/script&gt;

      &lt;/body&gt;
  &lt;/html&gt;
  &quot;&quot;&quot;)
}

// Info
println(&quot;&quot;&quot;
*** WARNING: ***

THERE IS NO SIZE CHECKING! Driver might crash as DataFrame.collect is used.
Also, the display can only handle so and so many items at once. Too large
dataset and the visualization might be very laggy or crash.

***  USAGE:  ***

visualizeNetwork(TTTDF)

****************
&quot;&quot;&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>*** WARNING: ***

THERE IS NO SIZE CHECKING! Driver might crash as DataFrame.collect is used.
Also, the display can only handle so and so many items at once. Too large
dataset and the visualization might be very laggy or crash.

***  USAGE:  ***

visualizeNetwork(TTTDF)

****************

import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._
visualizeNetwork: (TTTDF: org.apache.spark.sql.DataFrame, width: Int, height: Int)Unit
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="function-for-visualizing-a-twitter-timeline"><a class="header" href="#function-for-visualizing-a-twitter-timeline">Function for visualizing a Twitter timeline</a></h1>
<p>by Olof Bjrck, Joakim Johansson, Rania Sahioun, Raazesh Sainudiin and Ivan Sadikov</p>
<p>This is part of <em>Project MEP: Meme Evolution Programme</em> and supported by databricks, AWS and a Swedish VR grant.</p>
<pre><code>Copyright 2016-2020 Olof Bjrck, Joakim Johansson, Rania Sahioun, Ivan Sadikov and Raazesh Sainudiin

Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.sql.DataFrame

/** Displays an interactive visualization of a Twitter timeline.
 *
 *  This function takes a DataFrame from TTTDFfunctions function tweetsDF2TTTDF()
 *  and displays it in an interactive visualization. The DataFrame content should
 *  be a Twitter user timeline.
 *  
 *  Visualization description:
 *
 *
 *  This is a visualization of a Twitter user timeline.
 *  Each circle represents a Tweet. The x-axis shows the time a Tweet was posted and 
 *  the y-axis shows the Tweet type of the Tweet. The displayed time interval can be 
 *  zoomed by scrolling. If you hover over a Tweet circle, the Tweet text content 
 *  will appear in the upper left corner and the Tweet circle you're hovering will 
 *  become enlarged. Above the Tweet text in the upper left corner, there's a search 
 *  box that let's you search all Tweet text content. If the character sequence you're 
 *  searching for appears in a Tweet text, Tweet circle where the character sequence 
 *  appears is colored yellow.
 *  
 *  @param TTTDF A DataFrame from TTTDFfunctions function tweetsDF2TTTDF() containing
 *               the data to be displayed.
 *  @param width The display width in pixels.
 *  @param height The display height in pixels.
 */
def visualizeTimeline(TTTDF: DataFrame, width: Int = 900, height: Int = 400): Unit = {
  
  // The CSS code
  val visualizeTimelineCSS: String = 
  s&quot;&quot;&quot;
  /*
  General styling
  */

  body {
      font-family: Sans-Serif;
      width: 100%;
      height: 100%;
      margin: 0;
      margin-bottom: 100px;
      background-color: #FFF;
      overflow: scroll;
  }

  /*
  Page content
  */

  div.start {
      text-align: center;
  }

  ul.start {
      margin-top: 0;
      display: inline-block;
      text-align: left;
  }

  div.text {
      text-align: left;
      margin-left: 19.1%;
      margin-right: 19.1%;
  }

  /*
  For visualizations
  */

  .visualization {
      display: block;
      position: relative;
      align: center;
  }

  .hidden {
      visibility: hidden;
  }

  .visible {
      visibility: visible;
  }

  .zoom {
      cursor: move;
      fill: none;
      pointer-events: fill;
  }

  .tweet {
	fill: #1da1f2;
	stroke: black;
	opacity: 0.5;
  }

  circle.clicked {
      stroke-width: 5;
  }

  .searchedTweet {
      fill: yellow;
      opacity: 0.9;
  }

  .infoDisplay {
      margin: 1%;
  }

  #tweetTextDiv {
      position: absolute;
      font-family: Helvetica;
      text-align: left;
      width: 20%;
  }

  #searchDiv {
      height: 20px;
  }

  .highlight {
    background-color: yellow;
  }

  path, line {
    fill: none;
    stroke: black;
  }

  .axis {
      position:relative;
      z-index:1000000;
  }

  .axis text {
      fill: black;
      font-size: 1em;
  }

  #userTimelineVisualizationDiv {
      width: auto;
      height: auto;
      position: relative;
      align: center;
  }

  .title {
      fill: black;
      font-family: sans-serif;
      text-anchor: middle;
      text-align: center;
      font-size: 1.5em;
  }

  .searchField {
      font-family: sans-serif;
  }

  #tweet {
      position: absolute;
      left: 60%;
      height: auto;
  }
  &quot;&quot;&quot;;
  
  // Convert the timeline TTT DataFrame to a JSON object
  val data = TTTDF.toJSON.collect

  // The JavaScript code
  val visualizeTimelineJS: String = 
  s&quot;&quot;&quot;
  /*******************************************************************************

  This user timeline visualisation let's you explore a Twitter timeline.

  Contained in the visualisation is:

   - All Tweets in the data
   - A search function to search Tweets
   - A click option to view the original Tweet on Twitter

  *******************************************************************************/

  // Specify display sizes.
  var width = ${width}, 
      height = ${height},
      margin = {
                top: 0.1 * height,
                right: 0.05 * width, 
                bottom: 0.2 * height,
                left: 0.05 * width
            };

  // Get div.
  var div = d3.select(&quot;#userTimelineVisualizationDiv&quot;);

  // Create svg.
  var svg = div.append('svg')
      //.attr(&quot;class&quot;, &quot;visualization&quot;)
      .style(&quot;z-index&quot;, &quot;-1&quot;)
      .attr('width', width + margin.left + margin.right)
      .attr('height', height + margin.top + margin.bottom)
      .append(&quot;g&quot;)
          .attr(&quot;transform&quot;,
              &quot;translate(&quot; + margin.left + &quot;,&quot; + margin.top + &quot;)&quot;);

  // Declare global searched string.
  var searchedStr = &quot;&quot;;

  // Create zoom object. Zooms x-axis.
  var zoom = d3.zoom()
      .on(&quot;zoom&quot;, zoomed);

  // Create zoomable area. Basically just an overlaid rectangle.
  var view = svg.append(&quot;rect&quot;)
      .attr(&quot;class&quot;, &quot;zoom&quot;)
      .attr(&quot;width&quot;, width)
      .attr(&quot;height&quot;, height)
      // Allow for zoom while hovering x-axis
      .attr(&quot;transform&quot;,
          &quot;translate(&quot; + 0 + &quot;,&quot; + margin.top + &quot;)&quot;)
      // Remove currently displayed tweet on click
      .on(&quot;click&quot;, function() { clickView(); })
      // Link to zoom
      .call(zoom);

  // Set various tweet radius
  var idleTweetRadius = 15;
  var activeTweetRadius = idleTweetRadius * 1.618;
  var highlightedActiveTweetRadius = activeTweetRadius * 1.618;

  // Add title to the figure.
  svg.append(&quot;text&quot;)
      .attr(&quot;class&quot;, &quot;title&quot;) // style in css
      .attr(&quot;x&quot;, width / 2)
      .attr(&quot;y&quot;, margin.top)
      .text(&quot;Twitter Timeline Visualization&quot;);

  // Create x-scale and set x-range.
  var xScale = d3.scaleTime()
      .range([0, width]);

  // Create xAxis.
  var xAxis = d3.axisBottom(xScale)
      .tickFormat(d3.timeFormat(&quot;%c&quot;)) // Set tick format date and time

  // Create x-axis g.
  var gXAxis = svg.append(&quot;g&quot;)
      .attr(&quot;class&quot;, &quot;x axis&quot;)
      .attr(&quot;transform&quot;, &quot;translate(&quot; + 0 + &quot;,&quot; + height + &quot;)&quot;)

  // Create x-axis label.
  svg.append(&quot;text&quot;)
      .attr(&quot;class&quot;, &quot;x label&quot;)
      .attr(&quot;text-anchor&quot;, &quot;end&quot;)
      .attr(&quot;x&quot;, width)
      .attr(&quot;y&quot;, height - 6)
      .text(&quot;Time of Tweet&quot;);

  // y-range. Sets data placement along y-axis.
  // y-axis is divided in 6 lines, including top/bottom of chart,
  // and data is placed in the middle, lines 2 to 5.
  var yRange = [2, 3, 4, 5].map(function(x) { return x * height / 6; });

  // y-domain. Specifies which data should be placed where along y-axis.
  // Important: Matches with data from file.
  var yDomain = [&quot;ReplyTweet&quot;,
                  &quot;QuotedTweet&quot;,
                  &quot;ReTweet&quot;,
                  &quot;OriginalTweet&quot;];

  // y-ticks to be displayed.
  var yTickValues = [&quot;Reply&quot;,
                      &quot;Quote&quot;,
                      &quot;Retweet&quot;,
                      &quot;Tweet&quot;];

  // Create the y-scale and set y-range
  var yScale = d3.scaleOrdinal()
      .range(yRange)
      .domain(yDomain);

  // Create y-axis.
  var yAxis = d3.axisLeft(yScale)
      .tickValues(yTickValues); // Set y-axis tick values

  // Display y-axis (and label) after circles are placed to put y-axis above the circles

  // Read data. Note: file needs to be chronologically structured so that
  // data[0] is newest and data[length - 1] is oldest
  var data = ${data.mkString(&quot;[&quot;, &quot;,\n&quot;, &quot;]&quot;)};
        
  // Create and display the x-axis
  createAndDisplayXAxis(data);

  // Create circle for each tweet
  svg.selectAll(&quot;g&quot;)
      .data(data)
      .enter().append(&quot;g&quot;).attr(&quot;id&quot;, function(d) {
          return getGID(d.CurrentTwID);
      })
          .append(&quot;circle&quot;)
          // Set class to tweet ID
          .attr(&quot;id&quot;, function(d) {
              return getTweetID(d.CurrentTwID);
          })
          // Set position
          .attr(&quot;cy&quot;, function(d) {
              return yScale(d.TweetType.replace(/\\s/g, ''));
          })
          .attr(&quot;cx&quot;, function(d) { // x-position by tweet date
              return xScale(new Date(d.CurrentTweetDate));
          })
          // Set circle radius
          .attr(&quot;r&quot;, idleTweetRadius)
          // Set stroke
          .attr(&quot;stroke&quot;, &quot;black&quot;)
          .attr(&quot;stroke-width&quot;, &quot;0&quot;)
          // Set color by tweet type
          .attr(&quot;class&quot;, function(d) {
              // remove whitespace and return TweetType
              return &quot;tweet &quot; + d.TweetType.replace(/\\s/g, '');
            })
            // Add tooltip and enlarge tweet on mouse hover
            .on(&quot;mouseover&quot;, mouseoverTweet)
            // Restore tweet on mouseout
            .on(&quot;mouseout&quot;, mouseoutTweet)
            // Show actual tweet on click
            .on(&quot;click&quot;, clickTweet);

      // Display y-axis.
      var gYAxis = svg.append(&quot;g&quot;)
          .attr(&quot;class&quot;, &quot;y axis&quot;) // Set class to y and axis
          .call(yAxis);

      // Create y-axis label.
      svg.append(&quot;text&quot;)
          .attr(&quot;class&quot;, &quot;y label&quot;)
          .attr(&quot;text-anchor&quot;, &quot;end&quot;)
          .attr(&quot;x&quot;, - 2 * height / 6)
          .attr(&quot;y&quot;, 6)
          .attr(&quot;dy&quot;, &quot;.75em&quot;)
          .attr(&quot;transform&quot;, &quot;rotate(-90)&quot;)
          .text(&quot;Type of Tweet&quot;);
          
      // display x-axis
      xAxis.ticks(5);
      gXAxis.call(xAxis);

      // Handle input search
      d3.select(&quot;#searchInput&quot;).on(&quot;input&quot;,
          function() {
              searchedStr = this.value.toLowerCase();
              searchTweets(data);
          });

  /**
   * Searches all tweets for a specific string.
   *
   * @param {string} searchStr - The string to search for
   */
  function searchTweets(data) {

      // Perform search if searched string is at least 3 chars long
      if (searchedStr.length &gt; 2) {

          // Loop through all rows
          for (i = 0; i &lt; data.length; i++) {

              // Get tweet text
              var tweetText = data[i].CurrentTweet;
              var tweet = d3.select(&quot;#&quot; + getTweetID(data[i].CurrentTwID));

              // If tweet includes search string, display
              if (tweetText.toLowerCase().includes(searchedStr)) {
                  // Set class to searched tweet and enlarge
                  tweet
                      .classed(&quot;searchedTweet&quot;, true)
                      .attr(&quot;r&quot;, activeTweetRadius)
              // else, restore tweet to normal
              } else {
                  tweet
                      .classed(&quot;searchedTweet&quot;, false)
                      .attr(&quot;r&quot;, idleTweetRadius)
              }
          }

          // Highlight the searched string
          highlight();

      // else, restore tweets and dehighlight
      } else {
          // Restore tweets
          d3.selectAll(&quot;.tweet&quot;)
              .classed(&quot;searchedTweet&quot;, false)
              .attr(&quot;r&quot;, idleTweetRadius)

          // Dehighlight the displayed tweet
          dehighlight();
      }
  }

  /**
   * Create and display x-axis based on newest
   * and oldest dates in the dataset. Also sets the x-scale domain.
   *
   * @param data - Twitter dataset
   */
  function createAndDisplayXAxis(data) {

      // Get oldest date (that is, date of first tweet in the data)
      var oldestDate = new Date(data[data.length - 1].CurrentTweetDate);
      // Get newest date (that is, date of latest tweet in the data)
      var newestDate = new Date(data[0].CurrentTweetDate);
      // Add 2 weeks at beginning and end of axis for prettier display
      oldestDate.setDate(oldestDate.getDate() - 14); // go back 14 days
      newestDate.setDate(newestDate.getDate() + 14); // go forward 14 days

      // Set x-scale domain from newest and oldest date
      xScale.domain([oldestDate, newestDate]);
  }

  /**
   * Handle mouseover for Tweet.
   *
   * @param {list} d - Row from Twitter dataset
   */
  function mouseoverTweet(d) {

      // Get tweet
      var tweet = d3.select(this)

      // Get tweet text div
      var tweetTextDiv = d3.select(&quot;#tweetTextDiv&quot;);

      // Remove old tweet
      tweetTextDiv.selectAll(&quot;span&quot;)
          .remove();

      // Display tweet text
      tweetTextDiv.append(&quot;span&quot;)
          .text(d.CurrentTweet);

      // Enlarge tweet
      tweet.attr(&quot;r&quot;, activeTweetRadius);

      // If the tweet is searched, highlight and enlarge it
      if (tweet.classed(&quot;searchedTweet&quot;)) {

          // Enlarge the tweet to active and highlighted
          tweet.attr(&quot;r&quot;, highlightedActiveTweetRadius);

          // Highlight the tweet
          highlight();

      // else (that is, tweet is not searched), just enlarge the tweet
      } else {

          // Enlarge tweet to active
          tweet.attr(&quot;r&quot;, activeTweetRadius);
      }
  }

  /**
   * Highlights the searched part of the tweet text.
   */
  function highlight() {

      // Get tweet text div
      var tweetTextDiv = d3.select(&quot;#tweetTextDiv&quot;);

      // Get tweet text (works although text is inside a &lt;span&gt;)
      var tweetText = tweetTextDiv.text();
      // Get tweet text in lower case (used to highlight without case sensitivity)
      var tweetTextLowerCase = tweetText.toLowerCase();

      // Highlight if string to highlight is currently displayed
      if (tweetTextLowerCase.includes(searchedStr)) {

          // Get string before the string to highlight
          var strBefore = tweetText.substr(0, (tweetTextLowerCase.indexOf(searchedStr)));
          // Get string after the string to highlight
          var strAfter = tweetText.substr((tweetTextLowerCase.indexOf(searchedStr) +
                                      searchedStr.length),
                                      (tweetText.length - 1));

          // Remove non highlighted tweet text (the old tweet text with 1 &lt;span&gt;)
          tweetTextDiv.selectAll(&quot;span&quot;).remove();

          // Append string before highlight
          tweetTextDiv.append(&quot;span&quot;)
              .text(strBefore);
          // Append highlighted string
          tweetTextDiv.append(&quot;span&quot;)
              .attr(&quot;class&quot;, &quot;highlight&quot;)
              .text(searchedStr);
          // Append string after highlight
          tweetTextDiv.append(&quot;span&quot;)
              .text(strAfter);
      }
  }

  /**
   * Dehighlights the tweet text.
   */
  function dehighlight() {

      // Get tweet text div
      var tweetTextDiv = d3.select(&quot;#tweetTextDiv&quot;);

      // Get tweet text
      var tweetText = tweetTextDiv.text();

      // Remove highlighted text (the old tweet text with 3 &lt;span&gt;s)
      tweetTextDiv.selectAll(&quot;span&quot;).remove();

      // Add non highlighted text
      tweetTextDiv.append(&quot;span&quot;).text(tweetText);

      // Add actual tweet

  }

  /**
   * Handle mouseout for Tweet.
   * Removes the tooltip displaying the tweet.
   *
   * @param {list} d - Row from Twitter dataset
   */
  function mouseoutTweet(d) {

      // Get tweet
      var tweet = d3.select(this)

      // Restore tweet to idle unless the tweet is searched
      if (!tweet.classed(&quot;searchedTweet&quot;)) {

          // Restore tweet
          tweet.attr(&quot;r&quot;, idleTweetRadius);

      // else (that is, tweet is searched), restore to active radius
      } else {
          // Restore tweet
          tweet.attr(&quot;r&quot;, activeTweetRadius);
      }
  }

  /**
   * Removes tooltip by ID.
   *
   * @param {string} id - The tooltip ID.
   */
  function removeTooltip(id) {
      d3.select(&quot;#&quot; + id).remove();
  }

  /**
   * Creates a tooltip ID from a raw data tweet ID.
   *
   * @param {string} id - The tweet ID.
   */
  function getTooltipID(currentTwID) {
      return &quot;tt&quot; + currentTwID;
  }

  /**
   * Creates a tweet ID from a raw data tweet ID.
   *
   * @param {string} id - The tweet ID.
   */
  function getTweetID(currentTwID) {
      return &quot;tw&quot; + currentTwID;
  }

  function getGID(currentTwID) {
      return &quot;g&quot; + currentTwID;
  }

  /**
   * Handle zoom: Zoom the x-axis.
   */
  function zoomed() {

      // Create new x-scale based on zoom
      var new_xScale = d3.event.transform.rescaleX(xScale);

      // Display new x-scale. .ticks(3) to prettify
      gXAxis.call(xAxis.ticks(5).scale(new_xScale));

      // Reposition tweets based on zoom
      var tweets = d3.selectAll(&quot;.tweet&quot;);
      tweets.attr(&quot;cx&quot;, function(d) {
          return new_xScale(new Date(d.CurrentTweetDate));
      });
  };

  /**
   * Handle click on zoomable area. That is, handle click outside a tweet which
   * is considered a deselecting click. So, deselect previously clicked tweets
   * and remove displayed tweets.
   */
  function clickView() {

      // Get all clicked tweets
      var clicked = d3.selectAll(&quot;.clicked&quot;);

      // Remove clicked status on clicked tweets
      clicked.attr(&quot;stroke-width&quot;, &quot;0&quot;);
      clicked.classed(&quot;clicked&quot;, false);

      // Remove tweet
      document.getElementById(&quot;tweet&quot;).innerHTML = &quot;&quot;
  }

  /**
   * Handle click on a tweet circle. Display the clicked tweet and let the tweet
   * appear selected by adding a stroke to it.
   */
  function clickTweet(d) {

      // Remove results from old click
      clickView();

      // Get tweet
      var tweet = d3.select(this)

      // Set tweet to clicked
      tweet.classed(&quot;clicked&quot;, true);

      // Get tweet div
      // Cannot do d3.select because twttr doesn't handle D3 selections
      var tweetDiv = document.getElementById(&quot;tweet&quot;);

      // Display tweet
      twttr.widgets.createTweet(d.CurrentTwID, tweetDiv)
  }
  &quot;&quot;&quot;
  
  // The HTML code
  displayHTML(s&quot;&quot;&quot;
  &lt;!DOCTYPE html&gt;
  &lt;html lang=&quot;en&quot;&gt;

      &lt;head&gt;

          &lt;meta charset=&quot;utf-8&quot;&gt;
          &lt;meta name=&quot;author&quot; content=&quot;Olof Bjrck&quot;&gt;
          &lt;title&gt;User Timeline&lt;/title&gt;

          &lt;style&gt;
            ${visualizeTimelineCSS}
          &lt;/style&gt;

      &lt;/head&gt;

      &lt;body&gt;

          &lt;!-- The input where you can search the Tweets --&gt;
          &lt;div id=&quot;searchDiv&quot; class=&quot;infoDisplay&quot;&gt;
              Search: &lt;input name=&quot;searchStr&quot; type=&quot;text&quot; id=&quot;searchInput&quot; class=&quot;searchField&quot;&gt;
          &lt;/div&gt;

          &lt;!-- The place where Tweet texts are displayed --&gt;
          &lt;div id=&quot;tweetTextDiv&quot; class=&quot;infoDisplay&quot;&gt;
          &lt;/div&gt;

          &lt;!-- The D3 visualization --&gt;
          &lt;div id=&quot;userTimelineVisualizationDiv&quot; class=&quot;visualization&quot;&gt;
              &lt;div id=&quot;tweet&quot;&gt;&lt;/div&gt;
          &lt;/div&gt;

          &lt;script sync src=&quot;https://platform.twitter.com/widgets.js&quot;&gt;&lt;/script&gt;
          &lt;script src=&quot;https://d3js.org/d3.v4.min.js&quot;&gt;&lt;/script&gt;
          &lt;script&gt;
            ${visualizeTimelineJS}
          &lt;/script&gt;

      &lt;/body&gt;
  &lt;/html&gt;

  &quot;&quot;&quot;)
}

// Info
println(&quot;&quot;&quot;
*** WARNING: ***

BUG: The first Tweet in the dataset doesn't display for some reason.

THERE IS NO SIZE CHECKING! Driver might crash as DataFrame.collect is used.
Also, the display can only handle so and so many items at once. Too large
dataset and the visualization might be very laggy or crash.

***  USAGE:  ***

visualizeTimeline(TTTDF)
 
****************
&quot;&quot;&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>*** WARNING: ***

THERE IS NO SIZE CHECKING! Driver might crash as DataFrame.collect is used.
Also, the display can only handle so and so many items at once. Too large
dataset and the visualization might be very laggy or crash.

***  USAGE:  ***

visualizeTimeline(TTTDF)
 
****************

import org.apache.spark.sql.DataFrame
visualizeTimeline: (TTTDF: org.apache.spark.sql.DataFrame, width: Int, height: Int)Unit
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->

        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </body>
</html>
